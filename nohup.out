PyTorch version 1.9.0+cu102 available.
10/08/2021 21:37:12 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:37:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 21:37:12 - INFO - __main__ -   Seed = 12
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:37:25 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 21:37:45 - INFO - root -   save model
10/08/2021 21:37:45 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:37:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:37:51 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:38:09 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:38:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 21:38:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/08/2021 21:38:09 - INFO - root -   Trying to decide if add adapter
10/08/2021 21:38:09 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 21:38:09 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 21:38:09 - INFO - __main__ -   Language = en
10/08/2021 21:38:09 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 21:38:26 - INFO - __main__ -   Language adapter for bn not found, using en instead
10/08/2021 21:38:26 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:38:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 21:38:27 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 21:38:27 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:38:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.43it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.67it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.89it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.13it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.28it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.28it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.37it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.37it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.34it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.37it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.40it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.38it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.42it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.45it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.48it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.45it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.43it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.44it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.47it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.44it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.46it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.48it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.48it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.50it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.45it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.47it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.47it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.44it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.43it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.44it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.41it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.54it/s]
10/08/2021 21:38:31 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 21:38:31 - INFO - __main__ -     f1 = 0.528955768353853
10/08/2021 21:38:31 - INFO - __main__ -     loss = 1.0327228549867868
10/08/2021 21:38:31 - INFO - __main__ -     precision = 0.5253623188405797
10/08/2021 21:38:31 - INFO - __main__ -     recall = 0.5325987144168962
10/08/2021 21:38:31 - INFO - __main__ -   Language adapter for mr not found, using en instead
10/08/2021 21:38:31 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:38:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 21:38:31 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 21:38:31 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:38:31 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.63it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.44it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.48it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.51it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.46it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.46it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.44it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.46it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.49it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.48it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.49it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.47it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.45it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.46it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.45it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.44it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.46it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.43it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.44it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.42it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.39it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.43it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.42it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.40it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.41it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.21it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.27it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.26it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.31it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.36it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.40it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.58it/s]
10/08/2021 21:38:36 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 21:38:36 - INFO - __main__ -     f1 = 0.45711940910956095
10/08/2021 21:38:36 - INFO - __main__ -     loss = 0.8072491521015763
10/08/2021 21:38:36 - INFO - __main__ -     precision = 0.47485080988917305
10/08/2021 21:38:36 - INFO - __main__ -     recall = 0.44066455696202533
10/08/2021 21:38:36 - INFO - __main__ -   Language adapter for ta not found, using en instead
10/08/2021 21:38:36 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:38:36 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 21:38:36 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 21:38:36 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:38:36 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.57it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.45it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.42it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.32it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.38it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.41it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.42it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.43it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.43it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.44it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.40it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.41it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.44it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.41it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.41it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.35it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.37it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.41it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.40it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.40it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.42it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.41it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.42it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.43it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.43it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.43it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.42it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.33it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.31it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.32it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.31it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.55it/s]
10/08/2021 21:38:40 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 21:38:40 - INFO - __main__ -     f1 = 0.17960908610670892
10/08/2021 21:38:40 - INFO - __main__ -     loss = 1.3596679382026196
10/08/2021 21:38:40 - INFO - __main__ -     precision = 0.2607361963190184
10/08/2021 21:38:40 - INFO - __main__ -     recall = 0.136986301369863
10/08/2021 21:38:40 - INFO - __main__ -   Language adapter for is not found, using en instead
10/08/2021 21:38:40 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:38:40 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
10/08/2021 21:38:41 - INFO - __main__ -   ***** Running evaluation  in is *****
10/08/2021 21:38:41 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:38:41 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.60it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.45it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:06,  4.28it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:05,  5.16it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  5.80it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:04,  6.28it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.60it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.83it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.02it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  7.12it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.23it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.31it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.32it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  7.34it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.36it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.35it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.35it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.35it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.34it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.36it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  7.39it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  7.37it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.32it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.35it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.33it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.34it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.34it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  7.34it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  7.34it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.32it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.35it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.18it/s]
10/08/2021 21:38:45 - INFO - __main__ -   ***** Evaluation result  in is *****
10/08/2021 21:38:45 - INFO - __main__ -     f1 = 0.6487317448116834
10/08/2021 21:38:45 - INFO - __main__ -     loss = 0.4425821052864194
10/08/2021 21:38:45 - INFO - __main__ -     precision = 0.6129266521423384
10/08/2021 21:38:45 - INFO - __main__ -     recall = 0.6889795918367347
10/08/2021 21:38:45 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:39:14 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:39:14 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:39:33 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 21:39:35 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:39:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 21:39:35 - INFO - __main__ -   Seed = 22
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:39:46 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 21:40:00 - INFO - root -   save model
10/08/2021 21:40:00 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:40:04 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:40:04 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:40:16 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:40:16 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 21:40:16 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/08/2021 21:40:16 - INFO - root -   Trying to decide if add adapter
10/08/2021 21:40:16 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 21:40:16 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 21:40:16 - INFO - __main__ -   Language = en
10/08/2021 21:40:16 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 21:40:32 - INFO - __main__ -   Language adapter for bn not found, using en instead
10/08/2021 21:40:32 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:40:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 21:40:32 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 21:40:32 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:40:32 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.58it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.34it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.78it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.08it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.25it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.35it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.41it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.43it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.47it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.50it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.50it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.51it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.52it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.52it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.53it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.53it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.53it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.54it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.54it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.54it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.49it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.50it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.49it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.49it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.50it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.52it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.52it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.52it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.50it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.75it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.94it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.49it/s]
10/08/2021 21:40:37 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 21:40:37 - INFO - __main__ -     f1 = 0.4889929742388759
10/08/2021 21:40:37 - INFO - __main__ -     loss = 4.376011699438095
10/08/2021 21:40:37 - INFO - __main__ -     precision = 0.49904397705544934
10/08/2021 21:40:37 - INFO - __main__ -     recall = 0.4793388429752066
10/08/2021 21:40:37 - INFO - __main__ -   Language adapter for mr not found, using en instead
10/08/2021 21:40:37 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:40:37 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 21:40:37 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 21:40:37 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:40:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.60it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.58it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.57it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.58it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.56it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.55it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.56it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.55it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.53it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.54it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.55it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.54it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.54it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.54it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.51it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.52it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.52it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.52it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.51it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.51it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.51it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.52it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.51it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.51it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.50it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.50it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.49it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.48it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.46it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.45it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.45it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.68it/s]
10/08/2021 21:40:41 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 21:40:41 - INFO - __main__ -     f1 = 0.45916666666666667
10/08/2021 21:40:41 - INFO - __main__ -     loss = 2.6568188965320587
10/08/2021 21:40:41 - INFO - __main__ -     precision = 0.4850352112676056
10/08/2021 21:40:41 - INFO - __main__ -     recall = 0.43591772151898733
10/08/2021 21:40:41 - INFO - __main__ -   Language adapter for ta not found, using en instead
10/08/2021 21:40:41 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:40:41 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 21:40:41 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 21:40:41 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:40:41 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.52it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.46it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.48it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.50it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.52it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.51it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.49it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.48it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.48it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.49it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.50it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.48it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.48it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.46it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.47it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.47it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.48it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.48it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.48it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.48it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.46it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.47it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.47it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.48it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.48it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.47it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.47it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.46it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.46it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.46it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.45it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.64it/s]
10/08/2021 21:40:45 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 21:40:45 - INFO - __main__ -     f1 = 0.18119824646858254
10/08/2021 21:40:45 - INFO - __main__ -     loss = 4.427195876836777
10/08/2021 21:40:45 - INFO - __main__ -     precision = 0.229064039408867
10/08/2021 21:40:45 - INFO - __main__ -     recall = 0.1498791297340854
10/08/2021 21:40:45 - INFO - __main__ -   Language adapter for is not found, using en instead
10/08/2021 21:40:45 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:40:45 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
10/08/2021 21:40:46 - INFO - __main__ -   ***** Running evaluation  in is *****
10/08/2021 21:40:46 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:40:46 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.50it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.49it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.51it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:05,  5.10it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  5.77it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.26it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.62it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.88it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.04it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  7.17it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.26it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.33it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.38it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  7.41it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.44it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.44it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.44it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.46it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.46it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.46it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.43it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  7.45it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.45it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.44it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.45it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.43it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.41it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.39it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  7.39it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.39it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.41it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.34it/s]
10/08/2021 21:40:50 - INFO - __main__ -   ***** Evaluation result  in is *****
10/08/2021 21:40:50 - INFO - __main__ -     f1 = 0.6256564141035259
10/08/2021 21:40:50 - INFO - __main__ -     loss = 1.665244122967124
10/08/2021 21:40:50 - INFO - __main__ -     precision = 0.5787647467036781
10/08/2021 21:40:50 - INFO - __main__ -     recall = 0.6808163265306122
10/08/2021 21:40:50 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:41:03 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:41:03 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:41:18 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 21:41:20 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:41:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 21:41:20 - INFO - __main__ -   Seed = 32
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:41:30 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 21:41:44 - INFO - root -   save model
10/08/2021 21:41:44 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:41:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:41:48 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:42:01 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:42:01 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 21:42:01 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/08/2021 21:42:01 - INFO - root -   Trying to decide if add adapter
10/08/2021 21:42:01 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 21:42:01 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 21:42:01 - INFO - __main__ -   Language = en
10/08/2021 21:42:01 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 21:42:17 - INFO - __main__ -   Language adapter for bn not found, using en instead
10/08/2021 21:42:17 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:42:17 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 21:42:17 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 21:42:17 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:42:17 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.32it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.19it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.67it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  7.00it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.19it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.30it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.38it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.42it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.46it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.49it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.49it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.50it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.52it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.52it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.53it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.54it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.53it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.53it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.51it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.51it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.50it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.48it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.49it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.49it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.49it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.48it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.42it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.41it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.44it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.43it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.44it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.54it/s]
10/08/2021 21:42:21 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 21:42:21 - INFO - __main__ -     f1 = 0.5264100268576544
10/08/2021 21:42:21 - INFO - __main__ -     loss = 3.9108278676867485
10/08/2021 21:42:21 - INFO - __main__ -     precision = 0.5135371179039301
10/08/2021 21:42:21 - INFO - __main__ -     recall = 0.5399449035812672
10/08/2021 21:42:21 - INFO - __main__ -   Language adapter for mr not found, using en instead
10/08/2021 21:42:21 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:42:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 21:42:22 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 21:42:22 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:42:22 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.55it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.51it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.50it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.48it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.51it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.48it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.50it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.49it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.48it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.49it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.50it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.50it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.48it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.48it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.50it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.50it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.49it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.49it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.47it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.46it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.44it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.43it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.42it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.44it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.45it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.46it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.46it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.46it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.45it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.43it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.41it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.63it/s]
10/08/2021 21:42:26 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 21:42:26 - INFO - __main__ -     f1 = 0.4429967426710098
10/08/2021 21:42:26 - INFO - __main__ -     loss = 2.8395832143723965
10/08/2021 21:42:26 - INFO - __main__ -     precision = 0.4082721814543029
10/08/2021 21:42:26 - INFO - __main__ -     recall = 0.48417721518987344
10/08/2021 21:42:26 - INFO - __main__ -   Language adapter for ta not found, using en instead
10/08/2021 21:42:26 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:42:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 21:42:26 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 21:42:26 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:42:26 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.51it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.48it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.47it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.46it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.45it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.46it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.44it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.44it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.45it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.46it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.46it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.46it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.44it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.44it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.44it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.44it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.44it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.44it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.43it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.42it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.40it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.40it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.40it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.40it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.40it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.40it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.42it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.42it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.41it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.40it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.40it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.59it/s]
10/08/2021 21:42:31 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 21:42:31 - INFO - __main__ -     f1 = 0.2713915298184961
10/08/2021 21:42:31 - INFO - __main__ -     loss = 3.734482266008854
10/08/2021 21:42:31 - INFO - __main__ -     precision = 0.29263746505125815
10/08/2021 21:42:31 - INFO - __main__ -     recall = 0.25302175664786464
10/08/2021 21:42:31 - INFO - __main__ -   Language adapter for is not found, using en instead
10/08/2021 21:42:31 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:42:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
10/08/2021 21:42:31 - INFO - __main__ -   ***** Running evaluation  in is *****
10/08/2021 21:42:31 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:42:31 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.42it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.44it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.46it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.46it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.55it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.84it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.04it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.17it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.25it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  7.32it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.34it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.37it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.40it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.41it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.41it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.42it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.42it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.40it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.40it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.39it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.39it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  7.37it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.36it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.37it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.39it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.39it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.40it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.39it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.38it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.38it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.39it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.49it/s]
10/08/2021 21:42:35 - INFO - __main__ -   ***** Evaluation result  in is *****
10/08/2021 21:42:35 - INFO - __main__ -     f1 = 0.6463878326996199
10/08/2021 21:42:35 - INFO - __main__ -     loss = 1.5377771258354187
10/08/2021 21:42:35 - INFO - __main__ -     precision = 0.604982206405694
10/08/2021 21:42:35 - INFO - __main__ -     recall = 0.6938775510204082
10/08/2021 21:42:35 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:42:48 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:42:48 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:43:07 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 21:43:10 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:43:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 21:43:10 - INFO - __main__ -   Seed = 42
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:43:23 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 21:43:41 - INFO - root -   save model
10/08/2021 21:43:41 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:43:45 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:43:45 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:43:56 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:43:56 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 21:43:56 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/08/2021 21:43:56 - INFO - root -   Trying to decide if add adapter
10/08/2021 21:43:56 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 21:43:56 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 21:43:56 - INFO - __main__ -   Language = en
10/08/2021 21:43:56 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 21:44:12 - INFO - __main__ -   Language adapter for bn not found, using en instead
10/08/2021 21:44:12 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:44:12 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 21:44:13 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 21:44:13 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:44:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.43it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.88it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  7.18it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.32it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.40it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.45it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.50it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.50it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.51it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.51it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.52it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.51it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.52it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.53it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.51it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.51it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.53it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.53it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.53it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.52it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.52it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.49it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.43it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.44it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.48it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.49it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.50it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.51it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.48it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.45it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.45it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.62it/s]
10/08/2021 21:44:17 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 21:44:17 - INFO - __main__ -     f1 = 0.5402138540213854
10/08/2021 21:44:17 - INFO - __main__ -     loss = 4.150186933577061
10/08/2021 21:44:17 - INFO - __main__ -     precision = 0.5470809792843692
10/08/2021 21:44:17 - INFO - __main__ -     recall = 0.5335169880624426
10/08/2021 21:44:17 - INFO - __main__ -   Language adapter for mr not found, using en instead
10/08/2021 21:44:17 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:44:17 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 21:44:17 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 21:44:17 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:44:17 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.64it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.57it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.55it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.55it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.54it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.53it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.53it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.53it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.52it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.54it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.54it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.50it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.49it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.50it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.50it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.51it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.50it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.49it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.50it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.49it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.49it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.50it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.49it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.49it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.43it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.43it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.45it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.31it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.34it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.37it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.39it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.64it/s]
10/08/2021 21:44:21 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 21:44:21 - INFO - __main__ -     f1 = 0.4077892325315005
10/08/2021 21:44:21 - INFO - __main__ -     loss = 2.9534711688756943
10/08/2021 21:44:21 - INFO - __main__ -     precision = 0.3940959409594096
10/08/2021 21:44:21 - INFO - __main__ -     recall = 0.4224683544303797
10/08/2021 21:44:21 - INFO - __main__ -   Language adapter for ta not found, using en instead
10/08/2021 21:44:21 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:44:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 21:44:21 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 21:44:21 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:44:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.55it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.50it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.51it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.50it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.50it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.50it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.49it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.48it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.49it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.50it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.49it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.48it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.46it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.46it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.47it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.48it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.47it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.47it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.48it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.47it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.45it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.46it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.46it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.47it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.46it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.46it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.46it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.45it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.45it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.43it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.43it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.63it/s]
10/08/2021 21:44:26 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 21:44:26 - INFO - __main__ -     f1 = 0.23783783783783785
10/08/2021 21:44:26 - INFO - __main__ -     loss = 4.138149857521057
10/08/2021 21:44:26 - INFO - __main__ -     precision = 0.2696629213483146
10/08/2021 21:44:26 - INFO - __main__ -     recall = 0.2127316680096696
10/08/2021 21:44:26 - INFO - __main__ -   Language adapter for is not found, using en instead
10/08/2021 21:44:26 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:44:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
10/08/2021 21:44:26 - INFO - __main__ -   ***** Running evaluation  in is *****
10/08/2021 21:44:26 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:44:26 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.56it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.51it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.08it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.56it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.87it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.07it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  7.21it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.29it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.35it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.38it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.40it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.42it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.45it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.45it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.45it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.46it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.44it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.45it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.45it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.44it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.43it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  7.44it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.45it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.43it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.44it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.46it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.42it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.40it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.41it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.39it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.39it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.49it/s]
10/08/2021 21:44:30 - INFO - __main__ -   ***** Evaluation result  in is *****
10/08/2021 21:44:30 - INFO - __main__ -     f1 = 0.6177253708634463
10/08/2021 21:44:30 - INFO - __main__ -     loss = 1.7589807491749525
10/08/2021 21:44:30 - INFO - __main__ -     precision = 0.5783475783475783
10/08/2021 21:44:30 - INFO - __main__ -     recall = 0.6628571428571428
10/08/2021 21:44:30 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:44:42 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:44:42 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:45:02 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 21:45:04 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:45:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 21:45:04 - INFO - __main__ -   Seed = 52
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:45:27 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 21:45:42 - INFO - root -   save model
10/08/2021 21:45:42 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:45:46 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 21:45:46 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:45:58 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:45:58 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 21:45:58 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/08/2021 21:45:58 - INFO - root -   Trying to decide if add adapter
10/08/2021 21:45:58 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 21:45:58 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 21:45:58 - INFO - __main__ -   Language = en
10/08/2021 21:45:58 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 21:46:13 - INFO - __main__ -   Language adapter for bn not found, using en instead
10/08/2021 21:46:13 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:46:13 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 21:46:13 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 21:46:13 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:46:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.93it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.30it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.42it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.47it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.51it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.52it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.54it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.53it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.54it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.52it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.53it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.54it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.53it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.54it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.54it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.52it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.53it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.54it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.50it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.51it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.50it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.48it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.49it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.49it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.47it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.46it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.47it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.46it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.45it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.45it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.48it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.66it/s]
10/08/2021 21:46:18 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 21:46:18 - INFO - __main__ -     f1 = 0.5041095890410958
10/08/2021 21:46:18 - INFO - __main__ -     loss = 3.9634538143873215
10/08/2021 21:46:18 - INFO - __main__ -     precision = 0.5013623978201635
10/08/2021 21:46:18 - INFO - __main__ -     recall = 0.5068870523415978
10/08/2021 21:46:18 - INFO - __main__ -   Language adapter for mr not found, using en instead
10/08/2021 21:46:18 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:46:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 21:46:18 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 21:46:18 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:46:18 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.57it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.57it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.58it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.58it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.56it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.55it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.54it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.54it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.52it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.51it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.50it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.50it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.52it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.52it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.53it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.54it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.53it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.51it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.50it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:02,  5.97it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  6.34it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.65it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.86it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.03it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.14it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.23it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.28it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.33it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.35it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.37it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.41it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.46it/s]
10/08/2021 21:46:22 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 21:46:22 - INFO - __main__ -     f1 = 0.4288577154308617
10/08/2021 21:46:22 - INFO - __main__ -     loss = 2.7849283814430237
10/08/2021 21:46:22 - INFO - __main__ -     precision = 0.4346060113728676
10/08/2021 21:46:22 - INFO - __main__ -     recall = 0.42325949367088606
10/08/2021 21:46:22 - INFO - __main__ -   Language adapter for ta not found, using en instead
10/08/2021 21:46:22 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:46:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 21:46:23 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 21:46:23 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:46:23 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.55it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.52it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.51it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.51it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.51it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.49it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.48it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.49it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.48it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.46it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.47it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.46it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.46it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.46it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.46it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.46it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.47it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.47it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.48it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.48it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.46it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.46it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.46it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.46it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.45it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.44it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.43it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.44it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.42it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.42it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.41it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.63it/s]
10/08/2021 21:46:27 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 21:46:27 - INFO - __main__ -     f1 = 0.16683417085427138
10/08/2021 21:46:27 - INFO - __main__ -     loss = 4.5315265879035
10/08/2021 21:46:27 - INFO - __main__ -     precision = 0.22162883845126835
10/08/2021 21:46:27 - INFO - __main__ -     recall = 0.13376309427880742
10/08/2021 21:46:27 - INFO - __main__ -   Language adapter for is not found, using en instead
10/08/2021 21:46:27 - INFO - __main__ -   Set active language adapter to en
10/08/2021 21:46:27 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
10/08/2021 21:46:27 - INFO - __main__ -   ***** Running evaluation  in is *****
10/08/2021 21:46:27 - INFO - __main__ -     Num examples = 1000
10/08/2021 21:46:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.50it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.49it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.50it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.50it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.51it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.50it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.48it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.47it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.47it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.48it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.49it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.49it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.48it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.47it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.46it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.45it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.46it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.46it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.45it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.45it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.45it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.45it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.46it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.44it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.41it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:01,  5.03it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  5.56it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  6.01it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.36it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.63it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.85it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.25it/s]
10/08/2021 21:46:32 - INFO - __main__ -   ***** Evaluation result  in is *****
10/08/2021 21:46:32 - INFO - __main__ -     f1 = 0.6048448145344437
10/08/2021 21:46:32 - INFO - __main__ -     loss = 1.7314767390489578
10/08/2021 21:46:32 - INFO - __main__ -     precision = 0.5638673253352152
10/08/2021 21:46:32 - INFO - __main__ -     recall = 0.6522448979591837
10/08/2021 21:46:32 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:46:45 - INFO - __main__ -   Using lang2id = None
10/08/2021 21:46:45 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 21:47:02 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 22:05:47 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:05:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 22:05:47 - INFO - __main__ -   Seed = 12
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:05:55 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 22:06:11 - INFO - root -   save model
10/08/2021 22:06:11 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:06:14 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:06:14 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:06:28 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:06:28 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 22:06:28 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/08/2021 22:06:28 - INFO - root -   Trying to decide if add adapter
10/08/2021 22:06:28 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 22:06:28 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 22:06:28 - INFO - __main__ -   Language = en
10/08/2021 22:06:28 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 22:06:44 - INFO - __main__ -   Language adapter for ar not found, using en instead
10/08/2021 22:06:44 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:06:44 - INFO - __main__ -   all languages = ar
10/08/2021 22:06:44 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/ar/test.bert-base-multilingual-cased in language ar
10/08/2021 22:06:44 - INFO - utils_tag -   lang_id=0, lang=ar, lang2id=None
10/08/2021 22:06:44 - INFO - utils_tag -   Writing example 0 of 10000
10/08/2021 22:06:44 - INFO - utils_tag -   *** Example ***
10/08/2021 22:06:44 - INFO - utils_tag -   guid: ar-1
10/08/2021 22:06:44 - INFO - utils_tag -   tokens: [CLS] ت ##علم في جامعة نور ##ث و ##ستر ##ن في . [SEP]
10/08/2021 22:06:44 - INFO - utils_tag -   input_ids: 101 766 35263 10210 23215 44668 15909 791 43138 10582 10210 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   label_ids: -100 6 -100 6 1 4 -100 4 -100 -100 4 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:06:44 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:06:44 - INFO - utils_tag -   *** Example ***
10/08/2021 22:06:44 - INFO - utils_tag -   guid: ar-2
10/08/2021 22:06:44 - INFO - utils_tag -   tokens: [CLS] ت ##حويل ده ##شهر ( مقاطعة كل ##ارد ##شت ) [SEP]
10/08/2021 22:06:44 - INFO - utils_tag -   input_ids: 101 766 105997 40084 48221 113 29471 16333 62689 17444 114 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   label_ids: -100 6 -100 0 -100 3 3 3 -100 -100 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:06:44 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:06:44 - INFO - utils_tag -   *** Example ***
10/08/2021 22:06:44 - INFO - utils_tag -   guid: ar-3
10/08/2021 22:06:44 - INFO - utils_tag -   tokens: [CLS] ت ##حويل و ##يل ##فر ##يد أو ##رب ##اي ##ن إ ##يل ##في ##س إن ##د ##زان ##غا [SEP]
10/08/2021 22:06:44 - INFO - utils_tag -   input_ids: 101 766 105997 791 15951 24455 14472 11843 30075 67261 10582 761 15951 21970 11091 26351 10658 60367 81959 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   label_ids: -100 6 -100 2 -100 -100 -100 5 -100 -100 -100 5 -100 -100 -100 5 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:06:44 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:06:44 - INFO - utils_tag -   *** Example ***
10/08/2021 22:06:44 - INFO - utils_tag -   guid: ar-4
10/08/2021 22:06:44 - INFO - utils_tag -   tokens: [CLS] ت ##حويل مقاطعة روش ( كان ##ساس ) [SEP]
10/08/2021 22:06:44 - INFO - utils_tag -   input_ids: 101 766 105997 29471 44563 113 12326 81107 114 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   label_ids: -100 6 -100 0 3 3 3 -100 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:06:44 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:06:44 - INFO - utils_tag -   *** Example ***
10/08/2021 22:06:44 - INFO - utils_tag -   guid: ar-5
10/08/2021 22:06:44 - INFO - utils_tag -   tokens: [CLS] ت ##حويل قائمة ال ##مو ##اض ##يع الأساسية في علم ال ##اج ##تم ##اع [SEP]
10/08/2021 22:06:44 - INFO - utils_tag -   input_ids: 101 766 105997 24754 59901 54037 34201 29336 79193 10210 23172 59901 24728 31498 21337 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:06:44 - INFO - utils_tag -   label_ids: -100 6 -100 0 3 -100 -100 -100 3 3 3 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:06:44 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:06:47 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128, len(features)=10000
10/08/2021 22:06:49 - INFO - __main__ -   ***** Running evaluation  in ar *****
10/08/2021 22:06:50 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:06:50 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.11it/s]Evaluating:   1%|          | 2/313 [00:00<00:47,  6.56it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.88it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:43,  7.15it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:42,  7.31it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:41,  7.39it/s]Evaluating:   2%|▏         | 7/313 [00:00<00:41,  7.45it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:40,  7.49it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:40,  7.52it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:40,  7.54it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:40,  7.55it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:39,  7.54it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:39,  7.53it/s]Evaluating:   4%|▍         | 14/313 [00:01<00:39,  7.55it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:39,  7.56it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:39,  7.54it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:39,  7.53it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:39,  7.52it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:39,  7.52it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:38,  7.52it/s]Evaluating:   7%|▋         | 21/313 [00:02<00:38,  7.50it/s]Evaluating:   7%|▋         | 22/313 [00:02<00:38,  7.49it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:38,  7.49it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:38,  7.47it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:38,  7.49it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:38,  7.48it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:38,  7.49it/s]Evaluating:   9%|▉         | 28/313 [00:03<00:38,  7.50it/s]Evaluating:   9%|▉         | 29/313 [00:03<00:37,  7.50it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:37,  7.48it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:37,  7.47it/s]Evaluating:  10%|█         | 32/313 [00:04<00:37,  7.48it/s]Evaluating:  11%|█         | 33/313 [00:04<00:37,  7.49it/s]Evaluating:  11%|█         | 34/313 [00:04<00:37,  7.48it/s]Evaluating:  11%|█         | 35/313 [00:04<00:37,  7.49it/s]Evaluating:  12%|█▏        | 36/313 [00:04<00:37,  7.48it/s]Evaluating:  12%|█▏        | 37/313 [00:04<00:36,  7.48it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:36,  7.44it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:36,  7.43it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:36,  7.43it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:36,  7.43it/s]Evaluating:  13%|█▎        | 42/313 [00:05<00:36,  7.44it/s]Evaluating:  14%|█▎        | 43/313 [00:05<00:36,  7.46it/s]Evaluating:  14%|█▍        | 44/313 [00:05<00:36,  7.45it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:35,  7.45it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:35,  7.45it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:35,  7.45it/s]Evaluating:  15%|█▌        | 48/313 [00:06<00:35,  7.46it/s]Evaluating:  16%|█▌        | 49/313 [00:06<00:35,  7.47it/s]Evaluating:  16%|█▌        | 50/313 [00:06<00:35,  7.45it/s]Evaluating:  16%|█▋        | 51/313 [00:06<00:35,  7.46it/s]Evaluating:  17%|█▋        | 52/313 [00:06<00:34,  7.47it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:34,  7.47it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:34,  7.48it/s]Evaluating:  18%|█▊        | 55/313 [00:07<00:44,  5.74it/s]Evaluating:  18%|█▊        | 56/313 [00:07<00:41,  6.17it/s]Evaluating:  18%|█▊        | 57/313 [00:07<00:39,  6.51it/s]Evaluating:  19%|█▊        | 58/313 [00:07<00:37,  6.76it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:36,  6.95it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:35,  7.10it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:34,  7.21it/s]Evaluating:  20%|█▉        | 62/313 [00:08<00:34,  7.27it/s]Evaluating:  20%|██        | 63/313 [00:08<00:34,  7.32it/s]Evaluating:  20%|██        | 64/313 [00:08<00:33,  7.35it/s]Evaluating:  21%|██        | 65/313 [00:08<00:33,  7.37it/s]Evaluating:  21%|██        | 66/313 [00:08<00:33,  7.39it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:33,  7.40it/s]Evaluating:  22%|██▏       | 68/313 [00:09<00:33,  7.41it/s]Evaluating:  22%|██▏       | 69/313 [00:09<00:32,  7.41it/s]Evaluating:  22%|██▏       | 70/313 [00:09<00:32,  7.40it/s]Evaluating:  23%|██▎       | 71/313 [00:09<00:32,  7.42it/s]Evaluating:  23%|██▎       | 72/313 [00:09<00:32,  7.40it/s]Evaluating:  23%|██▎       | 73/313 [00:09<00:32,  7.40it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:32,  7.39it/s]Evaluating:  24%|██▍       | 75/313 [00:10<00:32,  7.32it/s]Evaluating:  24%|██▍       | 76/313 [00:10<00:32,  7.35it/s]Evaluating:  25%|██▍       | 77/313 [00:10<00:32,  7.37it/s]Evaluating:  25%|██▍       | 78/313 [00:10<00:31,  7.38it/s]Evaluating:  25%|██▌       | 79/313 [00:10<00:31,  7.39it/s]Evaluating:  26%|██▌       | 80/313 [00:10<00:31,  7.39it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:31,  7.38it/s]Evaluating:  26%|██▌       | 82/313 [00:11<00:31,  7.36it/s]Evaluating:  27%|██▋       | 83/313 [00:11<00:31,  7.31it/s]Evaluating:  27%|██▋       | 84/313 [00:11<00:31,  7.27it/s]Evaluating:  27%|██▋       | 85/313 [00:11<00:31,  7.26it/s]Evaluating:  27%|██▋       | 86/313 [00:11<00:31,  7.30it/s]Evaluating:  28%|██▊       | 87/313 [00:11<00:30,  7.33it/s]Evaluating:  28%|██▊       | 88/313 [00:11<00:30,  7.34it/s]Evaluating:  28%|██▊       | 89/313 [00:12<00:30,  7.34it/s]Evaluating:  29%|██▉       | 90/313 [00:12<00:30,  7.33it/s]Evaluating:  29%|██▉       | 91/313 [00:12<00:34,  6.49it/s]Evaluating:  29%|██▉       | 92/313 [00:12<00:32,  6.73it/s]Evaluating:  30%|██▉       | 93/313 [00:12<00:31,  6.91it/s]Evaluating:  30%|███       | 94/313 [00:12<00:31,  7.03it/s]Evaluating:  30%|███       | 95/313 [00:12<00:30,  7.12it/s]Evaluating:  31%|███       | 96/313 [00:13<00:30,  7.18it/s]Evaluating:  31%|███       | 97/313 [00:13<00:29,  7.23it/s]Evaluating:  31%|███▏      | 98/313 [00:13<00:29,  7.27it/s]Evaluating:  32%|███▏      | 99/313 [00:13<00:29,  7.28it/s]Evaluating:  32%|███▏      | 100/313 [00:13<00:29,  7.30it/s]Evaluating:  32%|███▏      | 101/313 [00:13<00:28,  7.32it/s]Evaluating:  33%|███▎      | 102/313 [00:13<00:28,  7.31it/s]Evaluating:  33%|███▎      | 103/313 [00:14<00:28,  7.31it/s]Evaluating:  33%|███▎      | 104/313 [00:14<00:29,  7.20it/s]Evaluating:  34%|███▎      | 105/313 [00:14<00:29,  7.16it/s]Evaluating:  34%|███▍      | 106/313 [00:14<00:28,  7.21it/s]Evaluating:  34%|███▍      | 107/313 [00:14<00:28,  7.24it/s]Evaluating:  35%|███▍      | 108/313 [00:14<00:28,  7.26it/s]Evaluating:  35%|███▍      | 109/313 [00:14<00:28,  7.27it/s]Evaluating:  35%|███▌      | 110/313 [00:15<00:27,  7.29it/s]Evaluating:  35%|███▌      | 111/313 [00:15<00:27,  7.29it/s]Evaluating:  36%|███▌      | 112/313 [00:15<00:27,  7.27it/s]Evaluating:  36%|███▌      | 113/313 [00:15<00:27,  7.28it/s]Evaluating:  36%|███▋      | 114/313 [00:15<00:27,  7.29it/s]Evaluating:  37%|███▋      | 115/313 [00:15<00:27,  7.30it/s]Evaluating:  37%|███▋      | 116/313 [00:15<00:26,  7.31it/s]Evaluating:  37%|███▋      | 117/313 [00:16<00:26,  7.29it/s]Evaluating:  38%|███▊      | 118/313 [00:16<00:26,  7.29it/s]Evaluating:  38%|███▊      | 119/313 [00:16<00:26,  7.29it/s]Evaluating:  38%|███▊      | 120/313 [00:16<00:26,  7.29it/s]Evaluating:  39%|███▊      | 121/313 [00:16<00:26,  7.29it/s]Evaluating:  39%|███▉      | 122/313 [00:16<00:26,  7.29it/s]Evaluating:  39%|███▉      | 123/313 [00:16<00:26,  7.28it/s]Evaluating:  40%|███▉      | 124/313 [00:16<00:25,  7.27it/s]Evaluating:  40%|███▉      | 125/313 [00:17<00:25,  7.27it/s]Evaluating:  40%|████      | 126/313 [00:17<00:25,  7.26it/s]Evaluating:  41%|████      | 127/313 [00:17<00:30,  6.17it/s]Evaluating:  41%|████      | 128/313 [00:17<00:28,  6.46it/s]Evaluating:  41%|████      | 129/313 [00:17<00:27,  6.68it/s]Evaluating:  42%|████▏     | 130/313 [00:17<00:26,  6.84it/s]Evaluating:  42%|████▏     | 131/313 [00:18<00:26,  6.96it/s]Evaluating:  42%|████▏     | 132/313 [00:18<00:25,  7.04it/s]Evaluating:  42%|████▏     | 133/313 [00:18<00:25,  7.11it/s]Evaluating:  43%|████▎     | 134/313 [00:18<00:25,  7.16it/s]Evaluating:  43%|████▎     | 135/313 [00:18<00:24,  7.19it/s]Evaluating:  43%|████▎     | 136/313 [00:18<00:24,  7.21it/s]Evaluating:  44%|████▍     | 137/313 [00:18<00:24,  7.23it/s]Evaluating:  44%|████▍     | 138/313 [00:18<00:24,  7.23it/s]Evaluating:  44%|████▍     | 139/313 [00:19<00:24,  7.24it/s]Evaluating:  45%|████▍     | 140/313 [00:19<00:23,  7.24it/s]Evaluating:  45%|████▌     | 141/313 [00:19<00:23,  7.24it/s]Evaluating:  45%|████▌     | 142/313 [00:19<00:23,  7.24it/s]Evaluating:  46%|████▌     | 143/313 [00:19<00:23,  7.24it/s]Evaluating:  46%|████▌     | 144/313 [00:19<00:23,  7.22it/s]Evaluating:  46%|████▋     | 145/313 [00:19<00:23,  7.23it/s]Evaluating:  47%|████▋     | 146/313 [00:20<00:23,  7.23it/s]Evaluating:  47%|████▋     | 147/313 [00:20<00:22,  7.23it/s]Evaluating:  47%|████▋     | 148/313 [00:20<00:22,  7.22it/s]Evaluating:  48%|████▊     | 149/313 [00:20<00:22,  7.23it/s]Evaluating:  48%|████▊     | 150/313 [00:20<00:22,  7.23it/s]Evaluating:  48%|████▊     | 151/313 [00:20<00:22,  7.23it/s]Evaluating:  49%|████▊     | 152/313 [00:20<00:22,  7.22it/s]Evaluating:  49%|████▉     | 153/313 [00:21<00:22,  7.22it/s]Evaluating:  49%|████▉     | 154/313 [00:21<00:22,  7.21it/s]Evaluating:  50%|████▉     | 155/313 [00:21<00:21,  7.21it/s]Evaluating:  50%|████▉     | 156/313 [00:21<00:21,  7.22it/s]Evaluating:  50%|█████     | 157/313 [00:21<00:21,  7.21it/s]Evaluating:  50%|█████     | 158/313 [00:21<00:21,  7.21it/s]Evaluating:  51%|█████     | 159/313 [00:21<00:21,  7.21it/s]Evaluating:  51%|█████     | 160/313 [00:22<00:21,  7.20it/s]Evaluating:  51%|█████▏    | 161/313 [00:22<00:21,  7.20it/s]Evaluating:  52%|█████▏    | 162/313 [00:22<00:20,  7.20it/s]Evaluating:  52%|█████▏    | 163/313 [00:22<00:23,  6.52it/s]Evaluating:  52%|█████▏    | 164/313 [00:22<00:22,  6.70it/s]Evaluating:  53%|█████▎    | 165/313 [00:22<00:21,  6.84it/s]Evaluating:  53%|█████▎    | 166/313 [00:22<00:21,  6.94it/s]Evaluating:  53%|█████▎    | 167/313 [00:23<00:20,  7.01it/s]Evaluating:  54%|█████▎    | 168/313 [00:23<00:20,  7.06it/s]Evaluating:  54%|█████▍    | 169/313 [00:23<00:20,  7.10it/s]Evaluating:  54%|█████▍    | 170/313 [00:23<00:20,  7.12it/s]Evaluating:  55%|█████▍    | 171/313 [00:23<00:19,  7.13it/s]Evaluating:  55%|█████▍    | 172/313 [00:23<00:19,  7.14it/s]Evaluating:  55%|█████▌    | 173/313 [00:23<00:19,  7.15it/s]Evaluating:  56%|█████▌    | 174/313 [00:24<00:19,  7.13it/s]Evaluating:  56%|█████▌    | 175/313 [00:24<00:19,  7.15it/s]Evaluating:  56%|█████▌    | 176/313 [00:24<00:19,  7.14it/s]Evaluating:  57%|█████▋    | 177/313 [00:24<00:19,  7.15it/s]Evaluating:  57%|█████▋    | 178/313 [00:24<00:18,  7.15it/s]Evaluating:  57%|█████▋    | 179/313 [00:24<00:18,  7.16it/s]Evaluating:  58%|█████▊    | 180/313 [00:24<00:18,  7.17it/s]Evaluating:  58%|█████▊    | 181/313 [00:25<00:18,  7.16it/s]Evaluating:  58%|█████▊    | 182/313 [00:25<00:18,  7.16it/s]Evaluating:  58%|█████▊    | 183/313 [00:25<00:18,  7.16it/s]Evaluating:  59%|█████▉    | 184/313 [00:25<00:18,  7.15it/s]Evaluating:  59%|█████▉    | 185/313 [00:25<00:17,  7.15it/s]Evaluating:  59%|█████▉    | 186/313 [00:25<00:17,  7.15it/s]Evaluating:  60%|█████▉    | 187/313 [00:25<00:17,  7.14it/s]Evaluating:  60%|██████    | 188/313 [00:25<00:17,  7.14it/s]Evaluating:  60%|██████    | 189/313 [00:26<00:17,  7.15it/s]Evaluating:  61%|██████    | 190/313 [00:26<00:17,  7.14it/s]Evaluating:  61%|██████    | 191/313 [00:26<00:17,  7.14it/s]Evaluating:  61%|██████▏   | 192/313 [00:26<00:16,  7.15it/s]Evaluating:  62%|██████▏   | 193/313 [00:26<00:16,  7.15it/s]Evaluating:  62%|██████▏   | 194/313 [00:26<00:16,  7.14it/s]Evaluating:  62%|██████▏   | 195/313 [00:26<00:16,  7.14it/s]Evaluating:  63%|██████▎   | 196/313 [00:27<00:16,  7.12it/s]Evaluating:  63%|██████▎   | 197/313 [00:27<00:16,  7.13it/s]Evaluating:  63%|██████▎   | 198/313 [00:27<00:16,  7.14it/s]Evaluating:  64%|██████▎   | 199/313 [00:27<00:15,  7.13it/s]Evaluating:  64%|██████▍   | 200/313 [00:27<00:15,  7.13it/s]Evaluating:  64%|██████▍   | 201/313 [00:27<00:15,  7.10it/s]Evaluating:  65%|██████▍   | 202/313 [00:27<00:15,  7.07it/s]Evaluating:  65%|██████▍   | 203/313 [00:28<00:15,  7.07it/s]Evaluating:  65%|██████▌   | 204/313 [00:28<00:15,  7.07it/s]Evaluating:  65%|██████▌   | 205/313 [00:28<00:15,  7.07it/s]Evaluating:  66%|██████▌   | 206/313 [00:28<00:15,  7.08it/s]Evaluating:  66%|██████▌   | 207/313 [00:28<00:14,  7.07it/s]Evaluating:  66%|██████▋   | 208/313 [00:28<00:14,  7.08it/s]Evaluating:  67%|██████▋   | 209/313 [00:28<00:14,  7.09it/s]Evaluating:  67%|██████▋   | 210/313 [00:29<00:14,  7.08it/s]Evaluating:  67%|██████▋   | 211/313 [00:29<00:14,  7.07it/s]Evaluating:  68%|██████▊   | 212/313 [00:29<00:14,  7.05it/s]Evaluating:  68%|██████▊   | 213/313 [00:29<00:14,  7.05it/s]Evaluating:  68%|██████▊   | 214/313 [00:29<00:14,  7.01it/s]Evaluating:  69%|██████▊   | 215/313 [00:29<00:14,  6.96it/s]Evaluating:  69%|██████▉   | 216/313 [00:29<00:13,  6.98it/s]Evaluating:  69%|██████▉   | 217/313 [00:30<00:13,  7.02it/s]Evaluating:  70%|██████▉   | 218/313 [00:30<00:13,  7.04it/s]Evaluating:  70%|██████▉   | 219/313 [00:30<00:13,  7.06it/s]Evaluating:  70%|███████   | 220/313 [00:30<00:13,  7.07it/s]Evaluating:  71%|███████   | 221/313 [00:30<00:12,  7.08it/s]Evaluating:  71%|███████   | 222/313 [00:30<00:12,  7.08it/s]Evaluating:  71%|███████   | 223/313 [00:30<00:12,  7.08it/s]Evaluating:  72%|███████▏  | 224/313 [00:31<00:12,  7.04it/s]Evaluating:  72%|███████▏  | 225/313 [00:31<00:12,  6.98it/s]Evaluating:  72%|███████▏  | 226/313 [00:31<00:12,  6.98it/s]Evaluating:  73%|███████▎  | 227/313 [00:31<00:12,  6.99it/s]Evaluating:  73%|███████▎  | 228/313 [00:31<00:12,  7.00it/s]Evaluating:  73%|███████▎  | 229/313 [00:31<00:11,  7.01it/s]Evaluating:  73%|███████▎  | 230/313 [00:31<00:11,  7.02it/s]Evaluating:  74%|███████▍  | 231/313 [00:32<00:11,  7.03it/s]Evaluating:  74%|███████▍  | 232/313 [00:32<00:11,  7.05it/s]Evaluating:  74%|███████▍  | 233/313 [00:32<00:11,  7.05it/s]Evaluating:  75%|███████▍  | 234/313 [00:32<00:11,  7.02it/s]Evaluating:  75%|███████▌  | 235/313 [00:32<00:11,  6.97it/s]Evaluating:  75%|███████▌  | 236/313 [00:32<00:11,  6.95it/s]Evaluating:  76%|███████▌  | 237/313 [00:32<00:10,  6.98it/s]Evaluating:  76%|███████▌  | 238/313 [00:33<00:10,  7.02it/s]Evaluating:  76%|███████▋  | 239/313 [00:33<00:10,  7.04it/s]Evaluating:  77%|███████▋  | 240/313 [00:33<00:10,  7.03it/s]Evaluating:  77%|███████▋  | 241/313 [00:33<00:10,  7.04it/s]Evaluating:  77%|███████▋  | 242/313 [00:33<00:10,  7.02it/s]Evaluating:  78%|███████▊  | 243/313 [00:33<00:09,  7.03it/s]Evaluating:  78%|███████▊  | 244/313 [00:33<00:09,  7.01it/s]Evaluating:  78%|███████▊  | 245/313 [00:34<00:09,  6.98it/s]Evaluating:  79%|███████▊  | 246/313 [00:34<00:09,  6.95it/s]Evaluating:  79%|███████▉  | 247/313 [00:34<00:09,  6.79it/s]Evaluating:  79%|███████▉  | 248/313 [00:34<00:09,  6.68it/s]Evaluating:  80%|███████▉  | 249/313 [00:34<00:09,  6.67it/s]Evaluating:  80%|███████▉  | 250/313 [00:34<00:09,  6.78it/s]Evaluating:  80%|████████  | 251/313 [00:34<00:09,  6.85it/s]Evaluating:  81%|████████  | 252/313 [00:35<00:08,  6.90it/s]Evaluating:  81%|████████  | 253/313 [00:35<00:08,  6.92it/s]Evaluating:  81%|████████  | 254/313 [00:35<00:08,  6.91it/s]Evaluating:  81%|████████▏ | 255/313 [00:35<00:08,  6.88it/s]Evaluating:  82%|████████▏ | 256/313 [00:35<00:08,  6.66it/s]Evaluating:  82%|████████▏ | 257/313 [00:35<00:08,  6.71it/s]Evaluating:  82%|████████▏ | 258/313 [00:36<00:08,  6.52it/s]Evaluating:  83%|████████▎ | 259/313 [00:36<00:08,  6.49it/s]Evaluating:  83%|████████▎ | 260/313 [00:36<00:08,  6.53it/s]Evaluating:  83%|████████▎ | 261/313 [00:36<00:07,  6.55it/s]Evaluating:  84%|████████▎ | 262/313 [00:36<00:07,  6.51it/s]Evaluating:  84%|████████▍ | 263/313 [00:36<00:07,  6.50it/s]Evaluating:  84%|████████▍ | 264/313 [00:36<00:07,  6.54it/s]Evaluating:  85%|████████▍ | 265/313 [00:37<00:07,  6.63it/s]Evaluating:  85%|████████▍ | 266/313 [00:37<00:07,  6.69it/s]Evaluating:  85%|████████▌ | 267/313 [00:37<00:06,  6.75it/s]Evaluating:  86%|████████▌ | 268/313 [00:37<00:06,  6.82it/s]Evaluating:  86%|████████▌ | 269/313 [00:37<00:06,  6.87it/s]Evaluating:  86%|████████▋ | 270/313 [00:37<00:06,  6.90it/s]Evaluating:  87%|████████▋ | 271/313 [00:37<00:06,  6.93it/s]Evaluating:  87%|████████▋ | 272/313 [00:38<00:05,  6.94it/s]Evaluating:  87%|████████▋ | 273/313 [00:38<00:05,  6.96it/s]Evaluating:  88%|████████▊ | 274/313 [00:38<00:05,  6.97it/s]Evaluating:  88%|████████▊ | 275/313 [00:38<00:05,  6.98it/s]Evaluating:  88%|████████▊ | 276/313 [00:38<00:05,  6.92it/s]Evaluating:  88%|████████▊ | 277/313 [00:38<00:05,  6.85it/s]Evaluating:  89%|████████▉ | 278/313 [00:38<00:05,  6.84it/s]Evaluating:  89%|████████▉ | 279/313 [00:39<00:04,  6.87it/s]Evaluating:  89%|████████▉ | 280/313 [00:39<00:04,  6.91it/s]Evaluating:  90%|████████▉ | 281/313 [00:39<00:04,  6.94it/s]Evaluating:  90%|█████████ | 282/313 [00:39<00:04,  6.88it/s]Evaluating:  90%|█████████ | 283/313 [00:39<00:04,  6.91it/s]Evaluating:  91%|█████████ | 284/313 [00:39<00:04,  6.92it/s]Evaluating:  91%|█████████ | 285/313 [00:39<00:04,  6.93it/s]Evaluating:  91%|█████████▏| 286/313 [00:40<00:03,  6.89it/s]Evaluating:  92%|█████████▏| 287/313 [00:40<00:03,  6.90it/s]Evaluating:  92%|█████████▏| 288/313 [00:40<00:03,  6.87it/s]Evaluating:  92%|█████████▏| 289/313 [00:40<00:03,  6.86it/s]Evaluating:  93%|█████████▎| 290/313 [00:40<00:03,  6.79it/s]Evaluating:  93%|█████████▎| 291/313 [00:40<00:03,  6.72it/s]Evaluating:  93%|█████████▎| 292/313 [00:41<00:03,  6.73it/s]Evaluating:  94%|█████████▎| 293/313 [00:41<00:02,  6.77it/s]Evaluating:  94%|█████████▍| 294/313 [00:41<00:02,  6.81it/s]Evaluating:  94%|█████████▍| 295/313 [00:41<00:02,  6.86it/s]Evaluating:  95%|█████████▍| 296/313 [00:41<00:02,  6.88it/s]Evaluating:  95%|█████████▍| 297/313 [00:41<00:02,  6.86it/s]Evaluating:  95%|█████████▌| 298/313 [00:41<00:02,  6.85it/s]Evaluating:  96%|█████████▌| 299/313 [00:42<00:02,  6.86it/s]Evaluating:  96%|█████████▌| 300/313 [00:42<00:01,  6.87it/s]Evaluating:  96%|█████████▌| 301/313 [00:42<00:01,  6.87it/s]Evaluating:  96%|█████████▋| 302/313 [00:42<00:03,  3.45it/s]Evaluating:  97%|█████████▋| 303/313 [00:43<00:02,  4.00it/s]Evaluating:  97%|█████████▋| 304/313 [00:43<00:01,  4.52it/s]Evaluating:  97%|█████████▋| 305/313 [00:43<00:01,  4.95it/s]Evaluating:  98%|█████████▊| 306/313 [00:43<00:01,  5.30it/s]Evaluating:  98%|█████████▊| 307/313 [00:43<00:01,  5.61it/s]Evaluating:  98%|█████████▊| 308/313 [00:43<00:00,  5.87it/s]Evaluating:  99%|█████████▊| 309/313 [00:44<00:00,  6.09it/s]Evaluating:  99%|█████████▉| 310/313 [00:44<00:00,  6.27it/s]Evaluating:  99%|█████████▉| 311/313 [00:44<00:00,  6.43it/s]Evaluating: 100%|█████████▉| 312/313 [00:44<00:00,  6.53it/s]Evaluating: 100%|██████████| 313/313 [00:44<00:00,  7.03it/s]
10/08/2021 22:07:35 - INFO - __main__ -   ***** Evaluation result  in ar *****
10/08/2021 22:07:35 - INFO - __main__ -     f1 = 0.28056556695314666
10/08/2021 22:07:35 - INFO - __main__ -     loss = 1.6974028249899038
10/08/2021 22:07:35 - INFO - __main__ -     precision = 0.29240104016180296
10/08/2021 22:07:35 - INFO - __main__ -     recall = 0.2696509459099387
10/08/2021 22:07:35 - INFO - __main__ -   Language adapter for bh not found, using en instead
10/08/2021 22:07:35 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:07:35 - INFO - __main__ -   all languages = bh
10/08/2021 22:07:35 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/bh/test.bert-base-multilingual-cased in language bh
10/08/2021 22:07:35 - INFO - utils_tag -   lang_id=0, lang=bh, lang2id=None
10/08/2021 22:07:35 - INFO - utils_tag -   Writing example 0 of 102
10/08/2021 22:07:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:35 - INFO - utils_tag -   guid: bh-1
10/08/2021 22:07:35 - INFO - utils_tag -   tokens: [CLS] अ ##ले ##क ##्ज ##ें ##ड ##र फ ##ॉन हम ##् ##ब ##ोल ##्ट ( 1769 [UNK] 1859 ) [UNK] considered the father of modern ge ##ography . [SEP]
10/08/2021 22:07:35 - INFO - utils_tag -   input_ids: 101 851 13665 12151 73649 42057 20691 11549 886 69016 105127 20429 18351 51140 25695 113 28842 100 13948 114 100 14289 10105 13194 10108 13456 46503 34850 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   label_ids: -100 2 -100 -100 -100 -100 -100 -100 5 -100 5 -100 -100 -100 -100 6 6 -100 -100 6 6 6 6 6 6 6 6 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:35 - INFO - utils_tag -   guid: bh-2
10/08/2021 22:07:35 - INFO - utils_tag -   tokens: [CLS] पर ##ि ##णी ##ति च ##ोप ##ड़ा [SEP]
10/08/2021 22:07:35 - INFO - utils_tag -   input_ids: 101 12213 12878 29624 24877 870 65430 44302 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   label_ids: -100 2 -100 -100 -100 5 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:35 - INFO - utils_tag -   guid: bh-3
10/08/2021 22:07:35 - INFO - utils_tag -   tokens: [CLS] प ##च ##्छ ##िम च ##ं ##पा ##रण जिला [SEP]
10/08/2021 22:07:35 - INFO - utils_tag -   input_ids: 101 885 16940 33345 50419 870 14018 42035 31857 70697 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   label_ids: -100 0 -100 -100 -100 3 -100 -100 -100 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:35 - INFO - utils_tag -   guid: bh-4
10/08/2021 22:07:35 - INFO - utils_tag -   tokens: [CLS] भारत क पर ##धान ##मंत्री लोग [SEP]
10/08/2021 22:07:35 - INFO - utils_tag -   input_ids: 101 14311 865 12213 69473 109520 51348 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   label_ids: -100 2 5 5 -100 -100 5 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:35 - INFO - utils_tag -   guid: bh-5
10/08/2021 22:07:35 - INFO - utils_tag -   tokens: [CLS] फ ##ि ##जी [SEP]
10/08/2021 22:07:35 - INFO - utils_tag -   input_ids: 101 886 12878 30800 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:35 - INFO - utils_tag -   label_ids: -100 0 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:35 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bh_bert-base-multilingual-cased_128, len(features)=102
10/08/2021 22:07:35 - INFO - __main__ -   ***** Running evaluation  in bh *****
10/08/2021 22:07:35 - INFO - __main__ -     Num examples = 102
10/08/2021 22:07:35 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  6.83it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  6.80it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  6.79it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.42it/s]
10/08/2021 22:07:36 - INFO - __main__ -   ***** Evaluation result  in bh *****
10/08/2021 22:07:36 - INFO - __main__ -     f1 = 0.4881889763779528
10/08/2021 22:07:36 - INFO - __main__ -     loss = 0.7368542551994324
10/08/2021 22:07:36 - INFO - __main__ -     precision = 0.44285714285714284
10/08/2021 22:07:36 - INFO - __main__ -     recall = 0.543859649122807
10/08/2021 22:07:36 - INFO - __main__ -   Language adapter for hi not found, using en instead
10/08/2021 22:07:36 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:07:36 - INFO - __main__ -   all languages = hi
10/08/2021 22:07:36 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/hi/test.bert-base-multilingual-cased in language hi
10/08/2021 22:07:36 - INFO - utils_tag -   lang_id=0, lang=hi, lang2id=None
10/08/2021 22:07:36 - INFO - utils_tag -   Writing example 0 of 1000
10/08/2021 22:07:36 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:36 - INFO - utils_tag -   guid: hi-1
10/08/2021 22:07:36 - INFO - utils_tag -   tokens: [CLS] न ##न्दा देवी पर ##्वत [SEP]
10/08/2021 22:07:36 - INFO - utils_tag -   input_ids: 101 884 57987 99207 12213 95877 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   label_ids: -100 0 -100 3 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:36 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:36 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:36 - INFO - utils_tag -   guid: hi-2
10/08/2021 22:07:36 - INFO - utils_tag -   tokens: [CLS] = = = आ ##गर ##ा में आम ##ंत्रण और प ##ला ##यन = = = [SEP]
10/08/2021 22:07:36 - INFO - utils_tag -   input_ids: 101 134 134 134 852 45086 11208 10532 70575 107229 10977 885 14334 65469 134 134 134 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   label_ids: -100 6 -100 -100 0 -100 -100 6 6 -100 6 6 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:36 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:36 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:36 - INFO - utils_tag -   guid: hi-3
10/08/2021 22:07:36 - INFO - utils_tag -   tokens: [CLS] ' ' ' अ ##ट ##ला ##ंट ##ा फ ##ाल ##्क ##न् ##स ' ' ' [SEP]
10/08/2021 22:07:36 - INFO - utils_tag -   input_ids: 101 112 112 112 851 14835 14334 58935 11208 886 35133 28197 41013 13432 112 112 112 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   label_ids: -100 6 6 -100 1 -100 -100 -100 -100 4 -100 -100 -100 -100 6 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:36 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:36 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:36 - INFO - utils_tag -   guid: hi-4
10/08/2021 22:07:36 - INFO - utils_tag -   tokens: [CLS] को ##टा , राजस्थान [SEP]
10/08/2021 22:07:36 - INFO - utils_tag -   input_ids: 101 11267 47026 117 102399 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   label_ids: -100 0 -100 3 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:36 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:36 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:36 - INFO - utils_tag -   guid: hi-5
10/08/2021 22:07:36 - INFO - utils_tag -   tokens: [CLS] संयुक्त राष्ट्र खा ##द्य एवं क ##ृ ##षि सं ##ग ##ठन [SEP]
10/08/2021 22:07:36 - INFO - utils_tag -   input_ids: 101 34475 72998 64566 97110 21850 865 111207 84456 28466 19741 63869 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:36 - INFO - utils_tag -   label_ids: -100 1 4 4 -100 4 4 -100 -100 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:36 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:36 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128, len(features)=1000
10/08/2021 22:07:36 - INFO - __main__ -   ***** Running evaluation  in hi *****
10/08/2021 22:07:36 - INFO - __main__ -     Num examples = 1000
10/08/2021 22:07:36 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.77it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.77it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.77it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.77it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.77it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.77it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.77it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.76it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.76it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.76it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.76it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.76it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.76it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.76it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.76it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.76it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.76it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.76it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.76it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.76it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.75it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.74it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.74it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.74it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.74it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.73it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  6.73it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.74it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.74it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.74it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.74it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.90it/s]
10/08/2021 22:07:41 - INFO - __main__ -   ***** Evaluation result  in hi *****
10/08/2021 22:07:41 - INFO - __main__ -     f1 = 0.5414736842105263
10/08/2021 22:07:41 - INFO - __main__ -     loss = 0.773034792393446
10/08/2021 22:07:41 - INFO - __main__ -     precision = 0.5605928509154315
10/08/2021 22:07:41 - INFO - __main__ -     recall = 0.5236156351791531
10/08/2021 22:07:41 - INFO - __main__ -   Language adapter for fo not found, using en instead
10/08/2021 22:07:41 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:07:41 - INFO - __main__ -   all languages = fo
10/08/2021 22:07:41 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/fo/test.bert-base-multilingual-cased in language fo
10/08/2021 22:07:41 - INFO - utils_tag -   lang_id=0, lang=fo, lang2id=None
10/08/2021 22:07:41 - INFO - utils_tag -   Writing example 0 of 100
10/08/2021 22:07:41 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:41 - INFO - utils_tag -   guid: fo-1
10/08/2021 22:07:41 - INFO - utils_tag -   tokens: [CLS] By ##rta - Af ##tur og aftur [SEP]
10/08/2021 22:07:41 - INFO - utils_tag -   input_ids: 101 12716 16294 118 71164 15698 10156 87168 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   label_ids: -100 1 -100 6 6 -100 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:41 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:41 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:41 - INFO - utils_tag -   guid: fo-2
10/08/2021 22:07:41 - INFO - utils_tag -   tokens: [CLS] ' ' ' Há ##ls ##hv ##ít ##ur ná ##pur ' ' ' [SEP]
10/08/2021 22:07:41 - INFO - utils_tag -   input_ids: 101 112 112 112 55664 11747 96389 25462 10546 33873 20413 112 112 112 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   label_ids: -100 6 6 -100 0 -100 -100 -100 -100 3 -100 6 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:41 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:41 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:41 - INFO - utils_tag -   guid: fo-3
10/08/2021 22:07:41 - INFO - utils_tag -   tokens: [CLS] Universiteti ##ð í Tübingen varð sto ##vna ##ð [SEP]
10/08/2021 22:07:41 - INFO - utils_tag -   input_ids: 101 75101 12332 267 38458 44883 47264 28492 12332 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   label_ids: -100 1 -100 4 4 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:41 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:41 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:41 - INFO - utils_tag -   guid: fo-4
10/08/2021 22:07:41 - INFO - utils_tag -   tokens: [CLS] Vi ##nnar ##in var it ##ali ##uma ##ður ##in Gregorio Pal ##trin ##ieri , i ##ð vann við tí ##ði ##ni 7 : 44 . 98 . [SEP]
10/08/2021 22:07:41 - INFO - utils_tag -   input_ids: 101 31826 60736 10245 10299 10271 13133 16746 17225 10245 36167 78722 109163 38102 117 177 12332 20956 16922 35857 18326 10342 128 131 11126 119 12327 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   label_ids: -100 6 -100 -100 6 6 -100 -100 -100 -100 2 5 -100 -100 6 6 -100 6 6 6 -100 -100 6 -100 -100 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:41 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:41 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:41 - INFO - utils_tag -   guid: fo-5
10/08/2021 22:07:41 - INFO - utils_tag -   tokens: [CLS] Í prosent ##um er Samba ##nds ##f ##lok ##kur ##in st ##ør ##sti fl ##okk ##ur á ting ##i , men he ##vu ##r eina ##ns 7 ting ##limi ##r . [SEP]
10/08/2021 22:07:41 - INFO - utils_tag -   input_ids: 101 238 99544 10465 10163 83832 27090 10575 84321 24260 10245 28780 18437 11964 58768 98013 10546 255 79436 10116 117 10588 10261 13048 10129 85334 10891 128 79436 89337 10129 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:41 - INFO - utils_tag -   label_ids: -100 6 6 -100 6 1 -100 -100 -100 -100 -100 6 -100 -100 6 -100 -100 6 6 -100 6 6 6 -100 -100 6 -100 6 6 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:41 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:41 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128, len(features)=100
10/08/2021 22:07:41 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/08/2021 22:07:41 - INFO - __main__ -     Num examples = 100
10/08/2021 22:07:41 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  6.77it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  6.78it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  6.78it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.55it/s]
10/08/2021 22:07:42 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/08/2021 22:07:42 - INFO - __main__ -     f1 = 0.6770428015564203
10/08/2021 22:07:42 - INFO - __main__ -     loss = 0.4112555533647537
10/08/2021 22:07:42 - INFO - __main__ -     precision = 0.635036496350365
10/08/2021 22:07:42 - INFO - __main__ -     recall = 0.725
10/08/2021 22:07:42 - INFO - __main__ -   Language adapter for no not found, using en instead
10/08/2021 22:07:42 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:07:42 - INFO - __main__ -   all languages = no
10/08/2021 22:07:42 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/no/test.bert-base-multilingual-cased in language no
10/08/2021 22:07:42 - INFO - utils_tag -   lang_id=0, lang=no, lang2id=None
10/08/2021 22:07:42 - INFO - utils_tag -   Writing example 0 of 10000
10/08/2021 22:07:42 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:42 - INFO - utils_tag -   guid: no-1
10/08/2021 22:07:42 - INFO - utils_tag -   tokens: [CLS] Is ##hockey - VM 2011 ( divisjon II ) [SEP]
10/08/2021 22:07:42 - INFO - utils_tag -   input_ids: 101 12034 65143 118 18618 10158 113 63601 10335 114 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   label_ids: -100 0 -100 -100 -100 3 3 3 3 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:42 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:42 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:42 - INFO - utils_tag -   guid: no-2
10/08/2021 22:07:42 - INFO - utils_tag -   tokens: [CLS] Han var med på det amerikanske laget som vant basket ##tur ##neri ##ngen . [SEP]
10/08/2021 22:07:42 - INFO - utils_tag -   input_ids: 101 10818 10299 10172 10217 10349 23448 25640 10181 23266 47178 15698 54839 15717 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   label_ids: -100 6 6 6 6 6 0 6 6 6 6 -100 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:42 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:42 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:42 - INFO - utils_tag -   guid: no-3
10/08/2021 22:07:42 - INFO - utils_tag -   tokens: [CLS] Am ##para - distriktet vender mot Bengal ##buk ##ta i øst . [SEP]
10/08/2021 22:07:42 - INFO - utils_tag -   input_ids: 101 11500 30978 118 25402 59500 12082 32599 58074 10213 177 32988 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   label_ids: -100 6 -100 -100 -100 6 6 0 -100 -100 6 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:42 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:42 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:42 - INFO - utils_tag -   guid: no-4
10/08/2021 22:07:42 - INFO - utils_tag -   tokens: [CLS] I tillegg har sangen ##e gjort det bra i en rekke land , og lå blant annet på tredje plass på dance ##top - listen i Latvia . [SEP]
10/08/2021 22:07:42 - INFO - utils_tag -   input_ids: 101 146 26532 10453 55855 10112 27492 10349 67603 177 10110 29821 11773 117 10156 35469 17834 20980 10217 25011 25673 10217 20412 37253 118 55129 177 55926 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   label_ids: -100 6 6 6 6 -100 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 -100 -100 -100 6 0 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:42 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:42 - INFO - utils_tag -   *** Example ***
10/08/2021 22:07:42 - INFO - utils_tag -   guid: no-5
10/08/2021 22:07:42 - INFO - utils_tag -   tokens: [CLS] i nordøst mot Ad ##riate ##r ##havet , [SEP]
10/08/2021 22:07:42 - INFO - utils_tag -   input_ids: 101 177 101639 12082 25474 88958 10129 39633 117 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:07:42 - INFO - utils_tag -   label_ids: -100 6 6 6 0 -100 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:07:42 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:07:46 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128, len(features)=10000
10/08/2021 22:07:48 - INFO - __main__ -   ***** Running evaluation  in no *****
10/08/2021 22:07:48 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:07:48 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.74it/s]Evaluating:   1%|          | 2/313 [00:00<00:44,  7.04it/s]Evaluating:   1%|          | 3/313 [00:00<00:42,  7.23it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:42,  7.32it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:41,  7.36it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:41,  7.40it/s]Evaluating:   2%|▏         | 7/313 [00:00<00:41,  7.42it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:41,  7.43it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:40,  7.43it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:40,  7.43it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:40,  7.42it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:40,  7.43it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:40,  7.43it/s]Evaluating:   4%|▍         | 14/313 [00:01<00:40,  7.43it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:40,  7.42it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:40,  7.41it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:39,  7.41it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:39,  7.42it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:39,  7.42it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:39,  7.41it/s]Evaluating:   7%|▋         | 21/313 [00:02<00:39,  7.42it/s]Evaluating:   7%|▋         | 22/313 [00:02<00:39,  7.42it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:39,  7.42it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:39,  7.40it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:39,  7.38it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:39,  7.34it/s]Evaluating:   9%|▊         | 27/313 [00:03<01:02,  4.58it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:55,  5.16it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:50,  5.68it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:46,  6.09it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:43,  6.43it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.69it/s]Evaluating:  11%|█         | 33/313 [00:04<00:40,  6.88it/s]Evaluating:  11%|█         | 34/313 [00:04<00:39,  7.03it/s]Evaluating:  11%|█         | 35/313 [00:05<00:38,  7.14it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:38,  7.21it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:38,  7.26it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:37,  7.28it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:37,  7.30it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:37,  7.31it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:37,  7.33it/s]Evaluating:  13%|█▎        | 42/313 [00:05<00:36,  7.35it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:36,  7.37it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:36,  7.36it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:36,  7.37it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:36,  7.35it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:36,  7.35it/s]Evaluating:  15%|█▌        | 48/313 [00:06<00:36,  7.33it/s]Evaluating:  16%|█▌        | 49/313 [00:06<00:36,  7.32it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:36,  7.30it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:35,  7.30it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:35,  7.28it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:35,  7.27it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:35,  7.26it/s]Evaluating:  18%|█▊        | 55/313 [00:07<00:35,  7.24it/s]Evaluating:  18%|█▊        | 56/313 [00:07<00:35,  7.19it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:35,  7.16it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:35,  7.14it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:35,  7.13it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:35,  7.11it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:35,  7.07it/s]Evaluating:  20%|█▉        | 62/313 [00:08<00:35,  7.01it/s]Evaluating:  20%|██        | 63/313 [00:08<00:35,  6.98it/s]Evaluating:  20%|██        | 64/313 [00:09<00:35,  6.96it/s]Evaluating:  21%|██        | 65/313 [00:09<00:35,  6.96it/s]Evaluating:  21%|██        | 66/313 [00:09<00:35,  6.96it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:35,  6.94it/s]Evaluating:  22%|██▏       | 68/313 [00:09<00:35,  6.95it/s]Evaluating:  22%|██▏       | 69/313 [00:09<00:34,  6.99it/s]Evaluating:  22%|██▏       | 70/313 [00:09<00:34,  7.01it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:34,  7.03it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:34,  7.04it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:34,  7.04it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:33,  7.05it/s]Evaluating:  24%|██▍       | 75/313 [00:10<00:33,  7.00it/s]Evaluating:  24%|██▍       | 76/313 [00:10<00:33,  7.02it/s]Evaluating:  25%|██▍       | 77/313 [00:10<00:33,  7.05it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:33,  7.07it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:33,  7.06it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:33,  7.05it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:32,  7.06it/s]Evaluating:  26%|██▌       | 82/313 [00:11<00:32,  7.05it/s]Evaluating:  27%|██▋       | 83/313 [00:11<00:32,  7.06it/s]Evaluating:  27%|██▋       | 84/313 [00:11<00:32,  7.06it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:32,  7.03it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:32,  6.97it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:32,  6.96it/s]Evaluating:  28%|██▊       | 88/313 [00:12<00:32,  6.94it/s]Evaluating:  28%|██▊       | 89/313 [00:12<00:32,  6.94it/s]Evaluating:  29%|██▉       | 90/313 [00:12<00:32,  6.96it/s]Evaluating:  29%|██▉       | 91/313 [00:12<00:31,  6.99it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:31,  7.01it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:31,  7.03it/s]Evaluating:  30%|███       | 94/313 [00:13<00:31,  7.03it/s]Evaluating:  30%|███       | 95/313 [00:13<00:30,  7.06it/s]Evaluating:  31%|███       | 96/313 [00:13<00:30,  7.09it/s]Evaluating:  31%|███       | 97/313 [00:13<00:30,  7.13it/s]Evaluating:  31%|███▏      | 98/313 [00:13<00:31,  6.73it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:31,  6.87it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:30,  6.98it/s]Evaluating:  32%|███▏      | 101/313 [00:14<00:30,  7.06it/s]Evaluating:  33%|███▎      | 102/313 [00:14<00:29,  7.12it/s]Evaluating:  33%|███▎      | 103/313 [00:14<00:29,  7.17it/s]Evaluating:  33%|███▎      | 104/313 [00:14<00:29,  7.19it/s]Evaluating:  34%|███▎      | 105/313 [00:14<00:28,  7.21it/s]Evaluating:  34%|███▍      | 106/313 [00:14<00:28,  7.22it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:28,  7.21it/s]Evaluating:  35%|███▍      | 108/313 [00:15<00:28,  7.20it/s]Evaluating:  35%|███▍      | 109/313 [00:15<00:28,  7.19it/s]Evaluating:  35%|███▌      | 110/313 [00:15<00:28,  7.16it/s]Evaluating:  35%|███▌      | 111/313 [00:15<00:28,  7.10it/s]Evaluating:  36%|███▌      | 112/313 [00:15<00:28,  7.04it/s]Evaluating:  36%|███▌      | 113/313 [00:15<00:28,  7.02it/s]Evaluating:  36%|███▋      | 114/313 [00:16<00:28,  6.94it/s]Evaluating:  37%|███▋      | 115/313 [00:16<00:28,  6.86it/s]Evaluating:  37%|███▋      | 116/313 [00:16<00:28,  6.85it/s]Evaluating:  37%|███▋      | 117/313 [00:16<00:29,  6.69it/s]Evaluating:  38%|███▊      | 118/313 [00:16<00:29,  6.70it/s]Evaluating:  38%|███▊      | 119/313 [00:16<00:28,  6.80it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:28,  6.88it/s]Evaluating:  39%|███▊      | 121/313 [00:17<00:27,  6.93it/s]Evaluating:  39%|███▉      | 122/313 [00:17<00:27,  6.95it/s]Evaluating:  39%|███▉      | 123/313 [00:17<00:27,  6.99it/s]Evaluating:  40%|███▉      | 124/313 [00:17<00:26,  7.02it/s]Evaluating:  40%|███▉      | 125/313 [00:17<00:26,  7.05it/s]Evaluating:  40%|████      | 126/313 [00:17<00:26,  7.06it/s]Evaluating:  41%|████      | 127/313 [00:18<00:26,  7.08it/s]Evaluating:  41%|████      | 128/313 [00:18<00:26,  7.05it/s]Evaluating:  41%|████      | 129/313 [00:18<00:26,  7.03it/s]Evaluating:  42%|████▏     | 130/313 [00:18<00:26,  7.01it/s]Evaluating:  42%|████▏     | 131/313 [00:18<00:26,  6.94it/s]Evaluating:  42%|████▏     | 132/313 [00:18<00:26,  6.89it/s]Evaluating:  42%|████▏     | 133/313 [00:18<00:26,  6.88it/s]Evaluating:  43%|████▎     | 134/313 [00:19<00:26,  6.87it/s]Evaluating:  43%|████▎     | 135/313 [00:19<00:26,  6.84it/s]Evaluating:  43%|████▎     | 136/313 [00:19<00:25,  6.84it/s]Evaluating:  44%|████▍     | 137/313 [00:19<00:25,  6.88it/s]Evaluating:  44%|████▍     | 138/313 [00:19<00:25,  6.84it/s]Evaluating:  44%|████▍     | 139/313 [00:19<00:25,  6.80it/s]Evaluating:  45%|████▍     | 140/313 [00:19<00:25,  6.79it/s]Evaluating:  45%|████▌     | 141/313 [00:20<00:25,  6.77it/s]Evaluating:  45%|████▌     | 142/313 [00:20<00:25,  6.78it/s]Evaluating:  46%|████▌     | 143/313 [00:20<00:24,  6.81it/s]Evaluating:  46%|████▌     | 144/313 [00:20<00:24,  6.85it/s]Evaluating:  46%|████▋     | 145/313 [00:20<00:24,  6.85it/s]Evaluating:  47%|████▋     | 146/313 [00:20<00:24,  6.84it/s]Evaluating:  47%|████▋     | 147/313 [00:20<00:24,  6.86it/s]Evaluating:  47%|████▋     | 148/313 [00:21<00:24,  6.87it/s]Evaluating:  48%|████▊     | 149/313 [00:21<00:23,  6.89it/s]Evaluating:  48%|████▊     | 150/313 [00:21<00:23,  6.90it/s]Evaluating:  48%|████▊     | 151/313 [00:21<00:23,  6.91it/s]Evaluating:  49%|████▊     | 152/313 [00:21<00:23,  6.87it/s]Evaluating:  49%|████▉     | 153/313 [00:21<00:23,  6.84it/s]Evaluating:  49%|████▉     | 154/313 [00:21<00:23,  6.86it/s]Evaluating:  50%|████▉     | 155/313 [00:22<00:22,  6.88it/s]Evaluating:  50%|████▉     | 156/313 [00:22<00:22,  6.90it/s]Evaluating:  50%|█████     | 157/313 [00:22<00:22,  6.88it/s]Evaluating:  50%|█████     | 158/313 [00:22<00:22,  6.86it/s]Evaluating:  51%|█████     | 159/313 [00:22<00:22,  6.82it/s]Evaluating:  51%|█████     | 160/313 [00:22<00:22,  6.76it/s]Evaluating:  51%|█████▏    | 161/313 [00:22<00:22,  6.70it/s]Evaluating:  52%|█████▏    | 162/313 [00:23<00:22,  6.68it/s]Evaluating:  52%|█████▏    | 163/313 [00:23<00:22,  6.72it/s]Evaluating:  52%|█████▏    | 164/313 [00:23<00:21,  6.77it/s]Evaluating:  53%|█████▎    | 165/313 [00:23<00:21,  6.82it/s]Evaluating:  53%|█████▎    | 166/313 [00:23<00:21,  6.78it/s]Evaluating:  53%|█████▎    | 167/313 [00:23<00:21,  6.72it/s]Evaluating:  54%|█████▎    | 168/313 [00:24<00:21,  6.69it/s]Evaluating:  54%|█████▍    | 169/313 [00:24<00:21,  6.66it/s]Evaluating:  54%|█████▍    | 170/313 [00:24<00:21,  6.62it/s]Evaluating:  55%|█████▍    | 171/313 [00:24<00:21,  6.59it/s]Evaluating:  55%|█████▍    | 172/313 [00:24<00:21,  6.58it/s]Evaluating:  55%|█████▌    | 173/313 [00:24<00:21,  6.59it/s]Evaluating:  56%|█████▌    | 174/313 [00:24<00:21,  6.57it/s]Evaluating:  56%|█████▌    | 175/313 [00:25<00:21,  6.56it/s]Evaluating:  56%|█████▌    | 176/313 [00:25<00:20,  6.56it/s]Evaluating:  57%|█████▋    | 177/313 [00:25<00:20,  6.56it/s]Evaluating:  57%|█████▋    | 178/313 [00:25<00:20,  6.55it/s]Evaluating:  57%|█████▋    | 179/313 [00:25<00:20,  6.57it/s]Evaluating:  58%|█████▊    | 180/313 [00:25<00:20,  6.62it/s]Evaluating:  58%|█████▊    | 181/313 [00:25<00:19,  6.67it/s]Evaluating:  58%|█████▊    | 182/313 [00:26<00:19,  6.70it/s]Evaluating:  58%|█████▊    | 183/313 [00:26<00:19,  6.74it/s]Evaluating:  59%|█████▉    | 184/313 [00:26<00:19,  6.77it/s]Evaluating:  59%|█████▉    | 185/313 [00:26<00:19,  6.73it/s]Evaluating:  59%|█████▉    | 186/313 [00:26<00:18,  6.69it/s]Evaluating:  60%|█████▉    | 187/313 [00:26<00:18,  6.66it/s]Evaluating:  60%|██████    | 188/313 [00:27<00:18,  6.62it/s]Evaluating:  60%|██████    | 189/313 [00:27<00:18,  6.59it/s]Evaluating:  61%|██████    | 190/313 [00:27<00:18,  6.57it/s]Evaluating:  61%|██████    | 191/313 [00:27<00:18,  6.58it/s]Evaluating:  61%|██████▏   | 192/313 [00:27<00:18,  6.60it/s]Evaluating:  62%|██████▏   | 193/313 [00:27<00:18,  6.58it/s]Evaluating:  62%|██████▏   | 194/313 [00:27<00:18,  6.57it/s]Evaluating:  62%|██████▏   | 195/313 [00:28<00:17,  6.60it/s]Evaluating:  63%|██████▎   | 196/313 [00:28<00:17,  6.65it/s]Evaluating:  63%|██████▎   | 197/313 [00:28<00:17,  6.72it/s]Evaluating:  63%|██████▎   | 198/313 [00:28<00:17,  6.76it/s]Evaluating:  64%|██████▎   | 199/313 [00:28<00:16,  6.81it/s]Evaluating:  64%|██████▍   | 200/313 [00:28<00:16,  6.76it/s]Evaluating:  64%|██████▍   | 201/313 [00:28<00:16,  6.71it/s]Evaluating:  65%|██████▍   | 202/313 [00:29<00:16,  6.67it/s]Evaluating:  65%|██████▍   | 203/313 [00:29<00:16,  6.64it/s]Evaluating:  65%|██████▌   | 204/313 [00:29<00:16,  6.59it/s]Evaluating:  65%|██████▌   | 205/313 [00:29<00:16,  6.57it/s]Evaluating:  66%|██████▌   | 206/313 [00:29<00:16,  6.59it/s]Evaluating:  66%|██████▌   | 207/313 [00:29<00:16,  6.62it/s]Evaluating:  66%|██████▋   | 208/313 [00:30<00:15,  6.62it/s]Evaluating:  67%|██████▋   | 209/313 [00:30<00:15,  6.63it/s]Evaluating:  67%|██████▋   | 210/313 [00:30<00:15,  6.68it/s]Evaluating:  67%|██████▋   | 211/313 [00:30<00:15,  6.77it/s]Evaluating:  68%|██████▊   | 212/313 [00:30<00:14,  6.84it/s]Evaluating:  68%|██████▊   | 213/313 [00:30<00:14,  6.90it/s]Evaluating:  68%|██████▊   | 214/313 [00:30<00:14,  6.93it/s]Evaluating:  69%|██████▊   | 215/313 [00:31<00:14,  6.90it/s]Evaluating:  69%|██████▉   | 216/313 [00:31<00:15,  6.45it/s]Evaluating:  69%|██████▉   | 217/313 [00:31<00:14,  6.63it/s]Evaluating:  70%|██████▉   | 218/313 [00:31<00:14,  6.75it/s]Evaluating:  70%|██████▉   | 219/313 [00:31<00:13,  6.86it/s]Evaluating:  70%|███████   | 220/313 [00:31<00:13,  6.91it/s]Evaluating:  71%|███████   | 221/313 [00:31<00:13,  6.95it/s]Evaluating:  71%|███████   | 222/313 [00:32<00:13,  6.94it/s]Evaluating:  71%|███████   | 223/313 [00:32<00:13,  6.91it/s]Evaluating:  72%|███████▏  | 224/313 [00:32<00:12,  6.92it/s]Evaluating:  72%|███████▏  | 225/313 [00:32<00:12,  6.95it/s]Evaluating:  72%|███████▏  | 226/313 [00:32<00:12,  6.97it/s]Evaluating:  73%|███████▎  | 227/313 [00:32<00:12,  7.00it/s]Evaluating:  73%|███████▎  | 228/313 [00:32<00:12,  7.02it/s]Evaluating:  73%|███████▎  | 229/313 [00:33<00:11,  7.05it/s]Evaluating:  73%|███████▎  | 230/313 [00:33<00:11,  7.05it/s]Evaluating:  74%|███████▍  | 231/313 [00:33<00:11,  7.05it/s]Evaluating:  74%|███████▍  | 232/313 [00:33<00:11,  6.95it/s]Evaluating:  74%|███████▍  | 233/313 [00:33<00:11,  6.85it/s]Evaluating:  75%|███████▍  | 234/313 [00:33<00:11,  6.84it/s]Evaluating:  75%|███████▌  | 235/313 [00:33<00:11,  6.80it/s]Evaluating:  75%|███████▌  | 236/313 [00:34<00:11,  6.75it/s]Evaluating:  76%|███████▌  | 237/313 [00:34<00:11,  6.75it/s]Evaluating:  76%|███████▌  | 238/313 [00:34<00:11,  6.74it/s]Evaluating:  76%|███████▋  | 239/313 [00:34<00:11,  6.66it/s]Evaluating:  77%|███████▋  | 240/313 [00:34<00:11,  6.60it/s]Evaluating:  77%|███████▋  | 241/313 [00:34<00:10,  6.55it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:10,  6.51it/s]Evaluating:  78%|███████▊  | 243/313 [00:35<00:10,  6.50it/s]Evaluating:  78%|███████▊  | 244/313 [00:35<00:10,  6.48it/s]Evaluating:  78%|███████▊  | 245/313 [00:35<00:10,  6.49it/s]Evaluating:  79%|███████▊  | 246/313 [00:35<00:10,  6.50it/s]Evaluating:  79%|███████▉  | 247/313 [00:35<00:10,  6.37it/s]Evaluating:  79%|███████▉  | 248/313 [00:35<00:10,  6.35it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:10,  6.39it/s]Evaluating:  80%|███████▉  | 250/313 [00:36<00:12,  5.03it/s]Evaluating:  80%|████████  | 251/313 [00:36<00:11,  5.50it/s]Evaluating:  81%|████████  | 252/313 [00:36<00:10,  5.89it/s]Evaluating:  81%|████████  | 253/313 [00:36<00:09,  6.18it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:09,  6.38it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.51it/s]Evaluating:  82%|████████▏ | 256/313 [00:37<00:08,  6.62it/s]Evaluating:  82%|████████▏ | 257/313 [00:37<00:08,  6.69it/s]Evaluating:  82%|████████▏ | 258/313 [00:37<00:08,  6.78it/s]Evaluating:  83%|████████▎ | 259/313 [00:37<00:07,  6.85it/s]Evaluating:  83%|████████▎ | 260/313 [00:37<00:07,  6.89it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:07,  6.91it/s]Evaluating:  84%|████████▎ | 262/313 [00:38<00:07,  6.92it/s]Evaluating:  84%|████████▍ | 263/313 [00:38<00:07,  6.95it/s]Evaluating:  84%|████████▍ | 264/313 [00:38<00:07,  6.89it/s]Evaluating:  85%|████████▍ | 265/313 [00:38<00:06,  6.88it/s]Evaluating:  85%|████████▍ | 266/313 [00:38<00:06,  6.90it/s]Evaluating:  85%|████████▌ | 267/313 [00:38<00:06,  6.93it/s]Evaluating:  86%|████████▌ | 268/313 [00:39<00:06,  6.96it/s]Evaluating:  86%|████████▌ | 269/313 [00:39<00:06,  6.97it/s]Evaluating:  86%|████████▋ | 270/313 [00:39<00:06,  6.98it/s]Evaluating:  87%|████████▋ | 271/313 [00:39<00:06,  6.98it/s]Evaluating:  87%|████████▋ | 272/313 [00:39<00:05,  6.96it/s]Evaluating:  87%|████████▋ | 273/313 [00:39<00:05,  6.97it/s]Evaluating:  88%|████████▊ | 274/313 [00:39<00:05,  6.95it/s]Evaluating:  88%|████████▊ | 275/313 [00:40<00:05,  6.93it/s]Evaluating:  88%|████████▊ | 276/313 [00:40<00:05,  6.92it/s]Evaluating:  88%|████████▊ | 277/313 [00:40<00:05,  6.94it/s]Evaluating:  89%|████████▉ | 278/313 [00:40<00:05,  6.94it/s]Evaluating:  89%|████████▉ | 279/313 [00:40<00:04,  6.95it/s]Evaluating:  89%|████████▉ | 280/313 [00:40<00:04,  6.96it/s]Evaluating:  90%|████████▉ | 281/313 [00:40<00:04,  6.96it/s]Evaluating:  90%|█████████ | 282/313 [00:41<00:04,  6.96it/s]Evaluating:  90%|█████████ | 283/313 [00:41<00:04,  6.98it/s]Evaluating:  91%|█████████ | 284/313 [00:41<00:04,  6.98it/s]Evaluating:  91%|█████████ | 285/313 [00:41<00:04,  6.97it/s]Evaluating:  91%|█████████▏| 286/313 [00:41<00:03,  6.93it/s]Evaluating:  92%|█████████▏| 287/313 [00:41<00:03,  6.93it/s]Evaluating:  92%|█████████▏| 288/313 [00:41<00:03,  6.94it/s]Evaluating:  92%|█████████▏| 289/313 [00:42<00:03,  6.94it/s]Evaluating:  93%|█████████▎| 290/313 [00:42<00:03,  6.93it/s]Evaluating:  93%|█████████▎| 291/313 [00:42<00:03,  6.91it/s]Evaluating:  93%|█████████▎| 292/313 [00:42<00:03,  6.87it/s]Evaluating:  94%|█████████▎| 293/313 [00:42<00:02,  6.87it/s]Evaluating:  94%|█████████▍| 294/313 [00:42<00:02,  6.90it/s]Evaluating:  94%|█████████▍| 295/313 [00:42<00:02,  6.92it/s]Evaluating:  95%|█████████▍| 296/313 [00:43<00:02,  6.93it/s]Evaluating:  95%|█████████▍| 297/313 [00:43<00:02,  6.93it/s]Evaluating:  95%|█████████▌| 298/313 [00:43<00:02,  6.91it/s]Evaluating:  96%|█████████▌| 299/313 [00:43<00:02,  6.88it/s]Evaluating:  96%|█████████▌| 300/313 [00:43<00:01,  6.87it/s]Evaluating:  96%|█████████▌| 301/313 [00:43<00:01,  6.76it/s]Evaluating:  96%|█████████▋| 302/313 [00:43<00:01,  6.65it/s]Evaluating:  97%|█████████▋| 303/313 [00:44<00:01,  6.61it/s]Evaluating:  97%|█████████▋| 304/313 [00:44<00:01,  6.58it/s]Evaluating:  97%|█████████▋| 305/313 [00:44<00:01,  6.51it/s]Evaluating:  98%|█████████▊| 306/313 [00:44<00:01,  6.47it/s]Evaluating:  98%|█████████▊| 307/313 [00:44<00:00,  6.48it/s]Evaluating:  98%|█████████▊| 308/313 [00:44<00:00,  6.46it/s]Evaluating:  99%|█████████▊| 309/313 [00:45<00:00,  6.43it/s]Evaluating:  99%|█████████▉| 310/313 [00:45<00:00,  6.42it/s]Evaluating:  99%|█████████▉| 311/313 [00:45<00:00,  6.44it/s]Evaluating: 100%|█████████▉| 312/313 [00:45<00:00,  6.49it/s]Evaluating: 100%|██████████| 313/313 [00:45<00:00,  6.87it/s]
10/08/2021 22:08:35 - INFO - __main__ -   ***** Evaluation result  in no *****
10/08/2021 22:08:35 - INFO - __main__ -     f1 = 0.7780774481403788
10/08/2021 22:08:35 - INFO - __main__ -     loss = 0.2622234277403393
10/08/2021 22:08:35 - INFO - __main__ -     precision = 0.7474889642377327
10/08/2021 22:08:35 - INFO - __main__ -     recall = 0.8112762116372726
10/08/2021 22:08:35 - INFO - __main__ -   Language adapter for da not found, using en instead
10/08/2021 22:08:35 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:08:35 - INFO - __main__ -   all languages = da
10/08/2021 22:08:35 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/da/test.bert-base-multilingual-cased in language da
10/08/2021 22:08:35 - INFO - utils_tag -   lang_id=0, lang=da, lang2id=None
10/08/2021 22:08:35 - INFO - utils_tag -   Writing example 0 of 10001
10/08/2021 22:08:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:08:35 - INFO - utils_tag -   guid: da-1
10/08/2021 22:08:35 - INFO - utils_tag -   tokens: [CLS] Hvis Lys ##et Ta ##r Os ##s [SEP]
10/08/2021 22:08:35 - INFO - utils_tag -   input_ids: 101 71947 43633 10308 14248 10129 12087 10107 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   label_ids: -100 1 4 -100 4 -100 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:08:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:08:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:08:35 - INFO - utils_tag -   guid: da-2
10/08/2021 22:08:35 - INFO - utils_tag -   tokens: [CLS] Det anses for at være hjem ##sted for Br ##ah ##ma og andre gud ##dom ##me . [SEP]
10/08/2021 22:08:35 - INFO - utils_tag -   input_ids: 101 10666 61648 10142 10160 15195 59541 36018 10142 38508 12257 10369 10156 12435 51354 15561 10627 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   label_ids: -100 6 6 6 6 6 6 -100 6 2 -100 -100 6 6 6 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:08:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:08:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:08:35 - INFO - utils_tag -   guid: da-3
10/08/2021 22:08:35 - INFO - utils_tag -   tokens: [CLS] Viola ( fodboldspiller ) [SEP]
10/08/2021 22:08:35 - INFO - utils_tag -   input_ids: 101 36497 113 99964 114 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   label_ids: -100 2 5 5 5 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:08:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:08:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:08:35 - INFO - utils_tag -   guid: da-4
10/08/2021 22:08:35 - INFO - utils_tag -   tokens: [CLS] er Sigmund Freud også at finde ibland ##t dem . [SEP]
10/08/2021 22:08:35 - INFO - utils_tag -   input_ids: 101 10163 54957 39632 11443 10160 98572 66612 10123 10268 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   label_ids: -100 6 2 5 6 6 6 6 -100 6 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:08:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:08:35 - INFO - utils_tag -   *** Example ***
10/08/2021 22:08:35 - INFO - utils_tag -   guid: da-5
10/08/2021 22:08:35 - INFO - utils_tag -   tokens: [CLS] Hurt ##ig ##l ##øb på sk ##øj ##ter under vinte ##r - OL 2010 [UNK] Herr ##ernes 500 meter [SEP]
10/08/2021 22:08:35 - INFO - utils_tag -   input_ids: 101 83912 11142 10161 54828 10217 66998 79306 10877 10571 60243 10129 118 19768 10175 100 38000 97597 10757 10586 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:08:35 - INFO - utils_tag -   label_ids: -100 1 -100 -100 -100 4 4 -100 -100 4 4 -100 -100 -100 4 4 4 -100 4 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:08:35 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:08:39 - INFO - utils_tag -   Writing example 10000 of 10001
10/08/2021 22:08:39 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128, len(features)=10001
10/08/2021 22:08:41 - INFO - __main__ -   ***** Running evaluation  in da *****
10/08/2021 22:08:41 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:08:41 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.85it/s]Evaluating:   1%|          | 2/313 [00:00<00:43,  7.09it/s]Evaluating:   1%|          | 3/313 [00:00<00:43,  7.19it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:42,  7.23it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:42,  7.24it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:42,  7.23it/s]Evaluating:   2%|▏         | 7/313 [00:00<00:42,  7.24it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:42,  7.24it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:42,  7.22it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:42,  7.20it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:41,  7.20it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:41,  7.19it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:41,  7.17it/s]Evaluating:   4%|▍         | 14/313 [00:01<00:41,  7.14it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:41,  7.10it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:42,  7.06it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:42,  7.01it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:42,  6.96it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:42,  6.89it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:42,  6.84it/s]Evaluating:   7%|▋         | 21/313 [00:02<00:42,  6.80it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:42,  6.79it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.81it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.84it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:41,  6.90it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:41,  6.92it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:41,  6.91it/s]Evaluating:   9%|▉         | 28/313 [00:03<00:41,  6.91it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:40,  6.93it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:40,  6.92it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:40,  6.92it/s]Evaluating:  10%|█         | 32/313 [00:04<00:40,  6.92it/s]Evaluating:  11%|█         | 33/313 [00:04<00:40,  6.91it/s]Evaluating:  11%|█         | 34/313 [00:04<00:40,  6.89it/s]Evaluating:  11%|█         | 35/313 [00:05<00:40,  6.86it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:40,  6.82it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.78it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.76it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.73it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.73it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:40,  6.72it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.71it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.71it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.70it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.72it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.76it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.77it/s]Evaluating:  15%|█▌        | 48/313 [00:06<00:39,  6.79it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.75it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:38,  6.79it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.77it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.77it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.78it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:38,  6.80it/s]Evaluating:  18%|█▊        | 55/313 [00:07<00:37,  6.80it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:37,  6.80it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:37,  6.76it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:37,  6.73it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.70it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:37,  6.69it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.68it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.69it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.71it/s]Evaluating:  21%|██        | 65/313 [00:09<00:36,  6.73it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.75it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.75it/s]Evaluating:  22%|██▏       | 68/313 [00:09<00:36,  6.75it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.72it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.70it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:35,  6.69it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:35,  6.67it/s]Evaluating:  24%|██▍       | 75/313 [00:10<00:35,  6.64it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:35,  6.65it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:35,  6.65it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:34,  6.66it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.65it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.62it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.62it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.67it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:33,  6.77it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.85it/s]Evaluating:  28%|██▊       | 88/313 [00:12<00:32,  6.88it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:32,  6.90it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:32,  6.88it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:32,  6.84it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:32,  6.81it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:32,  6.79it/s]Evaluating:  30%|███       | 94/313 [00:13<00:32,  6.78it/s]Evaluating:  30%|███       | 95/313 [00:13<00:32,  6.76it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.75it/s]Evaluating:  31%|███       | 97/313 [00:14<00:31,  6.76it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:31,  6.76it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:31,  6.74it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:31,  6.77it/s]Evaluating:  32%|███▏      | 101/313 [00:14<00:31,  6.81it/s]Evaluating:  33%|███▎      | 102/313 [00:14<00:31,  6.80it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:30,  6.78it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:30,  6.78it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:30,  6.81it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:30,  6.83it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:30,  6.85it/s]Evaluating:  35%|███▍      | 108/313 [00:15<00:29,  6.85it/s]Evaluating:  35%|███▍      | 109/313 [00:15<00:29,  6.84it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:29,  6.84it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:29,  6.86it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:29,  6.85it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:29,  6.85it/s]Evaluating:  36%|███▋      | 114/313 [00:16<00:29,  6.86it/s]Evaluating:  37%|███▋      | 115/313 [00:16<00:28,  6.87it/s]Evaluating:  37%|███▋      | 116/313 [00:16<00:28,  6.86it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:28,  6.85it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:28,  6.85it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:28,  6.84it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:28,  6.83it/s]Evaluating:  39%|███▊      | 121/313 [00:17<00:28,  6.83it/s]Evaluating:  39%|███▉      | 122/313 [00:17<00:28,  6.82it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.76it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.64it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.66it/s]Evaluating:  40%|████      | 126/313 [00:18<00:28,  6.67it/s]Evaluating:  41%|████      | 127/313 [00:18<00:27,  6.70it/s]Evaluating:  41%|████      | 128/313 [00:18<00:27,  6.75it/s]Evaluating:  41%|████      | 129/313 [00:18<00:27,  6.79it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:26,  6.84it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:26,  6.91it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:25,  6.97it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:25,  7.03it/s]Evaluating:  43%|████▎     | 134/313 [00:19<00:25,  7.08it/s]Evaluating:  43%|████▎     | 135/313 [00:19<00:25,  7.12it/s]Evaluating:  43%|████▎     | 136/313 [00:19<00:24,  7.14it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:24,  7.16it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:24,  7.18it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:24,  7.18it/s]Evaluating:  45%|████▍     | 140/313 [00:20<00:24,  7.17it/s]Evaluating:  45%|████▌     | 141/313 [00:20<00:23,  7.18it/s]Evaluating:  45%|████▌     | 142/313 [00:20<00:23,  7.16it/s]Evaluating:  46%|████▌     | 143/313 [00:20<00:23,  7.13it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:23,  7.10it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:23,  7.08it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:23,  7.08it/s]Evaluating:  47%|████▋     | 147/313 [00:21<00:23,  7.08it/s]Evaluating:  47%|████▋     | 148/313 [00:21<00:23,  7.09it/s]Evaluating:  48%|████▊     | 149/313 [00:21<00:23,  7.09it/s]Evaluating:  48%|████▊     | 150/313 [00:21<00:22,  7.10it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:22,  7.09it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:22,  7.10it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:22,  7.13it/s]Evaluating:  49%|████▉     | 154/313 [00:22<00:22,  7.14it/s]Evaluating:  50%|████▉     | 155/313 [00:22<00:22,  7.15it/s]Evaluating:  50%|████▉     | 156/313 [00:22<00:21,  7.16it/s]Evaluating:  50%|█████     | 157/313 [00:22<00:21,  7.17it/s]Evaluating:  50%|█████     | 158/313 [00:22<00:21,  7.17it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:21,  7.18it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:21,  7.17it/s]Evaluating:  51%|█████▏    | 161/313 [00:23<00:21,  7.18it/s]Evaluating:  52%|█████▏    | 162/313 [00:23<00:21,  7.15it/s]Evaluating:  52%|█████▏    | 163/313 [00:23<00:21,  7.13it/s]Evaluating:  52%|█████▏    | 164/313 [00:23<00:20,  7.13it/s]Evaluating:  53%|█████▎    | 165/313 [00:23<00:20,  7.10it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:20,  7.08it/s]Evaluating:  53%|█████▎    | 167/313 [00:24<00:20,  7.06it/s]Evaluating:  54%|█████▎    | 168/313 [00:24<00:20,  7.04it/s]Evaluating:  54%|█████▍    | 169/313 [00:24<00:20,  7.01it/s]Evaluating:  54%|█████▍    | 170/313 [00:24<00:20,  7.00it/s]Evaluating:  55%|█████▍    | 171/313 [00:24<00:20,  7.02it/s]Evaluating:  55%|█████▍    | 172/313 [00:24<00:20,  7.04it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:19,  7.05it/s]Evaluating:  56%|█████▌    | 174/313 [00:25<00:19,  7.05it/s]Evaluating:  56%|█████▌    | 175/313 [00:25<00:19,  7.03it/s]Evaluating:  56%|█████▌    | 176/313 [00:25<00:19,  6.99it/s]Evaluating:  57%|█████▋    | 177/313 [00:25<00:19,  6.95it/s]Evaluating:  57%|█████▋    | 178/313 [00:25<00:19,  6.98it/s]Evaluating:  57%|█████▋    | 179/313 [00:25<00:19,  7.02it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:18,  7.05it/s]Evaluating:  58%|█████▊    | 181/313 [00:26<00:18,  7.08it/s]Evaluating:  58%|█████▊    | 182/313 [00:26<00:18,  7.09it/s]Evaluating:  58%|█████▊    | 183/313 [00:26<00:18,  7.09it/s]Evaluating:  59%|█████▉    | 184/313 [00:26<00:18,  7.10it/s]Evaluating:  59%|█████▉    | 185/313 [00:26<00:18,  7.11it/s]Evaluating:  59%|█████▉    | 186/313 [00:26<00:17,  7.10it/s]Evaluating:  60%|█████▉    | 187/313 [00:27<00:17,  7.10it/s]Evaluating:  60%|██████    | 188/313 [00:27<00:17,  7.10it/s]Evaluating:  60%|██████    | 189/313 [00:27<00:17,  7.11it/s]Evaluating:  61%|██████    | 190/313 [00:27<00:17,  7.09it/s]Evaluating:  61%|██████    | 191/313 [00:27<00:17,  7.03it/s]Evaluating:  61%|██████▏   | 192/313 [00:27<00:17,  6.97it/s]Evaluating:  62%|██████▏   | 193/313 [00:27<00:17,  6.94it/s]Evaluating:  62%|██████▏   | 194/313 [00:28<00:17,  6.90it/s]Evaluating:  62%|██████▏   | 195/313 [00:28<00:17,  6.89it/s]Evaluating:  63%|██████▎   | 196/313 [00:28<00:17,  6.88it/s]Evaluating:  63%|██████▎   | 197/313 [00:28<00:16,  6.88it/s]Evaluating:  63%|██████▎   | 198/313 [00:28<00:16,  6.85it/s]Evaluating:  64%|██████▎   | 199/313 [00:28<00:16,  6.81it/s]Evaluating:  64%|██████▍   | 200/313 [00:28<00:16,  6.79it/s]Evaluating:  64%|██████▍   | 201/313 [00:29<00:16,  6.77it/s]Evaluating:  65%|██████▍   | 202/313 [00:29<00:16,  6.75it/s]Evaluating:  65%|██████▍   | 203/313 [00:29<00:16,  6.76it/s]Evaluating:  65%|██████▌   | 204/313 [00:29<00:16,  6.76it/s]Evaluating:  65%|██████▌   | 205/313 [00:29<00:15,  6.78it/s]Evaluating:  66%|██████▌   | 206/313 [00:29<00:15,  6.78it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:15,  6.80it/s]Evaluating:  66%|██████▋   | 208/313 [00:30<00:15,  6.82it/s]Evaluating:  67%|██████▋   | 209/313 [00:30<00:15,  6.84it/s]Evaluating:  67%|██████▋   | 210/313 [00:30<00:15,  6.85it/s]Evaluating:  67%|██████▋   | 211/313 [00:30<00:14,  6.85it/s]Evaluating:  68%|██████▊   | 212/313 [00:30<00:14,  6.78it/s]Evaluating:  68%|██████▊   | 213/313 [00:30<00:14,  6.71it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:14,  6.67it/s]Evaluating:  69%|██████▊   | 215/313 [00:31<00:14,  6.62it/s]Evaluating:  69%|██████▉   | 216/313 [00:31<00:14,  6.58it/s]Evaluating:  69%|██████▉   | 217/313 [00:31<00:14,  6.59it/s]Evaluating:  70%|██████▉   | 218/313 [00:31<00:14,  6.64it/s]Evaluating:  70%|██████▉   | 219/313 [00:31<00:14,  6.69it/s]Evaluating:  70%|███████   | 220/313 [00:31<00:13,  6.71it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:13,  6.73it/s]Evaluating:  71%|███████   | 222/313 [00:32<00:13,  6.77it/s]Evaluating:  71%|███████   | 223/313 [00:32<00:13,  6.79it/s]Evaluating:  72%|███████▏  | 224/313 [00:32<00:13,  6.80it/s]Evaluating:  72%|███████▏  | 225/313 [00:32<00:12,  6.82it/s]Evaluating:  72%|███████▏  | 226/313 [00:32<00:12,  6.84it/s]Evaluating:  73%|███████▎  | 227/313 [00:32<00:12,  6.87it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  6.89it/s]Evaluating:  73%|███████▎  | 229/313 [00:33<00:12,  6.92it/s]Evaluating:  73%|███████▎  | 230/313 [00:33<00:11,  6.93it/s]Evaluating:  74%|███████▍  | 231/313 [00:33<00:11,  6.96it/s]Evaluating:  74%|███████▍  | 232/313 [00:33<00:11,  6.98it/s]Evaluating:  74%|███████▍  | 233/313 [00:33<00:11,  7.01it/s]Evaluating:  75%|███████▍  | 234/313 [00:33<00:11,  7.02it/s]Evaluating:  75%|███████▌  | 235/313 [00:34<00:11,  7.03it/s]Evaluating:  75%|███████▌  | 236/313 [00:34<00:10,  7.04it/s]Evaluating:  76%|███████▌  | 237/313 [00:34<00:10,  7.02it/s]Evaluating:  76%|███████▌  | 238/313 [00:34<00:10,  7.01it/s]Evaluating:  76%|███████▋  | 239/313 [00:34<00:10,  7.02it/s]Evaluating:  77%|███████▋  | 240/313 [00:34<00:10,  6.99it/s]Evaluating:  77%|███████▋  | 241/313 [00:34<00:10,  7.00it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:10,  7.01it/s]Evaluating:  78%|███████▊  | 243/313 [00:35<00:09,  7.02it/s]Evaluating:  78%|███████▊  | 244/313 [00:35<00:09,  7.03it/s]Evaluating:  78%|███████▊  | 245/313 [00:35<00:09,  7.02it/s]Evaluating:  79%|███████▊  | 246/313 [00:35<00:09,  7.02it/s]Evaluating:  79%|███████▉  | 247/313 [00:35<00:09,  6.87it/s]Evaluating:  79%|███████▉  | 248/313 [00:35<00:09,  6.76it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:09,  6.72it/s]Evaluating:  80%|███████▉  | 250/313 [00:36<00:09,  6.66it/s]Evaluating:  80%|████████  | 251/313 [00:36<00:09,  6.59it/s]Evaluating:  81%|████████  | 252/313 [00:36<00:09,  6.53it/s]Evaluating:  81%|████████  | 253/313 [00:36<00:09,  6.50it/s]Evaluating:  81%|████████  | 254/313 [00:36<00:09,  6.49it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.47it/s]Evaluating:  82%|████████▏ | 256/313 [00:37<00:08,  6.49it/s]Evaluating:  82%|████████▏ | 257/313 [00:37<00:08,  6.52it/s]Evaluating:  82%|████████▏ | 258/313 [00:37<00:08,  6.55it/s]Evaluating:  83%|████████▎ | 259/313 [00:37<00:08,  6.56it/s]Evaluating:  83%|████████▎ | 260/313 [00:37<00:08,  6.56it/s]Evaluating:  83%|████████▎ | 261/313 [00:37<00:07,  6.61it/s]Evaluating:  84%|████████▎ | 262/313 [00:38<00:07,  6.67it/s]Evaluating:  84%|████████▍ | 263/313 [00:38<00:07,  6.76it/s]Evaluating:  84%|████████▍ | 264/313 [00:38<00:07,  6.82it/s]Evaluating:  85%|████████▍ | 265/313 [00:38<00:06,  6.87it/s]Evaluating:  85%|████████▍ | 266/313 [00:38<00:06,  6.88it/s]Evaluating:  85%|████████▌ | 267/313 [00:38<00:06,  6.89it/s]Evaluating:  86%|████████▌ | 268/313 [00:38<00:06,  6.91it/s]Evaluating:  86%|████████▌ | 269/313 [00:39<00:06,  6.89it/s]Evaluating:  86%|████████▋ | 270/313 [00:39<00:06,  6.86it/s]Evaluating:  87%|████████▋ | 271/313 [00:39<00:06,  6.88it/s]Evaluating:  87%|████████▋ | 272/313 [00:39<00:05,  6.90it/s]Evaluating:  87%|████████▋ | 273/313 [00:39<00:05,  6.91it/s]Evaluating:  88%|████████▊ | 274/313 [00:39<00:05,  6.92it/s]Evaluating:  88%|████████▊ | 275/313 [00:40<00:05,  6.87it/s]Evaluating:  88%|████████▊ | 276/313 [00:40<00:05,  6.87it/s]Evaluating:  88%|████████▊ | 277/313 [00:40<00:05,  6.92it/s]Evaluating:  89%|████████▉ | 278/313 [00:40<00:05,  6.93it/s]Evaluating:  89%|████████▉ | 279/313 [00:40<00:05,  5.69it/s]Evaluating:  89%|████████▉ | 280/313 [00:40<00:05,  5.89it/s]Evaluating:  90%|████████▉ | 281/313 [00:40<00:05,  6.03it/s]Evaluating:  90%|█████████ | 282/313 [00:41<00:05,  6.16it/s]Evaluating:  90%|█████████ | 283/313 [00:41<00:04,  6.24it/s]Evaluating:  91%|█████████ | 284/313 [00:41<00:04,  6.32it/s]Evaluating:  91%|█████████ | 285/313 [00:41<00:04,  6.40it/s]Evaluating:  91%|█████████▏| 286/313 [00:41<00:04,  6.48it/s]Evaluating:  92%|█████████▏| 287/313 [00:41<00:03,  6.51it/s]Evaluating:  92%|█████████▏| 288/313 [00:42<00:03,  6.55it/s]Evaluating:  92%|█████████▏| 289/313 [00:42<00:03,  6.54it/s]Evaluating:  93%|█████████▎| 290/313 [00:42<00:03,  6.55it/s]Evaluating:  93%|█████████▎| 291/313 [00:42<00:03,  6.61it/s]Evaluating:  93%|█████████▎| 292/313 [00:42<00:03,  6.65it/s]Evaluating:  94%|█████████▎| 293/313 [00:42<00:03,  6.60it/s]Evaluating:  94%|█████████▍| 294/313 [00:42<00:02,  6.53it/s]Evaluating:  94%|█████████▍| 295/313 [00:43<00:02,  6.51it/s]Evaluating:  95%|█████████▍| 296/313 [00:43<00:02,  6.49it/s]Evaluating:  95%|█████████▍| 297/313 [00:43<00:02,  6.45it/s]Evaluating:  95%|█████████▌| 298/313 [00:43<00:02,  6.43it/s]Evaluating:  96%|█████████▌| 299/313 [00:43<00:02,  6.42it/s]Evaluating:  96%|█████████▌| 300/313 [00:43<00:02,  6.40it/s]Evaluating:  96%|█████████▌| 301/313 [00:44<00:01,  6.39it/s]Evaluating:  96%|█████████▋| 302/313 [00:44<00:01,  6.07it/s]Evaluating:  97%|█████████▋| 303/313 [00:44<00:01,  6.19it/s]Evaluating:  97%|█████████▋| 304/313 [00:44<00:01,  6.39it/s]Evaluating:  97%|█████████▋| 305/313 [00:44<00:01,  6.52it/s]Evaluating:  98%|█████████▊| 306/313 [00:44<00:01,  6.64it/s]Evaluating:  98%|█████████▊| 307/313 [00:44<00:00,  6.73it/s]Evaluating:  98%|█████████▊| 308/313 [00:45<00:00,  6.77it/s]Evaluating:  99%|█████████▊| 309/313 [00:45<00:00,  6.80it/s]Evaluating:  99%|█████████▉| 310/313 [00:45<00:00,  6.82it/s]Evaluating:  99%|█████████▉| 311/313 [00:45<00:00,  6.83it/s]Evaluating: 100%|█████████▉| 312/313 [00:45<00:00,  6.83it/s]Evaluating: 100%|██████████| 313/313 [00:45<00:00,  6.83it/s]
10/08/2021 22:09:28 - INFO - __main__ -   ***** Evaluation result  in da *****
10/08/2021 22:09:28 - INFO - __main__ -     f1 = 0.8211494252873563
10/08/2021 22:09:28 - INFO - __main__ -     loss = 0.19629325057132938
10/08/2021 22:09:28 - INFO - __main__ -     precision = 0.7963915757700487
10/08/2021 22:09:28 - INFO - __main__ -     recall = 0.8474959848153015
10/08/2021 22:09:29 - INFO - __main__ -   Language adapter for ru not found, using en instead
10/08/2021 22:09:29 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:09:29 - INFO - __main__ -   all languages = ru
10/08/2021 22:09:29 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/ru/test.bert-base-multilingual-cased in language ru
10/08/2021 22:09:29 - INFO - utils_tag -   lang_id=0, lang=ru, lang2id=None
10/08/2021 22:09:29 - INFO - utils_tag -   Writing example 0 of 10002
10/08/2021 22:09:29 - INFO - utils_tag -   *** Example ***
10/08/2021 22:09:29 - INFO - utils_tag -   guid: ru-1
10/08/2021 22:09:29 - INFO - utils_tag -   tokens: [CLS] ' ' ' К ##рус А ##су ##ль ' ' ' [SEP]
10/08/2021 22:09:29 - INFO - utils_tag -   input_ids: 101 112 112 112 519 39401 509 16417 12118 112 112 112 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   label_ids: -100 6 6 -100 1 -100 4 -100 -100 6 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:09:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:09:29 - INFO - utils_tag -   *** Example ***
10/08/2021 22:09:29 - INFO - utils_tag -   guid: ru-2
10/08/2021 22:09:29 - INFO - utils_tag -   tokens: [CLS] = = Награда Республики Г ##вине ##я = = [SEP]
10/08/2021 22:09:29 - INFO - utils_tag -   input_ids: 101 134 134 82630 14940 512 69913 10385 134 134 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   label_ids: -100 6 -100 6 0 3 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:09:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:09:29 - INFO - utils_tag -   *** Example ***
10/08/2021 22:09:29 - INFO - utils_tag -   guid: ru-3
10/08/2021 22:09:29 - INFO - utils_tag -   tokens: [CLS] К тому же сериал ITV Ми ##стер Се ##л ##ф ##рид ##ж был более поп ##уля ##рен . [SEP]
10/08/2021 22:09:29 - INFO - utils_tag -   input_ids: 101 519 15278 11815 57244 54821 84078 24519 52203 10517 13582 63958 12025 10702 13106 48138 85118 27332 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   label_ids: -100 6 6 6 6 1 1 -100 4 -100 -100 -100 -100 6 6 6 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:09:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:09:29 - INFO - utils_tag -   *** Example ***
10/08/2021 22:09:29 - INFO - utils_tag -   guid: ru-4
10/08/2021 22:09:29 - INFO - utils_tag -   tokens: [CLS] Эта песня стала после ##дним сингл ##ом с альбома Pink Friday . [SEP]
10/08/2021 22:09:29 - INFO - utils_tag -   input_ids: 101 46724 44893 15309 11921 51176 42825 10364 558 29928 23413 30767 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   label_ids: -100 6 6 6 6 -100 6 -100 6 6 1 4 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:09:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:09:29 - INFO - utils_tag -   *** Example ***
10/08/2021 22:09:29 - INFO - utils_tag -   guid: ru-5
10/08/2021 22:09:29 - INFO - utils_tag -   tokens: [CLS] Па ##ро ( река ) [SEP]
10/08/2021 22:09:29 - INFO - utils_tag -   input_ids: 101 47041 14315 113 18947 114 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:09:29 - INFO - utils_tag -   label_ids: -100 0 -100 3 3 3 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:09:29 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:09:33 - INFO - utils_tag -   Writing example 10000 of 10002
10/08/2021 22:09:33 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128, len(features)=10002
10/08/2021 22:09:35 - INFO - __main__ -   ***** Running evaluation  in ru *****
10/08/2021 22:09:35 - INFO - __main__ -     Num examples = 10002
10/08/2021 22:09:35 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.75it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.89it/s]Evaluating:   1%|          | 3/313 [00:00<00:43,  7.06it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:43,  7.16it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:42,  7.22it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:42,  7.26it/s]Evaluating:   2%|▏         | 7/313 [00:00<00:42,  7.26it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:42,  7.26it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:41,  7.25it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:41,  7.23it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:41,  7.21it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:41,  7.18it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:41,  7.14it/s]Evaluating:   4%|▍         | 14/313 [00:01<00:42,  7.11it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:42,  7.09it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:42,  7.06it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:42,  7.04it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:42,  7.02it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:41,  7.01it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:42,  6.97it/s]Evaluating:   7%|▋         | 21/313 [00:02<00:42,  6.92it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:42,  6.87it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.84it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.82it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.80it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.78it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.76it/s]Evaluating:   9%|▉         | 28/313 [00:03<00:42,  6.74it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.73it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.73it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.72it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.72it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.71it/s]Evaluating:  11%|█         | 34/313 [00:04<00:41,  6.71it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.71it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.71it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.71it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.71it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.71it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.71it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:46,  5.90it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:43,  6.24it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:41,  6.52it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.87it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:38,  6.95it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:37,  7.00it/s]Evaluating:  15%|█▌        | 48/313 [00:06<00:37,  7.04it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:37,  7.05it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:37,  7.05it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:37,  7.05it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:36,  7.06it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:36,  7.07it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:36,  7.07it/s]Evaluating:  18%|█▊        | 55/313 [00:07<00:36,  7.05it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:36,  7.03it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:36,  6.98it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:36,  6.94it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.85it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.79it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:37,  6.76it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.73it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.71it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.70it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.69it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.70it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.71it/s]Evaluating:  22%|██▏       | 68/313 [00:09<00:36,  6.71it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.70it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.68it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:35,  6.68it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:35,  6.68it/s]Evaluating:  24%|██▍       | 75/313 [00:10<00:35,  6.64it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:35,  6.65it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:35,  6.64it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:34,  6.64it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.65it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.65it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.65it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:34,  6.65it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.66it/s]Evaluating:  28%|██▊       | 88/313 [00:12<00:33,  6.67it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.65it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.65it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:33,  6.64it/s]Evaluating:  30%|███       | 94/313 [00:13<00:32,  6.66it/s]Evaluating:  30%|███       | 95/313 [00:13<00:32,  6.68it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.68it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.67it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.65it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.63it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:32,  6.62it/s]Evaluating:  32%|███▏      | 101/313 [00:14<00:31,  6.63it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.69it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.72it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:30,  6.72it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:30,  6.73it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:30,  6.74it/s]Evaluating:  35%|███▍      | 108/313 [00:15<00:30,  6.75it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.79it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:29,  6.82it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:29,  6.81it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:29,  6.79it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:29,  6.80it/s]Evaluating:  36%|███▋      | 114/313 [00:16<00:29,  6.80it/s]Evaluating:  37%|███▋      | 115/313 [00:16<00:29,  6.82it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:28,  6.84it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:28,  6.86it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:28,  6.84it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:28,  6.82it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:28,  6.78it/s]Evaluating:  39%|███▊      | 121/313 [00:17<00:28,  6.76it/s]Evaluating:  39%|███▉      | 122/313 [00:17<00:28,  6.75it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.75it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.72it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.68it/s]Evaluating:  40%|████      | 126/313 [00:18<00:28,  6.65it/s]Evaluating:  41%|████      | 127/313 [00:18<00:28,  6.64it/s]Evaluating:  41%|████      | 128/313 [00:18<00:27,  6.62it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.63it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.62it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:27,  6.61it/s]Evaluating:  43%|████▎     | 134/313 [00:19<00:27,  6.59it/s]Evaluating:  43%|████▎     | 135/313 [00:19<00:27,  6.59it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.61it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.59it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.58it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.57it/s]Evaluating:  45%|████▍     | 140/313 [00:20<00:26,  6.57it/s]Evaluating:  45%|████▌     | 141/313 [00:20<00:26,  6.59it/s]Evaluating:  45%|████▌     | 142/313 [00:20<00:25,  6.63it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:35,  4.80it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:31,  5.33it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:29,  5.77it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:27,  6.14it/s]Evaluating:  47%|████▋     | 147/313 [00:21<00:25,  6.42it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.63it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.78it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:23,  6.90it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:23,  6.99it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:22,  7.04it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:22,  7.09it/s]Evaluating:  49%|████▉     | 154/313 [00:22<00:22,  7.11it/s]Evaluating:  50%|████▉     | 155/313 [00:22<00:22,  7.13it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:21,  7.14it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:21,  7.16it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:21,  7.07it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:21,  7.10it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:21,  7.06it/s]Evaluating:  51%|█████▏    | 161/313 [00:23<00:21,  7.01it/s]Evaluating:  52%|█████▏    | 162/313 [00:23<00:21,  7.00it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:21,  6.93it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:21,  6.89it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:21,  6.90it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:21,  6.93it/s]Evaluating:  53%|█████▎    | 167/313 [00:24<00:21,  6.94it/s]Evaluating:  54%|█████▎    | 168/313 [00:24<00:20,  6.94it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:20,  6.95it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:20,  6.92it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:20,  6.93it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.97it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:20,  7.00it/s]Evaluating:  56%|█████▌    | 174/313 [00:25<00:19,  7.03it/s]Evaluating:  56%|█████▌    | 175/313 [00:25<00:19,  7.06it/s]Evaluating:  56%|█████▌    | 176/313 [00:25<00:19,  7.09it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:33,  4.12it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:29,  4.63it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:26,  5.08it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:24,  5.44it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:23,  5.72it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:21,  5.96it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:21,  6.14it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:20,  6.32it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:19,  6.46it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:19,  6.57it/s]Evaluating:  60%|█████▉    | 187/313 [00:27<00:18,  6.67it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.77it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.84it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:17,  6.91it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:17,  6.98it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  7.01it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:17,  6.98it/s]Evaluating:  62%|██████▏   | 194/313 [00:28<00:17,  6.92it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.91it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:16,  6.89it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:16,  6.87it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:16,  6.83it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:16,  6.77it/s]Evaluating:  64%|██████▍   | 200/313 [00:29<00:16,  6.68it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:16,  6.64it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.62it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.59it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:16,  6.56it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:16,  6.59it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:16,  6.64it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:15,  6.72it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.76it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.83it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:14,  6.87it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:14,  6.82it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:14,  6.78it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:14,  6.79it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:15,  6.59it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.68it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.80it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:13,  6.86it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:13,  6.92it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:13,  6.96it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:13,  6.99it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:13,  7.01it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:12,  7.02it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:12,  7.03it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:12,  7.05it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:12,  7.04it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:12,  7.04it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:12,  7.04it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  7.01it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:11,  7.03it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:11,  7.04it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:11,  7.04it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:11,  7.03it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:11,  7.02it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:11,  7.01it/s]Evaluating:  75%|███████▌  | 235/313 [00:34<00:11,  7.00it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:10,  7.02it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:10,  7.02it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:10,  6.99it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:10,  6.98it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:10,  6.98it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:10,  6.96it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:10,  6.99it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:09,  7.01it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:09,  7.01it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:09,  6.80it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:09,  6.88it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:09,  6.82it/s]Evaluating:  79%|███████▉  | 248/313 [00:36<00:09,  6.75it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:09,  6.73it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.81it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.86it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:08,  6.91it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:08,  6.93it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:08,  6.96it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.94it/s]Evaluating:  82%|████████▏ | 256/313 [00:37<00:08,  6.92it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.90it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:07,  6.92it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:07,  6.92it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:07,  6.95it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:07,  6.94it/s]Evaluating:  84%|████████▎ | 262/313 [00:38<00:07,  6.95it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.94it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.93it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:06,  6.87it/s]Evaluating:  85%|████████▍ | 266/313 [00:39<00:06,  6.81it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:06,  6.79it/s]Evaluating:  86%|████████▌ | 268/313 [00:39<00:06,  6.77it/s]Evaluating:  86%|████████▌ | 269/313 [00:39<00:06,  6.77it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.78it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.82it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:05,  6.86it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:05,  6.90it/s]Evaluating:  88%|████████▊ | 274/313 [00:40<00:05,  6.93it/s]Evaluating:  88%|████████▊ | 275/313 [00:40<00:05,  6.94it/s]Evaluating:  88%|████████▊ | 276/313 [00:40<00:05,  6.95it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.97it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.95it/s]Evaluating:  89%|████████▉ | 279/313 [00:41<00:04,  6.91it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:04,  6.88it/s]Evaluating:  90%|████████▉ | 281/313 [00:41<00:04,  6.87it/s]Evaluating:  90%|█████████ | 282/313 [00:41<00:04,  6.85it/s]Evaluating:  90%|█████████ | 283/313 [00:41<00:04,  6.88it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.91it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.93it/s]Evaluating:  91%|█████████▏| 286/313 [00:42<00:03,  6.91it/s]Evaluating:  92%|█████████▏| 287/313 [00:42<00:03,  6.92it/s]Evaluating:  92%|█████████▏| 288/313 [00:42<00:03,  6.88it/s]Evaluating:  92%|█████████▏| 289/313 [00:42<00:03,  6.86it/s]Evaluating:  93%|█████████▎| 290/313 [00:42<00:03,  6.84it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.85it/s]Evaluating:  93%|█████████▎| 292/313 [00:43<00:03,  6.88it/s]Evaluating:  94%|█████████▎| 293/313 [00:43<00:02,  6.88it/s]Evaluating:  94%|█████████▍| 294/313 [00:43<00:02,  6.90it/s]Evaluating:  94%|█████████▍| 295/313 [00:43<00:02,  6.92it/s]Evaluating:  95%|█████████▍| 296/313 [00:43<00:02,  6.93it/s]Evaluating:  95%|█████████▍| 297/313 [00:43<00:02,  6.93it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.93it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.93it/s]Evaluating:  96%|█████████▌| 300/313 [00:44<00:01,  6.93it/s]Evaluating:  96%|█████████▌| 301/313 [00:44<00:01,  6.94it/s]Evaluating:  96%|█████████▋| 302/313 [00:44<00:01,  6.94it/s]Evaluating:  97%|█████████▋| 303/313 [00:44<00:01,  6.89it/s]Evaluating:  97%|█████████▋| 304/313 [00:44<00:01,  6.85it/s]Evaluating:  97%|█████████▋| 305/313 [00:45<00:01,  6.84it/s]Evaluating:  98%|█████████▊| 306/313 [00:45<00:01,  6.85it/s]Evaluating:  98%|█████████▊| 307/313 [00:45<00:00,  6.86it/s]Evaluating:  98%|█████████▊| 308/313 [00:45<00:00,  6.86it/s]Evaluating:  99%|█████████▊| 309/313 [00:45<00:00,  6.89it/s]Evaluating:  99%|█████████▉| 310/313 [00:45<00:00,  6.85it/s]Evaluating:  99%|█████████▉| 311/313 [00:45<00:00,  6.78it/s]Evaluating: 100%|█████████▉| 312/313 [00:46<00:00,  6.77it/s]Evaluating: 100%|██████████| 313/313 [00:46<00:00,  6.77it/s]
10/08/2021 22:10:22 - INFO - __main__ -   ***** Evaluation result  in ru *****
10/08/2021 22:10:22 - INFO - __main__ -     f1 = 0.5557556901915834
10/08/2021 22:10:22 - INFO - __main__ -     loss = 0.6887183297461215
10/08/2021 22:10:22 - INFO - __main__ -     precision = 0.5540772181862705
10/08/2021 22:10:22 - INFO - __main__ -     recall = 0.5574443623224111
10/08/2021 22:10:22 - INFO - __main__ -   Language adapter for bg not found, using en instead
10/08/2021 22:10:22 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:10:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/08/2021 22:10:23 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/08/2021 22:10:23 - INFO - __main__ -     Num examples = 10004
10/08/2021 22:10:23 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.76it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.76it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.76it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.75it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.75it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.75it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.75it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.76it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.77it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.75it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.76it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.76it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:53,  5.52it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:49,  5.98it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:46,  6.34it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:44,  6.60it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.79it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:41,  6.93it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:41,  7.04it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:40,  7.09it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:40,  7.11it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:40,  7.10it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:40,  7.09it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:40,  7.06it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:40,  7.03it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:40,  6.98it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:40,  6.91it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.85it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.82it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.79it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.77it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.76it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.75it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.74it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.73it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.73it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.73it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.72it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.72it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▋        | 51/313 [00:07<01:02,  4.22it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:54,  4.75it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:49,  5.20it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:46,  5.58it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:43,  5.87it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:42,  6.10it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:40,  6.27it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:39,  6.40it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:39,  6.48it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:38,  6.55it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.59it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.62it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.64it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.66it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.66it/s]Evaluating:  21%|██        | 66/313 [00:10<00:36,  6.68it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:36,  6.68it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.68it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.68it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.67it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.67it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:35,  6.71it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.71it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.70it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.69it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.67it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:35,  6.66it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:34,  6.66it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.67it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:34,  6.66it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:33,  6.66it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.66it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.65it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.65it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.64it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.64it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.64it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.65it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.65it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:31,  6.67it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:31,  6.69it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.71it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.70it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.73it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.69it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:30,  6.68it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:30,  6.70it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.73it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.74it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.75it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.71it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:29,  6.74it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:29,  6.80it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.85it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:28,  6.89it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:32,  6.05it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:31,  6.21it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:30,  6.33it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:30,  6.41it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.46it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:29,  6.50it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:29,  6.53it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.55it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.57it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.58it/s]Evaluating:  40%|████      | 126/313 [00:19<00:28,  6.60it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.62it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.64it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.67it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.71it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.73it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:26,  6.71it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:26,  6.68it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:26,  6.65it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.64it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.65it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.66it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.67it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:26,  6.68it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:25,  6.69it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.69it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.68it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.68it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.70it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.70it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:24,  6.73it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:24,  6.76it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.80it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:27,  6.06it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:25,  6.36it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.58it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:23,  6.72it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:23,  6.86it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:22,  6.95it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:22,  7.02it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:22,  7.07it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:22,  7.08it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:21,  7.10it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:22,  6.92it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:22,  6.94it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:22,  6.82it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.76it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.72it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.70it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:22,  6.68it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:21,  6.70it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:21,  6.75it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.78it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.79it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:20,  6.83it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:20,  6.86it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.87it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:20,  6.88it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.89it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.89it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:19,  6.91it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:19,  6.94it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:19,  6.98it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:19,  7.00it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:18,  7.00it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:18,  7.01it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:18,  6.98it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:18,  6.94it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:21,  6.09it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:20,  6.37it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:19,  6.58it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:18,  6.74it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.85it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:17,  6.93it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:17,  6.98it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:17,  7.00it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  7.02it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:17,  7.02it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:16,  7.03it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:16,  7.05it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:16,  7.02it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:16,  6.96it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:16,  6.92it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:16,  6.89it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:16,  6.89it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:16,  6.90it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.89it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.85it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:16,  6.81it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:15,  6.81it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:15,  6.77it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:15,  6.75it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.76it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.77it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.75it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:15,  6.72it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:15,  6.71it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:14,  6.70it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.64it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.64it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.63it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:14,  6.66it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:17,  5.38it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:16,  5.67it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:15,  5.89it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:15,  6.09it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:14,  6.28it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.45it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:13,  6.59it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:13,  6.70it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:12,  6.80it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.78it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.77it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.80it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.80it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.80it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:11,  6.83it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:11,  6.84it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.80it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.79it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.82it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.88it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:10,  6.91it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:10,  6.94it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:10,  6.95it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:10,  6.90it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.86it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.83it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.79it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.80it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:09,  6.84it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.73it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.67it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.66it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.78it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:10,  5.83it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:10,  6.00it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:09,  6.13it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:09,  6.22it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:09,  6.28it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:09,  6.33it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.35it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.38it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:08,  6.45it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.46it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:08,  6.44it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.45it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.48it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.52it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.49it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.47it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:07,  6.45it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.45it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.43it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.45it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.51it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:06,  6.57it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.53it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:06,  6.50it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.47it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.48it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.50it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.49it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.50it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.51it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.53it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.53it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.54it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.56it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.51it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.49it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.48it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.50it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.56it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.60it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.62it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.61it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.53it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.52it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.54it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.53it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.49it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.46it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.43it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  6.44it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.49it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.52it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.57it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  6.60it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.64it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.67it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.70it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.75it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.81it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.85it/s]Evaluating:  99%|█████████▉| 311/313 [00:46<00:00,  6.88it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.91it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.64it/s]
10/08/2021 22:11:12 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/08/2021 22:11:12 - INFO - __main__ -     f1 = 0.7051864370972853
10/08/2021 22:11:12 - INFO - __main__ -     loss = 0.39981635748007044
10/08/2021 22:11:12 - INFO - __main__ -     precision = 0.698326359832636
10/08/2021 22:11:12 - INFO - __main__ -     recall = 0.7121826328141668
10/08/2021 22:11:12 - INFO - __main__ -   Language adapter for uk not found, using en instead
10/08/2021 22:11:12 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:11:12 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/08/2021 22:11:13 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/08/2021 22:11:13 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:11:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.87it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.81it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.78it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.78it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.78it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.77it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.77it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.76it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.75it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.74it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.74it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:50,  5.84it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:47,  6.20it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:44,  6.48it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.67it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.80it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:41,  6.89it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:41,  6.94it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:40,  7.00it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:40,  7.04it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:40,  7.07it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:39,  7.08it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:39,  7.08it/s]Evaluating:  10%|█         | 32/313 [00:04<00:39,  7.07it/s]Evaluating:  11%|█         | 33/313 [00:04<00:39,  7.07it/s]Evaluating:  11%|█         | 34/313 [00:04<00:39,  7.06it/s]Evaluating:  11%|█         | 35/313 [00:05<00:39,  7.06it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:39,  7.05it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:39,  7.05it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:38,  7.05it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:39,  7.02it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:38,  7.00it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:38,  6.98it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:39,  6.94it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:39,  6.89it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.86it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.81it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.75it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.72it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:38,  6.70it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.73it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:37,  6.74it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.75it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.74it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.72it/s]Evaluating:  21%|██        | 65/313 [00:09<00:36,  6.73it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.71it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.72it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.72it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.72it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.72it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.71it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.63it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:36,  6.64it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:35,  6.64it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.65it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.70it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:34,  6.73it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:34,  6.73it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:34,  6.71it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:34,  6.70it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.69it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.67it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:34,  6.66it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.65it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:44,  5.08it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:40,  5.47it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:38,  5.78it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:36,  6.01it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:35,  6.19it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:34,  6.31it/s]Evaluating:  30%|███       | 94/313 [00:14<00:34,  6.41it/s]Evaluating:  30%|███       | 95/313 [00:14<00:33,  6.48it/s]Evaluating:  31%|███       | 96/313 [00:14<00:33,  6.55it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.57it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.59it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.61it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:32,  6.48it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:32,  6.58it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.70it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:30,  6.82it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:30,  6.89it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:29,  6.94it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:29,  6.97it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:29,  6.98it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:29,  6.99it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:29,  6.99it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:29,  6.96it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:29,  6.92it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:29,  6.89it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:29,  6.89it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.82it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.76it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.74it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.71it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.69it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.66it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:29,  6.64it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:37,  5.19it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:34,  5.55it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:32,  5.88it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:31,  6.08it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:30,  6.23it/s]Evaluating:  40%|████      | 126/313 [00:18<00:29,  6.35it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.43it/s]Evaluating:  41%|████      | 128/313 [00:19<00:28,  6.47it/s]Evaluating:  41%|████      | 129/313 [00:19<00:28,  6.51it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:28,  6.53it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.57it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.60it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:27,  6.62it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:26,  6.64it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.65it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.65it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.66it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.68it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.67it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:25,  6.67it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.67it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.68it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.65it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.62it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.62it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:25,  6.63it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:24,  6.67it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.68it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.76it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:23,  6.80it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:23,  6.83it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:23,  6.87it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:23,  6.91it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:22,  6.96it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:22,  7.02it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:22,  7.06it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:22,  6.98it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:22,  6.88it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:22,  6.94it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:21,  6.97it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:21,  6.99it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:21,  7.01it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:21,  7.02it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:21,  7.00it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:21,  6.97it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:21,  6.97it/s]Evaluating:  53%|█████▎    | 167/313 [00:24<00:21,  6.90it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.85it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.82it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.79it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:20,  6.80it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.80it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:20,  6.82it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.83it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.83it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.83it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:19,  6.85it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:19,  6.84it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:19,  6.80it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:19,  6.82it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.85it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.86it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:18,  6.89it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:18,  6.92it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:18,  6.96it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:18,  7.00it/s]Evaluating:  60%|█████▉    | 187/313 [00:27<00:17,  7.03it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:17,  6.99it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:17,  6.94it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:17,  6.90it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:17,  6.91it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  6.93it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:17,  6.96it/s]Evaluating:  62%|██████▏   | 194/313 [00:28<00:17,  6.97it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:16,  6.95it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:16,  6.92it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:16,  6.92it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:16,  6.81it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:16,  6.71it/s]Evaluating:  64%|██████▍   | 200/313 [00:29<00:17,  6.64it/s]Evaluating:  64%|██████▍   | 201/313 [00:29<00:16,  6.59it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.56it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.54it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:16,  6.55it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:16,  6.57it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:16,  6.53it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:16,  6.54it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.58it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.61it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.61it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:15,  6.62it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:15,  6.62it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:15,  6.56it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:14,  6.68it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.79it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.87it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:13,  6.88it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:13,  6.89it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:13,  6.92it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:13,  6.94it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:13,  6.94it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.95it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:12,  6.96it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:12,  6.93it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:12,  6.94it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:12,  6.96it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:12,  6.99it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  7.01it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:11,  7.02it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:11,  7.03it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:11,  7.02it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:11,  7.02it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:11,  7.02it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:11,  7.03it/s]Evaluating:  75%|███████▌  | 235/313 [00:34<00:11,  7.04it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:10,  7.04it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:10,  7.00it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:10,  6.97it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:10,  6.95it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:10,  6.94it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:10,  6.95it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:10,  6.98it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  7.00it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:09,  7.00it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:09,  6.98it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:09,  6.99it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:09,  6.93it/s]Evaluating:  79%|███████▉  | 248/313 [00:36<00:09,  6.87it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:09,  6.87it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.89it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:08,  6.91it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:08,  6.94it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:08,  6.96it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:08,  6.89it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.81it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:13,  4.38it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:11,  4.84it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:10,  5.29it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:09,  5.64it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:08,  5.91it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:08,  6.13it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:08,  6.32it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.48it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.58it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.67it/s]Evaluating:  85%|████████▍ | 266/313 [00:39<00:07,  6.64it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:06,  6.61it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.64it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.66it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.69it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.73it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:06,  6.78it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:05,  6.82it/s]Evaluating:  88%|████████▊ | 274/313 [00:40<00:05,  6.85it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.89it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.91it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.94it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.94it/s]Evaluating:  89%|████████▉ | 279/313 [00:41<00:04,  6.95it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:04,  6.89it/s]Evaluating:  90%|████████▉ | 281/313 [00:41<00:04,  6.81it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.75it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.79it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.79it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.81it/s]Evaluating:  91%|█████████▏| 286/313 [00:42<00:03,  6.84it/s]Evaluating:  92%|█████████▏| 287/313 [00:42<00:03,  6.80it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:04,  5.18it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:04,  5.48it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:04,  5.70it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  5.90it/s]Evaluating:  93%|█████████▎| 292/313 [00:43<00:03,  6.07it/s]Evaluating:  94%|█████████▎| 293/313 [00:43<00:03,  6.20it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:03,  6.13it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.08it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.30it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.47it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.60it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.68it/s]Evaluating:  96%|█████████▌| 300/313 [00:44<00:01,  6.75it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.80it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.84it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.87it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  6.89it/s]Evaluating:  97%|█████████▋| 305/313 [00:45<00:01,  6.88it/s]Evaluating:  98%|█████████▊| 306/313 [00:45<00:01,  6.90it/s]Evaluating:  98%|█████████▊| 307/313 [00:45<00:00,  6.89it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.89it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.90it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.54it/s]Evaluating:  99%|█████████▉| 311/313 [00:46<00:00,  6.56it/s]Evaluating: 100%|█████████▉| 312/313 [00:46<00:00,  6.50it/s]Evaluating: 100%|██████████| 313/313 [00:46<00:00,  6.69it/s]
10/08/2021 22:12:01 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/08/2021 22:12:01 - INFO - __main__ -     f1 = 0.5996950057186429
10/08/2021 22:12:01 - INFO - __main__ -     loss = 0.4808959999023535
10/08/2021 22:12:01 - INFO - __main__ -     precision = 0.613925532745297
10/08/2021 22:12:01 - INFO - __main__ -     recall = 0.5861092480810791
10/08/2021 22:12:01 - INFO - __main__ -   Language adapter for be not found, using en instead
10/08/2021 22:12:01 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:12:01 - INFO - __main__ -   all languages = be
10/08/2021 22:12:01 - INFO - __main__ -   Creating features from dataset file at /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/be/test.bert-base-multilingual-cased in language be
10/08/2021 22:12:01 - INFO - utils_tag -   lang_id=0, lang=be, lang2id=None
10/08/2021 22:12:01 - INFO - utils_tag -   Writing example 0 of 1001
10/08/2021 22:12:01 - INFO - utils_tag -   *** Example ***
10/08/2021 22:12:01 - INFO - utils_tag -   guid: be-1
10/08/2021 22:12:01 - INFO - utils_tag -   tokens: [CLS] Па про ##сь ##бе Д ##ыя ##ні ##са М ##ед ##эя а ##мал ##ад ##зіла іх , за ##тым яны сталі з ##орка ##мі . [SEP]
10/08/2021 22:12:01 - INFO - utils_tag -   input_ids: 101 47041 12709 11833 18106 513 20379 11108 12016 521 31742 108219 541 66611 20004 92041 20335 117 10234 33656 62692 92677 548 77797 15416 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   label_ids: -100 6 6 -100 -100 2 -100 -100 -100 2 -100 -100 6 -100 -100 -100 6 6 6 -100 6 6 6 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:12:01 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:12:01 - INFO - utils_tag -   *** Example ***
10/08/2021 22:12:01 - INFO - utils_tag -   guid: be-2
10/08/2021 22:12:01 - INFO - utils_tag -   tokens: [CLS] Б ##ер ##лін ##скі т ##эх ##ні ##чны ў ##нів ##ерс ##ітэт [SEP]
10/08/2021 22:12:01 - INFO - utils_tag -   input_ids: 101 510 11977 43608 20552 559 35664 11108 30420 587 31497 62408 91530 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   label_ids: -100 1 -100 -100 -100 4 -100 -100 -100 4 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:12:01 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:12:01 - INFO - utils_tag -   *** Example ***
10/08/2021 22:12:01 - INFO - utils_tag -   guid: be-3
10/08/2021 22:12:01 - INFO - utils_tag -   tokens: [CLS] ' ' ' Марк У ##іль ##ям ##с ' ' ' [SEP]
10/08/2021 22:12:01 - INFO - utils_tag -   input_ids: 101 112 112 112 33307 528 70510 27429 10513 112 112 112 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   label_ids: -100 6 6 -100 2 5 -100 -100 -100 6 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:12:01 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:12:01 - INFO - utils_tag -   *** Example ***
10/08/2021 22:12:01 - INFO - utils_tag -   guid: be-4
10/08/2021 22:12:01 - INFO - utils_tag -   tokens: [CLS] ' ' ' Я ##май ##ка ' ' ' [SEP]
10/08/2021 22:12:01 - INFO - utils_tag -   input_ids: 101 112 112 112 540 55655 10521 112 112 112 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   label_ids: -100 6 6 -100 0 -100 -100 6 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:12:01 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:12:01 - INFO - utils_tag -   *** Example ***
10/08/2021 22:12:01 - INFO - utils_tag -   guid: be-5
10/08/2021 22:12:01 - INFO - utils_tag -   tokens: [CLS] М ##я ##жу ##е на па ##ў ##но ##чным за ##хад ##зе з Э ##ква ##дора ##м , на по ##ў ##на ##чы [UNK] з К ##алу ##м ##бі ##я ##й , на ў ##сход ##зе [UNK] з Б ##раз ##ілі ##я ##й , на па ##ў ##д ##н ##ёв ##ым у ##сход ##зе [UNK] з Ба ##лів ##ія ##й і Ч ##ыл ##і . [SEP]
10/08/2021 22:12:01 - INFO - utils_tag -   input_ids: 101 521 10385 27557 10205 10122 12634 11384 10636 27819 10234 77889 15748 548 538 34790 83933 10241 117 10122 10297 11384 10409 16848 100 548 519 35714 10241 27032 10385 10384 117 10122 587 68874 15748 100 548 510 34556 45260 10385 10384 117 10122 12634 11384 10746 10267 27136 13480 560 68874 15748 100 548 101086 27106 12586 10384 579 532 37721 10352 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
10/08/2021 22:12:01 - INFO - utils_tag -   label_ids: -100 6 -100 -100 -100 6 6 -100 -100 -100 6 -100 -100 6 0 -100 -100 -100 6 6 6 -100 -100 -100 6 6 0 -100 -100 -100 -100 -100 6 6 6 -100 -100 6 6 0 -100 -100 -100 -100 6 6 6 -100 -100 -100 -100 -100 6 -100 -100 6 6 0 -100 -100 -100 6 0 -100 -100 6 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100
10/08/2021 22:12:01 - INFO - utils_tag -   langs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
10/08/2021 22:12:02 - INFO - __main__ -   Saving features into cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128, len(features)=1001
10/08/2021 22:12:02 - INFO - __main__ -   ***** Running evaluation  in be *****
10/08/2021 22:12:02 - INFO - __main__ -     Num examples = 1001
10/08/2021 22:12:02 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.77it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.76it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.77it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.77it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.77it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.77it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.76it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.76it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.75it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.76it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.75it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.75it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.76it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.76it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.76it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.73it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.73it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.73it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.73it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.73it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.72it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.71it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.72it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.73it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.73it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.73it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.74it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.74it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  4.90it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  5.33it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  5.69it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.63it/s]
10/08/2021 22:12:07 - INFO - __main__ -   ***** Evaluation result  in be *****
10/08/2021 22:12:07 - INFO - __main__ -     f1 = 0.5962577962577963
10/08/2021 22:12:07 - INFO - __main__ -     loss = 0.45245046308264136
10/08/2021 22:12:07 - INFO - __main__ -     precision = 0.6035353535353535
10/08/2021 22:12:07 - INFO - __main__ -     recall = 0.5891536565324569
10/08/2021 22:12:07 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:12:20 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:12:20 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:12:52 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 22:12:54 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:12:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 22:12:54 - INFO - __main__ -   Seed = 22
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:13:19 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 22:13:30 - INFO - root -   save model
10/08/2021 22:13:30 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:13:35 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:13:35 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:13:47 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:13:47 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 22:13:47 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/08/2021 22:13:47 - INFO - root -   Trying to decide if add adapter
10/08/2021 22:13:47 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 22:13:47 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 22:13:47 - INFO - __main__ -   Language = en
10/08/2021 22:13:47 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 22:14:03 - INFO - __main__ -   Language adapter for ar not found, using en instead
10/08/2021 22:14:03 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:14:03 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
10/08/2021 22:14:04 - INFO - __main__ -   ***** Running evaluation  in ar *****
10/08/2021 22:14:04 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:14:04 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.69it/s]Evaluating:   1%|          | 3/313 [00:00<00:44,  7.03it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:42,  7.24it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:41,  7.34it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:41,  7.38it/s]Evaluating:   2%|▏         | 7/313 [00:00<00:41,  7.43it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:40,  7.47it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:40,  7.49it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:40,  7.49it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:40,  7.48it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:40,  7.50it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:40,  7.49it/s]Evaluating:   4%|▍         | 14/313 [00:01<00:39,  7.50it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:39,  7.49it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:39,  7.50it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:39,  7.51it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:39,  7.50it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:39,  7.50it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:39,  7.48it/s]Evaluating:   7%|▋         | 21/313 [00:02<00:39,  7.49it/s]Evaluating:   7%|▋         | 22/313 [00:02<00:38,  7.47it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:38,  7.48it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:38,  7.45it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:38,  7.44it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:38,  7.44it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:38,  7.45it/s]Evaluating:   9%|▉         | 28/313 [00:03<00:38,  7.44it/s]Evaluating:   9%|▉         | 29/313 [00:03<00:38,  7.43it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:38,  7.41it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:38,  7.39it/s]Evaluating:  10%|█         | 32/313 [00:04<00:37,  7.40it/s]Evaluating:  11%|█         | 33/313 [00:04<00:37,  7.40it/s]Evaluating:  11%|█         | 34/313 [00:04<00:37,  7.40it/s]Evaluating:  11%|█         | 35/313 [00:04<00:37,  7.41it/s]Evaluating:  12%|█▏        | 36/313 [00:04<00:37,  7.42it/s]Evaluating:  12%|█▏        | 37/313 [00:04<00:37,  7.42it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:37,  7.41it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:37,  7.39it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:36,  7.40it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:36,  7.41it/s]Evaluating:  13%|█▎        | 42/313 [00:05<00:36,  7.40it/s]Evaluating:  14%|█▎        | 43/313 [00:05<00:36,  7.40it/s]Evaluating:  14%|█▍        | 44/313 [00:05<00:36,  7.41it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:36,  7.42it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:36,  7.37it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:36,  7.38it/s]Evaluating:  15%|█▌        | 48/313 [00:06<00:35,  7.38it/s]Evaluating:  16%|█▌        | 49/313 [00:06<00:36,  7.33it/s]Evaluating:  16%|█▌        | 50/313 [00:06<00:35,  7.32it/s]Evaluating:  16%|█▋        | 51/313 [00:06<00:35,  7.33it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:35,  7.35it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:35,  7.37it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:35,  7.37it/s]Evaluating:  18%|█▊        | 55/313 [00:07<00:35,  7.36it/s]Evaluating:  18%|█▊        | 56/313 [00:07<00:35,  7.32it/s]Evaluating:  18%|█▊        | 57/313 [00:07<00:35,  7.30it/s]Evaluating:  19%|█▊        | 58/313 [00:07<00:34,  7.33it/s]Evaluating:  19%|█▉        | 59/313 [00:07<00:34,  7.32it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:34,  7.32it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:34,  7.32it/s]Evaluating:  20%|█▉        | 62/313 [00:08<00:34,  7.30it/s]Evaluating:  20%|██        | 63/313 [00:08<00:34,  7.28it/s]Evaluating:  20%|██        | 64/313 [00:08<00:34,  7.18it/s]Evaluating:  21%|██        | 65/313 [00:08<00:34,  7.15it/s]Evaluating:  21%|██        | 66/313 [00:08<00:34,  7.20it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:34,  7.21it/s]Evaluating:  22%|██▏       | 68/313 [00:09<00:33,  7.25it/s]Evaluating:  22%|██▏       | 69/313 [00:09<00:33,  7.27it/s]Evaluating:  22%|██▏       | 70/313 [00:09<00:33,  7.28it/s]Evaluating:  23%|██▎       | 71/313 [00:09<00:33,  7.28it/s]Evaluating:  23%|██▎       | 72/313 [00:09<00:33,  7.30it/s]Evaluating:  23%|██▎       | 73/313 [00:09<00:32,  7.30it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:32,  7.30it/s]Evaluating:  24%|██▍       | 75/313 [00:10<00:32,  7.30it/s]Evaluating:  24%|██▍       | 76/313 [00:10<00:32,  7.26it/s]Evaluating:  25%|██▍       | 77/313 [00:10<00:32,  7.27it/s]Evaluating:  25%|██▍       | 78/313 [00:10<00:32,  7.29it/s]Evaluating:  25%|██▌       | 79/313 [00:10<00:32,  7.30it/s]Evaluating:  26%|██▌       | 80/313 [00:10<00:31,  7.31it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:31,  7.30it/s]Evaluating:  26%|██▌       | 82/313 [00:11<00:32,  7.21it/s]Evaluating:  27%|██▋       | 83/313 [00:11<00:32,  7.19it/s]Evaluating:  27%|██▋       | 84/313 [00:11<00:32,  7.11it/s]Evaluating:  27%|██▋       | 85/313 [00:11<00:32,  6.96it/s]Evaluating:  27%|██▋       | 86/313 [00:11<00:32,  7.04it/s]Evaluating:  28%|██▊       | 87/313 [00:11<00:31,  7.10it/s]Evaluating:  28%|██▊       | 88/313 [00:12<00:31,  7.13it/s]Evaluating:  28%|██▊       | 89/313 [00:12<00:31,  7.15it/s]Evaluating:  29%|██▉       | 90/313 [00:12<00:31,  7.18it/s]Evaluating:  29%|██▉       | 91/313 [00:12<00:30,  7.19it/s]Evaluating:  29%|██▉       | 92/313 [00:12<00:30,  7.19it/s]Evaluating:  30%|██▉       | 93/313 [00:12<00:30,  7.20it/s]Evaluating:  30%|███       | 94/313 [00:12<00:30,  7.20it/s]Evaluating:  30%|███       | 95/313 [00:12<00:30,  7.20it/s]Evaluating:  31%|███       | 96/313 [00:13<00:30,  7.21it/s]Evaluating:  31%|███       | 97/313 [00:13<00:40,  5.36it/s]Evaluating:  31%|███▏      | 98/313 [00:13<00:37,  5.79it/s]Evaluating:  32%|███▏      | 99/313 [00:13<00:34,  6.14it/s]Evaluating:  32%|███▏      | 100/313 [00:13<00:33,  6.43it/s]Evaluating:  32%|███▏      | 101/313 [00:13<00:31,  6.65it/s]Evaluating:  33%|███▎      | 102/313 [00:14<00:31,  6.80it/s]Evaluating:  33%|███▎      | 103/313 [00:14<00:30,  6.92it/s]Evaluating:  33%|███▎      | 104/313 [00:14<00:29,  7.00it/s]Evaluating:  34%|███▎      | 105/313 [00:14<00:29,  7.07it/s]Evaluating:  34%|███▍      | 106/313 [00:14<00:29,  7.10it/s]Evaluating:  34%|███▍      | 107/313 [00:14<00:28,  7.13it/s]Evaluating:  35%|███▍      | 108/313 [00:14<00:28,  7.14it/s]Evaluating:  35%|███▍      | 109/313 [00:15<00:28,  7.15it/s]Evaluating:  35%|███▌      | 110/313 [00:15<00:28,  7.15it/s]Evaluating:  35%|███▌      | 111/313 [00:15<00:28,  7.17it/s]Evaluating:  36%|███▌      | 112/313 [00:15<00:28,  7.17it/s]Evaluating:  36%|███▌      | 113/313 [00:15<00:27,  7.16it/s]Evaluating:  36%|███▋      | 114/313 [00:15<00:27,  7.18it/s]Evaluating:  37%|███▋      | 115/313 [00:15<00:27,  7.18it/s]Evaluating:  37%|███▋      | 116/313 [00:16<00:27,  7.20it/s]Evaluating:  37%|███▋      | 117/313 [00:16<00:28,  6.93it/s]Evaluating:  38%|███▊      | 118/313 [00:16<00:28,  6.83it/s]Evaluating:  38%|███▊      | 119/313 [00:16<00:27,  6.94it/s]Evaluating:  38%|███▊      | 120/313 [00:16<00:27,  7.00it/s]Evaluating:  39%|███▊      | 121/313 [00:16<00:27,  7.04it/s]Evaluating:  39%|███▉      | 122/313 [00:16<00:27,  7.07it/s]Evaluating:  39%|███▉      | 123/313 [00:17<00:26,  7.12it/s]Evaluating:  40%|███▉      | 124/313 [00:17<00:26,  7.13it/s]Evaluating:  40%|███▉      | 125/313 [00:17<00:26,  7.13it/s]Evaluating:  40%|████      | 126/313 [00:17<00:26,  7.14it/s]Evaluating:  41%|████      | 127/313 [00:17<00:26,  7.14it/s]Evaluating:  41%|████      | 128/313 [00:17<00:25,  7.14it/s]Evaluating:  41%|████      | 129/313 [00:17<00:25,  7.15it/s]Evaluating:  42%|████▏     | 130/313 [00:18<00:25,  7.14it/s]Evaluating:  42%|████▏     | 131/313 [00:18<00:25,  7.14it/s]Evaluating:  42%|████▏     | 132/313 [00:18<00:25,  7.15it/s]Evaluating:  42%|████▏     | 133/313 [00:18<00:25,  7.14it/s]Evaluating:  43%|████▎     | 134/313 [00:18<00:25,  7.10it/s]Evaluating:  43%|████▎     | 135/313 [00:18<00:24,  7.12it/s]Evaluating:  43%|████▎     | 136/313 [00:18<00:24,  7.12it/s]Evaluating:  44%|████▍     | 137/313 [00:19<00:24,  7.12it/s]Evaluating:  44%|████▍     | 138/313 [00:19<00:24,  7.14it/s]Evaluating:  44%|████▍     | 139/313 [00:19<00:24,  7.15it/s]Evaluating:  45%|████▍     | 140/313 [00:19<00:24,  7.15it/s]Evaluating:  45%|████▌     | 141/313 [00:19<00:24,  7.15it/s]Evaluating:  45%|████▌     | 142/313 [00:19<00:23,  7.15it/s]Evaluating:  46%|████▌     | 143/313 [00:19<00:23,  7.14it/s]Evaluating:  46%|████▌     | 144/313 [00:19<00:23,  7.15it/s]Evaluating:  46%|████▋     | 145/313 [00:20<00:23,  7.12it/s]Evaluating:  47%|████▋     | 146/313 [00:20<00:23,  7.12it/s]Evaluating:  47%|████▋     | 147/313 [00:20<00:23,  7.13it/s]Evaluating:  47%|████▋     | 148/313 [00:20<00:23,  7.11it/s]Evaluating:  48%|████▊     | 149/313 [00:20<00:23,  7.11it/s]Evaluating:  48%|████▊     | 150/313 [00:20<00:22,  7.12it/s]Evaluating:  48%|████▊     | 151/313 [00:20<00:22,  7.11it/s]Evaluating:  49%|████▊     | 152/313 [00:21<00:22,  7.11it/s]Evaluating:  49%|████▉     | 153/313 [00:21<00:22,  7.09it/s]Evaluating:  49%|████▉     | 154/313 [00:21<00:22,  7.07it/s]Evaluating:  50%|████▉     | 155/313 [00:21<00:22,  7.08it/s]Evaluating:  50%|████▉     | 156/313 [00:21<00:22,  7.09it/s]Evaluating:  50%|█████     | 157/313 [00:21<00:22,  7.07it/s]Evaluating:  50%|█████     | 158/313 [00:21<00:21,  7.07it/s]Evaluating:  51%|█████     | 159/313 [00:22<00:21,  7.09it/s]Evaluating:  51%|█████     | 160/313 [00:22<00:21,  7.09it/s]Evaluating:  51%|█████▏    | 161/313 [00:22<00:21,  7.08it/s]Evaluating:  52%|█████▏    | 162/313 [00:22<00:21,  7.09it/s]Evaluating:  52%|█████▏    | 163/313 [00:22<00:21,  7.10it/s]Evaluating:  52%|█████▏    | 164/313 [00:22<00:21,  7.09it/s]Evaluating:  53%|█████▎    | 165/313 [00:22<00:20,  7.09it/s]Evaluating:  53%|█████▎    | 166/313 [00:23<00:20,  7.05it/s]Evaluating:  53%|█████▎    | 167/313 [00:23<00:20,  7.00it/s]Evaluating:  54%|█████▎    | 168/313 [00:23<00:20,  7.02it/s]Evaluating:  54%|█████▍    | 169/313 [00:23<00:20,  6.98it/s]Evaluating:  54%|█████▍    | 170/313 [00:23<00:20,  6.93it/s]Evaluating:  55%|█████▍    | 171/313 [00:23<00:20,  6.96it/s]Evaluating:  55%|█████▍    | 172/313 [00:23<00:20,  7.00it/s]Evaluating:  55%|█████▌    | 173/313 [00:24<00:19,  7.01it/s]Evaluating:  56%|█████▌    | 174/313 [00:24<00:19,  7.01it/s]Evaluating:  56%|█████▌    | 175/313 [00:24<00:19,  7.02it/s]Evaluating:  56%|█████▌    | 176/313 [00:24<00:19,  7.00it/s]Evaluating:  57%|█████▋    | 177/313 [00:24<00:19,  6.98it/s]Evaluating:  57%|█████▋    | 178/313 [00:24<00:19,  6.96it/s]Evaluating:  57%|█████▋    | 179/313 [00:24<00:19,  6.95it/s]Evaluating:  58%|█████▊    | 180/313 [00:25<00:19,  6.71it/s]Evaluating:  58%|█████▊    | 181/313 [00:25<00:19,  6.68it/s]Evaluating:  58%|█████▊    | 182/313 [00:25<00:19,  6.76it/s]Evaluating:  58%|█████▊    | 183/313 [00:25<00:19,  6.83it/s]Evaluating:  59%|█████▉    | 184/313 [00:25<00:18,  6.87it/s]Evaluating:  59%|█████▉    | 185/313 [00:25<00:18,  6.88it/s]Evaluating:  59%|█████▉    | 186/313 [00:26<00:18,  6.82it/s]Evaluating:  60%|█████▉    | 187/313 [00:26<00:18,  6.87it/s]Evaluating:  60%|██████    | 188/313 [00:26<00:18,  6.92it/s]Evaluating:  60%|██████    | 189/313 [00:26<00:17,  6.97it/s]Evaluating:  61%|██████    | 190/313 [00:26<00:17,  7.00it/s]Evaluating:  61%|██████    | 191/313 [00:26<00:17,  7.02it/s]Evaluating:  61%|██████▏   | 192/313 [00:26<00:17,  7.02it/s]Evaluating:  62%|██████▏   | 193/313 [00:27<00:17,  6.95it/s]Evaluating:  62%|██████▏   | 194/313 [00:27<00:17,  6.87it/s]Evaluating:  62%|██████▏   | 195/313 [00:27<00:17,  6.84it/s]Evaluating:  63%|██████▎   | 196/313 [00:27<00:17,  6.78it/s]Evaluating:  63%|██████▎   | 197/313 [00:27<00:17,  6.73it/s]Evaluating:  63%|██████▎   | 198/313 [00:27<00:17,  6.74it/s]Evaluating:  64%|██████▎   | 199/313 [00:27<00:16,  6.74it/s]Evaluating:  64%|██████▍   | 200/313 [00:28<00:16,  6.73it/s]Evaluating:  64%|██████▍   | 201/313 [00:28<00:16,  6.73it/s]Evaluating:  65%|██████▍   | 202/313 [00:28<00:20,  5.32it/s]Evaluating:  65%|██████▍   | 203/313 [00:28<00:19,  5.74it/s]Evaluating:  65%|██████▌   | 204/313 [00:28<00:17,  6.07it/s]Evaluating:  65%|██████▌   | 205/313 [00:28<00:17,  6.33it/s]Evaluating:  66%|██████▌   | 206/313 [00:29<00:16,  6.48it/s]Evaluating:  66%|██████▌   | 207/313 [00:29<00:16,  6.60it/s]Evaluating:  66%|██████▋   | 208/313 [00:29<00:15,  6.67it/s]Evaluating:  67%|██████▋   | 209/313 [00:29<00:15,  6.73it/s]Evaluating:  67%|██████▋   | 210/313 [00:29<00:15,  6.82it/s]Evaluating:  67%|██████▋   | 211/313 [00:29<00:14,  6.87it/s]Evaluating:  68%|██████▊   | 212/313 [00:29<00:14,  6.89it/s]Evaluating:  68%|██████▊   | 213/313 [00:30<00:14,  6.91it/s]Evaluating:  68%|██████▊   | 214/313 [00:30<00:14,  6.89it/s]Evaluating:  69%|██████▊   | 215/313 [00:30<00:14,  6.63it/s]Evaluating:  69%|██████▉   | 216/313 [00:30<00:14,  6.59it/s]Evaluating:  69%|██████▉   | 217/313 [00:30<00:14,  6.45it/s]Evaluating:  70%|██████▉   | 218/313 [00:30<00:14,  6.52it/s]Evaluating:  70%|██████▉   | 219/313 [00:30<00:14,  6.62it/s]Evaluating:  70%|███████   | 220/313 [00:31<00:13,  6.68it/s]Evaluating:  71%|███████   | 221/313 [00:31<00:13,  6.73it/s]Evaluating:  71%|███████   | 222/313 [00:31<00:13,  6.79it/s]Evaluating:  71%|███████   | 223/313 [00:31<00:13,  6.82it/s]Evaluating:  72%|███████▏  | 224/313 [00:31<00:13,  6.77it/s]Evaluating:  72%|███████▏  | 225/313 [00:31<00:12,  6.81it/s]Evaluating:  72%|███████▏  | 226/313 [00:32<00:12,  6.81it/s]Evaluating:  73%|███████▎  | 227/313 [00:32<00:12,  6.77it/s]Evaluating:  73%|███████▎  | 228/313 [00:32<00:12,  6.74it/s]Evaluating:  73%|███████▎  | 229/313 [00:32<00:12,  6.74it/s]Evaluating:  73%|███████▎  | 230/313 [00:32<00:12,  6.74it/s]Evaluating:  74%|███████▍  | 231/313 [00:32<00:12,  6.64it/s]Evaluating:  74%|███████▍  | 232/313 [00:32<00:12,  6.62it/s]Evaluating:  74%|███████▍  | 233/313 [00:33<00:12,  6.56it/s]Evaluating:  75%|███████▍  | 234/313 [00:33<00:12,  6.53it/s]Evaluating:  75%|███████▌  | 235/313 [00:33<00:12,  6.49it/s]Evaluating:  75%|███████▌  | 236/313 [00:33<00:11,  6.48it/s]Evaluating:  76%|███████▌  | 237/313 [00:33<00:11,  6.52it/s]Evaluating:  76%|███████▌  | 238/313 [00:33<00:11,  6.57it/s]Evaluating:  76%|███████▋  | 239/313 [00:33<00:11,  6.65it/s]Evaluating:  77%|███████▋  | 240/313 [00:34<00:10,  6.69it/s]Evaluating:  77%|███████▋  | 241/313 [00:34<00:10,  6.76it/s]Evaluating:  77%|███████▋  | 242/313 [00:34<00:10,  6.82it/s]Evaluating:  78%|███████▊  | 243/313 [00:34<00:10,  6.84it/s]Evaluating:  78%|███████▊  | 244/313 [00:34<00:10,  6.89it/s]Evaluating:  78%|███████▊  | 245/313 [00:34<00:09,  6.91it/s]Evaluating:  79%|███████▊  | 246/313 [00:35<00:10,  6.52it/s]Evaluating:  79%|███████▉  | 247/313 [00:35<00:10,  6.27it/s]Evaluating:  79%|███████▉  | 248/313 [00:35<00:10,  6.50it/s]Evaluating:  80%|███████▉  | 249/313 [00:35<00:09,  6.63it/s]Evaluating:  80%|███████▉  | 250/313 [00:35<00:09,  6.72it/s]Evaluating:  80%|████████  | 251/313 [00:35<00:09,  6.83it/s]Evaluating:  81%|████████  | 252/313 [00:35<00:08,  6.88it/s]Evaluating:  81%|████████  | 253/313 [00:36<00:08,  6.88it/s]Evaluating:  81%|████████  | 254/313 [00:36<00:08,  6.90it/s]Evaluating:  81%|████████▏ | 255/313 [00:36<00:08,  6.93it/s]Evaluating:  82%|████████▏ | 256/313 [00:36<00:08,  6.93it/s]Evaluating:  82%|████████▏ | 257/313 [00:36<00:08,  6.94it/s]Evaluating:  82%|████████▏ | 258/313 [00:36<00:07,  6.89it/s]Evaluating:  83%|████████▎ | 259/313 [00:36<00:07,  6.88it/s]Evaluating:  83%|████████▎ | 260/313 [00:37<00:07,  6.93it/s]Evaluating:  83%|████████▎ | 261/313 [00:37<00:07,  6.94it/s]Evaluating:  84%|████████▎ | 262/313 [00:37<00:07,  6.56it/s]Evaluating:  84%|████████▍ | 263/313 [00:37<00:07,  6.35it/s]Evaluating:  84%|████████▍ | 264/313 [00:37<00:07,  6.35it/s]Evaluating:  85%|████████▍ | 265/313 [00:37<00:07,  6.35it/s]Evaluating:  85%|████████▍ | 266/313 [00:38<00:07,  6.37it/s]Evaluating:  85%|████████▌ | 267/313 [00:38<00:07,  6.35it/s]Evaluating:  86%|████████▌ | 268/313 [00:38<00:07,  6.34it/s]Evaluating:  86%|████████▌ | 269/313 [00:38<00:06,  6.38it/s]Evaluating:  86%|████████▋ | 270/313 [00:38<00:06,  6.44it/s]Evaluating:  87%|████████▋ | 271/313 [00:38<00:06,  6.51it/s]Evaluating:  87%|████████▋ | 272/313 [00:38<00:06,  6.60it/s]Evaluating:  87%|████████▋ | 273/313 [00:39<00:05,  6.67it/s]Evaluating:  88%|████████▊ | 274/313 [00:39<00:05,  6.68it/s]Evaluating:  88%|████████▊ | 275/313 [00:39<00:05,  6.72it/s]Evaluating:  88%|████████▊ | 276/313 [00:39<00:05,  6.78it/s]Evaluating:  88%|████████▊ | 277/313 [00:39<00:05,  6.81it/s]Evaluating:  89%|████████▉ | 278/313 [00:39<00:05,  6.82it/s]Evaluating:  89%|████████▉ | 279/313 [00:39<00:04,  6.84it/s]Evaluating:  89%|████████▉ | 280/313 [00:40<00:04,  6.86it/s]Evaluating:  90%|████████▉ | 281/313 [00:40<00:04,  6.87it/s]Evaluating:  90%|█████████ | 282/313 [00:40<00:06,  5.02it/s]Evaluating:  90%|█████████ | 283/313 [00:40<00:05,  5.47it/s]Evaluating:  91%|█████████ | 284/313 [00:40<00:04,  5.84it/s]Evaluating:  91%|█████████ | 285/313 [00:41<00:04,  6.11it/s]Evaluating:  91%|█████████▏| 286/313 [00:41<00:04,  6.26it/s]Evaluating:  92%|█████████▏| 287/313 [00:41<00:04,  6.42it/s]Evaluating:  92%|█████████▏| 288/313 [00:41<00:03,  6.56it/s]Evaluating:  92%|█████████▏| 289/313 [00:41<00:03,  6.65it/s]Evaluating:  93%|█████████▎| 290/313 [00:41<00:03,  6.74it/s]Evaluating:  93%|█████████▎| 291/313 [00:41<00:03,  6.79it/s]Evaluating:  93%|█████████▎| 292/313 [00:42<00:03,  6.58it/s]Evaluating:  94%|█████████▎| 293/313 [00:42<00:03,  6.53it/s]Evaluating:  94%|█████████▍| 294/313 [00:42<00:02,  6.65it/s]Evaluating:  94%|█████████▍| 295/313 [00:42<00:02,  6.73it/s]Evaluating:  95%|█████████▍| 296/313 [00:42<00:02,  6.78it/s]Evaluating:  95%|█████████▍| 297/313 [00:42<00:02,  6.79it/s]Evaluating:  95%|█████████▌| 298/313 [00:42<00:02,  6.82it/s]Evaluating:  96%|█████████▌| 299/313 [00:43<00:02,  6.81it/s]Evaluating:  96%|█████████▌| 300/313 [00:43<00:01,  6.85it/s]Evaluating:  96%|█████████▌| 301/313 [00:43<00:01,  6.87it/s]Evaluating:  96%|█████████▋| 302/313 [00:43<00:01,  6.88it/s]Evaluating:  97%|█████████▋| 303/313 [00:43<00:01,  6.89it/s]Evaluating:  97%|█████████▋| 304/313 [00:43<00:01,  6.89it/s]Evaluating:  97%|█████████▋| 305/313 [00:43<00:01,  6.88it/s]Evaluating:  98%|█████████▊| 306/313 [00:44<00:01,  6.84it/s]Evaluating:  98%|█████████▊| 307/313 [00:44<00:00,  6.79it/s]Evaluating:  98%|█████████▊| 308/313 [00:44<00:00,  6.77it/s]Evaluating:  99%|█████████▊| 309/313 [00:44<00:00,  6.74it/s]Evaluating:  99%|█████████▉| 310/313 [00:44<00:00,  6.73it/s]Evaluating:  99%|█████████▉| 311/313 [00:44<00:00,  6.65it/s]Evaluating: 100%|█████████▉| 312/313 [00:45<00:00,  6.59it/s]Evaluating: 100%|██████████| 313/313 [00:45<00:00,  6.94it/s]
10/08/2021 22:14:51 - INFO - __main__ -   ***** Evaluation result  in ar *****
10/08/2021 22:14:51 - INFO - __main__ -     f1 = 0.32447895382100533
10/08/2021 22:14:51 - INFO - __main__ -     loss = 5.544163345148007
10/08/2021 22:14:51 - INFO - __main__ -     precision = 0.3818684621858843
10/08/2021 22:14:51 - INFO - __main__ -     recall = 0.2820854427569056
10/08/2021 22:14:51 - INFO - __main__ -   Language adapter for bh not found, using en instead
10/08/2021 22:14:51 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:14:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bh_bert-base-multilingual-cased_128
10/08/2021 22:14:51 - INFO - __main__ -   ***** Running evaluation  in bh *****
10/08/2021 22:14:51 - INFO - __main__ -     Num examples = 102
10/08/2021 22:14:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  6.78it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  6.78it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  6.77it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.38it/s]
10/08/2021 22:14:51 - INFO - __main__ -   ***** Evaluation result  in bh *****
10/08/2021 22:14:51 - INFO - __main__ -     f1 = 0.3442622950819672
10/08/2021 22:14:51 - INFO - __main__ -     loss = 3.1455380022525787
10/08/2021 22:14:51 - INFO - __main__ -     precision = 0.3230769230769231
10/08/2021 22:14:51 - INFO - __main__ -     recall = 0.3684210526315789
10/08/2021 22:14:51 - INFO - __main__ -   Language adapter for hi not found, using en instead
10/08/2021 22:14:51 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:14:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
10/08/2021 22:14:51 - INFO - __main__ -   ***** Running evaluation  in hi *****
10/08/2021 22:14:51 - INFO - __main__ -     Num examples = 1000
10/08/2021 22:14:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.78it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.79it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.78it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.77it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.77it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.77it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.77it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.77it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.75it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.77it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.75it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.72it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.72it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.72it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.68it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.69it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.69it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.69it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.70it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.68it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.67it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.68it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.68it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.70it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.71it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.71it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.72it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.72it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.70it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.69it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.69it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.87it/s]
10/08/2021 22:14:56 - INFO - __main__ -   ***** Evaluation result  in hi *****
10/08/2021 22:14:56 - INFO - __main__ -     f1 = 0.5147751605995717
10/08/2021 22:14:56 - INFO - __main__ -     loss = 2.8746046163141727
10/08/2021 22:14:56 - INFO - __main__ -     precision = 0.5429087624209575
10/08/2021 22:14:56 - INFO - __main__ -     recall = 0.48941368078175895
10/08/2021 22:14:56 - INFO - __main__ -   Language adapter for fo not found, using en instead
10/08/2021 22:14:56 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:14:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/08/2021 22:14:56 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/08/2021 22:14:56 - INFO - __main__ -     Num examples = 100
10/08/2021 22:14:56 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  7.49it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  7.37it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  7.36it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  9.29it/s]
10/08/2021 22:14:57 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/08/2021 22:14:57 - INFO - __main__ -     f1 = 0.6390977443609023
10/08/2021 22:14:57 - INFO - __main__ -     loss = 1.9067792147397995
10/08/2021 22:14:57 - INFO - __main__ -     precision = 0.5821917808219178
10/08/2021 22:14:57 - INFO - __main__ -     recall = 0.7083333333333334
10/08/2021 22:14:57 - INFO - __main__ -   Language adapter for no not found, using en instead
10/08/2021 22:14:57 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:14:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/08/2021 22:14:58 - INFO - __main__ -   ***** Running evaluation  in no *****
10/08/2021 22:14:58 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:14:58 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.75it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.77it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.77it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.78it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.77it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.76it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.75it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.74it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.74it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.73it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.72it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:47,  6.32it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:45,  6.57it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.72it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:43,  6.84it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:42,  6.93it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:42,  6.99it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:41,  7.02it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:41,  7.04it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:41,  7.04it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:41,  7.02it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:41,  7.00it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:41,  6.99it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:41,  6.96it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:41,  6.92it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:41,  6.91it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:41,  6.91it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:41,  6.90it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:41,  6.88it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.85it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.81it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.78it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.77it/s]Evaluating:  11%|█         | 34/313 [00:04<00:41,  6.75it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.70it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.71it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.71it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.71it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.70it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.70it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.71it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.73it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.75it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.75it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.76it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.77it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:38,  6.80it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:38,  6.77it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:38,  6.75it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:38,  6.69it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.69it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.71it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.71it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:37,  6.70it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.67it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.67it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.67it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.67it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.68it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.68it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.67it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.68it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.68it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:35,  6.71it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:35,  6.72it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.70it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.65it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.68it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:34,  6.70it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:34,  6.72it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:34,  6.73it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.73it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.71it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.71it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:33,  6.71it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:33,  6.75it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.76it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.77it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.77it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:32,  6.76it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:32,  6.75it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:32,  6.75it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:32,  6.71it/s]Evaluating:  30%|███       | 94/313 [00:13<00:33,  6.62it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.76it/s]Evaluating:  31%|███       | 97/313 [00:14<00:31,  6.76it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.72it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.68it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:31,  6.66it/s]Evaluating:  32%|███▏      | 101/313 [00:14<00:31,  6.68it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.72it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.73it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.68it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.64it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:31,  6.61it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:30,  6.65it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.63it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.65it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.63it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:30,  6.59it/s]Evaluating:  36%|███▋      | 114/313 [00:16<00:30,  6.58it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:36,  5.45it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:34,  5.76it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:32,  6.04it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:31,  6.24it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:30,  6.40it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:29,  6.51it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:30,  6.38it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:29,  6.46it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.57it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.68it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:27,  6.74it/s]Evaluating:  40%|████      | 126/313 [00:18<00:27,  6.75it/s]Evaluating:  41%|████      | 127/313 [00:18<00:27,  6.79it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.81it/s]Evaluating:  41%|████      | 129/313 [00:19<00:26,  6.82it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:26,  6.84it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:26,  6.88it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:26,  6.86it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:26,  6.84it/s]Evaluating:  43%|████▎     | 134/313 [00:19<00:26,  6.86it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:25,  6.87it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:25,  6.86it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:25,  6.87it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.73it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:25,  6.69it/s]Evaluating:  45%|████▍     | 140/313 [00:20<00:25,  6.69it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.79it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:24,  6.87it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:24,  6.93it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:24,  6.98it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:23,  7.04it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:23,  7.06it/s]Evaluating:  47%|████▋     | 147/313 [00:21<00:23,  7.06it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:23,  7.07it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:23,  7.07it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:23,  7.08it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:23,  6.91it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:23,  6.90it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:23,  6.95it/s]Evaluating:  49%|████▉     | 154/313 [00:22<00:22,  7.00it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:22,  7.03it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:22,  7.06it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:22,  7.07it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:21,  7.06it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:21,  7.02it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:22,  6.94it/s]Evaluating:  51%|█████▏    | 161/313 [00:23<00:21,  6.97it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:21,  7.00it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:21,  7.01it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:21,  7.02it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:21,  7.03it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:20,  7.02it/s]Evaluating:  53%|█████▎    | 167/313 [00:24<00:20,  7.03it/s]Evaluating:  54%|█████▎    | 168/313 [00:24<00:21,  6.82it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.78it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.73it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:20,  6.83it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.90it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:20,  6.96it/s]Evaluating:  56%|█████▌    | 174/313 [00:25<00:19,  6.99it/s]Evaluating:  56%|█████▌    | 175/313 [00:25<00:19,  7.01it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:19,  6.99it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:19,  6.97it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:19,  6.97it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:19,  6.98it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:18,  7.00it/s]Evaluating:  58%|█████▊    | 181/313 [00:26<00:18,  7.01it/s]Evaluating:  58%|█████▊    | 182/313 [00:26<00:18,  7.03it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:23,  5.51it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:22,  5.76it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:21,  5.96it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:20,  6.14it/s]Evaluating:  60%|█████▉    | 187/313 [00:27<00:19,  6.31it/s]Evaluating:  60%|██████    | 188/313 [00:27<00:19,  6.45it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.56it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.65it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:18,  6.72it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  6.74it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:17,  6.76it/s]Evaluating:  62%|██████▏   | 194/313 [00:28<00:17,  6.80it/s]Evaluating:  62%|██████▏   | 195/313 [00:28<00:17,  6.85it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.88it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:16,  6.90it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:16,  6.95it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:16,  6.71it/s]Evaluating:  64%|██████▍   | 200/313 [00:29<00:16,  6.71it/s]Evaluating:  64%|██████▍   | 201/313 [00:29<00:16,  6.81it/s]Evaluating:  65%|██████▍   | 202/313 [00:29<00:16,  6.87it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:15,  6.90it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:15,  6.94it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:15,  6.98it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:15,  7.01it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:15,  7.03it/s]Evaluating:  66%|██████▋   | 208/313 [00:30<00:14,  7.05it/s]Evaluating:  67%|██████▋   | 209/313 [00:30<00:14,  7.02it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:14,  6.99it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:14,  7.01it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:14,  7.02it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:14,  7.04it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:14,  7.05it/s]Evaluating:  69%|██████▊   | 215/313 [00:31<00:13,  7.06it/s]Evaluating:  69%|██████▉   | 216/313 [00:31<00:13,  7.05it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:13,  7.02it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:13,  7.04it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:13,  6.99it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:13,  6.91it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:13,  6.86it/s]Evaluating:  71%|███████   | 222/313 [00:32<00:13,  6.82it/s]Evaluating:  71%|███████   | 223/313 [00:32<00:13,  6.83it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:12,  6.85it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:12,  6.88it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:12,  6.88it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:12,  6.86it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  6.86it/s]Evaluating:  73%|███████▎  | 229/313 [00:33<00:12,  6.89it/s]Evaluating:  73%|███████▎  | 230/313 [00:33<00:11,  6.93it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:11,  6.97it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:11,  6.96it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:11,  6.98it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:11,  6.97it/s]Evaluating:  75%|███████▌  | 235/313 [00:34<00:11,  6.96it/s]Evaluating:  75%|███████▌  | 236/313 [00:34<00:11,  6.94it/s]Evaluating:  76%|███████▌  | 237/313 [00:34<00:10,  6.92it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:10,  6.91it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:10,  6.90it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:10,  6.90it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:10,  6.89it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:10,  6.92it/s]Evaluating:  78%|███████▊  | 243/313 [00:35<00:10,  6.93it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:09,  6.95it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:09,  6.95it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:09,  6.90it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:09,  6.86it/s]Evaluating:  79%|███████▉  | 248/313 [00:36<00:09,  6.87it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:09,  6.89it/s]Evaluating:  80%|███████▉  | 250/313 [00:36<00:09,  6.88it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.87it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:08,  6.89it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:08,  6.85it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:08,  6.82it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.81it/s]Evaluating:  82%|████████▏ | 256/313 [00:37<00:08,  6.80it/s]Evaluating:  82%|████████▏ | 257/313 [00:37<00:08,  6.81it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.82it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:07,  6.83it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:08,  6.41it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:08,  6.40it/s]Evaluating:  84%|████████▎ | 262/313 [00:38<00:07,  6.55it/s]Evaluating:  84%|████████▍ | 263/313 [00:38<00:07,  6.67it/s]Evaluating:  84%|████████▍ | 264/313 [00:38<00:07,  6.76it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.80it/s]Evaluating:  85%|████████▍ | 266/313 [00:39<00:06,  6.84it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:06,  6.87it/s]Evaluating:  86%|████████▌ | 268/313 [00:39<00:06,  6.88it/s]Evaluating:  86%|████████▌ | 269/313 [00:39<00:06,  6.89it/s]Evaluating:  86%|████████▋ | 270/313 [00:39<00:06,  6.90it/s]Evaluating:  87%|████████▋ | 271/313 [00:39<00:06,  6.85it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:06,  6.80it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:05,  6.78it/s]Evaluating:  88%|████████▊ | 274/313 [00:40<00:05,  6.77it/s]Evaluating:  88%|████████▊ | 275/313 [00:40<00:05,  6.76it/s]Evaluating:  88%|████████▊ | 276/313 [00:40<00:05,  6.75it/s]Evaluating:  88%|████████▊ | 277/313 [00:40<00:05,  6.77it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.81it/s]Evaluating:  89%|████████▉ | 279/313 [00:41<00:04,  6.84it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:04,  6.86it/s]Evaluating:  90%|████████▉ | 281/313 [00:41<00:04,  6.87it/s]Evaluating:  90%|█████████ | 282/313 [00:41<00:04,  6.87it/s]Evaluating:  90%|█████████ | 283/313 [00:41<00:04,  6.88it/s]Evaluating:  91%|█████████ | 284/313 [00:41<00:04,  6.90it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:05,  5.31it/s]Evaluating:  91%|█████████▏| 286/313 [00:42<00:04,  5.57it/s]Evaluating:  92%|█████████▏| 287/313 [00:42<00:04,  5.83it/s]Evaluating:  92%|█████████▏| 288/313 [00:42<00:04,  6.02it/s]Evaluating:  92%|█████████▏| 289/313 [00:42<00:03,  6.15it/s]Evaluating:  93%|█████████▎| 290/313 [00:42<00:03,  6.28it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.36it/s]Evaluating:  93%|█████████▎| 292/313 [00:43<00:03,  6.43it/s]Evaluating:  94%|█████████▎| 293/313 [00:43<00:03,  6.53it/s]Evaluating:  94%|█████████▍| 294/313 [00:43<00:02,  6.62it/s]Evaluating:  94%|█████████▍| 295/313 [00:43<00:02,  6.67it/s]Evaluating:  95%|█████████▍| 296/313 [00:43<00:02,  6.72it/s]Evaluating:  95%|█████████▍| 297/313 [00:43<00:02,  6.76it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.79it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.74it/s]Evaluating:  96%|█████████▌| 300/313 [00:44<00:01,  6.71it/s]Evaluating:  96%|█████████▌| 301/313 [00:44<00:01,  6.67it/s]Evaluating:  96%|█████████▋| 302/313 [00:44<00:01,  6.66it/s]Evaluating:  97%|█████████▋| 303/313 [00:44<00:01,  6.64it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  6.66it/s]Evaluating:  97%|█████████▋| 305/313 [00:45<00:01,  6.72it/s]Evaluating:  98%|█████████▊| 306/313 [00:45<00:01,  6.77it/s]Evaluating:  98%|█████████▊| 307/313 [00:45<00:00,  6.80it/s]Evaluating:  98%|█████████▊| 308/313 [00:45<00:00,  6.84it/s]Evaluating:  99%|█████████▊| 309/313 [00:45<00:00,  6.86it/s]Evaluating:  99%|█████████▉| 310/313 [00:45<00:00,  6.51it/s]Evaluating:  99%|█████████▉| 311/313 [00:46<00:00,  6.54it/s]Evaluating: 100%|█████████▉| 312/313 [00:46<00:00,  6.62it/s]Evaluating: 100%|██████████| 313/313 [00:46<00:00,  6.76it/s]
10/08/2021 22:15:46 - INFO - __main__ -   ***** Evaluation result  in no *****
10/08/2021 22:15:46 - INFO - __main__ -     f1 = 0.7461170848267621
10/08/2021 22:15:46 - INFO - __main__ -     loss = 0.993577056894668
10/08/2021 22:15:46 - INFO - __main__ -     precision = 0.7146217418944691
10/08/2021 22:15:46 - INFO - __main__ -     recall = 0.7805165949173726
10/08/2021 22:15:46 - INFO - __main__ -   Language adapter for da not found, using en instead
10/08/2021 22:15:46 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:15:46 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/08/2021 22:15:47 - INFO - __main__ -   ***** Running evaluation  in da *****
10/08/2021 22:15:47 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:15:47 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.69it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.74it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.74it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.74it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.75it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.75it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.74it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.77it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.81it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.83it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.82it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.80it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.76it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.75it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.74it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.74it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.73it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.72it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.73it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.71it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.70it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.70it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.71it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.71it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.71it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.71it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.70it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.70it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.69it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.69it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.69it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.69it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.69it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.68it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.69it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.70it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.70it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.70it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.69it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.69it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.69it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.68it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.69it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.69it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.69it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.69it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.71it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 53/313 [00:08<01:03,  4.09it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:55,  4.63it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:50,  5.10it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:46,  5.49it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:44,  5.80it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:42,  6.04it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:40,  6.22it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:39,  6.35it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:39,  6.45it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.48it/s]Evaluating:  20%|██        | 63/313 [00:09<00:38,  6.55it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.59it/s]Evaluating:  21%|██        | 65/313 [00:10<00:37,  6.62it/s]Evaluating:  21%|██        | 66/313 [00:10<00:37,  6.65it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:36,  6.69it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.71it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.72it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.71it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:36,  6.69it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:35,  6.68it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.67it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.67it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.63it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:35,  6.68it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:34,  6.70it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.72it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.72it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.69it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:35,  6.38it/s]Evaluating:  27%|██▋       | 85/313 [00:13<00:34,  6.57it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:33,  6.76it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:32,  6.91it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:32,  7.01it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:31,  7.08it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:31,  7.13it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:30,  7.16it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:31,  7.04it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:31,  7.02it/s]Evaluating:  30%|███       | 94/313 [00:14<00:30,  7.09it/s]Evaluating:  30%|███       | 95/313 [00:14<00:30,  7.13it/s]Evaluating:  31%|███       | 96/313 [00:14<00:30,  7.15it/s]Evaluating:  31%|███       | 97/313 [00:14<00:30,  7.15it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:30,  7.14it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:30,  7.09it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:30,  7.05it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:30,  7.02it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:30,  6.99it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:30,  6.96it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:30,  6.91it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:30,  6.88it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:30,  6.87it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:30,  6.87it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:29,  6.86it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.79it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.73it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.70it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.67it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.66it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.65it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.64it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.63it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.64it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.65it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:31,  6.09it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:30,  6.36it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:29,  6.54it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.68it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.77it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:27,  6.82it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:27,  6.87it/s]Evaluating:  40%|████      | 126/313 [00:18<00:27,  6.88it/s]Evaluating:  41%|████      | 127/313 [00:19<00:27,  6.87it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.81it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.73it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.68it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.65it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.63it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:27,  6.61it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:27,  6.60it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:27,  6.59it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.62it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.65it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.69it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:25,  6.71it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:25,  6.71it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.72it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.70it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.66it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.65it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.64it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:25,  6.63it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.61it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.59it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.61it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.62it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.64it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.69it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:23,  6.72it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:23,  6.70it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.68it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.67it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.67it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.62it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:23,  6.61it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.64it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:22,  6.67it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.66it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.62it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.60it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:22,  6.62it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:22,  6.62it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:21,  6.64it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.68it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.71it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.75it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:20,  6.78it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.79it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:21,  6.46it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:21,  6.55it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.63it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.72it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:20,  6.78it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:19,  6.80it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:19,  6.82it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:19,  6.88it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.92it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:18,  6.97it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:18,  7.01it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:18,  7.02it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:18,  7.05it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:17,  7.06it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:17,  7.06it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:17,  7.06it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:17,  7.07it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:17,  7.04it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:17,  7.04it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  7.05it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:17,  7.05it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:16,  7.06it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:16,  7.07it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:16,  7.07it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:16,  7.09it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:16,  7.09it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:16,  7.07it/s]Evaluating:  64%|██████▍   | 200/313 [00:29<00:16,  7.06it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:15,  7.04it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:15,  7.03it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:15,  7.00it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:15,  6.98it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:15,  7.00it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:15,  7.02it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:15,  7.03it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:14,  7.03it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:14,  7.03it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:14,  7.02it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:14,  6.99it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:14,  6.99it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:14,  6.92it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:14,  6.62it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.64it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.75it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:14,  6.79it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:13,  6.80it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:13,  6.81it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:13,  6.84it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:13,  6.88it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.92it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:12,  6.96it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:12,  6.97it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:12,  7.00it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:12,  7.00it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:12,  7.02it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  7.00it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:11,  7.03it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:11,  6.99it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:11,  6.94it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:11,  6.94it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:11,  6.95it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:11,  6.93it/s]Evaluating:  75%|███████▌  | 235/313 [00:34<00:11,  6.93it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.93it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.89it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:10,  6.87it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:10,  6.90it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:10,  6.93it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:10,  6.95it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:10,  6.98it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.99it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:09,  6.99it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:09,  6.96it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:09,  6.93it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:09,  6.92it/s]Evaluating:  79%|███████▉  | 248/313 [00:36<00:09,  6.93it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:09,  6.94it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.95it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:08,  6.97it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:08,  6.97it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:08,  6.86it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:08,  6.72it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.68it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.66it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.65it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.63it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:08,  6.60it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:08,  6.60it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:07,  6.64it/s]Evaluating:  84%|████████▎ | 262/313 [00:38<00:07,  6.69it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.74it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.79it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.85it/s]Evaluating:  85%|████████▍ | 266/313 [00:39<00:08,  5.70it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:07,  6.03it/s]Evaluating:  86%|████████▌ | 268/313 [00:39<00:07,  6.29it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.34it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.31it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.28it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:06,  6.25it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:06,  6.23it/s]Evaluating:  88%|████████▊ | 274/313 [00:40<00:06,  6.26it/s]Evaluating:  88%|████████▊ | 275/313 [00:40<00:06,  6.29it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.31it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.35it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.39it/s]Evaluating:  89%|████████▉ | 279/313 [00:41<00:05,  6.39it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:05,  6.39it/s]Evaluating:  90%|████████▉ | 281/313 [00:41<00:04,  6.42it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.45it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.43it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.46it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.51it/s]Evaluating:  91%|█████████▏| 286/313 [00:42<00:04,  6.53it/s]Evaluating:  92%|█████████▏| 287/313 [00:42<00:03,  6.56it/s]Evaluating:  92%|█████████▏| 288/313 [00:42<00:03,  6.56it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.61it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.64it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.67it/s]Evaluating:  93%|█████████▎| 292/313 [00:43<00:03,  6.71it/s]Evaluating:  94%|█████████▎| 293/313 [00:43<00:02,  6.75it/s]Evaluating:  94%|█████████▍| 294/313 [00:43<00:02,  6.79it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.83it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.86it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.87it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.87it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.85it/s]Evaluating:  96%|█████████▌| 300/313 [00:44<00:01,  6.85it/s]Evaluating:  96%|█████████▌| 301/313 [00:44<00:01,  6.83it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.85it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.87it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  6.76it/s]Evaluating:  97%|█████████▋| 305/313 [00:45<00:01,  6.61it/s]Evaluating:  98%|█████████▊| 306/313 [00:45<00:01,  6.50it/s]Evaluating:  98%|█████████▊| 307/313 [00:45<00:00,  6.43it/s]Evaluating:  98%|█████████▊| 308/313 [00:45<00:00,  6.41it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.39it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.39it/s]Evaluating:  99%|█████████▉| 311/313 [00:46<00:00,  6.40it/s]Evaluating: 100%|█████████▉| 312/313 [00:46<00:00,  6.43it/s]Evaluating: 100%|██████████| 313/313 [00:46<00:00,  6.70it/s]
10/08/2021 22:16:35 - INFO - __main__ -   ***** Evaluation result  in da *****
10/08/2021 22:16:35 - INFO - __main__ -     f1 = 0.7959407375976805
10/08/2021 22:16:35 - INFO - __main__ -     loss = 0.8267616137814598
10/08/2021 22:16:35 - INFO - __main__ -     precision = 0.7717890694644449
10/08/2021 22:16:35 - INFO - __main__ -     recall = 0.8216527960286173
10/08/2021 22:16:35 - INFO - __main__ -   Language adapter for ru not found, using en instead
10/08/2021 22:16:35 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:16:35 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
10/08/2021 22:16:37 - INFO - __main__ -   ***** Running evaluation  in ru *****
10/08/2021 22:16:37 - INFO - __main__ -     Num examples = 10002
10/08/2021 22:16:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:47,  6.63it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.71it/s]Evaluating:   1%|          | 3/313 [00:00<00:46,  6.73it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.75it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.75it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.76it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.76it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.76it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.76it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.75it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.75it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.74it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.74it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.73it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.72it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.73it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.74it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.74it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.74it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.72it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.72it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.71it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.71it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.70it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.70it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.71it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.72it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.72it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.71it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.72it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.71it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.72it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.72it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.72it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.73it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.73it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.72it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.71it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.71it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.71it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.71it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.69it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.69it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.69it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:40,  6.67it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.68it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.68it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.67it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.68it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.69it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:39,  6.69it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.69it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.69it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.69it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.68it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.68it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.68it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:38,  6.68it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.68it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.68it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.65it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.65it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.65it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.65it/s]Evaluating:  21%|██        | 66/313 [00:09<00:37,  6.65it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.66it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.66it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.66it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.66it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.67it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.66it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:36,  6.67it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.67it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.66it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.62it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:35,  6.65it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:35,  6.65it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.66it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.62it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.59it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.57it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:34,  6.62it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:33,  6.66it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.68it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.72it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:32,  6.76it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:32,  6.78it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:32,  6.82it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:32,  6.82it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.70it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.78it/s]Evaluating:  31%|███       | 97/313 [00:14<00:31,  6.86it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:31,  6.89it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:31,  6.89it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:30,  6.88it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:30,  6.87it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:30,  6.88it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:30,  6.91it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:30,  6.91it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:30,  6.89it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:30,  6.90it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:29,  6.90it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:29,  6.91it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:29,  6.90it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.76it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.73it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:29,  6.80it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:29,  6.84it/s]Evaluating:  36%|███▋      | 114/313 [00:16<00:29,  6.83it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.81it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:28,  6.82it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:28,  6.85it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:28,  6.88it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:27,  6.96it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:27,  6.94it/s]Evaluating:  39%|███▊      | 121/313 [00:17<00:27,  6.96it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:27,  7.01it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:26,  7.06it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:26,  7.10it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:26,  7.13it/s]Evaluating:  40%|████      | 126/313 [00:18<00:26,  7.14it/s]Evaluating:  41%|████      | 127/313 [00:18<00:26,  7.12it/s]Evaluating:  41%|████      | 128/313 [00:18<00:26,  6.95it/s]Evaluating:  41%|████      | 129/313 [00:19<00:26,  6.83it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:26,  6.89it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:26,  6.93it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:25,  6.98it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:25,  7.02it/s]Evaluating:  43%|████▎     | 134/313 [00:19<00:25,  7.04it/s]Evaluating:  43%|████▎     | 135/313 [00:19<00:25,  7.04it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:25,  7.01it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:28,  6.08it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:28,  6.21it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:28,  6.19it/s]Evaluating:  45%|████▍     | 140/313 [00:20<00:27,  6.21it/s]Evaluating:  45%|████▌     | 141/313 [00:20<00:27,  6.33it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:26,  6.38it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:26,  6.44it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:26,  6.48it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.50it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:25,  6.51it/s]Evaluating:  47%|████▋     | 147/313 [00:21<00:25,  6.53it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.53it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.56it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.58it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.59it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.63it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:23,  6.67it/s]Evaluating:  49%|████▉     | 154/313 [00:22<00:23,  6.63it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:24,  6.45it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:24,  6.40it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:24,  6.44it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.46it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:23,  6.50it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:23,  6.51it/s]Evaluating:  51%|█████▏    | 161/313 [00:23<00:23,  6.52it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:23,  6.52it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.53it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.53it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:22,  6.52it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:22,  6.52it/s]Evaluating:  53%|█████▎    | 167/313 [00:24<00:22,  6.55it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.59it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:22,  6.45it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:22,  6.41it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:21,  6.51it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:21,  6.61it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:20,  6.68it/s]Evaluating:  56%|█████▌    | 174/313 [00:25<00:20,  6.69it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.73it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.79it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:19,  6.82it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:19,  6.84it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:19,  6.86it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:19,  6.86it/s]Evaluating:  58%|█████▊    | 181/313 [00:26<00:19,  6.81it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.80it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.79it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:19,  6.71it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:19,  6.66it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:19,  6.59it/s]Evaluating:  60%|█████▉    | 187/313 [00:27<00:19,  6.51it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:19,  6.46it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.54it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.64it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:18,  6.73it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  6.73it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:17,  6.69it/s]Evaluating:  62%|██████▏   | 194/313 [00:28<00:17,  6.72it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.67it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.61it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.58it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:17,  6.60it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:17,  6.60it/s]Evaluating:  64%|██████▍   | 200/313 [00:29<00:17,  6.63it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:16,  6.67it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.76it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:17,  6.28it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:17,  6.30it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:16,  6.48it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:16,  6.63it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:15,  6.75it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.82it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.86it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:14,  6.90it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:14,  6.87it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:14,  6.84it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:14,  6.82it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:14,  6.85it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.90it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:13,  6.94it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:13,  6.96it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:13,  6.95it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:13,  6.92it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:13,  6.90it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:13,  6.93it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.94it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:12,  6.95it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:12,  6.97it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:12,  6.98it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:12,  6.97it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:12,  6.99it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  6.92it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.84it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.83it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.78it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:12,  6.74it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:11,  6.71it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:11,  6.68it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.60it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.55it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.51it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.46it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:11,  6.46it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:11,  6.45it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:11,  6.44it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:11,  6.42it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:11,  6.21it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:11,  6.24it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.43it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:10,  6.57it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:09,  6.70it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.79it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.85it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.88it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:08,  6.89it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:08,  6.89it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:08,  6.90it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:08,  6.90it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.91it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.88it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.84it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.83it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:07,  6.80it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:07,  6.81it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:07,  6.69it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.60it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.56it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.52it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.47it/s]Evaluating:  85%|████████▍ | 266/313 [00:39<00:07,  6.45it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:07,  6.44it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.45it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.47it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.46it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.42it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:06,  6.48it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:06,  6.52it/s]Evaluating:  88%|████████▊ | 274/313 [00:40<00:05,  6.57it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.62it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.63it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.66it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.72it/s]Evaluating:  89%|████████▉ | 279/313 [00:41<00:05,  6.78it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:04,  6.81it/s]Evaluating:  90%|████████▉ | 281/313 [00:41<00:04,  6.84it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.81it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.81it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.52it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.49it/s]Evaluating:  91%|█████████▏| 286/313 [00:42<00:04,  6.44it/s]Evaluating:  92%|█████████▏| 287/313 [00:42<00:04,  6.23it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:04,  5.91it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:04,  5.72it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  5.78it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  5.78it/s]Evaluating:  93%|█████████▎| 292/313 [00:43<00:03,  5.87it/s]Evaluating:  94%|█████████▎| 293/313 [00:43<00:03,  5.98it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:03,  6.03it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.10it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.15it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.18it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.23it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.27it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  6.30it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.30it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.30it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.38it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  4.78it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  5.17it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  5.51it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:01,  5.79it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  5.94it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.06it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.15it/s]Evaluating:  99%|█████████▉| 311/313 [00:46<00:00,  6.20it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.17it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.63it/s]
10/08/2021 22:17:25 - INFO - __main__ -   ***** Evaluation result  in ru *****
10/08/2021 22:17:25 - INFO - __main__ -     f1 = 0.5660818713450292
10/08/2021 22:17:25 - INFO - __main__ -     loss = 2.3972737564446445
10/08/2021 22:17:25 - INFO - __main__ -     precision = 0.5760435263113152
10/08/2021 22:17:25 - INFO - __main__ -     recall = 0.5564588979223125
10/08/2021 22:17:25 - INFO - __main__ -   Language adapter for bg not found, using en instead
10/08/2021 22:17:25 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:17:25 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/08/2021 22:17:27 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/08/2021 22:17:27 - INFO - __main__ -     Num examples = 10004
10/08/2021 22:17:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.74it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.75it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.75it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.76it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.76it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.75it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.75it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.75it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.75it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.75it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.73it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.70it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.71it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.71it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.72it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.72it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.71it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.71it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.71it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.69it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.69it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.69it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.69it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.69it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:43,  6.69it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.70it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.71it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.72it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.72it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.72it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.72it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.72it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.73it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.72it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.72it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.72it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.72it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.72it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.70it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.65it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:40,  6.64it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.65it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.64it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.65it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.66it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.67it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:39,  6.67it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:39,  6.59it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:39,  6.62it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:47,  5.45it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:44,  5.77it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:42,  6.02it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:41,  6.20it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:40,  6.34it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:39,  6.43it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.51it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.52it/s]Evaluating:  20%|██        | 63/313 [00:09<00:38,  6.56it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.59it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.60it/s]Evaluating:  21%|██        | 66/313 [00:09<00:37,  6.63it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:37,  6.64it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.65it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.65it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.65it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.65it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.65it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:36,  6.66it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.67it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.67it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.63it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:35,  6.64it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:35,  6.64it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.64it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.64it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.60it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.58it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:35,  6.50it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:34,  6.49it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:34,  6.53it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:49,  4.59it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:44,  5.06it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:40,  5.46it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:38,  5.78it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:36,  6.03it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:35,  6.21it/s]Evaluating:  30%|███       | 94/313 [00:14<00:34,  6.36it/s]Evaluating:  30%|███       | 95/313 [00:14<00:33,  6.46it/s]Evaluating:  31%|███       | 96/313 [00:14<00:33,  6.51it/s]Evaluating:  31%|███       | 97/313 [00:14<00:33,  6.54it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:32,  6.55it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.57it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:32,  6.57it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:32,  6.60it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.61it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.61it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.61it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:31,  6.62it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:31,  6.61it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.61it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:31,  6.61it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.60it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.60it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.60it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:30,  6.60it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.59it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:30,  6.61it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:30,  6.60it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.59it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.60it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:29,  6.61it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:29,  6.64it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:28,  6.69it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.73it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.75it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.77it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.64it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:28,  6.63it/s]Evaluating:  40%|████      | 126/313 [00:19<00:27,  6.78it/s]Evaluating:  41%|████      | 127/313 [00:19<00:27,  6.88it/s]Evaluating:  41%|████      | 128/313 [00:19<00:26,  6.94it/s]Evaluating:  41%|████      | 129/313 [00:19<00:26,  6.99it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:26,  7.03it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:25,  7.06it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:25,  7.07it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:25,  7.07it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:25,  7.08it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:25,  7.05it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:25,  7.01it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:25,  7.00it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:25,  6.91it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:25,  6.79it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:25,  6.72it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.69it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.64it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.62it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.59it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.58it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.58it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.57it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.59it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.61it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.61it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:24,  6.63it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.63it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.62it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.61it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.58it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.56it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.56it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:23,  6.55it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.56it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.55it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:23,  6.55it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.54it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.54it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:22,  6.55it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:22,  6.54it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:22,  6.55it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:22,  6.57it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.57it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.59it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:21,  6.61it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:21,  6.60it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:21,  6.58it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:21,  6.56it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.57it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.57it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:20,  6.56it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:20,  6.55it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:21,  6.35it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:20,  6.35it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:20,  6.34it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:20,  6.41it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:20,  6.48it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:19,  6.49it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:19,  6.49it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:19,  6.49it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:19,  6.52it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:19,  6.55it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.56it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.56it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:18,  6.58it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:18,  6.60it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:18,  6.56it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:18,  6.55it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:18,  6.55it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.53it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.53it/s]Evaluating:  63%|██████▎   | 198/313 [00:30<00:17,  6.52it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:17,  6.51it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:17,  6.50it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:17,  6.50it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:17,  6.50it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.50it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:16,  6.50it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:16,  6.49it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:16,  6.49it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:16,  6.53it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:16,  6.55it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.55it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.53it/s]Evaluating:  67%|██████▋   | 211/313 [00:32<00:15,  6.50it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:15,  6.49it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:15,  6.45it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:15,  6.46it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:15,  6.50it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.51it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:14,  6.49it/s]Evaluating:  70%|██████▉   | 218/313 [00:33<00:14,  6.49it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:14,  6.50it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:15,  5.84it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:15,  6.01it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:14,  6.13it/s]Evaluating:  71%|███████   | 223/313 [00:34<00:14,  6.23it/s]Evaluating:  72%|███████▏  | 224/313 [00:34<00:14,  6.32it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:13,  6.39it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:13,  6.43it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:13,  6.47it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:13,  6.51it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.52it/s]Evaluating:  73%|███████▎  | 230/313 [00:35<00:12,  6.50it/s]Evaluating:  74%|███████▍  | 231/313 [00:35<00:12,  6.51it/s]Evaluating:  74%|███████▍  | 232/313 [00:35<00:12,  6.53it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:12,  6.52it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:12,  6.49it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:12,  6.48it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.49it/s]Evaluating:  76%|███████▌  | 237/313 [00:36<00:11,  6.51it/s]Evaluating:  76%|███████▌  | 238/313 [00:36<00:11,  6.49it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:11,  6.49it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.49it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.49it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.48it/s]Evaluating:  78%|███████▊  | 243/313 [00:37<00:10,  6.46it/s]Evaluating:  78%|███████▊  | 244/313 [00:37<00:10,  6.47it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:10,  6.46it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:10,  6.45it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:10,  6.44it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:10,  6.47it/s]Evaluating:  80%|███████▉  | 249/313 [00:38<00:09,  6.47it/s]Evaluating:  80%|███████▉  | 250/313 [00:38<00:09,  6.48it/s]Evaluating:  80%|████████  | 251/313 [00:38<00:09,  6.50it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:09,  6.48it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:09,  6.51it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:09,  6.50it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.50it/s]Evaluating:  82%|████████▏ | 256/313 [00:39<00:08,  6.54it/s]Evaluating:  82%|████████▏ | 257/313 [00:39<00:08,  6.59it/s]Evaluating:  82%|████████▏ | 258/313 [00:39<00:08,  6.49it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.39it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.48it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:07,  6.64it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.74it/s]Evaluating:  84%|████████▍ | 263/313 [00:40<00:07,  6.81it/s]Evaluating:  84%|████████▍ | 264/313 [00:40<00:07,  6.86it/s]Evaluating:  85%|████████▍ | 265/313 [00:40<00:06,  6.87it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:06,  6.89it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:06,  6.92it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.93it/s]Evaluating:  86%|████████▌ | 269/313 [00:41<00:06,  6.80it/s]Evaluating:  86%|████████▋ | 270/313 [00:41<00:06,  6.80it/s]Evaluating:  87%|████████▋ | 271/313 [00:41<00:06,  6.84it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:05,  6.88it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:05,  6.90it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.92it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.94it/s]Evaluating:  88%|████████▊ | 276/313 [00:42<00:05,  6.92it/s]Evaluating:  88%|████████▊ | 277/313 [00:42<00:05,  6.91it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.92it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:04,  6.92it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:04,  6.93it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.94it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.95it/s]Evaluating:  90%|█████████ | 283/313 [00:43<00:04,  6.94it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.95it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.96it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:03,  6.90it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.30it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.48it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.60it/s]Evaluating:  93%|█████████▎| 290/313 [00:44<00:03,  6.71it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.79it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.81it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:02,  6.84it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.86it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.87it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.88it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:02,  6.81it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.75it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.70it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:01,  6.68it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.68it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.69it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.69it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.71it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.71it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.72it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.75it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.75it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.78it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  6.82it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.85it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.86it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  7.51it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.60it/s]
10/08/2021 22:18:16 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/08/2021 22:18:16 - INFO - __main__ -     f1 = 0.6828637348807941
10/08/2021 22:18:16 - INFO - __main__ -     loss = 1.536010288582823
10/08/2021 22:18:16 - INFO - __main__ -     precision = 0.6715022344448264
10/08/2021 22:18:16 - INFO - __main__ -     recall = 0.694616314629116
10/08/2021 22:18:16 - INFO - __main__ -   Language adapter for uk not found, using en instead
10/08/2021 22:18:16 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:18:16 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/08/2021 22:18:17 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/08/2021 22:18:17 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:18:17 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.78it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.77it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.78it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.77it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.77it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.77it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.77it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.77it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.76it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.76it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.75it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.75it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.75it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.74it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.73it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:52,  5.56it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:49,  5.86it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:47,  6.10it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:45,  6.27it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:44,  6.40it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:43,  6.50it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:43,  6.57it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.63it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.66it/s]Evaluating:  10%|█         | 32/313 [00:04<00:42,  6.67it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.70it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.71it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.71it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.71it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.72it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:40,  6.72it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.73it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.72it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.72it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:39,  6.71it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.70it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.70it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 56/313 [00:08<01:10,  3.64it/s]Evaluating:  18%|█▊        | 57/313 [00:08<01:01,  4.19it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:54,  4.71it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:49,  5.14it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:45,  5.52it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:43,  5.86it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:41,  6.07it/s]Evaluating:  20%|██        | 63/313 [00:09<00:39,  6.26it/s]Evaluating:  20%|██        | 64/313 [00:10<00:38,  6.44it/s]Evaluating:  21%|██        | 65/313 [00:10<00:37,  6.54it/s]Evaluating:  21%|██        | 66/313 [00:10<00:37,  6.57it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:37,  6.59it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:37,  6.61it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.63it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.62it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:36,  6.63it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:36,  6.63it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:36,  6.64it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.65it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.65it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.62it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.62it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:35,  6.64it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:35,  6.65it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:35,  6.65it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.65it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.65it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.60it/s]Evaluating:  27%|██▋       | 84/313 [00:13<00:34,  6.57it/s]Evaluating:  27%|██▋       | 85/313 [00:13<00:34,  6.55it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:34,  6.56it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:34,  6.46it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:34,  6.57it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.71it/s]Evaluating:  29%|██▉       | 91/313 [00:14<00:32,  6.73it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:32,  6.76it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:32,  6.78it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.77it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.77it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.71it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.67it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:32,  6.56it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.53it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:32,  6.53it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:32,  6.56it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.55it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.57it/s]Evaluating:  33%|███▎      | 104/313 [00:16<00:31,  6.58it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:31,  6.61it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:31,  6.63it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.64it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.61it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:30,  6.62it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:30,  6.62it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.62it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:30,  6.62it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.61it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.61it/s]Evaluating:  37%|███▋      | 117/313 [00:18<00:29,  6.61it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:29,  6.61it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:29,  6.61it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:38,  4.97it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:35,  5.37it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:33,  5.69it/s]Evaluating:  39%|███▉      | 123/313 [00:19<00:32,  5.94it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:30,  6.11it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:30,  6.25it/s]Evaluating:  40%|████      | 126/313 [00:19<00:29,  6.32it/s]Evaluating:  41%|████      | 127/313 [00:19<00:29,  6.39it/s]Evaluating:  41%|████      | 128/313 [00:19<00:28,  6.45it/s]Evaluating:  41%|████      | 129/313 [00:20<00:28,  6.48it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:28,  6.51it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:27,  6.53it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:27,  6.53it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:27,  6.54it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:27,  6.53it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:27,  6.54it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:27,  6.55it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:26,  6.55it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:26,  6.55it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:26,  6.55it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.58it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.58it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:26,  6.57it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:25,  6.57it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:25,  6.57it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:25,  6.57it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.56it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.56it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.56it/s]Evaluating:  48%|████▊     | 149/313 [00:23<00:24,  6.58it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:24,  6.58it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:25,  6.42it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:27,  5.87it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:25,  6.18it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.43it/s]Evaluating:  50%|████▉     | 155/313 [00:24<00:23,  6.63it/s]Evaluating:  50%|████▉     | 156/313 [00:24<00:23,  6.75it/s]Evaluating:  50%|█████     | 157/313 [00:24<00:22,  6.84it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:22,  6.90it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:22,  6.93it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:21,  6.96it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:21,  6.96it/s]Evaluating:  52%|█████▏    | 162/313 [00:25<00:21,  6.94it/s]Evaluating:  52%|█████▏    | 163/313 [00:25<00:21,  6.94it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:21,  6.95it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:21,  6.95it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:21,  6.96it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:20,  6.99it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:20,  7.00it/s]Evaluating:  54%|█████▍    | 169/313 [00:26<00:20,  7.00it/s]Evaluating:  54%|█████▍    | 170/313 [00:26<00:20,  7.02it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:20,  6.98it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:20,  6.93it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:20,  6.92it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.89it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.85it/s]Evaluating:  56%|█████▌    | 176/313 [00:27<00:20,  6.84it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:19,  6.83it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:20,  6.74it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:20,  6.67it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:19,  6.65it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.64it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.63it/s]Evaluating:  58%|█████▊    | 183/313 [00:28<00:19,  6.61it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:19,  6.60it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:19,  6.66it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:18,  6.70it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:18,  6.78it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.83it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.81it/s]Evaluating:  61%|██████    | 190/313 [00:29<00:18,  6.76it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:18,  6.76it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:17,  6.79it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:17,  6.82it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:17,  6.83it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.82it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.85it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:16,  6.86it/s]Evaluating:  63%|██████▎   | 198/313 [00:30<00:16,  6.89it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:16,  6.91it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:16,  6.93it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:16,  6.98it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:15,  7.00it/s]Evaluating:  65%|██████▍   | 203/313 [00:31<00:15,  6.95it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:15,  6.89it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:15,  6.86it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:15,  6.91it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:15,  6.95it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.97it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:14,  7.00it/s]Evaluating:  67%|██████▋   | 210/313 [00:32<00:14,  7.02it/s]Evaluating:  67%|██████▋   | 211/313 [00:32<00:14,  7.01it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:14,  7.01it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:14,  6.94it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.92it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.93it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:13,  6.94it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:13,  6.94it/s]Evaluating:  70%|██████▉   | 218/313 [00:33<00:13,  6.92it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:13,  6.88it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:14,  6.22it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:14,  6.40it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.53it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.62it/s]Evaluating:  72%|███████▏  | 224/313 [00:34<00:13,  6.71it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:12,  6.81it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:12,  6.84it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.82it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.73it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.70it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.66it/s]Evaluating:  74%|███████▍  | 231/313 [00:35<00:12,  6.66it/s]Evaluating:  74%|███████▍  | 232/313 [00:35<00:12,  6.66it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:12,  6.65it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:12,  6.36it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:12,  6.29it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:12,  6.35it/s]Evaluating:  76%|███████▌  | 237/313 [00:36<00:11,  6.47it/s]Evaluating:  76%|███████▌  | 238/313 [00:36<00:11,  6.53it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:11,  6.63it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:10,  6.69it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:10,  6.78it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.80it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.73it/s]Evaluating:  78%|███████▊  | 244/313 [00:37<00:10,  6.72it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:10,  6.73it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:09,  6.73it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.73it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.76it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.78it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.70it/s]Evaluating:  80%|████████  | 251/313 [00:38<00:09,  6.70it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:09,  6.70it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:08,  6.71it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.63it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.57it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.53it/s]Evaluating:  82%|████████▏ | 257/313 [00:39<00:08,  6.50it/s]Evaluating:  82%|████████▏ | 258/313 [00:39<00:08,  6.48it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.48it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.51it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:07,  6.55it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.53it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.53it/s]Evaluating:  84%|████████▍ | 264/313 [00:40<00:07,  6.51it/s]Evaluating:  85%|████████▍ | 265/313 [00:40<00:07,  6.52it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.49it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:07,  6.48it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.48it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.46it/s]Evaluating:  86%|████████▋ | 270/313 [00:41<00:06,  6.44it/s]Evaluating:  87%|████████▋ | 271/313 [00:41<00:06,  6.43it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.48it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.48it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:06,  6.45it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.47it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.52it/s]Evaluating:  88%|████████▊ | 277/313 [00:42<00:05,  6.58it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.58it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.55it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.56it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.58it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.54it/s]Evaluating:  90%|█████████ | 283/313 [00:43<00:04,  6.49it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.46it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.48it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.43it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:03,  6.51it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.56it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.65it/s]Evaluating:  93%|█████████▎| 290/313 [00:44<00:03,  6.73it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.73it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.73it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:02,  6.74it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.76it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.73it/s]Evaluating:  95%|█████████▍| 296/313 [00:45<00:02,  6.71it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:02,  6.72it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.72it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.74it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:01,  6.78it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.79it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.75it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.69it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.68it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.64it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.69it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.21it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  5.83it/s]Evaluating:  99%|█████████▊| 309/313 [00:47<00:00,  5.45it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  5.34it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  5.35it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  5.52it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.56it/s]
10/08/2021 22:19:06 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/08/2021 22:19:06 - INFO - __main__ -     f1 = 0.600195429945881
10/08/2021 22:19:06 - INFO - __main__ -     loss = 1.9585030170294424
10/08/2021 22:19:06 - INFO - __main__ -     precision = 0.6054287663962393
10/08/2021 22:19:06 - INFO - __main__ -     recall = 0.5950517922348908
10/08/2021 22:19:06 - INFO - __main__ -   Language adapter for be not found, using en instead
10/08/2021 22:19:06 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:19:06 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/08/2021 22:19:06 - INFO - __main__ -   ***** Running evaluation  in be *****
10/08/2021 22:19:06 - INFO - __main__ -     Num examples = 1001
10/08/2021 22:19:06 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.77it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.76it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.77it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.76it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.76it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.76it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.77it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.76it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.75it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.75it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.75it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.74it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.75it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.75it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.75it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.74it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.75it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.74it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.74it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.73it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.74it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.74it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.74it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.74it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.74it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.74it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.74it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.74it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  5.93it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.28it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.52it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.83it/s]
10/08/2021 22:19:11 - INFO - __main__ -   ***** Evaluation result  in be *****
10/08/2021 22:19:11 - INFO - __main__ -     f1 = 0.5837049047426023
10/08/2021 22:19:11 - INFO - __main__ -     loss = 1.638931890949607
10/08/2021 22:19:11 - INFO - __main__ -     precision = 0.576
10/08/2021 22:19:11 - INFO - __main__ -     recall = 0.5916187345932621
10/08/2021 22:19:11 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:19:24 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:19:24 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:19:40 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 22:19:42 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:19:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 22:19:42 - INFO - __main__ -   Seed = 32
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:19:52 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 22:20:13 - INFO - root -   save model
10/08/2021 22:20:13 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:20:18 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:20:18 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:20:33 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:20:33 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 22:20:33 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/08/2021 22:20:33 - INFO - root -   Trying to decide if add adapter
10/08/2021 22:20:33 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 22:20:33 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 22:20:33 - INFO - __main__ -   Language = en
10/08/2021 22:20:33 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 22:20:49 - INFO - __main__ -   Language adapter for ar not found, using en instead
10/08/2021 22:20:49 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:20:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
10/08/2021 22:20:51 - INFO - __main__ -   ***** Running evaluation  in ar *****
10/08/2021 22:20:51 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:20:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:49,  6.34it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.87it/s]Evaluating:   1%|          | 3/313 [00:00<00:43,  7.17it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:42,  7.31it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:41,  7.38it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:41,  7.43it/s]Evaluating:   2%|▏         | 7/313 [00:00<00:41,  7.46it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:40,  7.49it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:40,  7.49it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:40,  7.49it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:40,  7.52it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:40,  7.50it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:39,  7.50it/s]Evaluating:   4%|▍         | 14/313 [00:01<00:39,  7.51it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:39,  7.52it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:39,  7.50it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:39,  7.49it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:39,  7.51it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:39,  7.52it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:39,  7.49it/s]Evaluating:   7%|▋         | 21/313 [00:02<00:39,  7.47it/s]Evaluating:   7%|▋         | 22/313 [00:02<00:39,  7.46it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:38,  7.46it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:38,  7.45it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:38,  7.44it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:38,  7.45it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:38,  7.44it/s]Evaluating:   9%|▉         | 28/313 [00:03<00:38,  7.45it/s]Evaluating:   9%|▉         | 29/313 [00:03<00:38,  7.46it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:37,  7.45it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:37,  7.45it/s]Evaluating:  10%|█         | 32/313 [00:04<00:37,  7.44it/s]Evaluating:  11%|█         | 33/313 [00:04<00:37,  7.45it/s]Evaluating:  11%|█         | 34/313 [00:04<00:37,  7.44it/s]Evaluating:  11%|█         | 35/313 [00:04<00:37,  7.44it/s]Evaluating:  12%|█▏        | 36/313 [00:04<00:37,  7.44it/s]Evaluating:  12%|█▏        | 37/313 [00:04<00:37,  7.45it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:37,  7.41it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:36,  7.42it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:36,  7.41it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:36,  7.43it/s]Evaluating:  13%|█▎        | 42/313 [00:05<00:36,  7.41it/s]Evaluating:  14%|█▎        | 43/313 [00:05<00:36,  7.41it/s]Evaluating:  14%|█▍        | 44/313 [00:05<00:36,  7.41it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:36,  7.41it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:36,  7.41it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:35,  7.42it/s]Evaluating:  15%|█▌        | 48/313 [00:06<00:35,  7.39it/s]Evaluating:  16%|█▌        | 49/313 [00:06<00:35,  7.38it/s]Evaluating:  16%|█▌        | 50/313 [00:06<00:35,  7.38it/s]Evaluating:  16%|█▋        | 51/313 [00:06<00:35,  7.36it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:35,  7.38it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:35,  7.38it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:35,  7.38it/s]Evaluating:  18%|█▊        | 55/313 [00:07<00:35,  7.37it/s]Evaluating:  18%|█▊        | 56/313 [00:07<00:34,  7.39it/s]Evaluating:  18%|█▊        | 57/313 [00:07<00:34,  7.38it/s]Evaluating:  19%|█▊        | 58/313 [00:07<00:34,  7.39it/s]Evaluating:  19%|█▉        | 59/313 [00:07<00:34,  7.39it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:34,  7.36it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:34,  7.37it/s]Evaluating:  20%|█▉        | 62/313 [00:08<00:34,  7.33it/s]Evaluating:  20%|██        | 63/313 [00:08<00:33,  7.36it/s]Evaluating:  20%|██        | 64/313 [00:08<00:33,  7.37it/s]Evaluating:  21%|██        | 65/313 [00:08<00:33,  7.35it/s]Evaluating:  21%|██        | 66/313 [00:08<00:33,  7.34it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:33,  7.36it/s]Evaluating:  22%|██▏       | 68/313 [00:09<00:33,  7.33it/s]Evaluating:  22%|██▏       | 69/313 [00:09<00:33,  7.34it/s]Evaluating:  22%|██▏       | 70/313 [00:09<00:33,  7.33it/s]Evaluating:  23%|██▎       | 71/313 [00:09<00:32,  7.34it/s]Evaluating:  23%|██▎       | 72/313 [00:09<00:32,  7.33it/s]Evaluating:  23%|██▎       | 73/313 [00:09<00:32,  7.33it/s]Evaluating:  24%|██▎       | 74/313 [00:09<00:32,  7.34it/s]Evaluating:  24%|██▍       | 75/313 [00:10<00:32,  7.34it/s]Evaluating:  24%|██▍       | 76/313 [00:10<00:32,  7.29it/s]Evaluating:  25%|██▍       | 77/313 [00:10<00:32,  7.31it/s]Evaluating:  25%|██▍       | 78/313 [00:10<00:32,  7.31it/s]Evaluating:  25%|██▌       | 79/313 [00:10<00:31,  7.32it/s]Evaluating:  26%|██▌       | 80/313 [00:10<00:31,  7.32it/s]Evaluating:  26%|██▌       | 81/313 [00:10<00:31,  7.32it/s]Evaluating:  26%|██▌       | 82/313 [00:11<00:31,  7.32it/s]Evaluating:  27%|██▋       | 83/313 [00:11<00:31,  7.28it/s]Evaluating:  27%|██▋       | 84/313 [00:11<00:31,  7.26it/s]Evaluating:  27%|██▋       | 85/313 [00:11<00:31,  7.24it/s]Evaluating:  27%|██▋       | 86/313 [00:11<00:31,  7.26it/s]Evaluating:  28%|██▊       | 87/313 [00:11<00:31,  7.27it/s]Evaluating:  28%|██▊       | 88/313 [00:11<00:30,  7.28it/s]Evaluating:  28%|██▊       | 89/313 [00:12<00:30,  7.29it/s]Evaluating:  29%|██▉       | 90/313 [00:12<00:30,  7.31it/s]Evaluating:  29%|██▉       | 91/313 [00:12<00:30,  7.31it/s]Evaluating:  29%|██▉       | 92/313 [00:12<00:30,  7.27it/s]Evaluating:  30%|██▉       | 93/313 [00:12<00:30,  7.25it/s]Evaluating:  30%|███       | 94/313 [00:12<00:30,  7.24it/s]Evaluating:  30%|███       | 95/313 [00:12<00:30,  7.23it/s]Evaluating:  31%|███       | 96/313 [00:13<00:30,  7.21it/s]Evaluating:  31%|███       | 97/313 [00:13<00:29,  7.21it/s]Evaluating:  31%|███▏      | 98/313 [00:13<00:30,  7.08it/s]Evaluating:  32%|███▏      | 99/313 [00:13<00:30,  7.07it/s]Evaluating:  32%|███▏      | 100/313 [00:13<00:29,  7.11it/s]Evaluating:  32%|███▏      | 101/313 [00:13<00:29,  7.15it/s]Evaluating:  33%|███▎      | 102/313 [00:13<00:29,  7.19it/s]Evaluating:  33%|███▎      | 103/313 [00:14<00:29,  7.21it/s]Evaluating:  33%|███▎      | 104/313 [00:14<00:28,  7.23it/s]Evaluating:  34%|███▎      | 105/313 [00:14<00:28,  7.24it/s]Evaluating:  34%|███▍      | 106/313 [00:14<00:28,  7.25it/s]Evaluating:  34%|███▍      | 107/313 [00:14<00:28,  7.26it/s]Evaluating:  35%|███▍      | 108/313 [00:14<00:28,  7.25it/s]Evaluating:  35%|███▍      | 109/313 [00:14<00:28,  7.24it/s]Evaluating:  35%|███▌      | 110/313 [00:14<00:28,  7.25it/s]Evaluating:  35%|███▌      | 111/313 [00:15<00:27,  7.24it/s]Evaluating:  36%|███▌      | 112/313 [00:15<00:27,  7.22it/s]Evaluating:  36%|███▌      | 113/313 [00:15<00:27,  7.20it/s]Evaluating:  36%|███▋      | 114/313 [00:15<00:27,  7.18it/s]Evaluating:  37%|███▋      | 115/313 [00:15<00:27,  7.17it/s]Evaluating:  37%|███▋      | 116/313 [00:15<00:27,  7.19it/s]Evaluating:  37%|███▋      | 117/313 [00:15<00:27,  7.21it/s]Evaluating:  38%|███▊      | 118/313 [00:16<00:27,  7.21it/s]Evaluating:  38%|███▊      | 119/313 [00:16<00:26,  7.22it/s]Evaluating:  38%|███▊      | 120/313 [00:16<00:26,  7.23it/s]Evaluating:  39%|███▊      | 121/313 [00:16<00:26,  7.23it/s]Evaluating:  39%|███▉      | 122/313 [00:16<00:27,  7.07it/s]Evaluating:  39%|███▉      | 123/313 [00:16<00:26,  7.04it/s]Evaluating:  40%|███▉      | 124/313 [00:16<00:26,  7.02it/s]Evaluating:  40%|███▉      | 125/313 [00:17<00:26,  7.03it/s]Evaluating:  40%|████      | 126/313 [00:17<00:26,  7.02it/s]Evaluating:  41%|████      | 127/313 [00:17<00:26,  6.94it/s]Evaluating:  41%|████      | 128/313 [00:17<00:26,  6.98it/s]Evaluating:  41%|████      | 129/313 [00:17<00:26,  7.02it/s]Evaluating:  42%|████▏     | 130/313 [00:17<00:26,  7.03it/s]Evaluating:  42%|████▏     | 131/313 [00:17<00:25,  7.03it/s]Evaluating:  42%|████▏     | 132/313 [00:18<00:25,  7.05it/s]Evaluating:  42%|████▏     | 133/313 [00:18<00:25,  7.05it/s]Evaluating:  43%|████▎     | 134/313 [00:18<00:25,  7.06it/s]Evaluating:  43%|████▎     | 135/313 [00:18<00:25,  7.06it/s]Evaluating:  43%|████▎     | 136/313 [00:18<00:25,  7.03it/s]Evaluating:  44%|████▍     | 137/313 [00:18<00:25,  7.02it/s]Evaluating:  44%|████▍     | 138/313 [00:18<00:25,  7.00it/s]Evaluating:  44%|████▍     | 139/313 [00:19<00:24,  7.00it/s]Evaluating:  45%|████▍     | 140/313 [00:19<00:24,  7.02it/s]Evaluating:  45%|████▌     | 141/313 [00:19<00:24,  7.04it/s]Evaluating:  45%|████▌     | 142/313 [00:19<00:24,  7.02it/s]Evaluating:  46%|████▌     | 143/313 [00:19<00:29,  5.73it/s]Evaluating:  46%|████▌     | 144/313 [00:19<00:29,  5.70it/s]Evaluating:  46%|████▋     | 145/313 [00:20<00:28,  5.95it/s]Evaluating:  47%|████▋     | 146/313 [00:20<00:26,  6.26it/s]Evaluating:  47%|████▋     | 147/313 [00:20<00:25,  6.51it/s]Evaluating:  47%|████▋     | 148/313 [00:20<00:24,  6.67it/s]Evaluating:  48%|████▊     | 149/313 [00:20<00:24,  6.80it/s]Evaluating:  48%|████▊     | 150/313 [00:20<00:23,  6.88it/s]Evaluating:  48%|████▊     | 151/313 [00:20<00:23,  6.93it/s]Evaluating:  49%|████▊     | 152/313 [00:21<00:23,  6.98it/s]Evaluating:  49%|████▉     | 153/313 [00:21<00:22,  7.01it/s]Evaluating:  49%|████▉     | 154/313 [00:21<00:22,  7.02it/s]Evaluating:  50%|████▉     | 155/313 [00:21<00:22,  7.04it/s]Evaluating:  50%|████▉     | 156/313 [00:21<00:22,  7.03it/s]Evaluating:  50%|█████     | 157/313 [00:21<00:22,  6.99it/s]Evaluating:  50%|█████     | 158/313 [00:21<00:22,  6.95it/s]Evaluating:  51%|█████     | 159/313 [00:22<00:22,  6.97it/s]Evaluating:  51%|█████     | 160/313 [00:22<00:21,  6.97it/s]Evaluating:  51%|█████▏    | 161/313 [00:22<00:21,  6.99it/s]Evaluating:  52%|█████▏    | 162/313 [00:22<00:21,  7.00it/s]Evaluating:  52%|█████▏    | 163/313 [00:22<00:21,  7.00it/s]Evaluating:  52%|█████▏    | 164/313 [00:22<00:21,  7.02it/s]Evaluating:  53%|█████▎    | 165/313 [00:22<00:21,  7.05it/s]Evaluating:  53%|█████▎    | 166/313 [00:23<00:20,  7.05it/s]Evaluating:  53%|█████▎    | 167/313 [00:23<00:20,  7.04it/s]Evaluating:  54%|█████▎    | 168/313 [00:23<00:20,  7.00it/s]Evaluating:  54%|█████▍    | 169/313 [00:23<00:20,  6.95it/s]Evaluating:  54%|█████▍    | 170/313 [00:23<00:20,  6.93it/s]Evaluating:  55%|█████▍    | 171/313 [00:23<00:20,  6.93it/s]Evaluating:  55%|█████▍    | 172/313 [00:23<00:20,  6.93it/s]Evaluating:  55%|█████▌    | 173/313 [00:24<00:20,  6.97it/s]Evaluating:  56%|█████▌    | 174/313 [00:24<00:19,  7.01it/s]Evaluating:  56%|█████▌    | 175/313 [00:24<00:19,  7.01it/s]Evaluating:  56%|█████▌    | 176/313 [00:24<00:19,  7.02it/s]Evaluating:  57%|█████▋    | 177/313 [00:25<00:35,  3.83it/s]Evaluating:  57%|█████▋    | 178/313 [00:25<00:30,  4.42it/s]Evaluating:  57%|█████▋    | 179/313 [00:25<00:26,  4.99it/s]Evaluating:  58%|█████▊    | 180/313 [00:25<00:24,  5.48it/s]Evaluating:  58%|█████▊    | 181/313 [00:25<00:22,  5.87it/s]Evaluating:  58%|█████▊    | 182/313 [00:25<00:21,  6.18it/s]Evaluating:  58%|█████▊    | 183/313 [00:25<00:20,  6.42it/s]Evaluating:  59%|█████▉    | 184/313 [00:26<00:20,  6.34it/s]Evaluating:  59%|█████▉    | 185/313 [00:26<00:19,  6.48it/s]Evaluating:  59%|█████▉    | 186/313 [00:26<00:19,  6.59it/s]Evaluating:  60%|█████▉    | 187/313 [00:26<00:18,  6.66it/s]Evaluating:  60%|██████    | 188/313 [00:26<00:18,  6.75it/s]Evaluating:  60%|██████    | 189/313 [00:26<00:18,  6.74it/s]Evaluating:  61%|██████    | 190/313 [00:26<00:19,  6.38it/s]Evaluating:  61%|██████    | 191/313 [00:27<00:20,  5.99it/s]Evaluating:  61%|██████▏   | 192/313 [00:27<00:21,  5.76it/s]Evaluating:  62%|██████▏   | 193/313 [00:27<00:21,  5.65it/s]Evaluating:  62%|██████▏   | 194/313 [00:27<00:20,  5.72it/s]Evaluating:  62%|██████▏   | 195/313 [00:27<00:20,  5.81it/s]Evaluating:  63%|██████▎   | 196/313 [00:28<00:19,  5.92it/s]Evaluating:  63%|██████▎   | 197/313 [00:28<00:19,  6.04it/s]Evaluating:  63%|██████▎   | 198/313 [00:28<00:18,  6.15it/s]Evaluating:  64%|██████▎   | 199/313 [00:28<00:18,  6.22it/s]Evaluating:  64%|██████▍   | 200/313 [00:28<00:18,  6.24it/s]Evaluating:  64%|██████▍   | 201/313 [00:28<00:17,  6.26it/s]Evaluating:  65%|██████▍   | 202/313 [00:28<00:17,  6.30it/s]Evaluating:  65%|██████▍   | 203/313 [00:29<00:17,  6.32it/s]Evaluating:  65%|██████▌   | 204/313 [00:29<00:17,  6.32it/s]Evaluating:  65%|██████▌   | 205/313 [00:29<00:17,  6.33it/s]Evaluating:  66%|██████▌   | 206/313 [00:29<00:16,  6.38it/s]Evaluating:  66%|██████▌   | 207/313 [00:29<00:24,  4.41it/s]Evaluating:  66%|██████▋   | 208/313 [00:30<00:21,  4.88it/s]Evaluating:  67%|██████▋   | 209/313 [00:30<00:19,  5.27it/s]Evaluating:  67%|██████▋   | 210/313 [00:30<00:18,  5.59it/s]Evaluating:  67%|██████▋   | 211/313 [00:30<00:17,  5.83it/s]Evaluating:  68%|██████▊   | 212/313 [00:30<00:16,  6.02it/s]Evaluating:  68%|██████▊   | 213/313 [00:30<00:16,  6.07it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:15,  6.21it/s]Evaluating:  69%|██████▊   | 215/313 [00:31<00:16,  6.10it/s]Evaluating:  69%|██████▉   | 216/313 [00:31<00:16,  5.88it/s]Evaluating:  69%|██████▉   | 217/313 [00:31<00:15,  6.11it/s]Evaluating:  70%|██████▉   | 218/313 [00:31<00:14,  6.37it/s]Evaluating:  70%|██████▉   | 219/313 [00:31<00:14,  6.56it/s]Evaluating:  70%|███████   | 220/313 [00:31<00:13,  6.69it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:13,  6.80it/s]Evaluating:  71%|███████   | 222/313 [00:32<00:13,  6.84it/s]Evaluating:  71%|███████   | 223/313 [00:32<00:13,  6.86it/s]Evaluating:  72%|███████▏  | 224/313 [00:32<00:13,  6.81it/s]Evaluating:  72%|███████▏  | 225/313 [00:32<00:12,  6.79it/s]Evaluating:  72%|███████▏  | 226/313 [00:32<00:12,  6.78it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:12,  6.79it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  6.79it/s]Evaluating:  73%|███████▎  | 229/313 [00:33<00:12,  6.80it/s]Evaluating:  73%|███████▎  | 230/313 [00:33<00:12,  6.75it/s]Evaluating:  74%|███████▍  | 231/313 [00:33<00:12,  6.67it/s]Evaluating:  74%|███████▍  | 232/313 [00:33<00:12,  6.60it/s]Evaluating:  74%|███████▍  | 233/313 [00:33<00:12,  6.56it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:12,  6.56it/s]Evaluating:  75%|███████▌  | 235/313 [00:34<00:11,  6.52it/s]Evaluating:  75%|███████▌  | 236/313 [00:34<00:11,  6.50it/s]Evaluating:  76%|███████▌  | 237/313 [00:34<00:11,  6.50it/s]Evaluating:  76%|███████▌  | 238/313 [00:34<00:11,  6.49it/s]Evaluating:  76%|███████▋  | 239/313 [00:34<00:11,  6.48it/s]Evaluating:  77%|███████▋  | 240/313 [00:34<00:11,  6.49it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:11,  6.53it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:10,  6.52it/s]Evaluating:  78%|███████▊  | 243/313 [00:35<00:10,  6.50it/s]Evaluating:  78%|███████▊  | 244/313 [00:35<00:10,  6.50it/s]Evaluating:  78%|███████▊  | 245/313 [00:35<00:10,  6.52it/s]Evaluating:  79%|███████▊  | 246/313 [00:35<00:10,  6.53it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:10,  6.51it/s]Evaluating:  79%|███████▉  | 248/313 [00:36<00:10,  6.20it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:10,  6.20it/s]Evaluating:  80%|███████▉  | 250/313 [00:36<00:09,  6.36it/s]Evaluating:  80%|████████  | 251/313 [00:36<00:09,  6.44it/s]Evaluating:  81%|████████  | 252/313 [00:36<00:09,  6.52it/s]Evaluating:  81%|████████  | 253/313 [00:36<00:09,  6.61it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:08,  6.68it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.69it/s]Evaluating:  82%|████████▏ | 256/313 [00:37<00:08,  6.74it/s]Evaluating:  82%|████████▏ | 257/313 [00:37<00:08,  6.79it/s]Evaluating:  82%|████████▏ | 258/313 [00:37<00:08,  6.83it/s]Evaluating:  83%|████████▎ | 259/313 [00:37<00:07,  6.85it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:07,  6.80it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:07,  6.78it/s]Evaluating:  84%|████████▎ | 262/313 [00:38<00:07,  6.72it/s]Evaluating:  84%|████████▍ | 263/313 [00:38<00:07,  6.67it/s]Evaluating:  84%|████████▍ | 264/313 [00:38<00:07,  6.63it/s]Evaluating:  85%|████████▍ | 265/313 [00:38<00:07,  6.61it/s]Evaluating:  85%|████████▍ | 266/313 [00:38<00:07,  6.56it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:07,  6.51it/s]Evaluating:  86%|████████▌ | 268/313 [00:39<00:06,  6.52it/s]Evaluating:  86%|████████▌ | 269/313 [00:39<00:06,  6.50it/s]Evaluating:  86%|████████▋ | 270/313 [00:39<00:06,  6.48it/s]Evaluating:  87%|████████▋ | 271/313 [00:39<00:06,  6.28it/s]Evaluating:  87%|████████▋ | 272/313 [00:39<00:06,  6.45it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:06,  6.57it/s]Evaluating:  88%|████████▊ | 274/313 [00:40<00:05,  6.66it/s]Evaluating:  88%|████████▊ | 275/313 [00:40<00:05,  6.72it/s]Evaluating:  88%|████████▊ | 276/313 [00:40<00:05,  6.71it/s]Evaluating:  88%|████████▊ | 277/313 [00:40<00:05,  6.72it/s]Evaluating:  89%|████████▉ | 278/313 [00:40<00:05,  6.75it/s]Evaluating:  89%|████████▉ | 279/313 [00:40<00:05,  6.72it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:04,  6.69it/s]Evaluating:  90%|████████▉ | 281/313 [00:41<00:04,  6.61it/s]Evaluating:  90%|█████████ | 282/313 [00:41<00:04,  6.60it/s]Evaluating:  90%|█████████ | 283/313 [00:41<00:04,  6.63it/s]Evaluating:  91%|█████████ | 284/313 [00:41<00:04,  6.67it/s]Evaluating:  91%|█████████ | 285/313 [00:41<00:04,  6.59it/s]Evaluating:  91%|█████████▏| 286/313 [00:41<00:04,  6.50it/s]Evaluating:  92%|█████████▏| 287/313 [00:42<00:04,  6.37it/s]Evaluating:  92%|█████████▏| 288/313 [00:42<00:04,  6.24it/s]Evaluating:  92%|█████████▏| 289/313 [00:42<00:03,  6.15it/s]Evaluating:  93%|█████████▎| 290/313 [00:42<00:03,  6.23it/s]Evaluating:  93%|█████████▎| 291/313 [00:42<00:03,  6.30it/s]Evaluating:  93%|█████████▎| 292/313 [00:42<00:03,  6.37it/s]Evaluating:  94%|█████████▎| 293/313 [00:43<00:03,  6.40it/s]Evaluating:  94%|█████████▍| 294/313 [00:43<00:02,  6.40it/s]Evaluating:  94%|█████████▍| 295/313 [00:43<00:02,  6.46it/s]Evaluating:  95%|█████████▍| 296/313 [00:43<00:02,  6.52it/s]Evaluating:  95%|█████████▍| 297/313 [00:43<00:02,  6.58it/s]Evaluating:  95%|█████████▌| 298/313 [00:43<00:02,  6.56it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.51it/s]Evaluating:  96%|█████████▌| 300/313 [00:44<00:01,  6.53it/s]Evaluating:  96%|█████████▌| 301/313 [00:44<00:01,  6.51it/s]Evaluating:  96%|█████████▋| 302/313 [00:44<00:01,  6.48it/s]Evaluating:  97%|█████████▋| 303/313 [00:44<00:01,  6.46it/s]Evaluating:  97%|█████████▋| 304/313 [00:44<00:01,  6.46it/s]Evaluating:  97%|█████████▋| 305/313 [00:44<00:01,  6.47it/s]Evaluating:  98%|█████████▊| 306/313 [00:45<00:01,  6.45it/s]Evaluating:  98%|█████████▊| 307/313 [00:45<00:00,  6.44it/s]Evaluating:  98%|█████████▊| 308/313 [00:45<00:00,  6.45it/s]Evaluating:  99%|█████████▊| 309/313 [00:45<00:00,  6.46it/s]Evaluating:  99%|█████████▉| 310/313 [00:45<00:00,  6.39it/s]Evaluating:  99%|█████████▉| 311/313 [00:45<00:00,  6.39it/s]Evaluating: 100%|█████████▉| 312/313 [00:46<00:00,  6.39it/s]Evaluating: 100%|██████████| 313/313 [00:46<00:00,  6.79it/s]
10/08/2021 22:21:38 - INFO - __main__ -   ***** Evaluation result  in ar *****
10/08/2021 22:21:38 - INFO - __main__ -     f1 = 0.30617154354196735
10/08/2021 22:21:38 - INFO - __main__ -     loss = 5.576695292140729
10/08/2021 22:21:38 - INFO - __main__ -     precision = 0.301620130989314
10/08/2021 22:21:38 - INFO - __main__ -     recall = 0.3108624211741718
10/08/2021 22:21:38 - INFO - __main__ -   Language adapter for bh not found, using en instead
10/08/2021 22:21:38 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:21:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bh_bert-base-multilingual-cased_128
10/08/2021 22:21:38 - INFO - __main__ -   ***** Running evaluation  in bh *****
10/08/2021 22:21:38 - INFO - __main__ -     Num examples = 102
10/08/2021 22:21:38 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  6.79it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  6.78it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  6.78it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.40it/s]
10/08/2021 22:21:39 - INFO - __main__ -   ***** Evaluation result  in bh *****
10/08/2021 22:21:39 - INFO - __main__ -     f1 = 0.48708487084870855
10/08/2021 22:21:39 - INFO - __main__ -     loss = 3.018088787794113
10/08/2021 22:21:39 - INFO - __main__ -     precision = 0.42038216560509556
10/08/2021 22:21:39 - INFO - __main__ -     recall = 0.5789473684210527
10/08/2021 22:21:39 - INFO - __main__ -   Language adapter for hi not found, using en instead
10/08/2021 22:21:39 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:21:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
10/08/2021 22:21:39 - INFO - __main__ -   ***** Running evaluation  in hi *****
10/08/2021 22:21:39 - INFO - __main__ -     Num examples = 1000
10/08/2021 22:21:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.93it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.85it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.83it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.81it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.81it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.80it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.79it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.80it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.79it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.79it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.80it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  5.34it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  5.71it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:03,  5.99it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.21it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.37it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.49it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.58it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.62it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.66it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.69it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.16it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.44it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.67it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.84it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.94it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  7.00it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  7.00it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.97it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.91it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.85it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.75it/s]
10/08/2021 22:21:44 - INFO - __main__ -   ***** Evaluation result  in hi *****
10/08/2021 22:21:44 - INFO - __main__ -     f1 = 0.5456692913385828
10/08/2021 22:21:44 - INFO - __main__ -     loss = 2.7583304196596146
10/08/2021 22:21:44 - INFO - __main__ -     precision = 0.5282012195121951
10/08/2021 22:21:44 - INFO - __main__ -     recall = 0.5643322475570033
10/08/2021 22:21:44 - INFO - __main__ -   Language adapter for fo not found, using en instead
10/08/2021 22:21:44 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:21:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/08/2021 22:21:44 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/08/2021 22:21:44 - INFO - __main__ -     Num examples = 100
10/08/2021 22:21:44 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  6.81it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  6.80it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  6.80it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.52it/s]
10/08/2021 22:21:44 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/08/2021 22:21:44 - INFO - __main__ -     f1 = 0.6044776119402986
10/08/2021 22:21:44 - INFO - __main__ -     loss = 1.4626197069883347
10/08/2021 22:21:44 - INFO - __main__ -     precision = 0.5472972972972973
10/08/2021 22:21:44 - INFO - __main__ -     recall = 0.675
10/08/2021 22:21:44 - INFO - __main__ -   Language adapter for no not found, using en instead
10/08/2021 22:21:44 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:21:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/08/2021 22:21:46 - INFO - __main__ -   ***** Running evaluation  in no *****
10/08/2021 22:21:46 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:21:46 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.78it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.79it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.78it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:43,  6.78it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.78it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.79it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.77it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.74it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.74it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.73it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.73it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.73it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.74it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.74it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.75it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.75it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 34/313 [00:05<00:57,  4.82it/s]Evaluating:  11%|█         | 35/313 [00:05<00:52,  5.28it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:48,  5.67it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:46,  5.97it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:44,  6.19it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:43,  6.34it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:42,  6.46it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:41,  6.54it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:41,  6.60it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.64it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.64it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.65it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:40,  6.60it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.63it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.65it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.66it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.66it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.67it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:39,  6.68it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:38,  6.69it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.69it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.71it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.70it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:37,  6.70it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.69it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.65it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.66it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.66it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.67it/s]Evaluating:  21%|██        | 66/313 [00:10<00:52,  4.74it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:47,  5.18it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:44,  5.56it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:41,  5.86it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:39,  6.08it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:38,  6.24it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:37,  6.37it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:37,  6.46it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:36,  6.52it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:36,  6.56it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:36,  6.56it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.60it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:35,  6.60it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:35,  6.59it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:35,  6.58it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:35,  6.58it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:35,  6.59it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:35,  6.57it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.55it/s]Evaluating:  27%|██▋       | 85/313 [00:13<00:35,  6.46it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:35,  6.47it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:34,  6.54it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:34,  6.57it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.60it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.62it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.62it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:33,  6.63it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.64it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.64it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:32,  6.66it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.65it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:32,  6.64it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:31,  6.64it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.64it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.63it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.64it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:31,  6.64it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:31,  6.63it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.63it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.63it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.63it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:30,  6.63it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.61it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:30,  6.61it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:30,  6.60it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.60it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.61it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:29,  6.61it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:29,  6.62it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.62it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:29,  6.62it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.62it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.62it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.58it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:28,  6.49it/s]Evaluating:  40%|████      | 126/313 [00:19<00:29,  6.42it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.46it/s]Evaluating:  41%|████      | 128/313 [00:19<00:28,  6.50it/s]Evaluating:  41%|████      | 129/313 [00:19<00:28,  6.53it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.55it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:39,  4.63it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:35,  5.09it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:32,  5.45it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:31,  5.75it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:29,  5.98it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:28,  6.15it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:28,  6.28it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:27,  6.37it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:27,  6.43it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.48it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.51it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:26,  6.53it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:25,  6.54it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:25,  6.52it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:25,  6.57it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.61it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:24,  6.65it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.67it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.68it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:24,  6.67it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:24,  6.66it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:24,  6.65it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.66it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.62it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.62it/s]Evaluating:  50%|████▉     | 156/313 [00:24<00:23,  6.63it/s]Evaluating:  50%|█████     | 157/313 [00:24<00:23,  6.61it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:23,  6.62it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:23,  6.63it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.64it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:22,  6.66it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.66it/s]Evaluating:  52%|█████▏    | 163/313 [00:25<00:33,  4.53it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:30,  4.92it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:27,  5.31it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:26,  5.62it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:24,  5.88it/s]Evaluating:  54%|█████▎    | 168/313 [00:26<00:23,  6.11it/s]Evaluating:  54%|█████▍    | 169/313 [00:26<00:22,  6.27it/s]Evaluating:  54%|█████▍    | 170/313 [00:26<00:22,  6.37it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:22,  6.45it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:21,  6.48it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:21,  6.51it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:21,  6.52it/s]Evaluating:  56%|█████▌    | 175/313 [00:27<00:21,  6.53it/s]Evaluating:  56%|█████▌    | 176/313 [00:27<00:20,  6.54it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:20,  6.55it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:20,  6.54it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:20,  6.57it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:20,  6.60it/s]Evaluating:  58%|█████▊    | 181/313 [00:28<00:19,  6.62it/s]Evaluating:  58%|█████▊    | 182/313 [00:28<00:19,  6.65it/s]Evaluating:  58%|█████▊    | 183/313 [00:28<00:19,  6.69it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:19,  6.72it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:19,  6.71it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:19,  6.68it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:18,  6.69it/s]Evaluating:  60%|██████    | 188/313 [00:29<00:18,  6.71it/s]Evaluating:  60%|██████    | 189/313 [00:29<00:18,  6.70it/s]Evaluating:  61%|██████    | 190/313 [00:29<00:18,  6.69it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:18,  6.71it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:18,  6.70it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:17,  6.69it/s]Evaluating:  62%|██████▏   | 194/313 [00:30<00:32,  3.62it/s]Evaluating:  62%|██████▏   | 195/313 [00:30<00:28,  4.18it/s]Evaluating:  63%|██████▎   | 196/313 [00:30<00:24,  4.70it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:22,  5.15it/s]Evaluating:  63%|██████▎   | 198/313 [00:31<00:20,  5.52it/s]Evaluating:  64%|██████▎   | 199/313 [00:31<00:19,  5.80it/s]Evaluating:  64%|██████▍   | 200/313 [00:31<00:18,  6.03it/s]Evaluating:  64%|██████▍   | 201/313 [00:31<00:18,  6.20it/s]Evaluating:  65%|██████▍   | 202/313 [00:31<00:17,  6.32it/s]Evaluating:  65%|██████▍   | 203/313 [00:31<00:17,  6.40it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:17,  6.41it/s]Evaluating:  65%|██████▌   | 205/313 [00:32<00:16,  6.43it/s]Evaluating:  66%|██████▌   | 206/313 [00:32<00:16,  6.48it/s]Evaluating:  66%|██████▌   | 207/313 [00:32<00:16,  6.51it/s]Evaluating:  66%|██████▋   | 208/313 [00:32<00:16,  6.50it/s]Evaluating:  67%|██████▋   | 209/313 [00:32<00:15,  6.52it/s]Evaluating:  67%|██████▋   | 210/313 [00:32<00:15,  6.56it/s]Evaluating:  67%|██████▋   | 211/313 [00:33<00:15,  6.60it/s]Evaluating:  68%|██████▊   | 212/313 [00:33<00:15,  6.61it/s]Evaluating:  68%|██████▊   | 213/313 [00:33<00:15,  6.57it/s]Evaluating:  68%|██████▊   | 214/313 [00:33<00:15,  6.57it/s]Evaluating:  69%|██████▊   | 215/313 [00:33<00:14,  6.59it/s]Evaluating:  69%|██████▉   | 216/313 [00:33<00:14,  6.61it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:14,  6.61it/s]Evaluating:  70%|██████▉   | 218/313 [00:34<00:14,  6.36it/s]Evaluating:  70%|██████▉   | 219/313 [00:34<00:14,  6.32it/s]Evaluating:  70%|███████   | 220/313 [00:34<00:14,  6.37it/s]Evaluating:  71%|███████   | 221/313 [00:34<00:14,  6.41it/s]Evaluating:  71%|███████   | 222/313 [00:34<00:14,  6.46it/s]Evaluating:  71%|███████   | 223/313 [00:34<00:13,  6.49it/s]Evaluating:  72%|███████▏  | 224/313 [00:35<00:19,  4.53it/s]Evaluating:  72%|███████▏  | 225/313 [00:35<00:17,  4.98it/s]Evaluating:  72%|███████▏  | 226/313 [00:35<00:16,  5.35it/s]Evaluating:  73%|███████▎  | 227/313 [00:35<00:15,  5.65it/s]Evaluating:  73%|███████▎  | 228/313 [00:35<00:14,  5.89it/s]Evaluating:  73%|███████▎  | 229/313 [00:36<00:13,  6.07it/s]Evaluating:  73%|███████▎  | 230/313 [00:36<00:13,  6.21it/s]Evaluating:  74%|███████▍  | 231/313 [00:36<00:12,  6.33it/s]Evaluating:  74%|███████▍  | 232/313 [00:36<00:12,  6.43it/s]Evaluating:  74%|███████▍  | 233/313 [00:36<00:12,  6.55it/s]Evaluating:  75%|███████▍  | 234/313 [00:36<00:11,  6.62it/s]Evaluating:  75%|███████▌  | 235/313 [00:36<00:11,  6.67it/s]Evaluating:  75%|███████▌  | 236/313 [00:37<00:11,  6.65it/s]Evaluating:  76%|███████▌  | 237/313 [00:37<00:11,  6.61it/s]Evaluating:  76%|███████▌  | 238/313 [00:37<00:11,  6.59it/s]Evaluating:  76%|███████▋  | 239/313 [00:37<00:11,  6.58it/s]Evaluating:  77%|███████▋  | 240/313 [00:37<00:11,  6.54it/s]Evaluating:  77%|███████▋  | 241/313 [00:37<00:11,  6.52it/s]Evaluating:  77%|███████▋  | 242/313 [00:37<00:10,  6.55it/s]Evaluating:  78%|███████▊  | 243/313 [00:38<00:10,  6.58it/s]Evaluating:  78%|███████▊  | 244/313 [00:38<00:10,  6.58it/s]Evaluating:  78%|███████▊  | 245/313 [00:38<00:10,  6.60it/s]Evaluating:  79%|███████▊  | 246/313 [00:38<00:10,  6.64it/s]Evaluating:  79%|███████▉  | 247/313 [00:38<00:09,  6.71it/s]Evaluating:  79%|███████▉  | 248/313 [00:38<00:09,  6.75it/s]Evaluating:  80%|███████▉  | 249/313 [00:39<00:09,  6.82it/s]Evaluating:  80%|███████▉  | 250/313 [00:39<00:09,  6.87it/s]Evaluating:  80%|████████  | 251/313 [00:39<00:09,  6.84it/s]Evaluating:  81%|████████  | 252/313 [00:39<00:08,  6.79it/s]Evaluating:  81%|████████  | 253/313 [00:39<00:08,  6.77it/s]Evaluating:  81%|████████  | 254/313 [00:39<00:08,  6.75it/s]Evaluating:  81%|████████▏ | 255/313 [00:39<00:08,  6.45it/s]Evaluating:  82%|████████▏ | 256/313 [00:40<00:08,  6.49it/s]Evaluating:  82%|████████▏ | 257/313 [00:40<00:08,  6.64it/s]Evaluating:  82%|████████▏ | 258/313 [00:40<00:08,  6.19it/s]Evaluating:  83%|████████▎ | 259/313 [00:40<00:08,  6.26it/s]Evaluating:  83%|████████▎ | 260/313 [00:40<00:08,  6.31it/s]Evaluating:  83%|████████▎ | 261/313 [00:40<00:08,  6.35it/s]Evaluating:  84%|████████▎ | 262/313 [00:41<00:07,  6.38it/s]Evaluating:  84%|████████▍ | 263/313 [00:41<00:07,  6.40it/s]Evaluating:  84%|████████▍ | 264/313 [00:41<00:07,  6.41it/s]Evaluating:  85%|████████▍ | 265/313 [00:41<00:07,  6.42it/s]Evaluating:  85%|████████▍ | 266/313 [00:41<00:07,  6.46it/s]Evaluating:  85%|████████▌ | 267/313 [00:41<00:07,  6.48it/s]Evaluating:  86%|████████▌ | 268/313 [00:41<00:06,  6.49it/s]Evaluating:  86%|████████▌ | 269/313 [00:42<00:06,  6.56it/s]Evaluating:  86%|████████▋ | 270/313 [00:42<00:06,  6.60it/s]Evaluating:  87%|████████▋ | 271/313 [00:42<00:06,  6.56it/s]Evaluating:  87%|████████▋ | 272/313 [00:42<00:06,  6.52it/s]Evaluating:  87%|████████▋ | 273/313 [00:42<00:06,  6.50it/s]Evaluating:  88%|████████▊ | 274/313 [00:42<00:06,  6.49it/s]Evaluating:  88%|████████▊ | 275/313 [00:43<00:05,  6.48it/s]Evaluating:  88%|████████▊ | 276/313 [00:43<00:05,  6.50it/s]Evaluating:  88%|████████▊ | 277/313 [00:43<00:05,  6.53it/s]Evaluating:  89%|████████▉ | 278/313 [00:43<00:05,  6.57it/s]Evaluating:  89%|████████▉ | 279/313 [00:43<00:05,  6.63it/s]Evaluating:  89%|████████▉ | 280/313 [00:43<00:04,  6.67it/s]Evaluating:  90%|████████▉ | 281/313 [00:43<00:04,  6.69it/s]Evaluating:  90%|█████████ | 282/313 [00:44<00:04,  6.74it/s]Evaluating:  90%|█████████ | 283/313 [00:44<00:04,  6.79it/s]Evaluating:  91%|█████████ | 284/313 [00:44<00:04,  6.85it/s]Evaluating:  91%|█████████ | 285/313 [00:44<00:04,  6.86it/s]Evaluating:  91%|█████████▏| 286/313 [00:44<00:03,  6.81it/s]Evaluating:  92%|█████████▏| 287/313 [00:44<00:03,  6.86it/s]Evaluating:  92%|█████████▏| 288/313 [00:44<00:03,  6.85it/s]Evaluating:  92%|█████████▏| 289/313 [00:45<00:03,  6.88it/s]Evaluating:  93%|█████████▎| 290/313 [00:45<00:03,  6.90it/s]Evaluating:  93%|█████████▎| 291/313 [00:45<00:03,  6.92it/s]Evaluating:  93%|█████████▎| 292/313 [00:45<00:03,  6.94it/s]Evaluating:  94%|█████████▎| 293/313 [00:45<00:02,  6.95it/s]Evaluating:  94%|█████████▍| 294/313 [00:45<00:02,  6.97it/s]Evaluating:  94%|█████████▍| 295/313 [00:45<00:02,  6.64it/s]Evaluating:  95%|█████████▍| 296/313 [00:46<00:02,  6.70it/s]Evaluating:  95%|█████████▍| 297/313 [00:46<00:02,  6.74it/s]Evaluating:  95%|█████████▌| 298/313 [00:46<00:02,  6.76it/s]Evaluating:  96%|█████████▌| 299/313 [00:46<00:02,  6.78it/s]Evaluating:  96%|█████████▌| 300/313 [00:46<00:01,  6.78it/s]Evaluating:  96%|█████████▌| 301/313 [00:46<00:01,  6.77it/s]Evaluating:  96%|█████████▋| 302/313 [00:46<00:01,  6.82it/s]Evaluating:  97%|█████████▋| 303/313 [00:47<00:01,  6.85it/s]Evaluating:  97%|█████████▋| 304/313 [00:47<00:01,  6.87it/s]Evaluating:  97%|█████████▋| 305/313 [00:47<00:01,  6.88it/s]Evaluating:  98%|█████████▊| 306/313 [00:47<00:01,  6.89it/s]Evaluating:  98%|█████████▊| 307/313 [00:47<00:00,  6.85it/s]Evaluating:  98%|█████████▊| 308/313 [00:47<00:00,  6.82it/s]Evaluating:  99%|█████████▊| 309/313 [00:47<00:00,  6.83it/s]Evaluating:  99%|█████████▉| 310/313 [00:48<00:00,  6.86it/s]Evaluating:  99%|█████████▉| 311/313 [00:48<00:00,  6.87it/s]Evaluating: 100%|█████████▉| 312/313 [00:48<00:00,  6.89it/s]Evaluating: 100%|██████████| 313/313 [00:48<00:00,  6.45it/s]
10/08/2021 22:22:36 - INFO - __main__ -   ***** Evaluation result  in no *****
10/08/2021 22:22:36 - INFO - __main__ -     f1 = 0.7285789961594491
10/08/2021 22:22:36 - INFO - __main__ -     loss = 1.1306573051614122
10/08/2021 22:22:36 - INFO - __main__ -     precision = 0.6963042652828756
10/08/2021 22:22:36 - INFO - __main__ -     recall = 0.7639911123455075
10/08/2021 22:22:36 - INFO - __main__ -   Language adapter for da not found, using en instead
10/08/2021 22:22:36 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:22:36 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/08/2021 22:22:37 - INFO - __main__ -   ***** Running evaluation  in da *****
10/08/2021 22:22:37 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:22:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.77it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.78it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.81it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.81it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:44,  6.81it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.81it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.81it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.80it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.80it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.80it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.79it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:43,  6.78it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.78it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.77it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.75it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.74it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.74it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▉         | 28/313 [00:04<01:01,  4.66it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:55,  5.14it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:51,  5.53it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:48,  5.85it/s]Evaluating:  10%|█         | 32/313 [00:04<00:46,  6.10it/s]Evaluating:  11%|█         | 33/313 [00:05<00:44,  6.28it/s]Evaluating:  11%|█         | 34/313 [00:05<00:43,  6.42it/s]Evaluating:  11%|█         | 35/313 [00:05<00:42,  6.51it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:42,  6.58it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.63it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.66it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:41,  6.68it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:40,  6.67it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.68it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.67it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.68it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.70it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.70it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:39,  6.70it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:39,  6.70it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.71it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:38,  6.72it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.71it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.71it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.71it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:42,  5.93it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:40,  6.29it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.53it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.75it/s]Evaluating:  20%|██        | 64/313 [00:09<00:36,  6.90it/s]Evaluating:  21%|██        | 65/313 [00:09<00:35,  7.00it/s]Evaluating:  21%|██        | 66/313 [00:10<00:34,  7.07it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:34,  7.12it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:34,  7.14it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:34,  7.13it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:34,  7.11it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:33,  7.14it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:33,  7.15it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:33,  7.16it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:33,  7.15it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:33,  7.11it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:33,  7.04it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:33,  7.04it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:33,  7.04it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:33,  7.05it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:32,  7.07it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:32,  7.07it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:32,  7.07it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:32,  7.02it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:32,  6.97it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:32,  6.96it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:32,  7.01it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:32,  7.03it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:32,  7.01it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:32,  6.98it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:31,  6.98it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:31,  6.97it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:31,  6.94it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:31,  6.91it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.73it/s]Evaluating:  30%|███       | 95/313 [00:14<00:46,  4.70it/s]Evaluating:  31%|███       | 96/313 [00:14<00:42,  5.15it/s]Evaluating:  31%|███       | 97/313 [00:14<00:39,  5.52it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:36,  5.81it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:35,  6.05it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:34,  6.21it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:33,  6.33it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.43it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:32,  6.48it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:32,  6.53it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.56it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:31,  6.59it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.60it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:31,  6.61it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.63it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.62it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.63it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:30,  6.63it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.63it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.63it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.63it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.62it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.62it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.63it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:29,  6.62it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.62it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.62it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.62it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.62it/s]Evaluating:  40%|████      | 126/313 [00:19<00:28,  6.62it/s]Evaluating:  41%|████      | 127/313 [00:19<00:29,  6.21it/s]Evaluating:  41%|████      | 128/313 [00:19<00:28,  6.45it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.65it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.77it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:26,  6.84it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:26,  6.89it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:26,  6.89it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:25,  6.91it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:25,  6.92it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:25,  6.84it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.77it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.72it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.69it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:25,  6.66it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.64it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.64it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.64it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.64it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.65it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.65it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.63it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.64it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.64it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.65it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.65it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.66it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:23,  6.68it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:23,  6.66it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.63it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.62it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.61it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.60it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:23,  6.59it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:25,  5.95it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:24,  6.24it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:23,  6.46it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.61it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.74it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:21,  6.84it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:21,  6.92it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:20,  6.98it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:20,  7.03it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:20,  7.05it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:20,  7.03it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:20,  7.00it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.96it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:20,  6.89it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.83it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.79it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.73it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:20,  6.50it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:20,  6.51it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:20,  6.66it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:19,  6.77it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.80it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.79it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.84it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:18,  6.83it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:18,  6.87it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:18,  6.92it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:18,  6.96it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:17,  6.98it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:17,  6.94it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:17,  6.93it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:17,  6.91it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  6.90it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:17,  6.90it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:18,  6.32it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:18,  6.38it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:18,  6.42it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.45it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:17,  6.48it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:17,  6.48it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:17,  6.50it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:17,  6.54it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.59it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.61it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:16,  6.62it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:16,  6.65it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:16,  6.68it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:15,  6.72it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.76it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.82it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.79it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:15,  6.75it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:14,  6.78it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:14,  6.77it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.77it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.76it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.76it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:14,  6.77it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:13,  6.79it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:13,  6.85it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:13,  6.82it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:13,  6.72it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.69it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.66it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:13,  6.66it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:13,  6.67it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:12,  6.70it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.75it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.79it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.83it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.84it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.82it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:11,  6.80it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:11,  6.80it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.79it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.78it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.80it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.80it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.69it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:11,  6.57it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.47it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.40it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:11,  6.37it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.37it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.37it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.40it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:10,  6.42it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:10,  6.43it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:10,  6.49it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.54it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.60it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.67it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:09,  6.71it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:08,  6.73it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.80it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.83it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.80it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.76it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.70it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:08,  6.68it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:07,  6.68it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:09,  5.71it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:08,  5.98it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:08,  6.21it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.36it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.48it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.13it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:07,  5.79it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:08,  5.58it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:07,  5.54it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:07,  5.60it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:07,  5.68it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:07,  5.76it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  5.87it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:06,  5.99it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:06,  6.07it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:06,  6.14it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.21it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.28it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.33it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.35it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:05,  6.37it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.38it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.39it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.39it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.42it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.40it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.38it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.36it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.40it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.43it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:04,  5.04it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  5.42it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  5.73it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:03,  6.01it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.21it/s]Evaluating:  95%|█████████▍| 296/313 [00:45<00:02,  6.35it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:02,  6.49it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.59it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.63it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:01,  6.68it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.73it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.78it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.83it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.85it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.87it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.88it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.86it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.83it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.81it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  6.82it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.79it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.77it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.60it/s]
10/08/2021 22:23:26 - INFO - __main__ -   ***** Evaluation result  in da *****
10/08/2021 22:23:26 - INFO - __main__ -     f1 = 0.7979139504563234
10/08/2021 22:23:26 - INFO - __main__ -     loss = 0.8606576694848057
10/08/2021 22:23:26 - INFO - __main__ -     precision = 0.7712008718752129
10/08/2021 22:23:26 - INFO - __main__ -     recall = 0.8265440210249672
10/08/2021 22:23:26 - INFO - __main__ -   Language adapter for ru not found, using en instead
10/08/2021 22:23:26 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:23:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
10/08/2021 22:23:27 - INFO - __main__ -   ***** Running evaluation  in ru *****
10/08/2021 22:23:27 - INFO - __main__ -     Num examples = 10002
10/08/2021 22:23:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.81it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.81it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.78it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.78it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.80it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.78it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:43,  6.79it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.79it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.78it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.77it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.75it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.76it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.76it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.75it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.73it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.72it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.72it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.69it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.71it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.73it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.68it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.68it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.67it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.67it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.69it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.70it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.72it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:43,  5.79it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:40,  6.18it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.44it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.66it/s]Evaluating:  20%|██        | 64/313 [00:09<00:36,  6.85it/s]Evaluating:  21%|██        | 65/313 [00:09<00:35,  6.97it/s]Evaluating:  21%|██        | 66/313 [00:09<00:34,  7.06it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:34,  7.12it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:34,  7.14it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:34,  7.15it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:34,  7.14it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:33,  7.13it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:33,  7.12it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:33,  7.11it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:33,  7.10it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:33,  7.10it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:33,  7.04it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:33,  7.00it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:33,  6.93it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:33,  6.89it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:34,  6.85it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:34,  6.82it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.79it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.72it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.69it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.70it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:33,  6.76it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.80it/s]Evaluating:  28%|██▊       | 88/313 [00:12<00:33,  6.70it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.68it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.65it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:33,  6.65it/s]Evaluating:  30%|███       | 94/313 [00:14<00:43,  5.05it/s]Evaluating:  30%|███       | 95/313 [00:14<00:39,  5.45it/s]Evaluating:  31%|███       | 96/313 [00:14<00:37,  5.77it/s]Evaluating:  31%|███       | 97/313 [00:14<00:35,  6.01it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:34,  6.20it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:33,  6.34it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:33,  6.42it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:32,  6.49it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.53it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.57it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.60it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.62it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:31,  6.62it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.63it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.63it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.63it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.64it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.63it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.63it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:30,  6.64it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.63it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.63it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.63it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.65it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.65it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.66it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:28,  6.67it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.65it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.64it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.64it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.64it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.65it/s]Evaluating:  40%|████      | 126/313 [00:18<00:28,  6.62it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.59it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.62it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.62it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.62it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.63it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:27,  6.62it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:26,  6.63it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.64it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.63it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.63it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.62it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.62it/s]Evaluating:  45%|████▍     | 140/313 [00:20<00:26,  6.52it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.57it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.69it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.75it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:24,  6.82it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:24,  6.86it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:24,  6.82it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:24,  6.79it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.76it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.74it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.72it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.71it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.69it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:24,  6.65it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:23,  6.63it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.62it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.61it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.60it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.60it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:23,  6.60it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:23,  6.54it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.56it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.58it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.59it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.63it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:22,  6.62it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:22,  6.62it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:21,  6.67it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.67it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.68it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.72it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:21,  6.75it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:21,  6.71it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:21,  6.64it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:21,  6.61it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.60it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.59it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:21,  6.42it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:21,  6.42it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:20,  6.57it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:20,  6.63it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.66it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.71it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.72it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:19,  6.70it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:19,  6.73it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:18,  6.77it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:18,  6.80it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.85it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:17,  6.93it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:17,  6.98it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:17,  7.01it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  7.03it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:22,  5.25it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:21,  5.66it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:19,  6.02it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:18,  6.29it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.50it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:17,  6.65it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:16,  6.76it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:16,  6.86it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:16,  6.92it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.83it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.71it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:16,  6.62it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:16,  6.52it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:16,  6.48it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:16,  6.47it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:16,  6.48it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.51it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.53it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:15,  6.51it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:15,  6.51it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:15,  6.51it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:15,  6.48it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:15,  6.46it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.51it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:15,  6.34it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:14,  6.39it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:14,  6.45it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:14,  6.47it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:14,  6.54it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.57it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.65it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:13,  6.75it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:12,  6.83it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:12,  6.89it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.94it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.96it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.82it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.70it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.58it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:12,  6.51it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:12,  6.49it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:12,  6.48it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:12,  6.47it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.47it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.47it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.45it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:11,  6.47it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.50it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.53it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.56it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.60it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.61it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.64it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:10,  6.61it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:10,  6.58it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.61it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.65it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.66it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.73it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:08,  6.79it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:08,  6.85it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.90it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.90it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.89it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.88it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.83it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:09,  5.89it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.15it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:08,  6.34it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.50it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.58it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.63it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.67it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.71it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:06,  6.74it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.74it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.75it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.76it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.78it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:06,  6.48it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.49it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.60it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.60it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.55it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.55it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.56it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.57it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.51it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.49it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.49it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.49it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.50it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.50it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.48it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.48it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.50it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.55it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.64it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.54it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  5.60it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  5.69it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:03,  5.83it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:03,  5.91it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  5.99it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.07it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.15it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.19it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  6.24it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.28it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.33it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.32it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.35it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.07it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.24it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.38it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.48it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.55it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.63it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.65it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.64it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.61it/s]
10/08/2021 22:24:16 - INFO - __main__ -   ***** Evaluation result  in ru *****
10/08/2021 22:24:16 - INFO - __main__ -     f1 = 0.542504353332278
10/08/2021 22:24:16 - INFO - __main__ -     loss = 2.540714929088617
10/08/2021 22:24:16 - INFO - __main__ -     precision = 0.5235658085707738
10/08/2021 22:24:16 - INFO - __main__ -     recall = 0.5628644165229532
10/08/2021 22:24:16 - INFO - __main__ -   Language adapter for bg not found, using en instead
10/08/2021 22:24:16 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:24:16 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/08/2021 22:24:17 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/08/2021 22:24:17 - INFO - __main__ -     Num examples = 10004
10/08/2021 22:24:17 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.81it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.76it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.78it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.78it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.79it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.78it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.78it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.75it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.75it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.76it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.76it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.76it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.75it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.74it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.74it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.75it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.75it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.75it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.75it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.76it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.76it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:40,  6.76it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.76it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.75it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.75it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.75it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.74it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.73it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.73it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.73it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.72it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.72it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.72it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.72it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.70it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:39,  6.68it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.68it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.67it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.68it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.69it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.68it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.69it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.69it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.69it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:50,  4.95it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:46,  5.34it/s]Evaluating:  20%|██        | 63/313 [00:09<00:44,  5.67it/s]Evaluating:  20%|██        | 64/313 [00:09<00:41,  5.94it/s]Evaluating:  21%|██        | 65/313 [00:09<00:40,  6.14it/s]Evaluating:  21%|██        | 66/313 [00:09<00:39,  6.30it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:38,  6.41it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:37,  6.50it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:37,  6.56it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.59it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.62it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.64it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:36,  6.65it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.66it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.67it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.68it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.68it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.69it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:34,  6.69it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:34,  6.69it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.69it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.64it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.58it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.60it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:33,  6.68it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:33,  6.67it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:34,  6.54it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:34,  6.53it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.57it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.58it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.61it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.62it/s]Evaluating:  30%|███       | 94/313 [00:14<00:33,  6.63it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.65it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.65it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:32,  6.65it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.66it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.66it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.66it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:31,  6.65it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:30,  6.65it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.67it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.69it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.67it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.66it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.65it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.65it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.64it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.63it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.64it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.64it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.62it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.64it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.64it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.63it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.63it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.56it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.57it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.55it/s]Evaluating:  40%|████      | 126/313 [00:19<00:48,  3.85it/s]Evaluating:  41%|████      | 127/313 [00:19<00:42,  4.39it/s]Evaluating:  41%|████      | 128/313 [00:19<00:37,  4.88it/s]Evaluating:  41%|████      | 129/313 [00:19<00:35,  5.21it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:33,  5.51it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:31,  5.83it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:30,  6.02it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:29,  6.18it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:28,  6.33it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:27,  6.43it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:27,  6.50it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:26,  6.55it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:26,  6.59it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:26,  6.62it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.64it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.65it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.65it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.66it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:25,  6.63it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:25,  6.64it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.65it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.63it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.61it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.61it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:24,  6.60it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:24,  6.59it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:24,  6.59it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.59it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.58it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:24,  6.57it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.57it/s]Evaluating:  50%|█████     | 157/313 [00:24<00:26,  5.98it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:24,  6.29it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:23,  6.53it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:22,  6.69it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:22,  6.82it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:21,  6.91it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:21,  6.95it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:21,  6.99it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:21,  7.02it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:21,  6.99it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:21,  6.92it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:20,  6.91it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.73it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.74it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:20,  6.82it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:20,  6.91it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:20,  6.96it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:19,  6.98it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:19,  7.02it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:19,  7.01it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:19,  6.97it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:19,  6.95it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:19,  6.94it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:19,  6.95it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:18,  6.96it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:18,  6.99it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:18,  6.99it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:18,  6.95it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:18,  6.93it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:18,  6.95it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:18,  6.98it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:17,  7.02it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:17,  7.04it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:17,  7.05it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:23,  5.27it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:21,  5.59it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:20,  5.84it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:19,  6.03it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:19,  6.16it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:18,  6.26it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:18,  6.33it/s]Evaluating:  63%|██████▎   | 198/313 [00:30<00:17,  6.40it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:17,  6.43it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:17,  6.45it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:17,  6.48it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:17,  6.52it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.58it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:16,  6.64it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:16,  6.69it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:15,  6.78it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:15,  6.85it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.88it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.83it/s]Evaluating:  67%|██████▋   | 210/313 [00:32<00:15,  6.84it/s]Evaluating:  67%|██████▋   | 211/313 [00:32<00:14,  6.83it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:14,  6.82it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:14,  6.84it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.85it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.87it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.85it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:14,  6.58it/s]Evaluating:  70%|██████▉   | 218/313 [00:33<00:14,  6.60it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:14,  6.70it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:13,  6.73it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:13,  6.76it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.79it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.81it/s]Evaluating:  72%|███████▏  | 224/313 [00:34<00:13,  6.43it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:13,  6.59it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:12,  6.70it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.77it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.70it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.62it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.57it/s]Evaluating:  74%|███████▍  | 231/313 [00:35<00:12,  6.54it/s]Evaluating:  74%|███████▍  | 232/313 [00:35<00:12,  6.52it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:12,  6.51it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:16,  4.75it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:15,  5.18it/s]Evaluating:  75%|███████▌  | 236/313 [00:36<00:13,  5.57it/s]Evaluating:  76%|███████▌  | 237/313 [00:36<00:12,  5.89it/s]Evaluating:  76%|███████▌  | 238/313 [00:36<00:12,  6.12it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:11,  6.32it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.48it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:10,  6.62it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.71it/s]Evaluating:  78%|███████▊  | 243/313 [00:37<00:10,  6.77it/s]Evaluating:  78%|███████▊  | 244/313 [00:37<00:10,  6.80it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:09,  6.84it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:09,  6.82it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.80it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.81it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.81it/s]Evaluating:  80%|███████▉  | 250/313 [00:38<00:09,  6.79it/s]Evaluating:  80%|████████  | 251/313 [00:38<00:09,  6.51it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:09,  6.49it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:09,  6.59it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.66it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.72it/s]Evaluating:  82%|████████▏ | 256/313 [00:39<00:08,  6.74it/s]Evaluating:  82%|████████▏ | 257/313 [00:39<00:08,  6.73it/s]Evaluating:  82%|████████▏ | 258/313 [00:39<00:08,  6.69it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.69it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:07,  6.72it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:07,  6.73it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.71it/s]Evaluating:  84%|████████▍ | 263/313 [00:40<00:07,  6.70it/s]Evaluating:  84%|████████▍ | 264/313 [00:40<00:07,  6.70it/s]Evaluating:  85%|████████▍ | 265/313 [00:40<00:07,  6.70it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:06,  6.72it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:06,  6.76it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.79it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.79it/s]Evaluating:  86%|████████▋ | 270/313 [00:41<00:06,  6.74it/s]Evaluating:  87%|████████▋ | 271/313 [00:41<00:06,  6.70it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.69it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.61it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.58it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.59it/s]Evaluating:  88%|████████▊ | 276/313 [00:42<00:05,  6.60it/s]Evaluating:  88%|████████▊ | 277/313 [00:42<00:05,  6.56it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.52it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.53it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.57it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.61it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.62it/s]Evaluating:  90%|█████████ | 283/313 [00:43<00:04,  6.66it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.70it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.73it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.65it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:03,  6.54it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.45it/s]Evaluating:  92%|█████████▏| 289/313 [00:44<00:03,  6.17it/s]Evaluating:  93%|█████████▎| 290/313 [00:44<00:03,  6.23it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.25it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.00it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.12it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:03,  6.20it/s]Evaluating:  94%|█████████▍| 295/313 [00:45<00:02,  6.27it/s]Evaluating:  95%|█████████▍| 296/313 [00:45<00:02,  6.32it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:02,  6.35it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.37it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.44it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:01,  6.52it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.60it/s]Evaluating:  96%|█████████▋| 302/313 [00:46<00:01,  6.69it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.73it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.24it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  5.84it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  5.42it/s]Evaluating:  98%|█████████▊| 307/313 [00:47<00:01,  5.29it/s]Evaluating:  98%|█████████▊| 308/313 [00:47<00:00,  5.32it/s]Evaluating:  99%|█████████▊| 309/313 [00:47<00:00,  5.38it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  5.54it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  5.69it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  5.82it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.50it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.52it/s]
10/08/2021 22:25:07 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/08/2021 22:25:07 - INFO - __main__ -     f1 = 0.6794228998260513
10/08/2021 22:25:07 - INFO - __main__ -     loss = 1.632035830055182
10/08/2021 22:25:07 - INFO - __main__ -     precision = 0.6527723161620134
10/08/2021 22:25:07 - INFO - __main__ -     recall = 0.7083422231704715
10/08/2021 22:25:07 - INFO - __main__ -   Language adapter for uk not found, using en instead
10/08/2021 22:25:07 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:25:07 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/08/2021 22:25:08 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/08/2021 22:25:08 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:25:08 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.78it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.80it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.79it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:43,  6.79it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.78it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.78it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:47,  6.19it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:44,  6.49it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.71it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.85it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:41,  6.98it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:40,  7.04it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:40,  7.12it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:39,  7.16it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:39,  7.19it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:39,  7.19it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:39,  7.17it/s]Evaluating:  10%|█         | 32/313 [00:04<00:39,  7.15it/s]Evaluating:  11%|█         | 33/313 [00:04<00:39,  7.14it/s]Evaluating:  11%|█         | 34/313 [00:04<00:39,  7.11it/s]Evaluating:  11%|█         | 35/313 [00:05<00:39,  7.07it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:39,  7.03it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:39,  6.91it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.83it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.81it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.78it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:40,  6.75it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.74it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.71it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:38,  6.71it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:46,  5.55it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:43,  5.85it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:42,  6.09it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:40,  6.26it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:39,  6.38it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:39,  6.47it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.54it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.56it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.58it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.63it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.63it/s]Evaluating:  21%|██        | 66/313 [00:09<00:37,  6.64it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.65it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.66it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.68it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.68it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.67it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.60it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:36,  6.63it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:36,  6.63it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.63it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.67it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:35,  6.68it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:34,  6.68it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.68it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.64it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.62it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.60it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:34,  6.62it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:34,  6.64it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.64it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.66it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:33,  6.66it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.67it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.67it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.62it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.58it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.57it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.57it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:32,  6.55it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:32,  6.57it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.59it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.60it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.61it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.62it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:31,  6.51it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.53it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.65it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.77it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:29,  6.80it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:29,  6.84it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:29,  6.88it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:28,  6.92it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:28,  6.94it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:28,  6.98it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:28,  7.00it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:27,  7.01it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:27,  7.00it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:27,  6.99it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:27,  6.93it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:27,  6.87it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:29,  6.44it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.60it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.68it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:27,  6.72it/s]Evaluating:  40%|████      | 126/313 [00:18<00:27,  6.75it/s]Evaluating:  41%|████      | 127/313 [00:18<00:27,  6.76it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.78it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.79it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:26,  6.80it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:26,  6.76it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:26,  6.73it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:26,  6.70it/s]Evaluating:  43%|████▎     | 134/313 [00:19<00:26,  6.68it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.65it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.63it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.64it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.63it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.62it/s]Evaluating:  45%|████▍     | 140/313 [00:20<00:26,  6.62it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.61it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.61it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:25,  6.60it/s]Evaluating:  47%|████▋     | 147/313 [00:21<00:25,  6.46it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.48it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.57it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.57it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.56it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.56it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:24,  6.57it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.57it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:24,  6.42it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.62it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.78it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:22,  6.89it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:22,  6.97it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:21,  6.99it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:21,  7.00it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:21,  6.97it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:21,  6.95it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:21,  6.95it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:21,  6.95it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:21,  6.97it/s]Evaluating:  53%|█████▎    | 167/313 [00:24<00:20,  6.98it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:20,  6.96it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:20,  6.94it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:20,  6.91it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:20,  6.85it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.78it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:20,  6.74it/s]Evaluating:  56%|█████▌    | 174/313 [00:25<00:20,  6.68it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.64it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.66it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:20,  6.69it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:20,  6.74it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:19,  6.79it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:19,  6.85it/s]Evaluating:  58%|█████▊    | 181/313 [00:26<00:19,  6.87it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.84it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.82it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:18,  6.81it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:18,  6.81it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:18,  6.79it/s]Evaluating:  60%|█████▉    | 187/313 [00:27<00:18,  6.80it/s]Evaluating:  60%|██████    | 188/313 [00:27<00:18,  6.81it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.73it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.66it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:18,  6.64it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:18,  6.66it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:18,  6.66it/s]Evaluating:  62%|██████▏   | 194/313 [00:28<00:17,  6.66it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.63it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.62it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.60it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:17,  6.59it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:17,  6.60it/s]Evaluating:  64%|██████▍   | 200/313 [00:29<00:17,  6.60it/s]Evaluating:  64%|██████▍   | 201/313 [00:29<00:16,  6.63it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.65it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:17,  6.47it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:16,  6.53it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:16,  6.61it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:15,  6.71it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:15,  6.80it/s]Evaluating:  66%|██████▋   | 208/313 [00:30<00:15,  6.79it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.76it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.79it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:14,  6.81it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:14,  6.83it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:14,  6.87it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:14,  6.90it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.86it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.83it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:13,  6.87it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:13,  6.91it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:13,  6.92it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:13,  6.92it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:13,  6.96it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  7.00it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:15,  5.91it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:14,  6.19it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:13,  6.39it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:13,  6.51it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:12,  6.62it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  6.69it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.71it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.64it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.61it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:12,  6.58it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:12,  6.63it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:11,  6.64it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.66it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.69it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.73it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.76it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:10,  6.82it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:11,  6.63it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:11,  6.35it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:11,  6.21it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:11,  6.17it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:11,  6.11it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:11,  6.14it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:10,  6.20it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:10,  6.24it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:10,  6.29it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:10,  6.33it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.37it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.40it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:09,  6.38it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:09,  6.40it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:09,  6.46it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.49it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.53it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.57it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.63it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:08,  6.69it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:07,  6.76it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:08,  6.41it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.44it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.59it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.67it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.69it/s]Evaluating:  85%|████████▍ | 266/313 [00:39<00:07,  6.71it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:06,  6.71it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.71it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.72it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.64it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.57it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:06,  6.53it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:06,  6.53it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:06,  6.49it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.50it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.52it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.55it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.57it/s]Evaluating:  89%|████████▉ | 279/313 [00:41<00:05,  6.54it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:05,  6.56it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.61it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.55it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.52it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.53it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.56it/s]Evaluating:  91%|█████████▏| 286/313 [00:42<00:04,  6.53it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.50it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:04,  5.09it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:04,  5.49it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  5.84it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.14it/s]Evaluating:  93%|█████████▎| 292/313 [00:43<00:03,  6.32it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.49it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.59it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.34it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.47it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.55it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.64it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.68it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:01,  6.70it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.67it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.68it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.68it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  6.71it/s]Evaluating:  97%|█████████▋| 305/313 [00:45<00:01,  6.68it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.66it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.59it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.52it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.50it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.49it/s]Evaluating:  99%|█████████▉| 311/313 [00:46<00:00,  6.46it/s]Evaluating: 100%|█████████▉| 312/313 [00:46<00:00,  6.44it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.66it/s]
10/08/2021 22:25:56 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/08/2021 22:25:56 - INFO - __main__ -     f1 = 0.5880510018214936
10/08/2021 22:25:56 - INFO - __main__ -     loss = 2.0055902322260337
10/08/2021 22:25:56 - INFO - __main__ -     precision = 0.5752262846554059
10/08/2021 22:25:56 - INFO - __main__ -     recall = 0.6014606155451225
10/08/2021 22:25:57 - INFO - __main__ -   Language adapter for be not found, using en instead
10/08/2021 22:25:57 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:25:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/08/2021 22:25:57 - INFO - __main__ -   ***** Running evaluation  in be *****
10/08/2021 22:25:57 - INFO - __main__ -     Num examples = 1001
10/08/2021 22:25:57 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.78it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.79it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.79it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.78it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.79it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.78it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.79it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.78it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.78it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.77it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.77it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.77it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.77it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.77it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.77it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.76it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.75it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.76it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.76it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.75it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.75it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.74it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.73it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.74it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.75it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.75it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  6.76it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.75it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.73it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.68it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  5.92it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.81it/s]
10/08/2021 22:26:02 - INFO - __main__ -   ***** Evaluation result  in be *****
10/08/2021 22:26:02 - INFO - __main__ -     f1 = 0.6339741074931345
10/08/2021 22:26:02 - INFO - __main__ -     loss = 1.600527685135603
10/08/2021 22:26:02 - INFO - __main__ -     precision = 0.6066066066066066
10/08/2021 22:26:02 - INFO - __main__ -     recall = 0.6639276910435498
10/08/2021 22:26:02 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:26:15 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:26:15 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:26:41 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 22:26:44 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:26:44 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 22:26:44 - INFO - __main__ -   Seed = 42
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:27:00 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 22:27:17 - INFO - root -   save model
10/08/2021 22:27:17 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:27:22 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:27:22 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:27:49 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:27:49 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 22:27:49 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/08/2021 22:27:49 - INFO - root -   Trying to decide if add adapter
10/08/2021 22:27:49 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 22:27:49 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 22:27:49 - INFO - __main__ -   Language = en
10/08/2021 22:27:49 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 22:28:05 - INFO - __main__ -   Language adapter for ar not found, using en instead
10/08/2021 22:28:05 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:28:05 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
10/08/2021 22:28:07 - INFO - __main__ -   ***** Running evaluation  in ar *****
10/08/2021 22:28:07 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:28:07 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:54,  5.74it/s]Evaluating:   1%|          | 2/313 [00:00<01:06,  4.68it/s]Evaluating:   1%|          | 3/313 [00:00<00:54,  5.64it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:49,  6.28it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:46,  6.69it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:44,  6.96it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:42,  7.15it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:41,  7.27it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:41,  7.35it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:40,  7.40it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:40,  7.41it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:40,  7.43it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:40,  7.44it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:39,  7.48it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:39,  7.50it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:39,  7.50it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:39,  7.50it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:39,  7.51it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:39,  7.50it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:39,  7.50it/s]Evaluating:   7%|▋         | 21/313 [00:02<00:38,  7.50it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:38,  7.49it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:38,  7.47it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:38,  7.45it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:38,  7.46it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:38,  7.46it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:38,  7.47it/s]Evaluating:   9%|▉         | 28/313 [00:03<00:38,  7.46it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:38,  7.45it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:38,  7.45it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:37,  7.45it/s]Evaluating:  10%|█         | 32/313 [00:04<00:37,  7.46it/s]Evaluating:  11%|█         | 33/313 [00:04<00:37,  7.46it/s]Evaluating:  11%|█         | 34/313 [00:04<00:37,  7.44it/s]Evaluating:  11%|█         | 35/313 [00:04<00:37,  7.46it/s]Evaluating:  12%|█▏        | 36/313 [00:04<00:37,  7.45it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:37,  7.45it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:36,  7.44it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:37,  7.39it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:37,  7.36it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:36,  7.36it/s]Evaluating:  13%|█▎        | 42/313 [00:05<00:36,  7.38it/s]Evaluating:  14%|█▎        | 43/313 [00:05<00:36,  7.39it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:36,  7.40it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:36,  7.41it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:36,  7.41it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:35,  7.42it/s]Evaluating:  15%|█▌        | 48/313 [00:06<00:35,  7.43it/s]Evaluating:  16%|█▌        | 49/313 [00:06<00:35,  7.42it/s]Evaluating:  16%|█▌        | 50/313 [00:06<00:35,  7.42it/s]Evaluating:  16%|█▋        | 51/313 [00:06<00:35,  7.41it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:35,  7.41it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:35,  7.40it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:34,  7.40it/s]Evaluating:  18%|█▊        | 55/313 [00:07<00:34,  7.41it/s]Evaluating:  18%|█▊        | 56/313 [00:07<00:34,  7.41it/s]Evaluating:  18%|█▊        | 57/313 [00:07<00:34,  7.40it/s]Evaluating:  19%|█▊        | 58/313 [00:07<00:34,  7.38it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:34,  7.38it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:34,  7.39it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:34,  7.40it/s]Evaluating:  20%|█▉        | 62/313 [00:08<00:33,  7.39it/s]Evaluating:  20%|██        | 63/313 [00:08<00:33,  7.38it/s]Evaluating:  20%|██        | 64/313 [00:08<00:33,  7.38it/s]Evaluating:  21%|██        | 65/313 [00:08<00:33,  7.37it/s]Evaluating:  21%|██        | 66/313 [00:09<00:33,  7.37it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:33,  7.37it/s]Evaluating:  22%|██▏       | 68/313 [00:09<00:33,  7.36it/s]Evaluating:  22%|██▏       | 69/313 [00:09<00:33,  7.38it/s]Evaluating:  22%|██▏       | 70/313 [00:09<00:32,  7.37it/s]Evaluating:  23%|██▎       | 71/313 [00:09<00:32,  7.38it/s]Evaluating:  23%|██▎       | 72/313 [00:09<00:32,  7.35it/s]Evaluating:  23%|██▎       | 73/313 [00:09<00:32,  7.36it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:32,  7.32it/s]Evaluating:  24%|██▍       | 75/313 [00:10<00:32,  7.33it/s]Evaluating:  24%|██▍       | 76/313 [00:10<00:37,  6.39it/s]Evaluating:  25%|██▍       | 77/313 [00:10<00:35,  6.62it/s]Evaluating:  25%|██▍       | 78/313 [00:10<00:34,  6.81it/s]Evaluating:  25%|██▌       | 79/313 [00:10<00:33,  6.93it/s]Evaluating:  26%|██▌       | 80/313 [00:10<00:33,  7.02it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:32,  7.07it/s]Evaluating:  26%|██▌       | 82/313 [00:11<00:32,  7.14it/s]Evaluating:  27%|██▋       | 83/313 [00:11<00:32,  7.15it/s]Evaluating:  27%|██▋       | 84/313 [00:11<00:31,  7.16it/s]Evaluating:  27%|██▋       | 85/313 [00:11<00:31,  7.21it/s]Evaluating:  27%|██▋       | 86/313 [00:11<00:31,  7.25it/s]Evaluating:  28%|██▊       | 87/313 [00:11<00:31,  7.27it/s]Evaluating:  28%|██▊       | 88/313 [00:12<00:31,  7.26it/s]Evaluating:  28%|██▊       | 89/313 [00:12<00:31,  7.21it/s]Evaluating:  29%|██▉       | 90/313 [00:12<00:31,  7.19it/s]Evaluating:  29%|██▉       | 91/313 [00:12<00:30,  7.23it/s]Evaluating:  29%|██▉       | 92/313 [00:12<00:30,  7.27it/s]Evaluating:  30%|██▉       | 93/313 [00:12<00:30,  7.28it/s]Evaluating:  30%|███       | 94/313 [00:12<00:30,  7.30it/s]Evaluating:  30%|███       | 95/313 [00:13<00:29,  7.30it/s]Evaluating:  31%|███       | 96/313 [00:13<00:29,  7.31it/s]Evaluating:  31%|███       | 97/313 [00:13<00:29,  7.32it/s]Evaluating:  31%|███▏      | 98/313 [00:13<00:29,  7.32it/s]Evaluating:  32%|███▏      | 99/313 [00:13<00:29,  7.30it/s]Evaluating:  32%|███▏      | 100/313 [00:13<00:29,  7.29it/s]Evaluating:  32%|███▏      | 101/313 [00:13<00:29,  7.26it/s]Evaluating:  33%|███▎      | 102/313 [00:14<00:29,  7.24it/s]Evaluating:  33%|███▎      | 103/313 [00:14<00:28,  7.26it/s]Evaluating:  33%|███▎      | 104/313 [00:14<00:28,  7.28it/s]Evaluating:  34%|███▎      | 105/313 [00:14<00:28,  7.29it/s]Evaluating:  34%|███▍      | 106/313 [00:14<00:28,  7.29it/s]Evaluating:  34%|███▍      | 107/313 [00:14<00:28,  7.29it/s]Evaluating:  35%|███▍      | 108/313 [00:14<00:28,  7.28it/s]Evaluating:  35%|███▍      | 109/313 [00:14<00:28,  7.28it/s]Evaluating:  35%|███▌      | 110/313 [00:15<00:27,  7.30it/s]Evaluating:  35%|███▌      | 111/313 [00:15<00:27,  7.29it/s]Evaluating:  36%|███▌      | 112/313 [00:15<00:27,  7.18it/s]Evaluating:  36%|███▌      | 113/313 [00:15<00:27,  7.22it/s]Evaluating:  36%|███▋      | 114/313 [00:15<00:27,  7.22it/s]Evaluating:  37%|███▋      | 115/313 [00:15<00:27,  7.24it/s]Evaluating:  37%|███▋      | 116/313 [00:15<00:27,  7.24it/s]Evaluating:  37%|███▋      | 117/313 [00:16<00:27,  7.25it/s]Evaluating:  38%|███▊      | 118/313 [00:16<00:26,  7.26it/s]Evaluating:  38%|███▊      | 119/313 [00:16<00:26,  7.26it/s]Evaluating:  38%|███▊      | 120/313 [00:16<00:27,  7.11it/s]Evaluating:  39%|███▊      | 121/313 [00:16<00:27,  7.09it/s]Evaluating:  39%|███▉      | 122/313 [00:16<00:27,  7.07it/s]Evaluating:  39%|███▉      | 123/313 [00:16<00:26,  7.12it/s]Evaluating:  40%|███▉      | 124/313 [00:17<00:26,  7.15it/s]Evaluating:  40%|███▉      | 125/313 [00:17<00:26,  7.12it/s]Evaluating:  40%|████      | 126/313 [00:17<00:26,  7.13it/s]Evaluating:  41%|████      | 127/313 [00:17<00:25,  7.16it/s]Evaluating:  41%|████      | 128/313 [00:17<00:25,  7.13it/s]Evaluating:  41%|████      | 129/313 [00:17<00:25,  7.09it/s]Evaluating:  42%|████▏     | 130/313 [00:17<00:25,  7.09it/s]Evaluating:  42%|████▏     | 131/313 [00:18<00:25,  7.01it/s]Evaluating:  42%|████▏     | 132/313 [00:18<00:25,  7.01it/s]Evaluating:  42%|████▏     | 133/313 [00:18<00:25,  7.03it/s]Evaluating:  43%|████▎     | 134/313 [00:18<00:25,  7.08it/s]Evaluating:  43%|████▎     | 135/313 [00:18<00:25,  7.06it/s]Evaluating:  43%|████▎     | 136/313 [00:18<00:24,  7.08it/s]Evaluating:  44%|████▍     | 137/313 [00:18<00:24,  7.11it/s]Evaluating:  44%|████▍     | 138/313 [00:19<00:24,  7.09it/s]Evaluating:  44%|████▍     | 139/313 [00:19<00:24,  7.07it/s]Evaluating:  45%|████▍     | 140/313 [00:19<00:24,  7.08it/s]Evaluating:  45%|████▌     | 141/313 [00:19<00:24,  7.03it/s]Evaluating:  45%|████▌     | 142/313 [00:19<00:24,  6.96it/s]Evaluating:  46%|████▌     | 143/313 [00:19<00:24,  6.92it/s]Evaluating:  46%|████▌     | 144/313 [00:19<00:24,  6.87it/s]Evaluating:  46%|████▋     | 145/313 [00:20<00:24,  6.84it/s]Evaluating:  47%|████▋     | 146/313 [00:20<00:24,  6.82it/s]Evaluating:  47%|████▋     | 147/313 [00:20<00:28,  5.85it/s]Evaluating:  47%|████▋     | 148/313 [00:20<00:26,  6.12it/s]Evaluating:  48%|████▊     | 149/313 [00:20<00:25,  6.37it/s]Evaluating:  48%|████▊     | 150/313 [00:20<00:24,  6.54it/s]Evaluating:  48%|████▊     | 151/313 [00:21<00:24,  6.69it/s]Evaluating:  49%|████▊     | 152/313 [00:21<00:23,  6.83it/s]Evaluating:  49%|████▉     | 153/313 [00:21<00:23,  6.93it/s]Evaluating:  49%|████▉     | 154/313 [00:21<00:22,  7.00it/s]Evaluating:  50%|████▉     | 155/313 [00:21<00:22,  7.03it/s]Evaluating:  50%|████▉     | 156/313 [00:21<00:22,  7.02it/s]Evaluating:  50%|█████     | 157/313 [00:21<00:22,  7.00it/s]Evaluating:  50%|█████     | 158/313 [00:22<00:22,  6.99it/s]Evaluating:  51%|█████     | 159/313 [00:22<00:21,  7.00it/s]Evaluating:  51%|█████     | 160/313 [00:22<00:21,  7.03it/s]Evaluating:  51%|█████▏    | 161/313 [00:22<00:21,  7.05it/s]Evaluating:  52%|█████▏    | 162/313 [00:22<00:21,  7.07it/s]Evaluating:  52%|█████▏    | 163/313 [00:22<00:21,  7.07it/s]Evaluating:  52%|█████▏    | 164/313 [00:22<00:21,  6.86it/s]Evaluating:  53%|█████▎    | 165/313 [00:23<00:21,  6.89it/s]Evaluating:  53%|█████▎    | 166/313 [00:23<00:21,  6.91it/s]Evaluating:  53%|█████▎    | 167/313 [00:23<00:23,  6.34it/s]Evaluating:  54%|█████▎    | 168/313 [00:23<00:22,  6.56it/s]Evaluating:  54%|█████▍    | 169/313 [00:23<00:21,  6.73it/s]Evaluating:  54%|█████▍    | 170/313 [00:23<00:21,  6.77it/s]Evaluating:  55%|█████▍    | 171/313 [00:23<00:20,  6.86it/s]Evaluating:  55%|█████▍    | 172/313 [00:24<00:20,  6.94it/s]Evaluating:  55%|█████▌    | 173/313 [00:24<00:20,  7.00it/s]Evaluating:  56%|█████▌    | 174/313 [00:24<00:19,  7.04it/s]Evaluating:  56%|█████▌    | 175/313 [00:24<00:19,  7.06it/s]Evaluating:  56%|█████▌    | 176/313 [00:24<00:19,  7.05it/s]Evaluating:  57%|█████▋    | 177/313 [00:24<00:19,  7.01it/s]Evaluating:  57%|█████▋    | 178/313 [00:24<00:19,  7.01it/s]Evaluating:  57%|█████▋    | 179/313 [00:25<00:19,  6.99it/s]Evaluating:  58%|█████▊    | 180/313 [00:25<00:19,  6.98it/s]Evaluating:  58%|█████▊    | 181/313 [00:25<00:18,  7.00it/s]Evaluating:  58%|█████▊    | 182/313 [00:25<00:18,  6.99it/s]Evaluating:  58%|█████▊    | 183/313 [00:25<00:18,  6.91it/s]Evaluating:  59%|█████▉    | 184/313 [00:25<00:18,  6.88it/s]Evaluating:  59%|█████▉    | 185/313 [00:25<00:18,  6.88it/s]Evaluating:  59%|█████▉    | 186/313 [00:26<00:18,  6.87it/s]Evaluating:  60%|█████▉    | 187/313 [00:26<00:18,  6.87it/s]Evaluating:  60%|██████    | 188/313 [00:26<00:18,  6.85it/s]Evaluating:  60%|██████    | 189/313 [00:26<00:18,  6.88it/s]Evaluating:  61%|██████    | 190/313 [00:26<00:18,  6.82it/s]Evaluating:  61%|██████    | 191/313 [00:26<00:18,  6.75it/s]Evaluating:  61%|██████▏   | 192/313 [00:26<00:18,  6.72it/s]Evaluating:  62%|██████▏   | 193/313 [00:27<00:17,  6.70it/s]Evaluating:  62%|██████▏   | 194/313 [00:27<00:17,  6.70it/s]Evaluating:  62%|██████▏   | 195/313 [00:27<00:17,  6.73it/s]Evaluating:  63%|██████▎   | 196/313 [00:27<00:17,  6.75it/s]Evaluating:  63%|██████▎   | 197/313 [00:27<00:17,  6.79it/s]Evaluating:  63%|██████▎   | 198/313 [00:27<00:16,  6.83it/s]Evaluating:  64%|██████▎   | 199/313 [00:27<00:16,  6.88it/s]Evaluating:  64%|██████▍   | 200/313 [00:28<00:16,  6.92it/s]Evaluating:  64%|██████▍   | 201/313 [00:28<00:16,  6.97it/s]Evaluating:  65%|██████▍   | 202/313 [00:28<00:16,  6.70it/s]Evaluating:  65%|██████▍   | 203/313 [00:28<00:23,  4.60it/s]Evaluating:  65%|██████▌   | 204/313 [00:29<00:39,  2.76it/s]Evaluating:  65%|██████▌   | 205/313 [00:29<00:38,  2.83it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:34,  3.12it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:30,  3.52it/s]Evaluating:  66%|██████▋   | 208/313 [00:30<00:27,  3.87it/s]Evaluating:  67%|██████▋   | 209/313 [00:30<00:24,  4.20it/s]Evaluating:  67%|██████▋   | 210/313 [00:30<00:22,  4.60it/s]Evaluating:  67%|██████▋   | 211/313 [00:30<00:20,  4.94it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:19,  5.26it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:18,  5.54it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:17,  5.75it/s]Evaluating:  69%|██████▊   | 215/313 [00:31<00:16,  5.94it/s]Evaluating:  69%|██████▉   | 216/313 [00:31<00:15,  6.08it/s]Evaluating:  69%|██████▉   | 217/313 [00:31<00:15,  6.20it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:15,  6.29it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:14,  6.36it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:14,  6.40it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:14,  6.43it/s]Evaluating:  71%|███████   | 222/313 [00:32<00:14,  6.45it/s]Evaluating:  71%|███████   | 223/313 [00:32<00:13,  6.45it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:13,  6.46it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:13,  6.49it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:13,  6.52it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:13,  6.55it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:12,  6.55it/s]Evaluating:  73%|███████▎  | 229/313 [00:33<00:12,  6.54it/s]Evaluating:  73%|███████▎  | 230/313 [00:33<00:12,  6.53it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.51it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:12,  6.50it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:12,  6.51it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:12,  6.51it/s]Evaluating:  75%|███████▌  | 235/313 [00:34<00:12,  6.49it/s]Evaluating:  75%|███████▌  | 236/313 [00:34<00:12,  6.29it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.35it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.45it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:14,  5.02it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:13,  5.45it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:12,  5.83it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:11,  6.14it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.37it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.53it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.67it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:09,  6.76it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:09,  6.82it/s]Evaluating:  79%|███████▉  | 248/313 [00:36<00:09,  6.88it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:09,  6.93it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.96it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:08,  6.99it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:08,  6.96it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:08,  6.89it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:08,  6.84it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.81it/s]Evaluating:  82%|████████▏ | 256/313 [00:37<00:08,  6.81it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.81it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.74it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:07,  6.77it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:07,  6.78it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:07,  6.79it/s]Evaluating:  84%|████████▎ | 262/313 [00:38<00:07,  6.81it/s]Evaluating:  84%|████████▍ | 263/313 [00:38<00:07,  6.87it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.90it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.81it/s]Evaluating:  85%|████████▍ | 266/313 [00:39<00:07,  6.65it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:06,  6.58it/s]Evaluating:  86%|████████▌ | 268/313 [00:39<00:06,  6.52it/s]Evaluating:  86%|████████▌ | 269/313 [00:39<00:06,  6.51it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.50it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.48it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:07,  5.59it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:06,  5.84it/s]Evaluating:  88%|████████▊ | 274/313 [00:40<00:06,  6.06it/s]Evaluating:  88%|████████▊ | 275/313 [00:40<00:06,  6.25it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.40it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.52it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.62it/s]Evaluating:  89%|████████▉ | 279/313 [00:41<00:05,  6.71it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:04,  6.77it/s]Evaluating:  90%|████████▉ | 281/313 [00:41<00:04,  6.83it/s]Evaluating:  90%|█████████ | 282/313 [00:41<00:04,  6.57it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.45it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.34it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.30it/s]Evaluating:  91%|█████████▏| 286/313 [00:42<00:04,  6.26it/s]Evaluating:  92%|█████████▏| 287/313 [00:42<00:04,  6.22it/s]Evaluating:  92%|█████████▏| 288/313 [00:42<00:03,  6.25it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.28it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.28it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.33it/s]Evaluating:  93%|█████████▎| 292/313 [00:43<00:03,  6.41it/s]Evaluating:  94%|█████████▎| 293/313 [00:43<00:03,  6.22it/s]Evaluating:  94%|█████████▍| 294/313 [00:43<00:03,  6.21it/s]Evaluating:  94%|█████████▍| 295/313 [00:43<00:02,  6.33it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.43it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.51it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.55it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.51it/s]Evaluating:  96%|█████████▌| 300/313 [00:44<00:01,  6.58it/s]Evaluating:  96%|█████████▌| 301/313 [00:44<00:01,  6.53it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.47it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.54it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  4.66it/s]Evaluating:  97%|█████████▋| 305/313 [00:45<00:01,  5.07it/s]Evaluating:  98%|█████████▊| 306/313 [00:45<00:01,  5.42it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:01,  5.71it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  5.91it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.09it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.24it/s]Evaluating:  99%|█████████▉| 311/313 [00:46<00:00,  6.33it/s]Evaluating: 100%|█████████▉| 312/313 [00:46<00:00,  6.42it/s]Evaluating: 100%|██████████| 313/313 [00:46<00:00,  6.68it/s]
10/08/2021 22:28:55 - INFO - __main__ -   ***** Evaluation result  in ar *****
10/08/2021 22:28:55 - INFO - __main__ -     f1 = 0.2958695439898447
10/08/2021 22:28:55 - INFO - __main__ -     loss = 5.478063976421905
10/08/2021 22:28:55 - INFO - __main__ -     precision = 0.3285265098124255
10/08/2021 22:28:55 - INFO - __main__ -     recall = 0.2691180389022116
10/08/2021 22:28:55 - INFO - __main__ -   Language adapter for bh not found, using en instead
10/08/2021 22:28:55 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:28:55 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bh_bert-base-multilingual-cased_128
10/08/2021 22:28:55 - INFO - __main__ -   ***** Running evaluation  in bh *****
10/08/2021 22:28:55 - INFO - __main__ -     Num examples = 102
10/08/2021 22:28:55 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  6.78it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  6.77it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  6.77it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.39it/s]
10/08/2021 22:28:55 - INFO - __main__ -   ***** Evaluation result  in bh *****
10/08/2021 22:28:55 - INFO - __main__ -     f1 = 0.417910447761194
10/08/2021 22:28:55 - INFO - __main__ -     loss = 3.028359353542328
10/08/2021 22:28:55 - INFO - __main__ -     precision = 0.36363636363636365
10/08/2021 22:28:55 - INFO - __main__ -     recall = 0.49122807017543857
10/08/2021 22:28:55 - INFO - __main__ -   Language adapter for hi not found, using en instead
10/08/2021 22:28:55 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:28:55 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
10/08/2021 22:28:55 - INFO - __main__ -   ***** Running evaluation  in hi *****
10/08/2021 22:28:55 - INFO - __main__ -     Num examples = 1000
10/08/2021 22:28:55 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.80it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.78it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.78it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.78it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.79it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.79it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.78it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.78it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.78it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.78it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:05,  4.09it/s]Evaluating:  38%|███▊      | 12/32 [00:02<00:04,  4.65it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  5.14it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:03,  5.54it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  5.87it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.11it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.29it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.43it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:01,  6.52it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.58it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.63it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.65it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.68it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.70it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.72it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.72it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.73it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.73it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.70it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.70it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.71it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.46it/s]
10/08/2021 22:29:00 - INFO - __main__ -   ***** Evaluation result  in hi *****
10/08/2021 22:29:00 - INFO - __main__ -     f1 = 0.5397727272727273
10/08/2021 22:29:00 - INFO - __main__ -     loss = 2.9565800800919533
10/08/2021 22:29:00 - INFO - __main__ -     precision = 0.5380258899676376
10/08/2021 22:29:00 - INFO - __main__ -     recall = 0.5415309446254072
10/08/2021 22:29:00 - INFO - __main__ -   Language adapter for fo not found, using en instead
10/08/2021 22:29:00 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:29:00 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/08/2021 22:29:00 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/08/2021 22:29:00 - INFO - __main__ -     Num examples = 100
10/08/2021 22:29:00 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  7.49it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  7.41it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  7.37it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  9.07it/s]
10/08/2021 22:29:01 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/08/2021 22:29:01 - INFO - __main__ -     f1 = 0.6367041198501873
10/08/2021 22:29:01 - INFO - __main__ -     loss = 2.1897925436496735
10/08/2021 22:29:01 - INFO - __main__ -     precision = 0.5782312925170068
10/08/2021 22:29:01 - INFO - __main__ -     recall = 0.7083333333333334
10/08/2021 22:29:01 - INFO - __main__ -   Language adapter for no not found, using en instead
10/08/2021 22:29:01 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:29:01 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/08/2021 22:29:02 - INFO - __main__ -   ***** Running evaluation  in no *****
10/08/2021 22:29:02 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:29:02 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.78it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.78it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.79it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.78it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.77it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:43,  6.77it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.78it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.73it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.72it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.71it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.71it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.72it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.73it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.73it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.70it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.69it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.70it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.70it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.71it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.70it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.71it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.71it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.73it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.72it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.72it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.72it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.73it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.68it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.69it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.70it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.69it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.70it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.69it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.70it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.70it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.71it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.71it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.71it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.70it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.69it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.70it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.70it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.70it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:44,  5.56it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:41,  5.97it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:38,  6.28it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:37,  6.50it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.67it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:35,  6.74it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:35,  6.80it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.78it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:34,  6.84it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:34,  6.86it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:34,  6.87it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:34,  6.87it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:34,  6.88it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:34,  6.82it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.78it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.72it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.62it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.60it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.63it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:34,  6.64it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.65it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.65it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.66it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:33,  6.66it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.65it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.66it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.64it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:32,  6.64it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:33,  6.34it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.48it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.62it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.73it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:30,  6.85it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:29,  6.92it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:29,  6.96it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:29,  6.99it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:29,  6.97it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:29,  6.99it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:28,  7.01it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:28,  6.99it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:28,  6.94it/s]Evaluating:  36%|███▋      | 114/313 [00:16<00:28,  6.87it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.81it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.77it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.72it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.69it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.66it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:29,  6.65it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.64it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.63it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|████      | 126/313 [00:18<00:28,  6.62it/s]Evaluating:  41%|████      | 127/313 [00:18<00:28,  6.63it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.62it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.63it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.65it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.67it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.67it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:27,  6.63it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:27,  6.61it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:28,  6.36it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:27,  6.47it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:27,  6.51it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.55it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.60it/s]Evaluating:  45%|████▍     | 140/313 [00:20<00:26,  6.62it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.61it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.61it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.54it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.55it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:25,  6.56it/s]Evaluating:  47%|████▋     | 147/313 [00:21<00:25,  6.55it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.56it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.59it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.54it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.54it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.56it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:24,  6.56it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.56it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:24,  6.57it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.57it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.57it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.58it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:23,  6.58it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:23,  6.60it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:22,  6.61it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.62it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.60it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.59it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:22,  6.61it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:22,  6.62it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:22,  6.63it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.63it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.63it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.65it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:21,  6.61it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:21,  6.64it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:20,  6.70it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.67it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.63it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.63it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:20,  6.61it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:20,  6.59it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:20,  6.57it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:20,  6.58it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:20,  6.51it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:20,  6.52it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.52it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:19,  6.52it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:19,  6.52it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:19,  6.53it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:19,  6.56it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.59it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.62it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.61it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:18,  6.61it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:18,  6.59it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:18,  6.60it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:17,  6.63it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.70it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.76it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.74it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:17,  6.70it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:17,  6.68it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:20,  5.54it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:19,  5.82it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:18,  6.05it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:17,  6.18it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:17,  6.28it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:16,  6.35it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:16,  6.40it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:16,  6.44it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:16,  6.46it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:16,  6.49it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.52it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:15,  6.54it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:15,  6.51it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:15,  6.53it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:15,  6.56it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.58it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.60it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:14,  6.64it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:14,  6.60it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:14,  6.57it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:14,  6.58it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:13,  6.57it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.55it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.54it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:13,  6.56it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:13,  6.57it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:13,  6.55it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:13,  6.55it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.55it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.59it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.60it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.62it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:12,  6.64it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:12,  6.66it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.65it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.63it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.63it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.61it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.57it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:11,  6.54it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.53it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.51it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.49it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.49it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.42it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.45it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:10,  6.45it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:10,  6.46it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.51it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.56it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.56it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.54it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:09,  6.54it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:09,  6.57it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:09,  6.55it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.56it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.58it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.60it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.63it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.64it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:07,  6.64it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:07,  6.66it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.69it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.74it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.78it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.78it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.07it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:07,  6.32it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.51it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.65it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.74it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.81it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:05,  6.86it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:05,  6.88it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.92it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.59it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.22it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.05it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.00it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  5.95it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  5.93it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:05,  6.00it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:05,  6.10it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.14it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.20it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.25it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.12it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.25it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.34it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.42it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.48it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.46it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.43it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.49it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.40it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.37it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.38it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:03,  5.23it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  5.61it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  5.92it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  6.16it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.36it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.54it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.65it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.76it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.78it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.76it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.77it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.80it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.85it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  6.81it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.76it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.73it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.60it/s]
10/08/2021 22:29:51 - INFO - __main__ -   ***** Evaluation result  in no *****
10/08/2021 22:29:51 - INFO - __main__ -     f1 = 0.7525405833443315
10/08/2021 22:29:51 - INFO - __main__ -     loss = 1.009679929707378
10/08/2021 22:29:51 - INFO - __main__ -     precision = 0.7169621526468
10/08/2021 22:29:51 - INFO - __main__ -     recall = 0.7918344674350785
10/08/2021 22:29:51 - INFO - __main__ -   Language adapter for da not found, using en instead
10/08/2021 22:29:51 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:29:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/08/2021 22:29:53 - INFO - __main__ -   ***** Running evaluation  in da *****
10/08/2021 22:29:53 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:29:53 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.79it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.77it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.77it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.77it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.76it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.76it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.74it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.75it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.74it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.73it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.73it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.73it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.74it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.73it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.73it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.72it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.71it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.71it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.72it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.69it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.71it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.71it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.73it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.74it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.73it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.73it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.73it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.74it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.74it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.74it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.74it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.72it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.72it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.73it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.70it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.71it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.68it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.64it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.64it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.65it/s]Evaluating:  21%|██        | 66/313 [00:10<00:55,  4.45it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:49,  4.94it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:45,  5.34it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:42,  5.68it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:40,  5.94it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:39,  6.14it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:38,  6.26it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:37,  6.35it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:37,  6.39it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:36,  6.48it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:36,  6.54it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.56it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.60it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:35,  6.62it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:35,  6.64it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.65it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.63it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.63it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.65it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:34,  6.66it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:33,  6.67it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.66it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.62it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.63it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.64it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.65it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.65it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.65it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.65it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:31,  6.66it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:31,  6.66it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.66it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.66it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:31,  6.65it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:30,  6.65it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.64it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.64it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.64it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.65it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.66it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:29,  6.67it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.66it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.65it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.66it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.65it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.64it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:29,  6.63it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.64it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.63it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.62it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.61it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.59it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.57it/s]Evaluating:  40%|████      | 126/313 [00:19<00:28,  6.57it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.62it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.63it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.64it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:33,  5.53it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:30,  5.89it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:29,  6.17it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:28,  6.39it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:27,  6.55it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.69it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.78it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:25,  6.85it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:25,  6.89it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:25,  6.91it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:25,  6.91it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.87it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.81it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.78it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.70it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.66it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.64it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.63it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.61it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.61it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.63it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.62it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:24,  6.58it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.53it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.56it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.59it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.58it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.59it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.57it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:23,  6.59it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.59it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:22,  6.61it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.63it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.66it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.64it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:22,  6.67it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:21,  6.69it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:21,  6.72it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.73it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.71it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.72it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:21,  6.71it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:20,  6.71it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:20,  6.74it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.75it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.73it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.70it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:20,  6.68it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:20,  6.69it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:19,  6.73it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:19,  6.78it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.82it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.76it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.70it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:19,  6.65it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:19,  6.63it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:19,  6.60it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:19,  6.59it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:19,  6.54it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.55it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.56it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:18,  6.55it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:18,  6.58it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:18,  6.60it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:18,  6.58it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.61it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.65it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.69it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:17,  6.70it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:16,  6.71it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:16,  6.75it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:16,  6.77it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.83it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:15,  6.91it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:15,  6.96it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:15,  6.99it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:15,  7.00it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:15,  7.02it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:14,  7.01it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:14,  6.99it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:14,  6.98it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:14,  6.93it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:14,  6.86it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:14,  6.84it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.83it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.79it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.77it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:14,  6.69it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:14,  6.62it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:14,  6.58it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:14,  6.56it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:14,  6.54it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.51it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.53it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:13,  6.55it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:13,  6.58it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:13,  6.62it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.67it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.72it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.80it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.39it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.58it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:12,  6.70it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:11,  6.76it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.81it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.84it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.86it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.81it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.77it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:10,  6.78it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:10,  6.79it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:10,  6.77it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.81it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.78it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.69it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.66it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:10,  6.66it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.66it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.59it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.59it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.56it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.57it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:09,  6.56it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:09,  6.53it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:09,  6.53it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.50it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.48it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.49it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.48it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.48it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.47it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:08,  6.41it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.42it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:08,  5.82it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:08,  6.11it/s]Evaluating:  85%|████████▍ | 265/313 [00:40<00:07,  6.32it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.50it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:06,  6.62it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.67it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.72it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.74it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.73it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.70it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:05,  6.69it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.69it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.68it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.63it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.57it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.53it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.49it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.47it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.46it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.45it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.44it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.43it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.43it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.45it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.47it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.54it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.60it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.68it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.67it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.61it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.54it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.50it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.49it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:03,  4.44it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:03,  4.90it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  5.28it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  5.59it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  5.83it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.00it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.14it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.25it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.35it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.41it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.44it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.47it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.51it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.53it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  6.58it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.66it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.69it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.59it/s]
10/08/2021 22:30:41 - INFO - __main__ -   ***** Evaluation result  in da *****
10/08/2021 22:30:41 - INFO - __main__ -     f1 = 0.7951849512178003
10/08/2021 22:30:41 - INFO - __main__ -     loss = 0.8534512027574424
10/08/2021 22:30:41 - INFO - __main__ -     precision = 0.7656799134901324
10/08/2021 22:30:41 - INFO - __main__ -     recall = 0.8270550445320485
10/08/2021 22:30:42 - INFO - __main__ -   Language adapter for ru not found, using en instead
10/08/2021 22:30:42 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:30:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
10/08/2021 22:30:43 - INFO - __main__ -   ***** Running evaluation  in ru *****
10/08/2021 22:30:43 - INFO - __main__ -     Num examples = 10002
10/08/2021 22:30:43 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.78it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.79it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.78it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.78it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.77it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.77it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.77it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.76it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.76it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.76it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.75it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|▉         | 31/313 [00:04<01:08,  4.10it/s]Evaluating:  10%|█         | 32/313 [00:05<01:00,  4.65it/s]Evaluating:  11%|█         | 33/313 [00:05<00:54,  5.13it/s]Evaluating:  11%|█         | 34/313 [00:05<00:50,  5.52it/s]Evaluating:  11%|█         | 35/313 [00:05<00:47,  5.84it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:45,  6.09it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:43,  6.27it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:42,  6.41it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:42,  6.50it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:41,  6.57it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:41,  6.62it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.65it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.67it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.70it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.71it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:39,  6.72it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:39,  6.72it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.72it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.72it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:38,  6.73it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:38,  6.73it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.69it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.72it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:37,  6.72it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:37,  6.71it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.71it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.70it/s]Evaluating:  20%|██        | 63/313 [00:09<01:00,  4.11it/s]Evaluating:  20%|██        | 64/313 [00:10<00:53,  4.65it/s]Evaluating:  21%|██        | 65/313 [00:10<00:48,  5.12it/s]Evaluating:  21%|██        | 66/313 [00:10<00:44,  5.51it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:42,  5.82it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:40,  6.05it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:39,  6.23it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:38,  6.36it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:37,  6.46it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:36,  6.53it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:36,  6.58it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:36,  6.61it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.63it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.65it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:35,  6.66it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:35,  6.66it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:35,  6.67it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:34,  6.68it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.69it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.64it/s]Evaluating:  27%|██▋       | 84/313 [00:13<00:34,  6.63it/s]Evaluating:  27%|██▋       | 85/313 [00:13<00:34,  6.65it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:34,  6.66it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:33,  6.66it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.67it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 90/313 [00:14<00:33,  6.67it/s]Evaluating:  29%|██▉       | 91/313 [00:14<00:33,  6.67it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:33,  6.66it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.66it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.66it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███       | 96/313 [00:15<00:39,  5.50it/s]Evaluating:  31%|███       | 97/313 [00:15<00:37,  5.80it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:35,  6.04it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:34,  6.21it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:33,  6.33it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:33,  6.42it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.49it/s]Evaluating:  33%|███▎      | 103/313 [00:16<00:32,  6.51it/s]Evaluating:  33%|███▎      | 104/313 [00:16<00:32,  6.52it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:31,  6.53it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:31,  6.52it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.52it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:31,  6.53it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:31,  6.53it/s]Evaluating:  35%|███▌      | 110/313 [00:17<00:31,  6.53it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:30,  6.54it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:30,  6.55it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.57it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:30,  6.59it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.62it/s]Evaluating:  37%|███▋      | 116/313 [00:18<00:29,  6.64it/s]Evaluating:  37%|███▋      | 117/313 [00:18<00:29,  6.65it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:29,  6.66it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:29,  6.64it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.64it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.64it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.63it/s]Evaluating:  39%|███▉      | 123/313 [00:19<00:28,  6.64it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:28,  6.65it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:28,  6.66it/s]Evaluating:  40%|████      | 126/313 [00:19<00:28,  6.64it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.64it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.63it/s]Evaluating:  41%|████      | 129/313 [00:20<00:41,  4.44it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:37,  4.93it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:34,  5.34it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:32,  5.65it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:30,  5.88it/s]Evaluating:  43%|████▎     | 134/313 [00:21<00:29,  6.09it/s]Evaluating:  43%|████▎     | 135/313 [00:21<00:28,  6.28it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:27,  6.39it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:27,  6.44it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:26,  6.49it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:26,  6.53it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.53it/s]Evaluating:  45%|████▌     | 141/313 [00:22<00:26,  6.55it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:26,  6.54it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:25,  6.56it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:25,  6.58it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:25,  6.58it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.58it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.59it/s]Evaluating:  47%|████▋     | 148/313 [00:23<00:24,  6.61it/s]Evaluating:  48%|████▊     | 149/313 [00:23<00:24,  6.63it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:24,  6.63it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:24,  6.62it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:24,  6.62it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.62it/s]Evaluating:  49%|████▉     | 154/313 [00:24<00:24,  6.61it/s]Evaluating:  50%|████▉     | 155/313 [00:24<00:23,  6.59it/s]Evaluating:  50%|████▉     | 156/313 [00:24<00:23,  6.59it/s]Evaluating:  50%|█████     | 157/313 [00:24<00:23,  6.60it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:23,  6.61it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:23,  6.60it/s]Evaluating:  51%|█████     | 160/313 [00:25<00:26,  5.80it/s]Evaluating:  51%|█████▏    | 161/313 [00:25<00:25,  6.01it/s]Evaluating:  52%|█████▏    | 162/313 [00:25<00:24,  6.17it/s]Evaluating:  52%|█████▏    | 163/313 [00:25<00:23,  6.28it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:23,  6.36it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:23,  6.43it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:22,  6.48it/s]Evaluating:  53%|█████▎    | 167/313 [00:26<00:22,  6.50it/s]Evaluating:  54%|█████▎    | 168/313 [00:26<00:22,  6.52it/s]Evaluating:  54%|█████▍    | 169/313 [00:26<00:22,  6.53it/s]Evaluating:  54%|█████▍    | 170/313 [00:26<00:21,  6.54it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:21,  6.56it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:21,  6.57it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:21,  6.56it/s]Evaluating:  56%|█████▌    | 174/313 [00:27<00:21,  6.56it/s]Evaluating:  56%|█████▌    | 175/313 [00:27<00:21,  6.57it/s]Evaluating:  56%|█████▌    | 176/313 [00:27<00:20,  6.57it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:20,  6.56it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:20,  6.57it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:20,  6.57it/s]Evaluating:  58%|█████▊    | 180/313 [00:28<00:20,  6.50it/s]Evaluating:  58%|█████▊    | 181/313 [00:28<00:20,  6.51it/s]Evaluating:  58%|█████▊    | 182/313 [00:28<00:20,  6.52it/s]Evaluating:  58%|█████▊    | 183/313 [00:28<00:19,  6.53it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:19,  6.53it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:19,  6.54it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:19,  6.54it/s]Evaluating:  60%|█████▉    | 187/313 [00:29<00:19,  6.57it/s]Evaluating:  60%|██████    | 188/313 [00:29<00:19,  6.58it/s]Evaluating:  60%|██████    | 189/313 [00:29<00:18,  6.59it/s]Evaluating:  61%|██████    | 190/313 [00:29<00:18,  6.60it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:18,  6.60it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:18,  6.61it/s]Evaluating:  62%|██████▏   | 193/313 [00:30<00:18,  6.52it/s]Evaluating:  62%|██████▏   | 194/313 [00:30<00:18,  6.56it/s]Evaluating:  62%|██████▏   | 195/313 [00:30<00:17,  6.62it/s]Evaluating:  63%|██████▎   | 196/313 [00:30<00:17,  6.59it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:17,  6.63it/s]Evaluating:  63%|██████▎   | 198/313 [00:30<00:17,  6.69it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:16,  6.75it/s]Evaluating:  64%|██████▍   | 200/313 [00:31<00:16,  6.72it/s]Evaluating:  64%|██████▍   | 201/313 [00:31<00:16,  6.68it/s]Evaluating:  65%|██████▍   | 202/313 [00:31<00:16,  6.67it/s]Evaluating:  65%|██████▍   | 203/313 [00:31<00:16,  6.65it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:16,  6.61it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:16,  6.59it/s]Evaluating:  66%|██████▌   | 206/313 [00:32<00:16,  6.57it/s]Evaluating:  66%|██████▌   | 207/313 [00:32<00:16,  6.55it/s]Evaluating:  66%|██████▋   | 208/313 [00:32<00:16,  6.54it/s]Evaluating:  67%|██████▋   | 209/313 [00:32<00:15,  6.54it/s]Evaluating:  67%|██████▋   | 210/313 [00:32<00:15,  6.54it/s]Evaluating:  67%|██████▋   | 211/313 [00:32<00:15,  6.53it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:15,  6.52it/s]Evaluating:  68%|██████▊   | 213/313 [00:33<00:15,  6.52it/s]Evaluating:  68%|██████▊   | 214/313 [00:33<00:15,  6.51it/s]Evaluating:  69%|██████▊   | 215/313 [00:33<00:15,  6.51it/s]Evaluating:  69%|██████▉   | 216/313 [00:33<00:14,  6.51it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:14,  6.51it/s]Evaluating:  70%|██████▉   | 218/313 [00:33<00:14,  6.53it/s]Evaluating:  70%|██████▉   | 219/313 [00:34<00:14,  6.51it/s]Evaluating:  70%|███████   | 220/313 [00:34<00:14,  6.52it/s]Evaluating:  71%|███████   | 221/313 [00:34<00:14,  6.54it/s]Evaluating:  71%|███████   | 222/313 [00:34<00:13,  6.56it/s]Evaluating:  71%|███████   | 223/313 [00:34<00:13,  6.57it/s]Evaluating:  72%|███████▏  | 224/313 [00:34<00:13,  6.53it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:13,  6.49it/s]Evaluating:  72%|███████▏  | 226/313 [00:35<00:16,  5.21it/s]Evaluating:  73%|███████▎  | 227/313 [00:35<00:15,  5.60it/s]Evaluating:  73%|███████▎  | 228/313 [00:35<00:14,  5.95it/s]Evaluating:  73%|███████▎  | 229/313 [00:35<00:13,  6.25it/s]Evaluating:  73%|███████▎  | 230/313 [00:35<00:12,  6.45it/s]Evaluating:  74%|███████▍  | 231/313 [00:35<00:12,  6.57it/s]Evaluating:  74%|███████▍  | 232/313 [00:36<00:12,  6.68it/s]Evaluating:  74%|███████▍  | 233/313 [00:36<00:11,  6.75it/s]Evaluating:  75%|███████▍  | 234/313 [00:36<00:11,  6.81it/s]Evaluating:  75%|███████▌  | 235/313 [00:36<00:11,  6.86it/s]Evaluating:  75%|███████▌  | 236/313 [00:36<00:11,  6.87it/s]Evaluating:  76%|███████▌  | 237/313 [00:36<00:11,  6.90it/s]Evaluating:  76%|███████▌  | 238/313 [00:36<00:10,  6.95it/s]Evaluating:  76%|███████▋  | 239/313 [00:37<00:10,  6.96it/s]Evaluating:  77%|███████▋  | 240/313 [00:37<00:10,  6.95it/s]Evaluating:  77%|███████▋  | 241/313 [00:37<00:10,  6.91it/s]Evaluating:  77%|███████▋  | 242/313 [00:37<00:10,  6.91it/s]Evaluating:  78%|███████▊  | 243/313 [00:37<00:10,  6.92it/s]Evaluating:  78%|███████▊  | 244/313 [00:37<00:09,  6.94it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:09,  6.96it/s]Evaluating:  79%|███████▊  | 246/313 [00:38<00:09,  6.88it/s]Evaluating:  79%|███████▉  | 247/313 [00:38<00:09,  6.78it/s]Evaluating:  79%|███████▉  | 248/313 [00:38<00:09,  6.72it/s]Evaluating:  80%|███████▉  | 249/313 [00:38<00:09,  6.68it/s]Evaluating:  80%|███████▉  | 250/313 [00:38<00:09,  6.68it/s]Evaluating:  80%|████████  | 251/313 [00:38<00:09,  6.66it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:09,  6.65it/s]Evaluating:  81%|████████  | 253/313 [00:39<00:09,  6.64it/s]Evaluating:  81%|████████  | 254/313 [00:39<00:09,  6.53it/s]Evaluating:  81%|████████▏ | 255/313 [00:39<00:08,  6.51it/s]Evaluating:  82%|████████▏ | 256/313 [00:39<00:08,  6.54it/s]Evaluating:  82%|████████▏ | 257/313 [00:39<00:08,  6.54it/s]Evaluating:  82%|████████▏ | 258/313 [00:39<00:08,  6.58it/s]Evaluating:  83%|████████▎ | 259/313 [00:40<00:08,  6.62it/s]Evaluating:  83%|████████▎ | 260/313 [00:40<00:09,  5.30it/s]Evaluating:  83%|████████▎ | 261/313 [00:40<00:09,  5.71it/s]Evaluating:  84%|████████▎ | 262/313 [00:40<00:08,  6.05it/s]Evaluating:  84%|████████▍ | 263/313 [00:40<00:07,  6.29it/s]Evaluating:  84%|████████▍ | 264/313 [00:40<00:07,  6.49it/s]Evaluating:  85%|████████▍ | 265/313 [00:41<00:07,  6.60it/s]Evaluating:  85%|████████▍ | 266/313 [00:41<00:07,  6.69it/s]Evaluating:  85%|████████▌ | 267/313 [00:41<00:06,  6.76it/s]Evaluating:  86%|████████▌ | 268/313 [00:41<00:06,  6.81it/s]Evaluating:  86%|████████▌ | 269/313 [00:41<00:06,  6.82it/s]Evaluating:  86%|████████▋ | 270/313 [00:41<00:06,  6.80it/s]Evaluating:  87%|████████▋ | 271/313 [00:41<00:06,  6.74it/s]Evaluating:  87%|████████▋ | 272/313 [00:42<00:06,  6.67it/s]Evaluating:  87%|████████▋ | 273/313 [00:42<00:06,  6.60it/s]Evaluating:  88%|████████▊ | 274/313 [00:42<00:05,  6.55it/s]Evaluating:  88%|████████▊ | 275/313 [00:42<00:05,  6.52it/s]Evaluating:  88%|████████▊ | 276/313 [00:42<00:05,  6.51it/s]Evaluating:  88%|████████▊ | 277/313 [00:42<00:05,  6.51it/s]Evaluating:  89%|████████▉ | 278/313 [00:43<00:05,  6.49it/s]Evaluating:  89%|████████▉ | 279/313 [00:43<00:05,  6.48it/s]Evaluating:  89%|████████▉ | 280/313 [00:43<00:05,  6.47it/s]Evaluating:  90%|████████▉ | 281/313 [00:43<00:04,  6.48it/s]Evaluating:  90%|█████████ | 282/313 [00:43<00:04,  6.50it/s]Evaluating:  90%|█████████ | 283/313 [00:43<00:04,  6.51it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.54it/s]Evaluating:  91%|█████████ | 285/313 [00:44<00:04,  6.60it/s]Evaluating:  91%|█████████▏| 286/313 [00:44<00:04,  6.65it/s]Evaluating:  92%|█████████▏| 287/313 [00:44<00:03,  6.65it/s]Evaluating:  92%|█████████▏| 288/313 [00:44<00:03,  6.65it/s]Evaluating:  92%|█████████▏| 289/313 [00:44<00:03,  6.66it/s]Evaluating:  93%|█████████▎| 290/313 [00:44<00:03,  6.68it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.70it/s]Evaluating:  93%|█████████▎| 292/313 [00:45<00:04,  4.42it/s]Evaluating:  94%|█████████▎| 293/313 [00:45<00:04,  4.88it/s]Evaluating:  94%|█████████▍| 294/313 [00:45<00:03,  5.26it/s]Evaluating:  94%|█████████▍| 295/313 [00:45<00:03,  5.56it/s]Evaluating:  95%|█████████▍| 296/313 [00:46<00:02,  5.80it/s]Evaluating:  95%|█████████▍| 297/313 [00:46<00:02,  5.97it/s]Evaluating:  95%|█████████▌| 298/313 [00:46<00:02,  6.12it/s]Evaluating:  96%|█████████▌| 299/313 [00:46<00:02,  6.25it/s]Evaluating:  96%|█████████▌| 300/313 [00:46<00:02,  6.38it/s]Evaluating:  96%|█████████▌| 301/313 [00:46<00:01,  6.50it/s]Evaluating:  96%|█████████▋| 302/313 [00:46<00:01,  6.59it/s]Evaluating:  97%|█████████▋| 303/313 [00:47<00:01,  6.62it/s]Evaluating:  97%|█████████▋| 304/313 [00:47<00:01,  6.63it/s]Evaluating:  97%|█████████▋| 305/313 [00:47<00:01,  6.68it/s]Evaluating:  98%|█████████▊| 306/313 [00:47<00:01,  6.72it/s]Evaluating:  98%|█████████▊| 307/313 [00:47<00:00,  6.73it/s]Evaluating:  98%|█████████▊| 308/313 [00:47<00:00,  6.76it/s]Evaluating:  99%|█████████▊| 309/313 [00:47<00:00,  6.79it/s]Evaluating:  99%|█████████▉| 310/313 [00:48<00:00,  6.78it/s]Evaluating:  99%|█████████▉| 311/313 [00:48<00:00,  6.79it/s]Evaluating: 100%|█████████▉| 312/313 [00:48<00:00,  6.78it/s]Evaluating: 100%|██████████| 313/313 [00:48<00:00,  6.46it/s]
10/08/2021 22:31:33 - INFO - __main__ -   ***** Evaluation result  in ru *****
10/08/2021 22:31:33 - INFO - __main__ -     f1 = 0.5540778327945329
10/08/2021 22:31:33 - INFO - __main__ -     loss = 2.5670769456476448
10/08/2021 22:31:33 - INFO - __main__ -     precision = 0.5522022838499184
10/08/2021 22:31:33 - INFO - __main__ -     recall = 0.5559661657222633
10/08/2021 22:31:33 - INFO - __main__ -   Language adapter for bg not found, using en instead
10/08/2021 22:31:33 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:31:33 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/08/2021 22:31:34 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/08/2021 22:31:34 - INFO - __main__ -     Num examples = 10004
10/08/2021 22:31:34 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.78it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.79it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.78it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.76it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.76it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.76it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.75it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.75it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.75it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.76it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.76it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.75it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.75it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.75it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.75it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.75it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.74it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.73it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.74it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.73it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.74it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.74it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.74it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.74it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.72it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:38,  6.72it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.72it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.73it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.71it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.72it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.72it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.72it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.72it/s]Evaluating:  21%|██        | 65/313 [00:09<00:36,  6.71it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.71it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.71it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.71it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.71it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.70it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.70it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:35,  6.70it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:35,  6.70it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:35,  6.69it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.69it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.69it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.66it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.64it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:35,  6.63it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:35,  6.64it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.65it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.67it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.63it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.65it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.66it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:34,  6.67it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.67it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.67it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.68it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.68it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.68it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:32,  6.68it/s]Evaluating:  30%|███       | 94/313 [00:14<00:39,  5.57it/s]Evaluating:  30%|███       | 95/313 [00:14<00:36,  5.92it/s]Evaluating:  31%|███       | 96/313 [00:14<00:34,  6.22it/s]Evaluating:  31%|███       | 97/313 [00:14<00:33,  6.44it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.61it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:31,  6.71it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:31,  6.79it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:30,  6.85it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:30,  6.89it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:30,  6.92it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:30,  6.91it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:30,  6.89it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:30,  6.89it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:29,  6.91it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:29,  6.92it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:29,  6.90it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:29,  6.88it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:29,  6.87it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:29,  6.84it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:29,  6.81it/s]Evaluating:  36%|███▋      | 114/313 [00:16<00:29,  6.73it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.74it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.77it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:28,  6.81it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:28,  6.78it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:28,  6.74it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:28,  6.73it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.74it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.73it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.71it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.71it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.70it/s]Evaluating:  40%|████      | 126/313 [00:18<00:28,  6.67it/s]Evaluating:  41%|████      | 127/313 [00:18<00:27,  6.65it/s]Evaluating:  41%|████      | 128/313 [00:19<00:36,  5.12it/s]Evaluating:  41%|████      | 129/313 [00:19<00:33,  5.50it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:31,  5.81it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:30,  6.04it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:29,  6.22it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:28,  6.35it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:27,  6.42it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:27,  6.48it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:27,  6.51it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.54it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.57it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.59it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.61it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.61it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.59it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:25,  6.59it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.59it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.54it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:25,  6.56it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.64it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.71it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:23,  6.77it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:23,  6.81it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:23,  6.77it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.78it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.78it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:22,  6.79it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.73it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:23,  6.68it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:24,  6.18it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.44it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.62it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.77it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:21,  6.86it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:21,  6.93it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:21,  6.96it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:20,  6.96it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:20,  6.95it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:20,  6.90it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:20,  6.86it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:20,  6.86it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.84it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:20,  6.83it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.82it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.81it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.75it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:20,  6.69it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:20,  6.66it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:20,  6.63it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:20,  6.60it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.60it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.59it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.51it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:19,  6.53it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:19,  6.55it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:19,  6.56it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:19,  6.59it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.61it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.62it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.62it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:18,  6.62it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:18,  6.62it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:19,  6.20it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:18,  6.45it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.64it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.78it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:16,  6.85it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:16,  6.87it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:16,  6.81it/s]Evaluating:  64%|██████▍   | 200/313 [00:29<00:16,  6.78it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:16,  6.77it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.81it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.83it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:15,  6.82it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:15,  6.83it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:15,  6.79it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:15,  6.72it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.67it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.67it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.66it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:15,  6.68it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:15,  6.71it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:14,  6.74it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.74it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.64it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.63it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:14,  6.60it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:14,  6.60it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:14,  6.58it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:14,  6.57it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:13,  6.58it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.58it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.64it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:13,  6.66it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:13,  6.66it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:13,  6.67it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.67it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.66it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.70it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.73it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.76it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:12,  6.74it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:11,  6.71it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.70it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.72it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.70it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.64it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.59it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:11,  6.55it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:11,  6.52it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.51it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.51it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.49it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.54it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.58it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:10,  6.61it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.68it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.73it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.70it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.68it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.69it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:09,  6.71it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:08,  6.73it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.71it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.68it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.66it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.68it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.66it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:08,  6.67it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:11,  4.76it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:10,  5.16it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:09,  5.50it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:08,  5.76it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:08,  5.95it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.10it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.20it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:07,  6.29it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:07,  6.35it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.39it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.41it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.43it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.43it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.44it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:06,  6.44it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.48it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.54it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.59it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.65it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.69it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:04,  6.70it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.73it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.73it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.73it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.70it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.66it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.62it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:03,  6.62it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.61it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.58it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.52it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.49it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.12it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.30it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.36it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.38it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.44it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.46it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.45it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.46it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  6.45it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.44it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.42it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.42it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  6.42it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.46it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.47it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.53it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.59it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.63it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.56it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.52it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.52it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  7.18it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.62it/s]
10/08/2021 22:32:23 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/08/2021 22:32:23 - INFO - __main__ -     f1 = 0.6803514987322428
10/08/2021 22:32:23 - INFO - __main__ -     loss = 1.711920443434304
10/08/2021 22:32:23 - INFO - __main__ -     precision = 0.6649015614392396
10/08/2021 22:32:23 - INFO - __main__ -     recall = 0.6965365194509636
10/08/2021 22:32:23 - INFO - __main__ -   Language adapter for uk not found, using en instead
10/08/2021 22:32:23 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:32:23 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/08/2021 22:32:24 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/08/2021 22:32:24 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:32:24 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.75it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.75it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.77it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.77it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.77it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.77it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.77it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.77it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.76it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.76it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.77it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.77it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.76it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.76it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.76it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.76it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.75it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.75it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.75it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.75it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.75it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.75it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.74it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.72it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.72it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.71it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.71it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.71it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.70it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.69it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.69it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.70it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:40,  6.66it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.70it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.67it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.68it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.63it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:39,  6.63it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:39,  6.66it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.67it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.66it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.62it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.68it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:45,  5.59it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:42,  5.92it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:40,  6.18it/s]Evaluating:  20%|██        | 63/313 [00:09<00:39,  6.36it/s]Evaluating:  20%|██        | 64/313 [00:09<00:38,  6.46it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.53it/s]Evaluating:  21%|██        | 66/313 [00:09<00:37,  6.59it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:37,  6.62it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.65it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.67it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.68it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.68it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:35,  6.69it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.68it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.69it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.69it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.68it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.68it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:35,  6.68it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:34,  6.68it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.68it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.67it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:33,  6.68it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:33,  6.68it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.67it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.67it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:33,  6.66it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.66it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.66it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.67it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:32,  6.65it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.65it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.66it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:31,  6.66it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:30,  6.67it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.67it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.66it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.66it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.61it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.61it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:30,  6.63it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:30,  6.62it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.64it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.63it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.64it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.64it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.63it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.60it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:29,  6.60it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.62it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.62it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.62it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|████      | 126/313 [00:19<00:35,  5.21it/s]Evaluating:  41%|████      | 127/313 [00:19<00:32,  5.66it/s]Evaluating:  41%|████      | 128/313 [00:19<00:30,  6.02it/s]Evaluating:  41%|████      | 129/313 [00:19<00:29,  6.29it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:28,  6.45it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.56it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.63it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:26,  6.68it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:26,  6.69it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.69it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.69it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.66it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.67it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.68it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:25,  6.68it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.66it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.64it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.63it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.62it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.61it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.63it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.63it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.57it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.59it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.65it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.67it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.67it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:23,  6.68it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:23,  6.68it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.68it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.65it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.63it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:45,  3.41it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:38,  3.99it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:33,  4.52it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:30,  4.99it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:28,  5.38it/s]Evaluating:  52%|█████▏    | 163/313 [00:25<00:26,  5.71it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:24,  5.96it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:24,  6.15it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:23,  6.32it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:22,  6.44it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:22,  6.54it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.61it/s]Evaluating:  54%|█████▍    | 170/313 [00:26<00:21,  6.64it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:21,  6.66it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:21,  6.65it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:21,  6.62it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.62it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.65it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.67it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:20,  6.67it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:20,  6.69it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:19,  6.72it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:19,  6.75it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.76it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.75it/s]Evaluating:  58%|█████▊    | 183/313 [00:28<00:19,  6.68it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:19,  6.64it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:19,  6.61it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:19,  6.61it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:19,  6.59it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:19,  6.57it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.59it/s]Evaluating:  61%|██████    | 190/313 [00:29<00:18,  6.60it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:18,  6.57it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:18,  6.57it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:18,  6.58it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:18,  6.58it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.59it/s]Evaluating:  63%|██████▎   | 196/313 [00:30<00:17,  6.57it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:17,  6.58it/s]Evaluating:  63%|██████▎   | 198/313 [00:30<00:17,  6.57it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:17,  6.55it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:17,  6.54it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:17,  6.54it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.54it/s]Evaluating:  65%|██████▍   | 203/313 [00:31<00:16,  6.55it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:16,  6.53it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:16,  6.54it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:16,  6.54it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:16,  6.54it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:16,  6.56it/s]Evaluating:  67%|██████▋   | 209/313 [00:32<00:15,  6.57it/s]Evaluating:  67%|██████▋   | 210/313 [00:32<00:15,  6.54it/s]Evaluating:  67%|██████▋   | 211/313 [00:32<00:15,  6.54it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:15,  6.53it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:15,  6.52it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:15,  6.53it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.55it/s]Evaluating:  69%|██████▉   | 216/313 [00:33<00:14,  6.54it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:14,  6.53it/s]Evaluating:  70%|██████▉   | 218/313 [00:33<00:14,  6.52it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:14,  6.54it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:14,  6.55it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:17,  5.36it/s]Evaluating:  71%|███████   | 222/313 [00:34<00:15,  5.78it/s]Evaluating:  71%|███████   | 223/313 [00:34<00:14,  6.11it/s]Evaluating:  72%|███████▏  | 224/313 [00:34<00:14,  6.34it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:13,  6.51it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:13,  6.64it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.77it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.85it/s]Evaluating:  73%|███████▎  | 229/313 [00:35<00:12,  6.93it/s]Evaluating:  73%|███████▎  | 230/313 [00:35<00:11,  6.97it/s]Evaluating:  74%|███████▍  | 231/313 [00:35<00:11,  7.01it/s]Evaluating:  74%|███████▍  | 232/313 [00:35<00:11,  7.02it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:11,  6.91it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.74it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.63it/s]Evaluating:  75%|███████▌  | 236/313 [00:36<00:11,  6.55it/s]Evaluating:  76%|███████▌  | 237/313 [00:36<00:11,  6.52it/s]Evaluating:  76%|███████▌  | 238/313 [00:36<00:11,  6.51it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:11,  6.50it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.47it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.49it/s]Evaluating:  77%|███████▋  | 242/313 [00:37<00:10,  6.51it/s]Evaluating:  78%|███████▊  | 243/313 [00:37<00:10,  6.55it/s]Evaluating:  78%|███████▊  | 244/313 [00:37<00:10,  6.59it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:10,  6.63it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:10,  6.64it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.65it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.66it/s]Evaluating:  80%|███████▉  | 249/313 [00:38<00:09,  6.69it/s]Evaluating:  80%|███████▉  | 250/313 [00:38<00:09,  6.72it/s]Evaluating:  80%|████████  | 251/313 [00:38<00:09,  6.77it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:08,  6.82it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:08,  6.82it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.80it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.80it/s]Evaluating:  82%|████████▏ | 256/313 [00:39<00:08,  6.83it/s]Evaluating:  82%|████████▏ | 257/313 [00:39<00:08,  6.85it/s]Evaluating:  82%|████████▏ | 258/313 [00:39<00:08,  6.85it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:07,  6.86it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:07,  6.81it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:07,  6.84it/s]Evaluating:  84%|████████▎ | 262/313 [00:40<00:07,  6.88it/s]Evaluating:  84%|████████▍ | 263/313 [00:40<00:07,  6.90it/s]Evaluating:  84%|████████▍ | 264/313 [00:40<00:07,  6.89it/s]Evaluating:  85%|████████▍ | 265/313 [00:40<00:07,  6.84it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:06,  6.80it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:06,  6.80it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.80it/s]Evaluating:  86%|████████▌ | 269/313 [00:41<00:06,  6.76it/s]Evaluating:  86%|████████▋ | 270/313 [00:41<00:06,  6.69it/s]Evaluating:  87%|████████▋ | 271/313 [00:41<00:06,  6.63it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.57it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.54it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.51it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.49it/s]Evaluating:  88%|████████▊ | 276/313 [00:42<00:05,  6.47it/s]Evaluating:  88%|████████▊ | 277/313 [00:42<00:05,  6.46it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.45it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.45it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.47it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.46it/s]Evaluating:  90%|█████████ | 282/313 [00:43<00:04,  6.48it/s]Evaluating:  90%|█████████ | 283/313 [00:43<00:04,  6.52it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.59it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.64it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.71it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:03,  6.79it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.84it/s]Evaluating:  92%|█████████▏| 289/313 [00:44<00:03,  6.80it/s]Evaluating:  93%|█████████▎| 290/313 [00:44<00:03,  6.73it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.67it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.65it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.64it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.58it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.55it/s]Evaluating:  95%|█████████▍| 296/313 [00:45<00:02,  6.51it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:02,  6.49it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.46it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.45it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  6.45it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.44it/s]Evaluating:  96%|█████████▋| 302/313 [00:46<00:01,  6.43it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.43it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.42it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.47it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.51it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.56it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.62it/s]Evaluating:  99%|█████████▊| 309/313 [00:47<00:00,  6.65it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  6.60it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.53it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.51it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.56it/s]
10/08/2021 22:33:13 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/08/2021 22:33:13 - INFO - __main__ -     f1 = 0.5832159537400846
10/08/2021 22:33:13 - INFO - __main__ -     loss = 1.8472846005671322
10/08/2021 22:33:13 - INFO - __main__ -     precision = 0.5802050298694594
10/08/2021 22:33:13 - INFO - __main__ -     recall = 0.5862582904836426
10/08/2021 22:33:13 - INFO - __main__ -   Language adapter for be not found, using en instead
10/08/2021 22:33:13 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:33:13 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/08/2021 22:33:14 - INFO - __main__ -   ***** Running evaluation  in be *****
10/08/2021 22:33:14 - INFO - __main__ -     Num examples = 1001
10/08/2021 22:33:14 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.79it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.79it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.79it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.79it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.79it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.79it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.79it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.79it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.78it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.78it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.77it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.77it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.77it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.78it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.78it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.77it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.76it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.75it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.75it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.73it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.74it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.74it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.75it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.75it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.75it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.75it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  6.75it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.75it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.75it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.75it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  5.73it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.78it/s]
10/08/2021 22:33:18 - INFO - __main__ -   ***** Evaluation result  in be *****
10/08/2021 22:33:18 - INFO - __main__ -     f1 = 0.6176706827309236
10/08/2021 22:33:18 - INFO - __main__ -     loss = 1.650340860709548
10/08/2021 22:33:18 - INFO - __main__ -     precision = 0.6040848389630793
10/08/2021 22:33:18 - INFO - __main__ -     recall = 0.6318816762530813
10/08/2021 22:33:18 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:33:43 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:33:43 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:34:24 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 22:34:27 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:34:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 22:34:27 - INFO - __main__ -   Seed = 52
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:34:35 - INFO - __main__ -   loading from existing model bert-base-multilingual-cased
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/08/2021 22:34:48 - INFO - root -   save model
10/08/2021 22:34:48 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:34:53 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='ar,bh,hi,fo,no,da,ru,bg,uk,be', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 22:34:53 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:35:06 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:35:06 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 22:35:06 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/08/2021 22:35:06 - INFO - root -   Trying to decide if add adapter
10/08/2021 22:35:06 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 22:35:06 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 22:35:06 - INFO - __main__ -   Language = en
10/08/2021 22:35:06 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 22:35:22 - INFO - __main__ -   Language adapter for ar not found, using en instead
10/08/2021 22:35:22 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:35:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ar_bert-base-multilingual-cased_128
10/08/2021 22:35:24 - INFO - __main__ -   ***** Running evaluation  in ar *****
10/08/2021 22:35:24 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:35:24 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:49,  6.30it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.81it/s]Evaluating:   1%|          | 3/313 [00:00<00:43,  7.13it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:42,  7.30it/s]Evaluating:   2%|▏         | 5/313 [00:00<01:03,  4.85it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:55,  5.52it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:51,  5.98it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:47,  6.39it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.72it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:43,  6.96it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:42,  7.08it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:41,  7.18it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:41,  7.28it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:40,  7.35it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:40,  7.39it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:39,  7.44it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:39,  7.47it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:39,  7.47it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:39,  7.47it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:39,  7.47it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:39,  7.48it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:38,  7.47it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:38,  7.49it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:38,  7.49it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:38,  7.46it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:38,  7.48it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:38,  7.49it/s]Evaluating:   9%|▉         | 28/313 [00:03<00:38,  7.46it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:38,  7.47it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:37,  7.48it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:37,  7.48it/s]Evaluating:  10%|█         | 32/313 [00:04<00:37,  7.46it/s]Evaluating:  11%|█         | 33/313 [00:04<00:37,  7.46it/s]Evaluating:  11%|█         | 34/313 [00:04<00:37,  7.45it/s]Evaluating:  11%|█         | 35/313 [00:04<00:37,  7.47it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:37,  7.44it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:37,  7.44it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:37,  7.41it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:37,  7.36it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:37,  7.37it/s]Evaluating:  13%|█▎        | 41/313 [00:05<00:43,  6.29it/s]Evaluating:  13%|█▎        | 42/313 [00:05<00:41,  6.60it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:39,  6.83it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:38,  7.01it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:37,  7.13it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:37,  7.16it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:36,  7.23it/s]Evaluating:  15%|█▌        | 48/313 [00:06<00:36,  7.27it/s]Evaluating:  16%|█▌        | 49/313 [00:06<00:36,  7.32it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:35,  7.34it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:35,  7.36it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:35,  7.37it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:35,  7.38it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:35,  7.39it/s]Evaluating:  18%|█▊        | 55/313 [00:07<00:34,  7.40it/s]Evaluating:  18%|█▊        | 56/313 [00:07<00:34,  7.40it/s]Evaluating:  18%|█▊        | 57/313 [00:07<00:34,  7.40it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:34,  7.40it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:34,  7.39it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:34,  7.39it/s]Evaluating:  19%|█▉        | 61/313 [00:08<00:34,  7.38it/s]Evaluating:  20%|█▉        | 62/313 [00:08<00:34,  7.33it/s]Evaluating:  20%|██        | 63/313 [00:08<00:34,  7.35it/s]Evaluating:  20%|██        | 64/313 [00:08<00:33,  7.36it/s]Evaluating:  21%|██        | 65/313 [00:09<00:33,  7.36it/s]Evaluating:  21%|██        | 66/313 [00:09<00:33,  7.37it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:33,  7.37it/s]Evaluating:  22%|██▏       | 68/313 [00:09<00:33,  7.38it/s]Evaluating:  22%|██▏       | 69/313 [00:09<00:33,  7.37it/s]Evaluating:  22%|██▏       | 70/313 [00:09<00:32,  7.37it/s]Evaluating:  23%|██▎       | 71/313 [00:09<00:32,  7.35it/s]Evaluating:  23%|██▎       | 72/313 [00:09<00:32,  7.35it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:32,  7.34it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:32,  7.35it/s]Evaluating:  24%|██▍       | 75/313 [00:10<00:32,  7.37it/s]Evaluating:  24%|██▍       | 76/313 [00:10<00:32,  7.36it/s]Evaluating:  25%|██▍       | 77/313 [00:10<00:32,  7.37it/s]Evaluating:  25%|██▍       | 78/313 [00:10<00:31,  7.36it/s]Evaluating:  25%|██▌       | 79/313 [00:10<00:31,  7.35it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:31,  7.35it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:31,  7.34it/s]Evaluating:  26%|██▌       | 82/313 [00:11<00:31,  7.35it/s]Evaluating:  27%|██▋       | 83/313 [00:11<00:31,  7.31it/s]Evaluating:  27%|██▋       | 84/313 [00:11<00:31,  7.26it/s]Evaluating:  27%|██▋       | 85/313 [00:11<00:31,  7.25it/s]Evaluating:  27%|██▋       | 86/313 [00:11<00:31,  7.27it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:31,  7.27it/s]Evaluating:  28%|██▊       | 88/313 [00:12<00:30,  7.27it/s]Evaluating:  28%|██▊       | 89/313 [00:12<00:30,  7.29it/s]Evaluating:  29%|██▉       | 90/313 [00:12<00:30,  7.29it/s]Evaluating:  29%|██▉       | 91/313 [00:12<00:30,  7.29it/s]Evaluating:  29%|██▉       | 92/313 [00:12<00:30,  7.30it/s]Evaluating:  30%|██▉       | 93/313 [00:12<00:30,  7.28it/s]Evaluating:  30%|███       | 94/313 [00:13<00:30,  7.30it/s]Evaluating:  30%|███       | 95/313 [00:13<00:29,  7.29it/s]Evaluating:  31%|███       | 96/313 [00:13<00:29,  7.30it/s]Evaluating:  31%|███       | 97/313 [00:13<00:29,  7.30it/s]Evaluating:  31%|███▏      | 98/313 [00:13<00:29,  7.30it/s]Evaluating:  32%|███▏      | 99/313 [00:13<00:29,  7.30it/s]Evaluating:  32%|███▏      | 100/313 [00:13<00:29,  7.28it/s]Evaluating:  32%|███▏      | 101/313 [00:13<00:29,  7.29it/s]Evaluating:  33%|███▎      | 102/313 [00:14<00:28,  7.30it/s]Evaluating:  33%|███▎      | 103/313 [00:14<00:28,  7.29it/s]Evaluating:  33%|███▎      | 104/313 [00:14<00:28,  7.29it/s]Evaluating:  34%|███▎      | 105/313 [00:14<00:28,  7.28it/s]Evaluating:  34%|███▍      | 106/313 [00:14<00:28,  7.28it/s]Evaluating:  34%|███▍      | 107/313 [00:14<00:28,  7.28it/s]Evaluating:  35%|███▍      | 108/313 [00:14<00:28,  7.28it/s]Evaluating:  35%|███▍      | 109/313 [00:15<00:28,  7.26it/s]Evaluating:  35%|███▌      | 110/313 [00:15<00:28,  7.22it/s]Evaluating:  35%|███▌      | 111/313 [00:15<00:28,  7.17it/s]Evaluating:  36%|███▌      | 112/313 [00:15<00:28,  6.98it/s]Evaluating:  36%|███▌      | 113/313 [00:15<00:28,  7.03it/s]Evaluating:  36%|███▋      | 114/313 [00:15<00:32,  6.07it/s]Evaluating:  37%|███▋      | 115/313 [00:15<00:30,  6.39it/s]Evaluating:  37%|███▋      | 116/313 [00:16<00:29,  6.62it/s]Evaluating:  37%|███▋      | 117/313 [00:16<00:28,  6.80it/s]Evaluating:  38%|███▊      | 118/313 [00:16<00:28,  6.93it/s]Evaluating:  38%|███▊      | 119/313 [00:16<00:27,  7.01it/s]Evaluating:  38%|███▊      | 120/313 [00:16<00:27,  7.09it/s]Evaluating:  39%|███▊      | 121/313 [00:16<00:26,  7.13it/s]Evaluating:  39%|███▉      | 122/313 [00:16<00:26,  7.16it/s]Evaluating:  39%|███▉      | 123/313 [00:17<00:26,  7.19it/s]Evaluating:  40%|███▉      | 124/313 [00:17<00:26,  7.20it/s]Evaluating:  40%|███▉      | 125/313 [00:17<00:26,  7.21it/s]Evaluating:  40%|████      | 126/313 [00:17<00:25,  7.22it/s]Evaluating:  41%|████      | 127/313 [00:17<00:25,  7.22it/s]Evaluating:  41%|████      | 128/313 [00:17<00:25,  7.21it/s]Evaluating:  41%|████      | 129/313 [00:17<00:25,  7.18it/s]Evaluating:  42%|████▏     | 130/313 [00:18<00:25,  7.18it/s]Evaluating:  42%|████▏     | 131/313 [00:18<00:25,  7.18it/s]Evaluating:  42%|████▏     | 132/313 [00:18<00:25,  7.19it/s]Evaluating:  42%|████▏     | 133/313 [00:18<00:25,  7.20it/s]Evaluating:  43%|████▎     | 134/313 [00:18<00:24,  7.20it/s]Evaluating:  43%|████▎     | 135/313 [00:18<00:24,  7.19it/s]Evaluating:  43%|████▎     | 136/313 [00:18<00:24,  7.18it/s]Evaluating:  44%|████▍     | 137/313 [00:19<00:24,  7.17it/s]Evaluating:  44%|████▍     | 138/313 [00:19<00:24,  7.12it/s]Evaluating:  44%|████▍     | 139/313 [00:19<00:24,  7.07it/s]Evaluating:  45%|████▍     | 140/313 [00:19<00:24,  7.02it/s]Evaluating:  45%|████▌     | 141/313 [00:19<00:24,  6.98it/s]Evaluating:  45%|████▌     | 142/313 [00:19<00:24,  6.98it/s]Evaluating:  46%|████▌     | 143/313 [00:19<00:24,  6.99it/s]Evaluating:  46%|████▌     | 144/313 [00:20<00:24,  7.01it/s]Evaluating:  46%|████▋     | 145/313 [00:20<00:23,  7.02it/s]Evaluating:  47%|████▋     | 146/313 [00:20<00:23,  7.02it/s]Evaluating:  47%|████▋     | 147/313 [00:20<00:23,  7.00it/s]Evaluating:  47%|████▋     | 148/313 [00:20<00:23,  6.95it/s]Evaluating:  48%|████▊     | 149/313 [00:21<00:40,  4.06it/s]Evaluating:  48%|████▊     | 150/313 [00:21<00:35,  4.58it/s]Evaluating:  48%|████▊     | 151/313 [00:21<00:32,  5.04it/s]Evaluating:  49%|████▊     | 152/313 [00:21<00:29,  5.42it/s]Evaluating:  49%|████▉     | 153/313 [00:21<00:27,  5.73it/s]Evaluating:  49%|████▉     | 154/313 [00:21<00:26,  5.96it/s]Evaluating:  50%|████▉     | 155/313 [00:22<00:25,  6.13it/s]Evaluating:  50%|████▉     | 156/313 [00:22<00:25,  6.26it/s]Evaluating:  50%|█████     | 157/313 [00:22<00:24,  6.36it/s]Evaluating:  50%|█████     | 158/313 [00:22<00:24,  6.42it/s]Evaluating:  51%|█████     | 159/313 [00:22<00:23,  6.47it/s]Evaluating:  51%|█████     | 160/313 [00:22<00:23,  6.51it/s]Evaluating:  51%|█████▏    | 161/313 [00:22<00:23,  6.56it/s]Evaluating:  52%|█████▏    | 162/313 [00:23<00:22,  6.58it/s]Evaluating:  52%|█████▏    | 163/313 [00:23<00:22,  6.58it/s]Evaluating:  52%|█████▏    | 164/313 [00:23<00:22,  6.59it/s]Evaluating:  53%|█████▎    | 165/313 [00:23<00:22,  6.58it/s]Evaluating:  53%|█████▎    | 166/313 [00:23<00:22,  6.59it/s]Evaluating:  53%|█████▎    | 167/313 [00:23<00:22,  6.64it/s]Evaluating:  54%|█████▎    | 168/313 [00:23<00:21,  6.67it/s]Evaluating:  54%|█████▍    | 169/313 [00:24<00:21,  6.64it/s]Evaluating:  54%|█████▍    | 170/313 [00:24<00:21,  6.62it/s]Evaluating:  55%|█████▍    | 171/313 [00:24<00:21,  6.61it/s]Evaluating:  55%|█████▍    | 172/313 [00:24<00:21,  6.59it/s]Evaluating:  55%|█████▌    | 173/313 [00:24<00:21,  6.58it/s]Evaluating:  56%|█████▌    | 174/313 [00:24<00:21,  6.60it/s]Evaluating:  56%|█████▌    | 175/313 [00:25<00:20,  6.63it/s]Evaluating:  56%|█████▌    | 176/313 [00:25<00:20,  6.65it/s]Evaluating:  57%|█████▋    | 177/313 [00:25<00:20,  6.64it/s]Evaluating:  57%|█████▋    | 178/313 [00:25<00:20,  6.63it/s]Evaluating:  57%|█████▋    | 179/313 [00:25<00:20,  6.60it/s]Evaluating:  58%|█████▊    | 180/313 [00:25<00:26,  5.07it/s]Evaluating:  58%|█████▊    | 181/313 [00:26<00:24,  5.49it/s]Evaluating:  58%|█████▊    | 182/313 [00:26<00:22,  5.84it/s]Evaluating:  58%|█████▊    | 183/313 [00:26<00:21,  6.14it/s]Evaluating:  59%|█████▉    | 184/313 [00:26<00:20,  6.37it/s]Evaluating:  59%|█████▉    | 185/313 [00:26<00:19,  6.53it/s]Evaluating:  59%|█████▉    | 186/313 [00:26<00:19,  6.64it/s]Evaluating:  60%|█████▉    | 187/313 [00:26<00:18,  6.69it/s]Evaluating:  60%|██████    | 188/313 [00:27<00:18,  6.70it/s]Evaluating:  60%|██████    | 189/313 [00:27<00:18,  6.73it/s]Evaluating:  61%|██████    | 190/313 [00:27<00:18,  6.78it/s]Evaluating:  61%|██████    | 191/313 [00:27<00:18,  6.71it/s]Evaluating:  61%|██████▏   | 192/313 [00:27<00:18,  6.68it/s]Evaluating:  62%|██████▏   | 193/313 [00:27<00:17,  6.77it/s]Evaluating:  62%|██████▏   | 194/313 [00:28<00:17,  6.74it/s]Evaluating:  62%|██████▏   | 195/313 [00:28<00:17,  6.72it/s]Evaluating:  63%|██████▎   | 196/313 [00:28<00:17,  6.74it/s]Evaluating:  63%|██████▎   | 197/313 [00:28<00:17,  6.78it/s]Evaluating:  63%|██████▎   | 198/313 [00:28<00:16,  6.80it/s]Evaluating:  64%|██████▎   | 199/313 [00:28<00:16,  6.87it/s]Evaluating:  64%|██████▍   | 200/313 [00:28<00:16,  6.88it/s]Evaluating:  64%|██████▍   | 201/313 [00:29<00:16,  6.88it/s]Evaluating:  65%|██████▍   | 202/313 [00:29<00:16,  6.86it/s]Evaluating:  65%|██████▍   | 203/313 [00:29<00:16,  6.86it/s]Evaluating:  65%|██████▌   | 204/313 [00:29<00:15,  6.84it/s]Evaluating:  65%|██████▌   | 205/313 [00:29<00:15,  6.84it/s]Evaluating:  66%|██████▌   | 206/313 [00:29<00:15,  6.84it/s]Evaluating:  66%|██████▌   | 207/313 [00:29<00:15,  6.85it/s]Evaluating:  66%|██████▋   | 208/313 [00:30<00:15,  6.85it/s]Evaluating:  67%|██████▋   | 209/313 [00:30<00:15,  6.85it/s]Evaluating:  67%|██████▋   | 210/313 [00:30<00:15,  6.87it/s]Evaluating:  67%|██████▋   | 211/313 [00:30<00:15,  6.76it/s]Evaluating:  68%|██████▊   | 212/313 [00:30<00:14,  6.80it/s]Evaluating:  68%|██████▊   | 213/313 [00:30<00:16,  6.06it/s]Evaluating:  68%|██████▊   | 214/313 [00:30<00:15,  6.30it/s]Evaluating:  69%|██████▊   | 215/313 [00:31<00:16,  6.11it/s]Evaluating:  69%|██████▉   | 216/313 [00:31<00:16,  5.90it/s]Evaluating:  69%|██████▉   | 217/313 [00:31<00:16,  5.70it/s]Evaluating:  70%|██████▉   | 218/313 [00:31<00:17,  5.58it/s]Evaluating:  70%|██████▉   | 219/313 [00:31<00:16,  5.59it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:16,  5.68it/s]Evaluating:  71%|███████   | 221/313 [00:32<00:15,  5.82it/s]Evaluating:  71%|███████   | 222/313 [00:32<00:15,  5.93it/s]Evaluating:  71%|███████   | 223/313 [00:32<00:14,  6.02it/s]Evaluating:  72%|███████▏  | 224/313 [00:32<00:14,  6.11it/s]Evaluating:  72%|███████▏  | 225/313 [00:32<00:14,  6.20it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:13,  6.28it/s]Evaluating:  73%|███████▎  | 227/313 [00:33<00:13,  6.35it/s]Evaluating:  73%|███████▎  | 228/313 [00:33<00:13,  6.39it/s]Evaluating:  73%|███████▎  | 229/313 [00:33<00:13,  6.42it/s]Evaluating:  73%|███████▎  | 230/313 [00:33<00:12,  6.43it/s]Evaluating:  74%|███████▍  | 231/313 [00:33<00:12,  6.46it/s]Evaluating:  74%|███████▍  | 232/313 [00:33<00:12,  6.37it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:12,  6.31it/s]Evaluating:  75%|███████▍  | 234/313 [00:34<00:12,  6.34it/s]Evaluating:  75%|███████▌  | 235/313 [00:34<00:12,  6.45it/s]Evaluating:  75%|███████▌  | 236/313 [00:34<00:11,  6.57it/s]Evaluating:  76%|███████▌  | 237/313 [00:34<00:11,  6.68it/s]Evaluating:  76%|███████▌  | 238/313 [00:34<00:11,  6.77it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:10,  6.82it/s]Evaluating:  77%|███████▋  | 240/313 [00:35<00:10,  6.90it/s]Evaluating:  77%|███████▋  | 241/313 [00:35<00:10,  6.93it/s]Evaluating:  77%|███████▋  | 242/313 [00:35<00:10,  6.87it/s]Evaluating:  78%|███████▊  | 243/313 [00:35<00:10,  6.71it/s]Evaluating:  78%|███████▊  | 244/313 [00:35<00:11,  5.95it/s]Evaluating:  78%|███████▊  | 245/313 [00:35<00:11,  6.14it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:10,  6.27it/s]Evaluating:  79%|███████▉  | 247/313 [00:36<00:10,  6.39it/s]Evaluating:  79%|███████▉  | 248/313 [00:36<00:10,  6.42it/s]Evaluating:  80%|███████▉  | 249/313 [00:36<00:09,  6.43it/s]Evaluating:  80%|███████▉  | 250/313 [00:36<00:09,  6.49it/s]Evaluating:  80%|████████  | 251/313 [00:36<00:09,  6.54it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:09,  6.57it/s]Evaluating:  81%|████████  | 253/313 [00:37<00:09,  6.60it/s]Evaluating:  81%|████████  | 254/313 [00:37<00:08,  6.61it/s]Evaluating:  81%|████████▏ | 255/313 [00:37<00:08,  6.53it/s]Evaluating:  82%|████████▏ | 256/313 [00:37<00:08,  6.58it/s]Evaluating:  82%|████████▏ | 257/313 [00:37<00:08,  6.64it/s]Evaluating:  82%|████████▏ | 258/313 [00:37<00:08,  6.67it/s]Evaluating:  83%|████████▎ | 259/313 [00:38<00:08,  6.60it/s]Evaluating:  83%|████████▎ | 260/313 [00:38<00:08,  6.57it/s]Evaluating:  83%|████████▎ | 261/313 [00:38<00:07,  6.53it/s]Evaluating:  84%|████████▎ | 262/313 [00:38<00:07,  6.53it/s]Evaluating:  84%|████████▍ | 263/313 [00:38<00:07,  6.53it/s]Evaluating:  84%|████████▍ | 264/313 [00:38<00:07,  6.53it/s]Evaluating:  85%|████████▍ | 265/313 [00:38<00:07,  6.60it/s]Evaluating:  85%|████████▍ | 266/313 [00:39<00:07,  6.64it/s]Evaluating:  85%|████████▌ | 267/313 [00:39<00:06,  6.63it/s]Evaluating:  86%|████████▌ | 268/313 [00:39<00:06,  6.64it/s]Evaluating:  86%|████████▌ | 269/313 [00:39<00:06,  6.67it/s]Evaluating:  86%|████████▋ | 270/313 [00:39<00:06,  6.71it/s]Evaluating:  87%|████████▋ | 271/313 [00:39<00:06,  6.69it/s]Evaluating:  87%|████████▋ | 272/313 [00:40<00:06,  6.69it/s]Evaluating:  87%|████████▋ | 273/313 [00:40<00:05,  6.70it/s]Evaluating:  88%|████████▊ | 274/313 [00:40<00:05,  6.74it/s]Evaluating:  88%|████████▊ | 275/313 [00:40<00:05,  6.77it/s]Evaluating:  88%|████████▊ | 276/313 [00:40<00:05,  6.74it/s]Evaluating:  88%|████████▊ | 277/313 [00:40<00:05,  6.48it/s]Evaluating:  89%|████████▉ | 278/313 [00:40<00:05,  6.20it/s]Evaluating:  89%|████████▉ | 279/313 [00:41<00:05,  6.07it/s]Evaluating:  89%|████████▉ | 280/313 [00:41<00:05,  5.98it/s]Evaluating:  90%|████████▉ | 281/313 [00:41<00:05,  5.97it/s]Evaluating:  90%|█████████ | 282/313 [00:41<00:05,  6.02it/s]Evaluating:  90%|█████████ | 283/313 [00:41<00:04,  6.11it/s]Evaluating:  91%|█████████ | 284/313 [00:41<00:04,  6.15it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.21it/s]Evaluating:  91%|█████████▏| 286/313 [00:42<00:04,  6.23it/s]Evaluating:  92%|█████████▏| 287/313 [00:42<00:04,  6.28it/s]Evaluating:  92%|█████████▏| 288/313 [00:42<00:03,  6.32it/s]Evaluating:  92%|█████████▏| 289/313 [00:42<00:03,  6.37it/s]Evaluating:  93%|█████████▎| 290/313 [00:42<00:03,  6.41it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.36it/s]Evaluating:  93%|█████████▎| 292/313 [00:43<00:03,  6.43it/s]Evaluating:  94%|█████████▎| 293/313 [00:43<00:03,  6.53it/s]Evaluating:  94%|█████████▍| 294/313 [00:43<00:02,  6.60it/s]Evaluating:  94%|█████████▍| 295/313 [00:43<00:02,  6.66it/s]Evaluating:  95%|█████████▍| 296/313 [00:43<00:02,  6.73it/s]Evaluating:  95%|█████████▍| 297/313 [00:43<00:02,  6.77it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.80it/s]Evaluating:  96%|█████████▌| 299/313 [00:44<00:02,  6.75it/s]Evaluating:  96%|█████████▌| 300/313 [00:44<00:01,  6.64it/s]Evaluating:  96%|█████████▌| 301/313 [00:44<00:01,  6.56it/s]Evaluating:  96%|█████████▋| 302/313 [00:44<00:01,  6.50it/s]Evaluating:  97%|█████████▋| 303/313 [00:44<00:01,  6.48it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  6.44it/s]Evaluating:  97%|█████████▋| 305/313 [00:45<00:01,  6.43it/s]Evaluating:  98%|█████████▊| 306/313 [00:45<00:01,  6.42it/s]Evaluating:  98%|█████████▊| 307/313 [00:45<00:00,  6.47it/s]Evaluating:  98%|█████████▊| 308/313 [00:45<00:00,  6.49it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:01,  3.87it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  4.39it/s]Evaluating:  99%|█████████▉| 311/313 [00:46<00:00,  4.84it/s]Evaluating: 100%|█████████▉| 312/313 [00:46<00:00,  5.22it/s]Evaluating: 100%|██████████| 313/313 [00:46<00:00,  6.70it/s]
10/08/2021 22:36:11 - INFO - __main__ -   ***** Evaluation result  in ar *****
10/08/2021 22:36:11 - INFO - __main__ -     f1 = 0.2740011361484568
10/08/2021 22:36:11 - INFO - __main__ -     loss = 5.759424570650339
10/08/2021 22:36:11 - INFO - __main__ -     precision = 0.29336036492650786
10/08/2021 22:36:11 - INFO - __main__ -     recall = 0.25703881339372947
10/08/2021 22:36:11 - INFO - __main__ -   Language adapter for bh not found, using en instead
10/08/2021 22:36:11 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:36:11 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bh_bert-base-multilingual-cased_128
10/08/2021 22:36:11 - INFO - __main__ -   ***** Running evaluation  in bh *****
10/08/2021 22:36:11 - INFO - __main__ -     Num examples = 102
10/08/2021 22:36:11 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  6.79it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  6.79it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  6.80it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.41it/s]
10/08/2021 22:36:12 - INFO - __main__ -   ***** Evaluation result  in bh *****
10/08/2021 22:36:12 - INFO - __main__ -     f1 = 0.437037037037037
10/08/2021 22:36:12 - INFO - __main__ -     loss = 3.198270797729492
10/08/2021 22:36:12 - INFO - __main__ -     precision = 0.3782051282051282
10/08/2021 22:36:12 - INFO - __main__ -     recall = 0.5175438596491229
10/08/2021 22:36:12 - INFO - __main__ -   Language adapter for hi not found, using en instead
10/08/2021 22:36:12 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:36:12 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_hi_bert-base-multilingual-cased_128
10/08/2021 22:36:12 - INFO - __main__ -   ***** Running evaluation  in hi *****
10/08/2021 22:36:12 - INFO - __main__ -     Num examples = 1000
10/08/2021 22:36:12 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.01it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.88it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.84it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.83it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.82it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.81it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.80it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.80it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.79it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.78it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.79it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.78it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.78it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.77it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.77it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.77it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.77it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.77it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.77it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.76it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.76it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.74it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.75it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.75it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.76it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.76it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  6.76it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.75it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.76it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.75it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.75it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.93it/s]
10/08/2021 22:36:17 - INFO - __main__ -   ***** Evaluation result  in hi *****
10/08/2021 22:36:17 - INFO - __main__ -     f1 = 0.5074380165289256
10/08/2021 22:36:17 - INFO - __main__ -     loss = 3.0414248183369637
10/08/2021 22:36:17 - INFO - __main__ -     precision = 0.5151006711409396
10/08/2021 22:36:17 - INFO - __main__ -     recall = 0.5
10/08/2021 22:36:17 - INFO - __main__ -   Language adapter for fo not found, using en instead
10/08/2021 22:36:17 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:36:17 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/08/2021 22:36:17 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/08/2021 22:36:17 - INFO - __main__ -     Num examples = 100
10/08/2021 22:36:17 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  7.08it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  6.90it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  6.89it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.54it/s]
10/08/2021 22:36:17 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/08/2021 22:36:17 - INFO - __main__ -     f1 = 0.6339622641509434
10/08/2021 22:36:17 - INFO - __main__ -     loss = 1.7264872193336487
10/08/2021 22:36:17 - INFO - __main__ -     precision = 0.5793103448275863
10/08/2021 22:36:17 - INFO - __main__ -     recall = 0.7
10/08/2021 22:36:17 - INFO - __main__ -   Language adapter for no not found, using en instead
10/08/2021 22:36:17 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:36:17 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/08/2021 22:36:19 - INFO - __main__ -   ***** Running evaluation  in no *****
10/08/2021 22:36:19 - INFO - __main__ -     Num examples = 10000
10/08/2021 22:36:19 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 5/313 [00:00<01:10,  4.37it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:01,  4.97it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:56,  5.44it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:52,  5.81it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:49,  6.08it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:48,  6.28it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:46,  6.44it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:46,  6.54it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:45,  6.61it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.66it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.70it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.73it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.74it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.75it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:43,  6.76it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:43,  6.77it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.77it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:42,  6.77it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.77it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.77it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.77it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:42,  6.77it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.78it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.77it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:41,  6.77it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.77it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.77it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.77it/s]Evaluating:  11%|█         | 33/313 [00:05<00:41,  6.77it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.74it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.74it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.74it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.75it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.75it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.75it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:40,  6.75it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.75it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.74it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.73it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:39,  6.73it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:39,  6.73it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:38,  6.73it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.74it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.70it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.70it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:37,  6.71it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.71it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.67it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.69it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.69it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.69it/s]Evaluating:  21%|██        | 66/313 [00:10<00:36,  6.69it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:36,  6.70it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.70it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.71it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.71it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:40,  5.96it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:38,  6.28it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:36,  6.54it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:35,  6.74it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:34,  6.89it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:33,  6.98it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:33,  7.04it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:33,  7.05it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:33,  7.05it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:33,  7.06it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:33,  7.03it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:32,  7.00it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:33,  6.94it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:33,  6.88it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:33,  6.84it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:33,  6.71it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:33,  6.75it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.80it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:32,  6.83it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:32,  6.82it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:32,  6.81it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:32,  6.80it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:32,  6.75it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.72it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.71it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.71it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.72it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:31,  6.73it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:31,  6.70it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:31,  6.68it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:31,  6.68it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.67it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.66it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.68it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.66it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:31,  6.65it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:30,  6.67it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.68it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.66it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.66it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.66it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.65it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:30,  6.66it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.68it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.68it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.66it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.65it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.65it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.62it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.62it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:29,  6.61it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.62it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.65it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.66it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.65it/s]Evaluating:  40%|████      | 126/313 [00:18<00:28,  6.64it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.63it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.63it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.67it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.69it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.67it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.65it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:27,  6.63it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:27,  6.62it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.62it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.63it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.64it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:28,  6.06it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:28,  6.21it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:27,  6.32it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.40it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:26,  6.47it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:26,  6.52it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.54it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.57it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.57it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.60it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.61it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.62it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.62it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.61it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.60it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.60it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.58it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:24,  6.58it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.61it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.62it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.63it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:23,  6.63it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.61it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.61it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.59it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.58it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.59it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:22,  6.52it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:22,  6.48it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:22,  6.56it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.60it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.61it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.59it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:21,  6.58it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:21,  6.56it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:21,  6.56it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:21,  6.56it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:21,  6.56it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.55it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:20,  6.55it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:20,  6.55it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:20,  6.55it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:20,  6.60it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.61it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.58it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.57it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:19,  6.56it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:19,  6.56it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:19,  6.55it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:19,  6.54it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:19,  6.53it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.54it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.53it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:18,  6.53it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:18,  6.52it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:18,  6.52it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:18,  6.51it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:18,  6.52it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.53it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.54it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:17,  6.53it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:17,  6.52it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:17,  6.53it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:17,  6.56it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.59it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:16,  6.60it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:20,  5.24it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:19,  5.63it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:17,  5.98it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:16,  6.27it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:16,  6.49it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.66it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.78it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:15,  6.80it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:15,  6.68it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:14,  6.76it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.79it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.85it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.90it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:13,  6.94it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:13,  6.91it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:13,  6.85it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:13,  6.83it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:13,  6.82it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.79it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.80it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:13,  6.79it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:12,  6.78it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:12,  6.76it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.78it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.76it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.76it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.76it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:12,  6.76it/s]Evaluating:  74%|███████▍  | 232/313 [00:35<00:11,  6.77it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:11,  6.84it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.89it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.94it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:11,  6.93it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:12,  5.93it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:12,  6.22it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:11,  6.43it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.60it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:10,  6.73it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.80it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.84it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.79it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:09,  6.81it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:09,  6.78it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.78it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.77it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.80it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.82it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.84it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:08,  6.83it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:08,  6.82it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.76it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.71it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.69it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:08,  6.73it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.78it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:07,  6.79it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:07,  6.79it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:07,  6.75it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.73it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.68it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.67it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.67it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.68it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:06,  6.70it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.71it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.68it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.65it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.63it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.63it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.65it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.69it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.68it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.65it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.62it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.60it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.60it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.59it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.63it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.66it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.66it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.63it/s]Evaluating:  91%|█████████ | 285/313 [00:42<00:04,  6.59it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.58it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:03,  6.58it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.54it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.51it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.51it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.52it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.54it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.57it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.60it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.64it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.65it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.63it/s]Evaluating:  95%|█████████▌| 298/313 [00:44<00:02,  6.60it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.61it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:01,  6.62it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.65it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.43it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  5.45it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  5.57it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  5.71it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  5.82it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:01,  5.92it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.02it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.10it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.17it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.24it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.30it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.61it/s]
10/08/2021 22:37:07 - INFO - __main__ -   ***** Evaluation result  in no *****
10/08/2021 22:37:07 - INFO - __main__ -     f1 = 0.7039648443905158
10/08/2021 22:37:07 - INFO - __main__ -     loss = 1.25092149571108
10/08/2021 22:37:07 - INFO - __main__ -     precision = 0.6670188304020881
10/08/2021 22:37:07 - INFO - __main__ -     recall = 0.7452437161505346
10/08/2021 22:37:07 - INFO - __main__ -   Language adapter for da not found, using en instead
10/08/2021 22:37:07 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:37:07 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/08/2021 22:37:09 - INFO - __main__ -   ***** Running evaluation  in da *****
10/08/2021 22:37:09 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:37:09 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:46,  6.67it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.71it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.74it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.76it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.76it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.77it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.75it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.76it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.76it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.77it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.76it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.77it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:41,  6.77it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.76it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.76it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:40,  6.76it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.76it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.75it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.75it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.75it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.70it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.71it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.71it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.71it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.72it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.71it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.68it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.71it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.70it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.70it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.70it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.71it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:38,  6.71it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.71it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.71it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.71it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.68it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.67it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.71it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.70it/s]Evaluating:  21%|██        | 65/313 [00:09<00:36,  6.70it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.71it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.71it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.71it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.70it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.70it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.71it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:58,  4.15it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:51,  4.67it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:46,  5.13it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:43,  5.52it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:40,  5.82it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:38,  6.06it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:37,  6.23it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:36,  6.35it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:36,  6.44it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:35,  6.51it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:35,  6.55it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:35,  6.55it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.54it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.58it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:34,  6.60it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:34,  6.61it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.62it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.62it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.63it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:33,  6.66it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.66it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.65it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.64it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.62it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.62it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:32,  6.62it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:32,  6.51it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.54it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:41,  5.03it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:38,  5.39it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:36,  5.71it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:34,  5.96it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:33,  6.15it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:32,  6.30it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:31,  6.39it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:31,  6.45it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:31,  6.50it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:30,  6.52it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.54it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:30,  6.57it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.61it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:30,  6.47it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:30,  6.50it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:29,  6.61it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:28,  6.70it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:28,  6.77it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.82it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:27,  6.84it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:27,  6.88it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:27,  6.87it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:27,  6.87it/s]Evaluating:  40%|████      | 126/313 [00:19<00:27,  6.90it/s]Evaluating:  41%|████      | 127/313 [00:19<00:26,  6.91it/s]Evaluating:  41%|████      | 128/313 [00:19<00:26,  6.87it/s]Evaluating:  41%|████      | 129/313 [00:19<00:26,  6.86it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:26,  6.86it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:26,  6.85it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:26,  6.82it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:26,  6.82it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:26,  6.76it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.71it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.68it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.65it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:26,  6.65it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:26,  6.62it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.63it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:25,  6.66it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.70it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.70it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.70it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:25,  6.72it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:24,  6.74it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:24,  6.77it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.60it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.57it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.56it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:24,  6.57it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:24,  6.56it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.58it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.57it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:24,  6.56it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.59it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.61it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:23,  6.58it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:23,  6.55it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.55it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.55it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:23,  6.55it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.54it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.56it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:22,  6.56it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:22,  6.55it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:22,  6.54it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:22,  6.56it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:29,  4.83it/s]Evaluating:  54%|█████▍    | 170/313 [00:26<00:27,  5.24it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:25,  5.57it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:24,  5.83it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:23,  6.02it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:22,  6.17it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:21,  6.28it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:21,  6.35it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:21,  6.40it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:20,  6.44it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:20,  6.48it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:20,  6.49it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:20,  6.50it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:20,  6.54it/s]Evaluating:  58%|█████▊    | 183/313 [00:28<00:19,  6.59it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:19,  6.64it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:19,  6.67it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:18,  6.69it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:18,  6.72it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.73it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.76it/s]Evaluating:  61%|██████    | 190/313 [00:29<00:18,  6.79it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:17,  6.81it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:17,  6.82it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:17,  6.84it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:17,  6.85it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.89it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:16,  6.94it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:16,  6.96it/s]Evaluating:  63%|██████▎   | 198/313 [00:30<00:16,  6.99it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:16,  7.02it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:16,  7.03it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:15,  7.05it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:18,  5.96it/s]Evaluating:  65%|██████▍   | 203/313 [00:31<00:17,  6.13it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:17,  6.24it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:17,  6.32it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:16,  6.38it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:16,  6.41it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:16,  6.22it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:16,  6.29it/s]Evaluating:  67%|██████▋   | 210/313 [00:32<00:15,  6.47it/s]Evaluating:  67%|██████▋   | 211/313 [00:32<00:15,  6.64it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:14,  6.76it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:14,  6.71it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.80it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.76it/s]Evaluating:  69%|██████▉   | 216/313 [00:33<00:14,  6.68it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:14,  6.64it/s]Evaluating:  70%|██████▉   | 218/313 [00:33<00:14,  6.52it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:14,  6.53it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:14,  6.54it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:14,  6.56it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.61it/s]Evaluating:  71%|███████   | 223/313 [00:34<00:13,  6.64it/s]Evaluating:  72%|███████▏  | 224/313 [00:34<00:13,  6.65it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:13,  6.68it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:12,  6.70it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.74it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.80it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.85it/s]Evaluating:  73%|███████▎  | 230/313 [00:35<00:12,  6.89it/s]Evaluating:  74%|███████▍  | 231/313 [00:35<00:11,  6.94it/s]Evaluating:  74%|███████▍  | 232/313 [00:35<00:11,  6.88it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:11,  6.80it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.79it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:14,  5.48it/s]Evaluating:  75%|███████▌  | 236/313 [00:36<00:13,  5.75it/s]Evaluating:  76%|███████▌  | 237/313 [00:36<00:12,  6.03it/s]Evaluating:  76%|███████▌  | 238/313 [00:36<00:12,  6.23it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:11,  6.32it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.38it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.43it/s]Evaluating:  77%|███████▋  | 242/313 [00:37<00:10,  6.52it/s]Evaluating:  78%|███████▊  | 243/313 [00:37<00:10,  6.61it/s]Evaluating:  78%|███████▊  | 244/313 [00:37<00:10,  6.68it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:10,  6.75it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:09,  6.84it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.89it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.91it/s]Evaluating:  80%|███████▉  | 249/313 [00:38<00:09,  6.87it/s]Evaluating:  80%|███████▉  | 250/313 [00:38<00:09,  6.84it/s]Evaluating:  80%|████████  | 251/313 [00:38<00:09,  6.81it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:09,  6.77it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:08,  6.69it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.63it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.63it/s]Evaluating:  82%|████████▏ | 256/313 [00:39<00:08,  6.59it/s]Evaluating:  82%|████████▏ | 257/313 [00:39<00:08,  6.54it/s]Evaluating:  82%|████████▏ | 258/313 [00:39<00:08,  6.56it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.59it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.62it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:07,  6.60it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.58it/s]Evaluating:  84%|████████▍ | 263/313 [00:40<00:07,  6.59it/s]Evaluating:  84%|████████▍ | 264/313 [00:40<00:07,  6.60it/s]Evaluating:  85%|████████▍ | 265/313 [00:40<00:07,  6.56it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.53it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:07,  6.52it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:07,  6.09it/s]Evaluating:  86%|████████▌ | 269/313 [00:41<00:07,  6.19it/s]Evaluating:  86%|████████▋ | 270/313 [00:41<00:06,  6.26it/s]Evaluating:  87%|████████▋ | 271/313 [00:41<00:06,  6.32it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.36it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.37it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:06,  6.39it/s]Evaluating:  88%|████████▊ | 275/313 [00:42<00:05,  6.45it/s]Evaluating:  88%|████████▊ | 276/313 [00:42<00:05,  6.51it/s]Evaluating:  88%|████████▊ | 277/313 [00:42<00:05,  6.59it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.61it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.61it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:04,  6.61it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.64it/s]Evaluating:  90%|█████████ | 282/313 [00:43<00:04,  6.66it/s]Evaluating:  90%|█████████ | 283/313 [00:43<00:04,  6.72it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.67it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.56it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.44it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.41it/s]Evaluating:  92%|█████████▏| 288/313 [00:44<00:03,  6.39it/s]Evaluating:  92%|█████████▏| 289/313 [00:44<00:03,  6.36it/s]Evaluating:  93%|█████████▎| 290/313 [00:44<00:03,  6.43it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.47it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.51it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.56it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.59it/s]Evaluating:  94%|█████████▍| 295/313 [00:45<00:02,  6.63it/s]Evaluating:  95%|█████████▍| 296/313 [00:45<00:02,  6.35it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:02,  6.00it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  5.80it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  5.80it/s]Evaluating:  96%|█████████▌| 300/313 [00:46<00:02,  5.22it/s]Evaluating:  96%|█████████▌| 301/313 [00:46<00:02,  5.47it/s]Evaluating:  96%|█████████▋| 302/313 [00:46<00:01,  5.68it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  5.85it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  5.98it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.09it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.18it/s]Evaluating:  98%|█████████▊| 307/313 [00:47<00:00,  6.24it/s]Evaluating:  98%|█████████▊| 308/313 [00:47<00:00,  6.28it/s]Evaluating:  99%|█████████▊| 309/313 [00:47<00:00,  6.34it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  6.36it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.39it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.46it/s]Evaluating: 100%|██████████| 313/313 [00:48<00:00,  6.52it/s]
10/08/2021 22:37:58 - INFO - __main__ -   ***** Evaluation result  in da *****
10/08/2021 22:37:58 - INFO - __main__ -     f1 = 0.7950903033491146
10/08/2021 22:37:58 - INFO - __main__ -     loss = 0.8292618969235176
10/08/2021 22:37:58 - INFO - __main__ -     precision = 0.7650671525949923
10/08/2021 22:37:58 - INFO - __main__ -     recall = 0.8275660680391298
10/08/2021 22:37:58 - INFO - __main__ -   Language adapter for ru not found, using en instead
10/08/2021 22:37:58 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:37:58 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ru_bert-base-multilingual-cased_128
10/08/2021 22:37:59 - INFO - __main__ -   ***** Running evaluation  in ru *****
10/08/2021 22:37:59 - INFO - __main__ -     Num examples = 10002
10/08/2021 22:37:59 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.74it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.78it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.78it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.80it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.80it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.80it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.80it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.79it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:43,  6.78it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.78it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.78it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.77it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.75it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.75it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.75it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.75it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.75it/s]Evaluating:  11%|█         | 35/313 [00:05<00:42,  6.50it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.65it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.75it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.76it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.77it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.76it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.74it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.74it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.74it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.74it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.74it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.73it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.74it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:38,  6.72it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.73it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.73it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.72it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.71it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.72it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.68it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.66it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.70it/s]Evaluating:  21%|██        | 65/313 [00:09<00:36,  6.72it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.71it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.71it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:41,  5.84it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:39,  6.20it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:37,  6.48it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.69it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:35,  6.84it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:34,  6.96it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:34,  7.03it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:33,  7.09it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:33,  7.12it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:33,  7.10it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:33,  7.06it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:33,  7.00it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:33,  6.93it/s]Evaluating:  26%|██▌       | 81/313 [00:11<00:33,  6.85it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:33,  6.80it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.73it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.69it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.69it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:33,  6.68it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.67it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.67it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.65it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.66it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.66it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:33,  6.65it/s]Evaluating:  30%|███       | 94/313 [00:13<00:32,  6.66it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.65it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.66it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.66it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:32,  6.65it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:31,  6.65it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.61it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.64it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.63it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:31,  6.64it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:31,  6.63it/s]Evaluating:  34%|███▍      | 107/313 [00:15<00:31,  6.61it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:31,  6.57it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.59it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.62it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.62it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.64it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:30,  6.64it/s]Evaluating:  36%|███▋      | 114/313 [00:16<00:30,  6.63it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.64it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.64it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.63it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.63it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.63it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:29,  6.63it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.64it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.65it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.65it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.64it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|████      | 126/313 [00:18<00:28,  6.63it/s]Evaluating:  41%|████      | 127/313 [00:18<00:28,  6.63it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.62it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:27,  6.61it/s]Evaluating:  43%|████▎     | 134/313 [00:19<00:27,  6.61it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.60it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.60it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.60it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:26,  6.60it/s]Evaluating:  44%|████▍     | 139/313 [00:20<00:26,  6.60it/s]Evaluating:  45%|████▍     | 140/313 [00:20<00:26,  6.60it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.60it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.60it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:25,  6.60it/s]Evaluating:  47%|████▋     | 146/313 [00:21<00:25,  6.59it/s]Evaluating:  47%|████▋     | 147/313 [00:21<00:25,  6.59it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:24,  6.61it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.63it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.63it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.63it/s]Evaluating:  49%|████▊     | 152/313 [00:22<00:24,  6.63it/s]Evaluating:  49%|████▉     | 153/313 [00:22<00:24,  6.64it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:23,  6.65it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.65it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.64it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.62it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.60it/s]Evaluating:  51%|█████     | 159/313 [00:23<00:23,  6.60it/s]Evaluating:  51%|█████     | 160/313 [00:23<00:23,  6.59it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.59it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.58it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.57it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.57it/s]Evaluating:  53%|█████▎    | 165/313 [00:24<00:22,  6.57it/s]Evaluating:  53%|█████▎    | 166/313 [00:24<00:22,  6.58it/s]Evaluating:  53%|█████▎    | 167/313 [00:24<00:22,  6.61it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:22,  6.38it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:22,  6.53it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:21,  6.61it/s]Evaluating:  55%|█████▍    | 171/313 [00:25<00:21,  6.67it/s]Evaluating:  55%|█████▍    | 172/313 [00:25<00:20,  6.74it/s]Evaluating:  55%|█████▌    | 173/313 [00:25<00:20,  6.78it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:20,  6.79it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.83it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:19,  6.86it/s]Evaluating:  57%|█████▋    | 177/313 [00:26<00:19,  6.89it/s]Evaluating:  57%|█████▋    | 178/313 [00:26<00:19,  6.92it/s]Evaluating:  57%|█████▋    | 179/313 [00:26<00:19,  6.96it/s]Evaluating:  58%|█████▊    | 180/313 [00:26<00:19,  6.98it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:18,  7.00it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:18,  7.04it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:18,  7.06it/s]Evaluating:  59%|█████▉    | 184/313 [00:27<00:18,  7.05it/s]Evaluating:  59%|█████▉    | 185/313 [00:27<00:18,  7.00it/s]Evaluating:  59%|█████▉    | 186/313 [00:27<00:18,  6.89it/s]Evaluating:  60%|█████▉    | 187/313 [00:27<00:18,  6.86it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.83it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.81it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:17,  6.86it/s]Evaluating:  61%|██████    | 191/313 [00:28<00:17,  6.88it/s]Evaluating:  61%|██████▏   | 192/313 [00:28<00:17,  6.91it/s]Evaluating:  62%|██████▏   | 193/313 [00:28<00:17,  6.94it/s]Evaluating:  62%|██████▏   | 194/313 [00:28<00:17,  6.91it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.92it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.84it/s]Evaluating:  63%|██████▎   | 197/313 [00:29<00:17,  6.81it/s]Evaluating:  63%|██████▎   | 198/313 [00:29<00:16,  6.82it/s]Evaluating:  64%|██████▎   | 199/313 [00:29<00:16,  6.85it/s]Evaluating:  64%|██████▍   | 200/313 [00:29<00:16,  6.87it/s]Evaluating:  64%|██████▍   | 201/313 [00:29<00:16,  6.86it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:18,  5.96it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:17,  6.18it/s]Evaluating:  65%|██████▌   | 204/313 [00:30<00:17,  6.34it/s]Evaluating:  65%|██████▌   | 205/313 [00:30<00:16,  6.50it/s]Evaluating:  66%|██████▌   | 206/313 [00:30<00:16,  6.58it/s]Evaluating:  66%|██████▌   | 207/313 [00:30<00:16,  6.61it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.66it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.71it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:15,  6.71it/s]Evaluating:  67%|██████▋   | 211/313 [00:31<00:15,  6.63it/s]Evaluating:  68%|██████▊   | 212/313 [00:31<00:15,  6.68it/s]Evaluating:  68%|██████▊   | 213/313 [00:31<00:14,  6.71it/s]Evaluating:  68%|██████▊   | 214/313 [00:31<00:14,  6.68it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.62it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.50it/s]Evaluating:  69%|██████▉   | 217/313 [00:32<00:14,  6.46it/s]Evaluating:  70%|██████▉   | 218/313 [00:32<00:14,  6.48it/s]Evaluating:  70%|██████▉   | 219/313 [00:32<00:14,  6.56it/s]Evaluating:  70%|███████   | 220/313 [00:32<00:14,  6.64it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:13,  6.67it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:14,  6.44it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:14,  6.13it/s]Evaluating:  72%|███████▏  | 224/313 [00:33<00:14,  6.02it/s]Evaluating:  72%|███████▏  | 225/313 [00:33<00:14,  5.96it/s]Evaluating:  72%|███████▏  | 226/313 [00:33<00:14,  6.01it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:14,  6.07it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:13,  6.13it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:13,  6.21it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:13,  6.26it/s]Evaluating:  74%|███████▍  | 231/313 [00:34<00:13,  6.31it/s]Evaluating:  74%|███████▍  | 232/313 [00:34<00:12,  6.35it/s]Evaluating:  74%|███████▍  | 233/313 [00:34<00:12,  6.39it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:13,  6.04it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:12,  6.22it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:12,  6.36it/s]Evaluating:  76%|███████▌  | 237/313 [00:35<00:11,  6.48it/s]Evaluating:  76%|███████▌  | 238/313 [00:35<00:11,  6.58it/s]Evaluating:  76%|███████▋  | 239/313 [00:35<00:11,  6.65it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:10,  6.70it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:10,  6.74it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.78it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.63it/s]Evaluating:  78%|███████▊  | 244/313 [00:36<00:10,  6.64it/s]Evaluating:  78%|███████▊  | 245/313 [00:36<00:10,  6.68it/s]Evaluating:  79%|███████▊  | 246/313 [00:36<00:09,  6.71it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:09,  6.73it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.75it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.78it/s]Evaluating:  80%|███████▉  | 250/313 [00:37<00:09,  6.75it/s]Evaluating:  80%|████████  | 251/313 [00:37<00:09,  6.45it/s]Evaluating:  81%|████████  | 252/313 [00:37<00:09,  6.12it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:10,  5.91it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:09,  5.91it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:09,  5.94it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:09,  6.01it/s]Evaluating:  82%|████████▏ | 257/313 [00:38<00:09,  6.11it/s]Evaluating:  82%|████████▏ | 258/313 [00:38<00:08,  6.17it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.23it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.29it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:08,  6.35it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.39it/s]Evaluating:  84%|████████▍ | 263/313 [00:39<00:07,  6.41it/s]Evaluating:  84%|████████▍ | 264/313 [00:39<00:07,  6.41it/s]Evaluating:  85%|████████▍ | 265/313 [00:39<00:07,  6.42it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  5.95it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:07,  6.09it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:07,  6.19it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:07,  6.26it/s]Evaluating:  86%|████████▋ | 270/313 [00:40<00:06,  6.31it/s]Evaluating:  87%|████████▋ | 271/313 [00:40<00:06,  6.34it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.37it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.43it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:06,  6.50it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.58it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.64it/s]Evaluating:  88%|████████▊ | 277/313 [00:41<00:05,  6.69it/s]Evaluating:  89%|████████▉ | 278/313 [00:41<00:05,  6.76it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.78it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:04,  6.73it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.73it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.74it/s]Evaluating:  90%|█████████ | 283/313 [00:42<00:04,  6.67it/s]Evaluating:  91%|█████████ | 284/313 [00:42<00:04,  6.59it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.55it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.51it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.48it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:03,  6.44it/s]Evaluating:  92%|█████████▏| 289/313 [00:43<00:03,  6.45it/s]Evaluating:  93%|█████████▎| 290/313 [00:43<00:03,  6.49it/s]Evaluating:  93%|█████████▎| 291/313 [00:43<00:03,  6.45it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.44it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.43it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.45it/s]Evaluating:  94%|█████████▍| 295/313 [00:44<00:02,  6.46it/s]Evaluating:  95%|█████████▍| 296/313 [00:44<00:02,  6.49it/s]Evaluating:  95%|█████████▍| 297/313 [00:44<00:02,  6.53it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.57it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.53it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  6.50it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.50it/s]Evaluating:  96%|█████████▋| 302/313 [00:45<00:01,  6.47it/s]Evaluating:  97%|█████████▋| 303/313 [00:45<00:01,  6.45it/s]Evaluating:  97%|█████████▋| 304/313 [00:45<00:01,  6.47it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.55it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.62it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.67it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.69it/s]Evaluating:  99%|█████████▊| 309/313 [00:46<00:00,  6.69it/s]Evaluating:  99%|█████████▉| 310/313 [00:46<00:00,  6.69it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.71it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.71it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.63it/s]
10/08/2021 22:38:48 - INFO - __main__ -   ***** Evaluation result  in ru *****
10/08/2021 22:38:48 - INFO - __main__ -     f1 = 0.5476276132230083
10/08/2021 22:38:48 - INFO - __main__ -     loss = 2.474687463654497
10/08/2021 22:38:48 - INFO - __main__ -     precision = 0.5334890965732088
10/08/2021 22:38:48 - INFO - __main__ -     recall = 0.5625359283895869
10/08/2021 22:38:48 - INFO - __main__ -   Language adapter for bg not found, using en instead
10/08/2021 22:38:48 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:38:48 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/08/2021 22:38:49 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/08/2021 22:38:49 - INFO - __main__ -     Num examples = 10004
10/08/2021 22:38:49 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 2/313 [00:00<01:30,  3.44it/s]Evaluating:   1%|          | 3/313 [00:00<01:09,  4.44it/s]Evaluating:   1%|▏         | 4/313 [00:00<01:00,  5.15it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:54,  5.64it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:51,  5.99it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:49,  6.23it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:47,  6.40it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:46,  6.51it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.59it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:45,  6.65it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:44,  6.69it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:44,  6.72it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.74it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.75it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.76it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.76it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.77it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:43,  6.77it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:43,  6.77it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.75it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.75it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.76it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:41,  6.77it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 33/313 [00:05<00:41,  6.76it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.75it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.75it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.75it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.75it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.75it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:40,  6.76it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:40,  6.76it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.76it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.75it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:39,  6.75it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.75it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:39,  6.75it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:39,  6.74it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.75it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.74it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:38,  6.74it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.74it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.75it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:48,  5.32it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:45,  5.68it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:43,  5.96it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:41,  6.17it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:40,  6.32it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:39,  6.43it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:39,  6.51it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:38,  6.57it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.62it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.64it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.67it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.67it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.69it/s]Evaluating:  21%|██        | 66/313 [00:10<00:36,  6.69it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:39,  6.27it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:37,  6.51it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.65it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:35,  6.75it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:35,  6.83it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:34,  6.91it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:34,  6.94it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:34,  6.94it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:34,  6.92it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:34,  6.92it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:34,  6.85it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:34,  6.81it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:34,  6.78it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:34,  6.76it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.74it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.73it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:33,  6.68it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:33,  6.68it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.68it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.68it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.67it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:33,  6.67it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:32,  6.67it/s]Evaluating:  30%|███       | 94/313 [00:14<00:32,  6.67it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.66it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.65it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.64it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:32,  6.64it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:35,  5.98it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:33,  6.28it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:32,  6.46it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.61it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:30,  6.73it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:30,  6.81it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:30,  6.82it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:30,  6.79it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:30,  6.78it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.74it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.71it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:30,  6.68it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.67it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.66it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.66it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.65it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.64it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.65it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:29,  6.65it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.64it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.63it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.64it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|████      | 126/313 [00:19<00:28,  6.62it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.59it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.61it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.60it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.61it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:27,  6.61it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:27,  6.60it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:31,  5.65it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:30,  5.91it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:29,  6.09it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:28,  6.23it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:27,  6.33it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:27,  6.41it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.47it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.51it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:26,  6.53it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:26,  6.53it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:25,  6.52it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:25,  6.54it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.55it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.56it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.56it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:24,  6.56it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.58it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:24,  6.57it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:24,  6.58it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.42it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.45it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:24,  6.48it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:24,  6.50it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:25,  6.03it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:24,  6.27it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:24,  6.41it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.50it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.57it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.62it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.59it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:22,  6.58it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:22,  6.57it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:24,  5.99it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:23,  6.14it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:23,  6.26it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:22,  6.34it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:22,  6.43it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:21,  6.47it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:21,  6.49it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:21,  6.52it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:21,  6.55it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:20,  6.60it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:20,  6.62it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:20,  6.67it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:20,  6.69it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:20,  6.66it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:20,  6.61it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:20,  6.59it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.57it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.56it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:19,  6.57it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:19,  6.57it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:19,  6.55it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:19,  6.54it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:19,  6.54it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.57it/s]Evaluating:  61%|██████    | 190/313 [00:29<00:18,  6.58it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:18,  6.58it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:18,  6.56it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:18,  6.57it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:18,  6.59it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.58it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.56it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:17,  6.57it/s]Evaluating:  63%|██████▎   | 198/313 [00:30<00:17,  6.54it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:17,  6.53it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:17,  6.54it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:17,  6.55it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:16,  6.57it/s]Evaluating:  65%|██████▍   | 203/313 [00:31<00:16,  6.57it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:16,  6.58it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:16,  6.61it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:16,  6.61it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:16,  6.62it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.64it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.65it/s]Evaluating:  67%|██████▋   | 210/313 [00:32<00:15,  6.69it/s]Evaluating:  67%|██████▋   | 211/313 [00:32<00:15,  6.75it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:14,  6.79it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:14,  6.71it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.69it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.69it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.79it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:13,  6.87it/s]Evaluating:  70%|██████▉   | 218/313 [00:33<00:13,  6.92it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:13,  6.92it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:13,  6.92it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:13,  6.91it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.90it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.92it/s]Evaluating:  72%|███████▏  | 224/313 [00:34<00:12,  6.87it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:12,  6.82it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:12,  6.85it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.84it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.83it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.83it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.84it/s]Evaluating:  74%|███████▍  | 231/313 [00:35<00:12,  6.83it/s]Evaluating:  74%|███████▍  | 232/313 [00:35<00:11,  6.87it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:11,  6.88it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:17,  4.45it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:15,  4.91it/s]Evaluating:  75%|███████▌  | 236/313 [00:36<00:14,  5.29it/s]Evaluating:  76%|███████▌  | 237/313 [00:36<00:13,  5.62it/s]Evaluating:  76%|███████▌  | 238/313 [00:36<00:12,  5.88it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:12,  6.06it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.21it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.30it/s]Evaluating:  77%|███████▋  | 242/313 [00:37<00:11,  6.36it/s]Evaluating:  78%|███████▊  | 243/313 [00:37<00:10,  6.37it/s]Evaluating:  78%|███████▊  | 244/313 [00:37<00:10,  6.41it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:10,  6.46it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:10,  6.51it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:10,  6.56it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:09,  6.61it/s]Evaluating:  80%|███████▉  | 249/313 [00:38<00:09,  6.67it/s]Evaluating:  80%|███████▉  | 250/313 [00:38<00:09,  6.68it/s]Evaluating:  80%|████████  | 251/313 [00:38<00:09,  6.62it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:09,  6.61it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:09,  6.62it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:08,  6.62it/s]Evaluating:  81%|████████▏ | 255/313 [00:39<00:08,  6.58it/s]Evaluating:  82%|████████▏ | 256/313 [00:39<00:08,  6.54it/s]Evaluating:  82%|████████▏ | 257/313 [00:39<00:08,  6.55it/s]Evaluating:  82%|████████▏ | 258/313 [00:39<00:08,  6.52it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.49it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.50it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:07,  6.54it/s]Evaluating:  84%|████████▎ | 262/313 [00:40<00:07,  6.56it/s]Evaluating:  84%|████████▍ | 263/313 [00:40<00:07,  6.53it/s]Evaluating:  84%|████████▍ | 264/313 [00:40<00:07,  6.51it/s]Evaluating:  85%|████████▍ | 265/313 [00:40<00:07,  6.56it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.57it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:07,  6.55it/s]Evaluating:  86%|████████▌ | 268/313 [00:41<00:06,  6.57it/s]Evaluating:  86%|████████▌ | 269/313 [00:41<00:06,  6.61it/s]Evaluating:  86%|████████▋ | 270/313 [00:41<00:06,  6.66it/s]Evaluating:  87%|████████▋ | 271/313 [00:41<00:06,  6.66it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.65it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:06,  6.65it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.67it/s]Evaluating:  88%|████████▊ | 275/313 [00:42<00:05,  6.72it/s]Evaluating:  88%|████████▊ | 276/313 [00:42<00:05,  6.77it/s]Evaluating:  88%|████████▊ | 277/313 [00:42<00:05,  6.70it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.60it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.52it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:05,  6.47it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.45it/s]Evaluating:  90%|█████████ | 282/313 [00:43<00:04,  6.46it/s]Evaluating:  90%|█████████ | 283/313 [00:43<00:04,  6.49it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  6.53it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  6.57it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  6.50it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  6.48it/s]Evaluating:  92%|█████████▏| 288/313 [00:44<00:03,  6.49it/s]Evaluating:  92%|█████████▏| 289/313 [00:44<00:03,  6.53it/s]Evaluating:  93%|█████████▎| 290/313 [00:44<00:03,  6.58it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.65it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.68it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:02,  6.71it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:02,  6.65it/s]Evaluating:  94%|█████████▍| 295/313 [00:45<00:02,  6.59it/s]Evaluating:  95%|█████████▍| 296/313 [00:45<00:02,  6.56it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:02,  6.57it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.54it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.51it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:02,  6.48it/s]Evaluating:  96%|█████████▌| 301/313 [00:46<00:01,  6.46it/s]Evaluating:  96%|█████████▋| 302/313 [00:46<00:01,  6.43it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.42it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.43it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.42it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.42it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.43it/s]Evaluating:  98%|█████████▊| 308/313 [00:47<00:00,  6.48it/s]Evaluating:  99%|█████████▊| 309/313 [00:47<00:00,  6.54it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  6.54it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.53it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.55it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  7.25it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.54it/s]
10/08/2021 22:39:38 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/08/2021 22:39:38 - INFO - __main__ -     f1 = 0.6738880568423252
10/08/2021 22:39:38 - INFO - __main__ -     loss = 1.618013704165864
10/08/2021 22:39:38 - INFO - __main__ -     precision = 0.6603412969283277
10/08/2021 22:39:38 - INFO - __main__ -     recall = 0.6880022757983074
10/08/2021 22:39:38 - INFO - __main__ -   Language adapter for uk not found, using en instead
10/08/2021 22:39:38 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:39:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/08/2021 22:39:40 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/08/2021 22:39:40 - INFO - __main__ -     Num examples = 10001
10/08/2021 22:39:40 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 2/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.80it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.79it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.79it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:44,  6.79it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.78it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.79it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.79it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:43,  6.79it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:43,  6.79it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:43,  6.79it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.79it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.78it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.78it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.76it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:42,  6.77it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:42,  6.77it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.75it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.76it/s]Evaluating:   9%|▊         | 27/313 [00:03<00:42,  6.76it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.76it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.76it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:41,  6.76it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.76it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.76it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.76it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.75it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:40,  6.74it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:40,  6.73it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:40,  6.74it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.74it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.74it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.72it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:39,  6.73it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:39,  6.74it/s]Evaluating:  15%|█▌        | 47/313 [00:06<00:39,  6.75it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:39,  6.75it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.74it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:38,  6.74it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:38,  6.74it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:38,  6.73it/s]Evaluating:  17%|█▋        | 54/313 [00:07<00:38,  6.73it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.73it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.73it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.74it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:37,  6.73it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 60/313 [00:08<00:37,  6.72it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.72it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.71it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.69it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.69it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.68it/s]Evaluating:  21%|██        | 66/313 [00:09<00:36,  6.68it/s]Evaluating:  21%|██▏       | 67/313 [00:09<00:36,  6.70it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.73it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.75it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:35,  6.76it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:35,  6.76it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:35,  6.73it/s]Evaluating:  23%|██▎       | 73/313 [00:10<00:35,  6.72it/s]Evaluating:  24%|██▎       | 74/313 [00:10<00:35,  6.71it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:35,  6.71it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.71it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:35,  6.70it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:35,  6.70it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:34,  6.69it/s]Evaluating:  26%|██▌       | 80/313 [00:11<00:34,  6.69it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:34,  6.69it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:34,  6.68it/s]Evaluating:  27%|██▋       | 86/313 [00:12<00:33,  6.68it/s]Evaluating:  28%|██▊       | 87/313 [00:12<00:33,  6.68it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:33,  6.68it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:33,  6.68it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.68it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.58it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:33,  6.54it/s]Evaluating:  30%|██▉       | 93/313 [00:13<00:33,  6.58it/s]Evaluating:  30%|███       | 94/313 [00:13<00:33,  6.60it/s]Evaluating:  30%|███       | 95/313 [00:14<00:32,  6.61it/s]Evaluating:  31%|███       | 96/313 [00:14<00:32,  6.62it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.63it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:32,  6.64it/s]Evaluating:  32%|███▏      | 99/313 [00:14<00:32,  6.64it/s]Evaluating:  32%|███▏      | 100/313 [00:14<00:32,  6.63it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:31,  6.64it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:31,  6.64it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.64it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:36,  5.78it/s]Evaluating:  34%|███▎      | 105/313 [00:15<00:38,  5.44it/s]Evaluating:  34%|███▍      | 106/313 [00:15<00:35,  5.85it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:33,  6.15it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:32,  6.37it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:31,  6.51it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:30,  6.59it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:30,  6.64it/s]Evaluating:  36%|███▌      | 112/313 [00:16<00:30,  6.68it/s]Evaluating:  36%|███▌      | 113/313 [00:16<00:29,  6.68it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:29,  6.66it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:29,  6.65it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:29,  6.64it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:29,  6.64it/s]Evaluating:  38%|███▊      | 118/313 [00:17<00:29,  6.63it/s]Evaluating:  38%|███▊      | 119/313 [00:17<00:29,  6.63it/s]Evaluating:  38%|███▊      | 120/313 [00:17<00:29,  6.63it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:28,  6.63it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:28,  6.63it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:28,  6.63it/s]Evaluating:  40%|███▉      | 124/313 [00:18<00:28,  6.62it/s]Evaluating:  40%|███▉      | 125/313 [00:18<00:28,  6.62it/s]Evaluating:  40%|████      | 126/313 [00:18<00:28,  6.62it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.62it/s]Evaluating:  41%|████      | 128/313 [00:19<00:27,  6.64it/s]Evaluating:  41%|████      | 129/313 [00:19<00:27,  6.65it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:27,  6.66it/s]Evaluating:  42%|████▏     | 131/313 [00:19<00:27,  6.67it/s]Evaluating:  42%|████▏     | 132/313 [00:19<00:27,  6.68it/s]Evaluating:  42%|████▏     | 133/313 [00:19<00:26,  6.68it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:26,  6.65it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:26,  6.63it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:26,  6.62it/s]Evaluating:  44%|████▍     | 137/313 [00:20<00:26,  6.61it/s]Evaluating:  44%|████▍     | 138/313 [00:20<00:39,  4.45it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:35,  4.93it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:32,  5.34it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:30,  5.66it/s]Evaluating:  45%|████▌     | 142/313 [00:21<00:28,  5.93it/s]Evaluating:  46%|████▌     | 143/313 [00:21<00:27,  6.13it/s]Evaluating:  46%|████▌     | 144/313 [00:21<00:26,  6.27it/s]Evaluating:  46%|████▋     | 145/313 [00:21<00:26,  6.38it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:25,  6.45it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:25,  6.49it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.52it/s]Evaluating:  48%|████▊     | 149/313 [00:22<00:25,  6.54it/s]Evaluating:  48%|████▊     | 150/313 [00:22<00:24,  6.56it/s]Evaluating:  48%|████▊     | 151/313 [00:22<00:24,  6.57it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:24,  6.57it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.59it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.61it/s]Evaluating:  50%|████▉     | 155/313 [00:23<00:23,  6.62it/s]Evaluating:  50%|████▉     | 156/313 [00:23<00:23,  6.63it/s]Evaluating:  50%|█████     | 157/313 [00:23<00:23,  6.61it/s]Evaluating:  50%|█████     | 158/313 [00:23<00:23,  6.60it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:23,  6.59it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.58it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.59it/s]Evaluating:  52%|█████▏    | 162/313 [00:24<00:22,  6.61it/s]Evaluating:  52%|█████▏    | 163/313 [00:24<00:22,  6.63it/s]Evaluating:  52%|█████▏    | 164/313 [00:24<00:22,  6.64it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:22,  6.64it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:22,  6.66it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:21,  6.69it/s]Evaluating:  54%|█████▎    | 168/313 [00:25<00:21,  6.70it/s]Evaluating:  54%|█████▍    | 169/313 [00:25<00:21,  6.71it/s]Evaluating:  54%|█████▍    | 170/313 [00:25<00:30,  4.70it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:27,  5.14it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:25,  5.49it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:24,  5.77it/s]Evaluating:  56%|█████▌    | 174/313 [00:26<00:23,  5.98it/s]Evaluating:  56%|█████▌    | 175/313 [00:26<00:22,  6.17it/s]Evaluating:  56%|█████▌    | 176/313 [00:26<00:21,  6.32it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:21,  6.44it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:20,  6.50it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:20,  6.54it/s]Evaluating:  58%|█████▊    | 180/313 [00:27<00:20,  6.62it/s]Evaluating:  58%|█████▊    | 181/313 [00:27<00:19,  6.64it/s]Evaluating:  58%|█████▊    | 182/313 [00:27<00:19,  6.68it/s]Evaluating:  58%|█████▊    | 183/313 [00:27<00:19,  6.73it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:19,  6.77it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:18,  6.77it/s]Evaluating:  59%|█████▉    | 186/313 [00:28<00:18,  6.76it/s]Evaluating:  60%|█████▉    | 187/313 [00:28<00:18,  6.76it/s]Evaluating:  60%|██████    | 188/313 [00:28<00:18,  6.75it/s]Evaluating:  60%|██████    | 189/313 [00:28<00:18,  6.76it/s]Evaluating:  61%|██████    | 190/313 [00:28<00:18,  6.77it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:17,  6.78it/s]Evaluating:  61%|██████▏   | 192/313 [00:29<00:17,  6.76it/s]Evaluating:  62%|██████▏   | 193/313 [00:29<00:17,  6.76it/s]Evaluating:  62%|██████▏   | 194/313 [00:29<00:17,  6.74it/s]Evaluating:  62%|██████▏   | 195/313 [00:29<00:17,  6.75it/s]Evaluating:  63%|██████▎   | 196/313 [00:29<00:17,  6.72it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:17,  6.71it/s]Evaluating:  63%|██████▎   | 198/313 [00:30<00:17,  6.72it/s]Evaluating:  64%|██████▎   | 199/313 [00:30<00:16,  6.73it/s]Evaluating:  64%|██████▍   | 200/313 [00:30<00:16,  6.74it/s]Evaluating:  64%|██████▍   | 201/313 [00:30<00:16,  6.76it/s]Evaluating:  65%|██████▍   | 202/313 [00:30<00:19,  5.75it/s]Evaluating:  65%|██████▍   | 203/313 [00:30<00:18,  6.02it/s]Evaluating:  65%|██████▌   | 204/313 [00:31<00:17,  6.30it/s]Evaluating:  65%|██████▌   | 205/313 [00:31<00:16,  6.52it/s]Evaluating:  66%|██████▌   | 206/313 [00:31<00:16,  6.68it/s]Evaluating:  66%|██████▌   | 207/313 [00:31<00:15,  6.79it/s]Evaluating:  66%|██████▋   | 208/313 [00:31<00:15,  6.86it/s]Evaluating:  67%|██████▋   | 209/313 [00:31<00:15,  6.89it/s]Evaluating:  67%|██████▋   | 210/313 [00:31<00:14,  6.93it/s]Evaluating:  67%|██████▋   | 211/313 [00:32<00:14,  6.95it/s]Evaluating:  68%|██████▊   | 212/313 [00:32<00:14,  6.93it/s]Evaluating:  68%|██████▊   | 213/313 [00:32<00:14,  6.80it/s]Evaluating:  68%|██████▊   | 214/313 [00:32<00:14,  6.73it/s]Evaluating:  69%|██████▊   | 215/313 [00:32<00:14,  6.68it/s]Evaluating:  69%|██████▉   | 216/313 [00:32<00:14,  6.75it/s]Evaluating:  69%|██████▉   | 217/313 [00:33<00:14,  6.82it/s]Evaluating:  70%|██████▉   | 218/313 [00:33<00:13,  6.80it/s]Evaluating:  70%|██████▉   | 219/313 [00:33<00:13,  6.81it/s]Evaluating:  70%|███████   | 220/313 [00:33<00:13,  6.85it/s]Evaluating:  71%|███████   | 221/313 [00:33<00:13,  6.88it/s]Evaluating:  71%|███████   | 222/313 [00:33<00:13,  6.86it/s]Evaluating:  71%|███████   | 223/313 [00:33<00:13,  6.78it/s]Evaluating:  72%|███████▏  | 224/313 [00:34<00:13,  6.78it/s]Evaluating:  72%|███████▏  | 225/313 [00:34<00:12,  6.78it/s]Evaluating:  72%|███████▏  | 226/313 [00:34<00:12,  6.79it/s]Evaluating:  73%|███████▎  | 227/313 [00:34<00:12,  6.78it/s]Evaluating:  73%|███████▎  | 228/313 [00:34<00:12,  6.81it/s]Evaluating:  73%|███████▎  | 229/313 [00:34<00:12,  6.84it/s]Evaluating:  73%|███████▎  | 230/313 [00:34<00:12,  6.78it/s]Evaluating:  74%|███████▍  | 231/313 [00:35<00:12,  6.72it/s]Evaluating:  74%|███████▍  | 232/313 [00:35<00:12,  6.71it/s]Evaluating:  74%|███████▍  | 233/313 [00:35<00:11,  6.71it/s]Evaluating:  75%|███████▍  | 234/313 [00:35<00:11,  6.71it/s]Evaluating:  75%|███████▌  | 235/313 [00:35<00:11,  6.74it/s]Evaluating:  75%|███████▌  | 236/313 [00:35<00:12,  6.13it/s]Evaluating:  76%|███████▌  | 237/313 [00:36<00:12,  6.23it/s]Evaluating:  76%|███████▌  | 238/313 [00:36<00:11,  6.31it/s]Evaluating:  76%|███████▋  | 239/313 [00:36<00:11,  6.37it/s]Evaluating:  77%|███████▋  | 240/313 [00:36<00:11,  6.40it/s]Evaluating:  77%|███████▋  | 241/313 [00:36<00:11,  6.43it/s]Evaluating:  77%|███████▋  | 242/313 [00:36<00:10,  6.49it/s]Evaluating:  78%|███████▊  | 243/313 [00:36<00:10,  6.49it/s]Evaluating:  78%|███████▊  | 244/313 [00:37<00:10,  6.55it/s]Evaluating:  78%|███████▊  | 245/313 [00:37<00:10,  6.51it/s]Evaluating:  79%|███████▊  | 246/313 [00:37<00:10,  6.50it/s]Evaluating:  79%|███████▉  | 247/313 [00:37<00:10,  6.48it/s]Evaluating:  79%|███████▉  | 248/313 [00:37<00:10,  6.48it/s]Evaluating:  80%|███████▉  | 249/313 [00:37<00:09,  6.47it/s]Evaluating:  80%|███████▉  | 250/313 [00:38<00:09,  6.46it/s]Evaluating:  80%|████████  | 251/313 [00:38<00:09,  6.51it/s]Evaluating:  81%|████████  | 252/313 [00:38<00:09,  6.52it/s]Evaluating:  81%|████████  | 253/313 [00:38<00:09,  6.50it/s]Evaluating:  81%|████████  | 254/313 [00:38<00:09,  6.49it/s]Evaluating:  81%|████████▏ | 255/313 [00:38<00:08,  6.52it/s]Evaluating:  82%|████████▏ | 256/313 [00:38<00:08,  6.53it/s]Evaluating:  82%|████████▏ | 257/313 [00:39<00:08,  6.51it/s]Evaluating:  82%|████████▏ | 258/313 [00:39<00:08,  6.50it/s]Evaluating:  83%|████████▎ | 259/313 [00:39<00:08,  6.48it/s]Evaluating:  83%|████████▎ | 260/313 [00:39<00:08,  6.50it/s]Evaluating:  83%|████████▎ | 261/313 [00:39<00:08,  6.48it/s]Evaluating:  84%|████████▎ | 262/313 [00:39<00:07,  6.49it/s]Evaluating:  84%|████████▍ | 263/313 [00:40<00:07,  6.53it/s]Evaluating:  84%|████████▍ | 264/313 [00:40<00:07,  6.57it/s]Evaluating:  85%|████████▍ | 265/313 [00:40<00:07,  6.58it/s]Evaluating:  85%|████████▍ | 266/313 [00:40<00:07,  6.64it/s]Evaluating:  85%|████████▌ | 267/313 [00:40<00:06,  6.66it/s]Evaluating:  86%|████████▌ | 268/313 [00:40<00:06,  6.73it/s]Evaluating:  86%|████████▌ | 269/313 [00:40<00:06,  6.75it/s]Evaluating:  86%|████████▋ | 270/313 [00:41<00:06,  6.76it/s]Evaluating:  87%|████████▋ | 271/313 [00:41<00:06,  6.78it/s]Evaluating:  87%|████████▋ | 272/313 [00:41<00:06,  6.76it/s]Evaluating:  87%|████████▋ | 273/313 [00:41<00:05,  6.74it/s]Evaluating:  88%|████████▊ | 274/313 [00:41<00:05,  6.78it/s]Evaluating:  88%|████████▊ | 275/313 [00:41<00:05,  6.79it/s]Evaluating:  88%|████████▊ | 276/313 [00:41<00:05,  6.73it/s]Evaluating:  88%|████████▊ | 277/313 [00:42<00:05,  6.68it/s]Evaluating:  89%|████████▉ | 278/313 [00:42<00:05,  6.66it/s]Evaluating:  89%|████████▉ | 279/313 [00:42<00:05,  6.63it/s]Evaluating:  89%|████████▉ | 280/313 [00:42<00:04,  6.63it/s]Evaluating:  90%|████████▉ | 281/313 [00:42<00:04,  6.68it/s]Evaluating:  90%|█████████ | 282/313 [00:42<00:04,  6.34it/s]Evaluating:  90%|█████████ | 283/313 [00:43<00:04,  6.09it/s]Evaluating:  91%|█████████ | 284/313 [00:43<00:04,  5.94it/s]Evaluating:  91%|█████████ | 285/313 [00:43<00:04,  5.78it/s]Evaluating:  91%|█████████▏| 286/313 [00:43<00:04,  5.72it/s]Evaluating:  92%|█████████▏| 287/313 [00:43<00:04,  5.80it/s]Evaluating:  92%|█████████▏| 288/313 [00:43<00:04,  5.88it/s]Evaluating:  92%|█████████▏| 289/313 [00:44<00:04,  5.98it/s]Evaluating:  93%|█████████▎| 290/313 [00:44<00:03,  6.06it/s]Evaluating:  93%|█████████▎| 291/313 [00:44<00:03,  6.14it/s]Evaluating:  93%|█████████▎| 292/313 [00:44<00:03,  6.21it/s]Evaluating:  94%|█████████▎| 293/313 [00:44<00:03,  6.27it/s]Evaluating:  94%|█████████▍| 294/313 [00:44<00:03,  6.33it/s]Evaluating:  94%|█████████▍| 295/313 [00:45<00:02,  6.39it/s]Evaluating:  95%|█████████▍| 296/313 [00:45<00:02,  6.43it/s]Evaluating:  95%|█████████▍| 297/313 [00:45<00:02,  6.47it/s]Evaluating:  95%|█████████▌| 298/313 [00:45<00:02,  6.51it/s]Evaluating:  96%|█████████▌| 299/313 [00:45<00:02,  6.58it/s]Evaluating:  96%|█████████▌| 300/313 [00:45<00:01,  6.63it/s]Evaluating:  96%|█████████▌| 301/313 [00:45<00:01,  6.67it/s]Evaluating:  96%|█████████▋| 302/313 [00:46<00:01,  6.69it/s]Evaluating:  97%|█████████▋| 303/313 [00:46<00:01,  6.70it/s]Evaluating:  97%|█████████▋| 304/313 [00:46<00:01,  6.69it/s]Evaluating:  97%|█████████▋| 305/313 [00:46<00:01,  6.72it/s]Evaluating:  98%|█████████▊| 306/313 [00:46<00:01,  6.62it/s]Evaluating:  98%|█████████▊| 307/313 [00:46<00:00,  6.56it/s]Evaluating:  98%|█████████▊| 308/313 [00:46<00:00,  6.55it/s]Evaluating:  99%|█████████▊| 309/313 [00:47<00:00,  6.58it/s]Evaluating:  99%|█████████▉| 310/313 [00:47<00:00,  6.57it/s]Evaluating:  99%|█████████▉| 311/313 [00:47<00:00,  6.57it/s]Evaluating: 100%|█████████▉| 312/313 [00:47<00:00,  6.54it/s]Evaluating: 100%|██████████| 313/313 [00:47<00:00,  6.57it/s]
10/08/2021 22:40:29 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/08/2021 22:40:29 - INFO - __main__ -     f1 = 0.5760532486196204
10/08/2021 22:40:29 - INFO - __main__ -     loss = 1.8680855310001312
10/08/2021 22:40:29 - INFO - __main__ -     precision = 0.5848114873685019
10/08/2021 22:40:29 - INFO - __main__ -     recall = 0.5675534689619197
10/08/2021 22:40:29 - INFO - __main__ -   Language adapter for be not found, using en instead
10/08/2021 22:40:29 - INFO - __main__ -   Set active language adapter to en
10/08/2021 22:40:29 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/08/2021 22:40:29 - INFO - __main__ -   ***** Running evaluation  in be *****
10/08/2021 22:40:29 - INFO - __main__ -     Num examples = 1001
10/08/2021 22:40:29 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.80it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.80it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.81it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.81it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  6.80it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.80it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.80it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.80it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.79it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.77it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.77it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.78it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.78it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.78it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.78it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.77it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.76it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.77it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.76it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.77it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.76it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.76it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.63it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.69it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.70it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.71it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  6.73it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.74it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.75it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.75it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.74it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.91it/s]
10/08/2021 22:40:34 - INFO - __main__ -   ***** Evaluation result  in be *****
10/08/2021 22:40:34 - INFO - __main__ -     f1 = 0.5843787940105221
10/08/2021 22:40:34 - INFO - __main__ -     loss = 1.5845774412155151
10/08/2021 22:40:34 - INFO - __main__ -     precision = 0.5757575757575758
10/08/2021 22:40:34 - INFO - __main__ -     recall = 0.5932621199671323
10/08/2021 22:40:34 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:40:46 - INFO - __main__ -   Using lang2id = None
10/08/2021 22:40:46 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 22:41:06 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 23:14:09 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=0, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s0/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:14:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 23:14:09 - INFO - __main__ -   Seed = 0
10/08/2021 23:14:09 - INFO - root -   save model
10/08/2021 23:14:09 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=0, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s0/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:14:09 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:14:31 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:14:31 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 23:14:31 - INFO - __main__ -   Task Adapter will be loaded from this path /home/abhijeet/rohan/emea/outputs/ner
10/08/2021 23:14:31 - INFO - root -   Trying to decide if add adapter
10/08/2021 23:14:31 - INFO - root -   loading task adapter
Loading module configuration from /home/abhijeet/rohan/emea/outputs/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from /home/abhijeet/rohan/emea/outputs/ner/pytorch_adapter.bin
Loading module configuration from /home/abhijeet/rohan/emea/outputs/ner/head_config.json
Loading module weights from /home/abhijeet/rohan/emea/outputs/ner/pytorch_model_head.bin
10/08/2021 23:14:31 - INFO - root -   loading lang adpater en/wiki@ukp
10/08/2021 23:14:31 - INFO - __main__ -   Language = en
10/08/2021 23:14:31 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 23:14:37 - INFO - __main__ -   Language adapter for bn not found, using en instead
10/08/2021 23:14:37 - INFO - __main__ -   Set active language adapter to en
10/08/2021 23:14:37 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 23:14:38 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 23:14:38 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:14:38 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  6.14it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.70it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  7.05it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.28it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.41it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.48it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.49it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.53it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.57it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.59it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.60it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.59it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.60it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.60it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.60it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.60it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.60it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.59it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.57it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.57it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.57it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.58it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.58it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.59it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.60it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.59it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.53it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.53it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.55it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.53it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.52it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.67it/s]
10/08/2021 23:14:42 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 23:14:42 - INFO - __main__ -     f1 = 0.5495495495495496
10/08/2021 23:14:42 - INFO - __main__ -     loss = 2.9823328033089638
10/08/2021 23:14:42 - INFO - __main__ -     precision = 0.5393457117595049
10/08/2021 23:14:42 - INFO - __main__ -     recall = 0.5601469237832875
10/08/2021 23:14:42 - INFO - __main__ -   Language adapter for mr not found, using en instead
10/08/2021 23:14:42 - INFO - __main__ -   Set active language adapter to en
10/08/2021 23:14:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 23:14:42 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 23:14:42 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:14:42 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.65it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.60it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.60it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.57it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.57it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.58it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.59it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.59it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.58it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.59it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.59it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.58it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.58it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.48it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.50it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.48it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.49it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.51it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.53it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.55it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.55it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.53it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.49it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.49it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.51it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.52it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.53it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.55it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.55it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.53it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.53it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.71it/s]
10/08/2021 23:14:46 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 23:14:46 - INFO - __main__ -     f1 = 0.4736434108527132
10/08/2021 23:14:46 - INFO - __main__ -     loss = 2.0860213078558445
10/08/2021 23:14:46 - INFO - __main__ -     precision = 0.4642857142857143
10/08/2021 23:14:46 - INFO - __main__ -     recall = 0.4833860759493671
10/08/2021 23:14:46 - INFO - __main__ -   Language adapter for ta not found, using en instead
10/08/2021 23:14:46 - INFO - __main__ -   Set active language adapter to en
10/08/2021 23:14:46 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 23:14:46 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 23:14:46 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:14:46 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.67it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.59it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.59it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.57it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.54it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.54it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.53it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.54it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.53it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.54it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.54it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.49it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.49it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.49it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.50it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.50it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.49it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.49it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.51it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.30it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  6.61it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.87it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.06it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.18it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.28it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.35it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.40it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.39it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.41it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.44it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.46it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.52it/s]
10/08/2021 23:14:51 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 23:14:51 - INFO - __main__ -     f1 = 0.322609472743521
10/08/2021 23:14:51 - INFO - __main__ -     loss = 2.8174578733742237
10/08/2021 23:14:51 - INFO - __main__ -     precision = 0.362086258776329
10/08/2021 23:14:51 - INFO - __main__ -     recall = 0.2908944399677679
10/08/2021 23:14:51 - INFO - __main__ -   Language adapter for is not found, using en instead
10/08/2021 23:14:51 - INFO - __main__ -   Set active language adapter to en
10/08/2021 23:14:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
10/08/2021 23:14:51 - INFO - __main__ -   ***** Running evaluation  in is *****
10/08/2021 23:14:51 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:14:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.60it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.56it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.56it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.57it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.56it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.56it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.55it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.55it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.53it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.52it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.52it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.52it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.51it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.50it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.47it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.44it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.38it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.38it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.41it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.41it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.43it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.37it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.37it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.39it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.42it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.43it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.42it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.36it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.35it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.35it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.34it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.61it/s]
10/08/2021 23:14:55 - INFO - __main__ -   ***** Evaluation result  in is *****
10/08/2021 23:14:55 - INFO - __main__ -     f1 = 0.6472148541114059
10/08/2021 23:14:55 - INFO - __main__ -     loss = 0.9845715211704373
10/08/2021 23:14:55 - INFO - __main__ -     precision = 0.6039603960396039
10/08/2021 23:14:55 - INFO - __main__ -     recall = 0.6971428571428572
10/08/2021 23:14:55 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:15:08 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:15:08 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:15:24 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 23:52:02 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:52:02 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 23:52:02 - INFO - __main__ -   Seed = 12
10/08/2021 23:52:02 - INFO - root -   save model
10/08/2021 23:52:02 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:52:02 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:52:15 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:52:15 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 23:52:15 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/08/2021 23:52:15 - INFO - root -   Trying to decide if add adapter
10/08/2021 23:52:15 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 23:52:15 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/08/2021 23:52:15 - INFO - __main__ -   Language = en
10/08/2021 23:52:15 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 23:52:17 - INFO - __main__ -   Language = hi
10/08/2021 23:52:17 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/08/2021 23:52:19 - INFO - __main__ -   Language = ar
10/08/2021 23:52:19 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/08/2021 23:52:24 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 23:52:24 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 23:52:24 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:52:24 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.65it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.92it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.09it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.16it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.19it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.21it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.22it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.23it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.23it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.24it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.24it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.23it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.24it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.24it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.24it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.24it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.23it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.22it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.23it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.22it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.22it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.20it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.20it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.20it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.21it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.21it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.21it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.21it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.21it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.21it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.22it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.34it/s]
10/08/2021 23:52:29 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 23:52:29 - INFO - __main__ -     f1 = 0.4784422809457581
10/08/2021 23:52:29 - INFO - __main__ -     loss = 1.152155227959156
10/08/2021 23:52:29 - INFO - __main__ -     precision = 0.48314606741573035
10/08/2021 23:52:29 - INFO - __main__ -     recall = 0.4738292011019284
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 902, in main
    result_json['adapter_weights'] = args.adapter_weights 
AttributeError: 'ModelArguments' object has no attribute 'adapter_weights'
PyTorch version 1.9.0+cu102 available.
10/08/2021 23:55:01 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:55:01 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 23:55:01 - INFO - __main__ -   Seed = 12
10/08/2021 23:55:01 - INFO - root -   save model
10/08/2021 23:55:01 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:55:01 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:55:14 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:55:14 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 23:55:14 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/08/2021 23:55:14 - INFO - root -   Trying to decide if add adapter
10/08/2021 23:55:14 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 23:55:14 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/08/2021 23:55:14 - INFO - __main__ -   Language = en
10/08/2021 23:55:14 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 23:55:15 - INFO - __main__ -   Language = hi
10/08/2021 23:55:15 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/08/2021 23:55:16 - INFO - __main__ -   Language = ar
10/08/2021 23:55:16 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/08/2021 23:55:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 23:55:21 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 23:55:21 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:55:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.34it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.65it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  5.92it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.05it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.11it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.14it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.17it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.18it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.19it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.20it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.20it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.19it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.19it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.19it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.18it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.18it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.15it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.16it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.17it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.17it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.17it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.18it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.17it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.14it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.13it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.14it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.15it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.12it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.14it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.14it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.13it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.26it/s]
10/08/2021 23:55:26 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 23:55:26 - INFO - __main__ -     f1 = 0.4784422809457581
10/08/2021 23:55:26 - INFO - __main__ -     loss = 1.152155227959156
10/08/2021 23:55:26 - INFO - __main__ -     precision = 0.48314606741573035
10/08/2021 23:55:26 - INFO - __main__ -     recall = 0.4738292011019284
10/08/2021 23:55:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 23:55:26 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 23:55:26 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:55:26 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.27it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.23it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.22it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.21it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  5.41it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:04,  5.65it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  5.82it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:04,  5.93it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.01it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.06it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.10it/s]Evaluating:  38%|███▊      | 12/32 [00:02<00:03,  6.12it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.15it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.16it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.17it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.18it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.17it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.17it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.17it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.16it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.14it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.15it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.15it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.09it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.11it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.13it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.14it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.13it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.00it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.04it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.06it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.21it/s]
10/08/2021 23:55:31 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 23:55:31 - INFO - __main__ -     f1 = 0.5677276091783864
10/08/2021 23:55:31 - INFO - __main__ -     loss = 0.5726263104006648
10/08/2021 23:55:31 - INFO - __main__ -     precision = 0.5333796940194715
10/08/2021 23:55:31 - INFO - __main__ -     recall = 0.6068037974683544
10/08/2021 23:55:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 23:55:31 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 23:55:31 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:55:31 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.21it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.15it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.16it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:05,  5.55it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  5.76it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:04,  5.88it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  5.96it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:04,  5.98it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.04it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.08it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.10it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.12it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.13it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.13it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.14it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.15it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.15it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.15it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.14it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.13it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.13it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.14it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.14it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.11it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.10it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.12it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.12it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.11it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.11it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.10it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.06it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.21it/s]
10/08/2021 23:55:37 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 23:55:37 - INFO - __main__ -     f1 = 0.23917322834645666
10/08/2021 23:55:37 - INFO - __main__ -     loss = 1.099341606721282
10/08/2021 23:55:37 - INFO - __main__ -     precision = 0.30720606826801516
10/08/2021 23:55:37 - INFO - __main__ -     recall = 0.19580983078162773
10/08/2021 23:55:37 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:55:51 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:55:51 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:56:08 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 23:56:54 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:56:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 23:56:54 - INFO - __main__ -   Seed = 12
10/08/2021 23:56:54 - INFO - root -   save model
10/08/2021 23:56:54 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:56:54 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:57:05 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:57:05 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 23:57:05 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/08/2021 23:57:05 - INFO - root -   Trying to decide if add adapter
10/08/2021 23:57:05 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 23:57:05 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/08/2021 23:57:05 - INFO - __main__ -   Language = en
10/08/2021 23:57:05 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 23:57:06 - INFO - __main__ -   Language = hi
10/08/2021 23:57:06 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/08/2021 23:57:07 - INFO - __main__ -   Language = ar
10/08/2021 23:57:07 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/08/2021 23:57:12 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 23:57:13 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 23:57:13 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:57:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  6.10it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.13it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.14it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.14it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.14it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.14it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.14it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.13it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.14it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.04it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  5.78it/s]Evaluating:  38%|███▊      | 12/32 [00:02<00:03,  5.10it/s]Evaluating:  41%|████      | 13/32 [00:02<00:04,  4.36it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:04,  3.70it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:04,  3.61it/s]Evaluating:  50%|█████     | 16/32 [00:03<00:04,  3.99it/s]Evaluating:  53%|█████▎    | 17/32 [00:03<00:03,  4.29it/s]Evaluating:  56%|█████▋    | 18/32 [00:03<00:03,  4.52it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  4.69it/s]Evaluating:  62%|██████▎   | 20/32 [00:04<00:02,  4.61it/s]Evaluating:  66%|██████▌   | 21/32 [00:04<00:02,  4.13it/s]Evaluating:  69%|██████▉   | 22/32 [00:04<00:02,  3.77it/s]Evaluating:  72%|███████▏  | 23/32 [00:05<00:02,  3.32it/s]Evaluating:  75%|███████▌  | 24/32 [00:05<00:02,  3.06it/s]Evaluating:  78%|███████▊  | 25/32 [00:05<00:02,  3.13it/s]Evaluating:  81%|████████▏ | 26/32 [00:05<00:01,  3.53it/s]Evaluating:  84%|████████▍ | 27/32 [00:06<00:01,  3.87it/s]Evaluating:  88%|████████▊ | 28/32 [00:06<00:00,  4.15it/s]Evaluating:  91%|█████████ | 29/32 [00:06<00:00,  4.37it/s]Evaluating:  94%|█████████▍| 30/32 [00:06<00:00,  4.17it/s]Evaluating:  97%|█████████▋| 31/32 [00:07<00:00,  3.80it/s]Evaluating: 100%|██████████| 32/32 [00:07<00:00,  3.78it/s]Evaluating: 100%|██████████| 32/32 [00:07<00:00,  4.34it/s]
10/08/2021 23:57:20 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 23:57:20 - INFO - __main__ -     f1 = 0.4784422809457581
10/08/2021 23:57:20 - INFO - __main__ -     loss = 1.152155227959156
10/08/2021 23:57:20 - INFO - __main__ -     precision = 0.48314606741573035
10/08/2021 23:57:20 - INFO - __main__ -     recall = 0.4738292011019284
10/08/2021 23:57:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 23:57:20 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 23:57:20 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:57:20 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:08,  3.82it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:06,  4.45it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:06,  4.71it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:05,  4.86it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:05,  4.93it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:05,  4.55it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:06,  3.97it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:06,  3.49it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:07,  3.11it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:07,  2.92it/s]Evaluating:  34%|███▍      | 11/32 [00:02<00:06,  3.35it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.72it/s]Evaluating:  41%|████      | 13/32 [00:03<00:04,  4.04it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:04,  4.29it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:03,  4.37it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:03,  4.05it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:04,  3.71it/s]Evaluating:  56%|█████▋    | 18/32 [00:04<00:04,  3.24it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:04,  3.00it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.41it/s]Evaluating:  66%|██████▌   | 21/32 [00:05<00:02,  3.77it/s]Evaluating:  69%|██████▉   | 22/32 [00:05<00:02,  4.07it/s]Evaluating:  72%|███████▏  | 23/32 [00:05<00:02,  4.31it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:01,  4.43it/s]Evaluating:  78%|███████▊  | 25/32 [00:06<00:01,  4.14it/s]Evaluating:  81%|████████▏ | 26/32 [00:06<00:01,  3.84it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.32it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:01,  3.03it/s]Evaluating:  91%|█████████ | 29/32 [00:07<00:00,  3.24it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.62it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  3.95it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.86it/s]
10/08/2021 23:57:29 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 23:57:29 - INFO - __main__ -     f1 = 0.5677276091783864
10/08/2021 23:57:29 - INFO - __main__ -     loss = 0.5726263104006648
10/08/2021 23:57:29 - INFO - __main__ -     precision = 0.5333796940194715
10/08/2021 23:57:29 - INFO - __main__ -     recall = 0.6068037974683544
10/08/2021 23:57:29 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 23:57:29 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 23:57:29 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:57:29 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:11,  2.62it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:09,  3.21it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:07,  3.84it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:06,  4.24it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:06,  4.49it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:05,  4.63it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.42it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:06,  3.95it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:06,  3.42it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:07,  3.08it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.20it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.60it/s]Evaluating:  41%|████      | 13/32 [00:03<00:04,  3.93it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:04,  4.21it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:03,  4.42it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:03,  4.30it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:03,  3.86it/s]Evaluating:  56%|█████▋    | 18/32 [00:04<00:04,  3.36it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:04,  3.05it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.25it/s]Evaluating:  66%|██████▌   | 21/32 [00:05<00:03,  3.63it/s]Evaluating:  69%|██████▉   | 22/32 [00:05<00:02,  3.95it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  4.22it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:01,  4.36it/s]Evaluating:  78%|███████▊  | 25/32 [00:06<00:01,  4.06it/s]Evaluating:  81%|████████▏ | 26/32 [00:06<00:01,  3.75it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.32it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:01,  3.02it/s]Evaluating:  91%|█████████ | 29/32 [00:08<00:01,  2.87it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.29it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  3.67it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.75it/s]
10/08/2021 23:57:37 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 23:57:37 - INFO - __main__ -     f1 = 0.23917322834645666
10/08/2021 23:57:37 - INFO - __main__ -     loss = 1.099341606721282
10/08/2021 23:57:37 - INFO - __main__ -     precision = 0.30720606826801516
10/08/2021 23:57:37 - INFO - __main__ -     recall = 0.19580983078162773
10/08/2021 23:57:37 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:57:49 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:57:49 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:58:05 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 23:58:07 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:58:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 23:58:07 - INFO - __main__ -   Seed = 22
10/08/2021 23:58:07 - INFO - root -   save model
10/08/2021 23:58:07 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:58:07 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:58:22 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:58:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 23:58:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/08/2021 23:58:22 - INFO - root -   Trying to decide if add adapter
10/08/2021 23:58:22 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 23:58:22 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/08/2021 23:58:22 - INFO - __main__ -   Language = en
10/08/2021 23:58:22 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 23:58:23 - INFO - __main__ -   Language = hi
10/08/2021 23:58:23 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/08/2021 23:58:25 - INFO - __main__ -   Language = ar
10/08/2021 23:58:25 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/08/2021 23:58:30 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 23:58:31 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 23:58:31 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:58:31 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:10,  3.08it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:07,  4.18it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:06,  4.70it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:05,  4.97it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:05,  5.17it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:05,  5.11it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.63it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:05,  4.04it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:06,  3.50it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.18it/s]Evaluating:  34%|███▍      | 11/32 [00:02<00:05,  3.63it/s]Evaluating:  38%|███▊      | 12/32 [00:02<00:04,  4.00it/s]Evaluating:  41%|████      | 13/32 [00:03<00:04,  4.30it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:03,  4.53it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:03,  4.46it/s]Evaluating:  50%|█████     | 16/32 [00:03<00:04,  3.97it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:04,  3.42it/s]Evaluating:  56%|█████▋    | 18/32 [00:04<00:04,  3.09it/s]Evaluating:  59%|█████▉    | 19/32 [00:04<00:03,  3.25it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.62it/s]Evaluating:  66%|██████▌   | 21/32 [00:05<00:02,  3.94it/s]Evaluating:  69%|██████▉   | 22/32 [00:05<00:02,  4.20it/s]Evaluating:  72%|███████▏  | 23/32 [00:05<00:02,  4.34it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:01,  4.15it/s]Evaluating:  78%|███████▊  | 25/32 [00:06<00:01,  3.73it/s]Evaluating:  81%|████████▏ | 26/32 [00:06<00:01,  3.27it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.28it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:01,  3.66it/s]Evaluating:  91%|█████████ | 29/32 [00:07<00:00,  3.90it/s]Evaluating:  94%|█████████▍| 30/32 [00:07<00:00,  4.19it/s]Evaluating:  97%|█████████▋| 31/32 [00:07<00:00,  4.34it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  4.64it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.97it/s]
10/08/2021 23:58:39 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/08/2021 23:58:39 - INFO - __main__ -     f1 = 0.4542020774315392
10/08/2021 23:58:39 - INFO - __main__ -     loss = 4.28633176535368
10/08/2021 23:58:39 - INFO - __main__ -     precision = 0.467444120505345
10/08/2021 23:58:39 - INFO - __main__ -     recall = 0.44168962350780533
10/08/2021 23:58:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/08/2021 23:58:39 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/08/2021 23:58:39 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:58:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:12,  2.51it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:08,  3.43it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:07,  4.00it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:06,  4.33it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:05,  4.55it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:05,  4.62it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.36it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:06,  3.88it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:06,  3.32it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.17it/s]Evaluating:  34%|███▍      | 11/32 [00:02<00:05,  3.51it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.85it/s]Evaluating:  41%|████      | 13/32 [00:03<00:04,  4.13it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:04,  4.35it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:03,  4.43it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:03,  4.08it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:04,  3.28it/s]Evaluating:  56%|█████▋    | 18/32 [00:04<00:04,  3.01it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:04,  3.04it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.44it/s]Evaluating:  66%|██████▌   | 21/32 [00:05<00:02,  3.79it/s]Evaluating:  69%|██████▉   | 22/32 [00:05<00:02,  4.08it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  4.29it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:01,  4.24it/s]Evaluating:  78%|███████▊  | 25/32 [00:06<00:01,  3.85it/s]Evaluating:  81%|████████▏ | 26/32 [00:06<00:01,  3.43it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.09it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:01,  3.00it/s]Evaluating:  91%|█████████ | 29/32 [00:07<00:00,  3.39it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.74it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  4.04it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.81it/s]
10/08/2021 23:58:47 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/08/2021 23:58:47 - INFO - __main__ -     f1 = 0.5384041759880686
10/08/2021 23:58:47 - INFO - __main__ -     loss = 2.095146529376507
10/08/2021 23:58:47 - INFO - __main__ -     precision = 0.5091678420310296
10/08/2021 23:58:47 - INFO - __main__ -     recall = 0.5712025316455697
10/08/2021 23:58:47 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/08/2021 23:58:48 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/08/2021 23:58:48 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:58:48 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:11,  2.62it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:11,  2.59it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:09,  3.15it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:07,  3.69it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:06,  4.07it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:06,  4.33it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.49it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:05,  4.29it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:05,  3.88it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.32it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.12it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.40it/s]Evaluating:  41%|████      | 13/32 [00:03<00:05,  3.76it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:04,  4.05it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:03,  4.29it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:03,  4.48it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:03,  4.25it/s]Evaluating:  56%|█████▋    | 18/32 [00:04<00:03,  3.84it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  3.45it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.09it/s]Evaluating:  66%|██████▌   | 21/32 [00:05<00:03,  2.89it/s]Evaluating:  69%|██████▉   | 22/32 [00:06<00:03,  3.19it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  3.57it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:02,  3.90it/s]Evaluating:  78%|███████▊  | 25/32 [00:06<00:01,  4.17it/s]Evaluating:  81%|████████▏ | 26/32 [00:06<00:01,  4.21it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.89it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:01,  3.53it/s]Evaluating:  91%|█████████ | 29/32 [00:08<00:00,  3.17it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  2.93it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  2.79it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.35it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.56it/s]
10/08/2021 23:58:57 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/08/2021 23:58:57 - INFO - __main__ -     f1 = 0.25820568927789933
10/08/2021 23:58:57 - INFO - __main__ -     loss = 3.8106623589992523
10/08/2021 23:58:57 - INFO - __main__ -     precision = 0.2825670498084291
10/08/2021 23:58:57 - INFO - __main__ -     recall = 0.23771152296535053
10/08/2021 23:58:57 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:59:09 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:59:09 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:59:27 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/08/2021 23:59:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:59:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/08/2021 23:59:29 - INFO - __main__ -   Seed = 32
10/08/2021 23:59:29 - INFO - root -   save model
10/08/2021 23:59:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/08/2021 23:59:29 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/08/2021 23:59:44 - INFO - __main__ -   Using lang2id = None
10/08/2021 23:59:44 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/08/2021 23:59:44 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/08/2021 23:59:44 - INFO - root -   Trying to decide if add adapter
10/08/2021 23:59:44 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/08/2021 23:59:44 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/08/2021 23:59:44 - INFO - __main__ -   Language = en
10/08/2021 23:59:44 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/08/2021 23:59:45 - INFO - __main__ -   Language = hi
10/08/2021 23:59:45 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/08/2021 23:59:46 - INFO - __main__ -   Language = ar
10/08/2021 23:59:46 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/08/2021 23:59:53 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/08/2021 23:59:53 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/08/2021 23:59:53 - INFO - __main__ -     Num examples = 1000
10/08/2021 23:59:53 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:09,  3.25it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:10,  2.86it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:10,  2.69it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:09,  3.10it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:07,  3.59it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:06,  3.97it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.26it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:05,  4.37it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:05,  4.04it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.64it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.28it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:06,  3.00it/s]Evaluating:  41%|████      | 13/32 [00:03<00:06,  2.85it/s]Evaluating:  44%|████▍     | 14/32 [00:04<00:06,  2.89it/s]Evaluating:  47%|████▋     | 15/32 [00:04<00:05,  3.31it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:04,  3.69it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:03,  4.01it/s]Evaluating:  56%|█████▋    | 18/32 [00:05<00:03,  4.24it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  4.11it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.77it/s]Evaluating:  66%|██████▌   | 21/32 [00:06<00:03,  3.42it/s]Evaluating:  69%|██████▉   | 22/32 [00:06<00:03,  3.08it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:03,  2.88it/s]Evaluating:  75%|███████▌  | 24/32 [00:07<00:02,  3.18it/s]Evaluating:  78%|███████▊  | 25/32 [00:07<00:01,  3.55it/s]Evaluating:  81%|████████▏ | 26/32 [00:07<00:01,  3.88it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  4.15it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:00,  4.15it/s]Evaluating:  91%|█████████ | 29/32 [00:08<00:00,  3.86it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.38it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  3.06it/s]Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.53it/s]
10/09/2021 00:00:03 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/09/2021 00:00:03 - INFO - __main__ -     f1 = 0.5255540479421077
10/09/2021 00:00:03 - INFO - __main__ -     loss = 3.8330188393592834
10/09/2021 00:00:03 - INFO - __main__ -     precision = 0.517825311942959
10/09/2021 00:00:03 - INFO - __main__ -     recall = 0.5335169880624426
10/09/2021 00:00:03 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/09/2021 00:00:03 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/09/2021 00:00:03 - INFO - __main__ -     Num examples = 1000
10/09/2021 00:00:03 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:11,  2.71it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:11,  2.62it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:10,  2.87it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:08,  3.45it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:06,  3.89it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:06,  4.21it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.44it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:05,  4.50it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:05,  4.37it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:05,  3.90it/s]Evaluating:  34%|███▍      | 11/32 [00:02<00:05,  3.87it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:04,  4.16it/s]Evaluating:  41%|████      | 13/32 [00:03<00:04,  4.38it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:03,  4.55it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:03,  4.76it/s]Evaluating:  50%|█████     | 16/32 [00:03<00:03,  4.99it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:02,  5.16it/s]Evaluating:  56%|█████▋    | 18/32 [00:04<00:02,  5.28it/s]Evaluating:  59%|█████▉    | 19/32 [00:04<00:02,  5.37it/s]Evaluating:  62%|██████▎   | 20/32 [00:04<00:02,  5.43it/s]Evaluating:  66%|██████▌   | 21/32 [00:04<00:02,  5.48it/s]Evaluating:  69%|██████▉   | 22/32 [00:04<00:01,  5.51it/s]Evaluating:  72%|███████▏  | 23/32 [00:05<00:01,  5.54it/s]Evaluating:  75%|███████▌  | 24/32 [00:05<00:01,  5.56it/s]Evaluating:  78%|███████▊  | 25/32 [00:05<00:01,  5.57it/s]Evaluating:  81%|████████▏ | 26/32 [00:05<00:01,  5.58it/s]Evaluating:  84%|████████▍ | 27/32 [00:05<00:00,  5.58it/s]Evaluating:  88%|████████▊ | 28/32 [00:06<00:00,  5.60it/s]Evaluating:  91%|█████████ | 29/32 [00:06<00:00,  5.60it/s]Evaluating:  94%|█████████▍| 30/32 [00:06<00:00,  5.59it/s]Evaluating:  97%|█████████▋| 31/32 [00:06<00:00,  5.59it/s]Evaluating: 100%|██████████| 32/32 [00:06<00:00,  4.84it/s]
10/09/2021 00:00:09 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/09/2021 00:00:09 - INFO - __main__ -     f1 = 0.5286404416839199
10/09/2021 00:00:09 - INFO - __main__ -     loss = 2.3239511474967003
10/09/2021 00:00:09 - INFO - __main__ -     precision = 0.46878824969400246
10/09/2021 00:00:09 - INFO - __main__ -     recall = 0.6060126582278481
10/09/2021 00:00:09 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/09/2021 00:00:10 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/09/2021 00:00:10 - INFO - __main__ -     Num examples = 1000
10/09/2021 00:00:10 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.62it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.61it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:05,  5.62it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  5.62it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  5.62it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:04,  5.61it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  5.62it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:04,  5.61it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:04,  5.62it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  5.61it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  5.60it/s]Evaluating:  38%|███▊      | 12/32 [00:02<00:03,  5.60it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  5.26it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:03,  4.60it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:03,  4.41it/s]Evaluating:  50%|█████     | 16/32 [00:03<00:03,  4.27it/s]Evaluating:  53%|█████▎    | 17/32 [00:03<00:03,  4.00it/s]Evaluating:  56%|█████▋    | 18/32 [00:03<00:03,  3.99it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:03,  3.70it/s]Evaluating:  62%|██████▎   | 20/32 [00:04<00:03,  3.99it/s]Evaluating:  66%|██████▌   | 21/32 [00:04<00:02,  4.30it/s]Evaluating:  69%|██████▉   | 22/32 [00:04<00:02,  4.62it/s]Evaluating:  72%|███████▏  | 23/32 [00:04<00:01,  4.90it/s]Evaluating:  75%|███████▌  | 24/32 [00:04<00:01,  5.12it/s]Evaluating:  78%|███████▊  | 25/32 [00:05<00:01,  5.30it/s]Evaluating:  81%|████████▏ | 26/32 [00:05<00:01,  5.42it/s]Evaluating:  84%|████████▍ | 27/32 [00:05<00:00,  5.48it/s]Evaluating:  88%|████████▊ | 28/32 [00:05<00:00,  5.53it/s]Evaluating:  91%|█████████ | 29/32 [00:05<00:00,  5.57it/s]Evaluating:  94%|█████████▍| 30/32 [00:05<00:00,  5.56it/s]Evaluating:  97%|█████████▋| 31/32 [00:06<00:00,  5.57it/s]Evaluating: 100%|██████████| 32/32 [00:06<00:00,  5.16it/s]
10/09/2021 00:00:16 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/09/2021 00:00:16 - INFO - __main__ -     f1 = 0.31500620090946674
10/09/2021 00:00:16 - INFO - __main__ -     loss = 3.3482093065977097
10/09/2021 00:00:16 - INFO - __main__ -     precision = 0.3234295415959253
10/09/2021 00:00:16 - INFO - __main__ -     recall = 0.30701047542304594
10/09/2021 00:00:16 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:00:29 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:00:29 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:00:47 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:00:49 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:00:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:00:49 - INFO - __main__ -   Seed = 42
10/09/2021 00:00:49 - INFO - root -   save model
10/09/2021 00:00:49 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:00:49 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:01:03 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:01:04 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:01:04 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/09/2021 00:01:04 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:01:04 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:01:04 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/09/2021 00:01:04 - INFO - __main__ -   Language = en
10/09/2021 00:01:04 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:01:05 - INFO - __main__ -   Language = hi
10/09/2021 00:01:05 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/09/2021 00:01:06 - INFO - __main__ -   Language = ar
10/09/2021 00:01:06 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/09/2021 00:01:12 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/09/2021 00:01:12 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/09/2021 00:01:12 - INFO - __main__ -     Num examples = 1000
10/09/2021 00:01:12 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:13,  2.33it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:12,  2.44it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:10,  2.79it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:08,  3.39it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:07,  3.84it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:06,  4.17it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.39it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:05,  4.25it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:06,  3.78it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.42it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.09it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:06,  2.89it/s]Evaluating:  41%|████      | 13/32 [00:04<00:06,  2.77it/s]Evaluating:  44%|████▍     | 14/32 [00:04<00:05,  3.12it/s]Evaluating:  47%|████▋     | 15/32 [00:04<00:04,  3.51it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:04,  3.87it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:03,  4.17it/s]Evaluating:  56%|█████▋    | 18/32 [00:05<00:03,  4.36it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  4.13it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.84it/s]Evaluating:  66%|██████▌   | 21/32 [00:06<00:03,  3.33it/s]Evaluating:  69%|██████▉   | 22/32 [00:06<00:03,  3.03it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  3.21it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:02,  3.59it/s]Evaluating:  78%|███████▊  | 25/32 [00:07<00:01,  3.92it/s]Evaluating:  81%|████████▏ | 26/32 [00:07<00:01,  4.19it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  4.34it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:00,  4.19it/s]Evaluating:  91%|█████████ | 29/32 [00:08<00:00,  3.84it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.34it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  3.20it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.60it/s]
10/09/2021 00:01:21 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/09/2021 00:01:21 - INFO - __main__ -     f1 = 0.5513413506012952
10/09/2021 00:01:21 - INFO - __main__ -     loss = 3.7530488297343254
10/09/2021 00:01:21 - INFO - __main__ -     precision = 0.5554520037278659
10/09/2021 00:01:21 - INFO - __main__ -     recall = 0.5472910927456383
10/09/2021 00:01:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/09/2021 00:01:21 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/09/2021 00:01:21 - INFO - __main__ -     Num examples = 1000
10/09/2021 00:01:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:11,  2.78it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:11,  2.66it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:09,  3.10it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:07,  3.67it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:06,  4.10it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:05,  4.41it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.60it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:05,  4.47it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:05,  4.06it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.47it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.12it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.35it/s]Evaluating:  41%|████      | 13/32 [00:03<00:05,  3.72it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:04,  4.04it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:03,  4.29it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:03,  4.48it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:03,  4.35it/s]Evaluating:  56%|█████▋    | 18/32 [00:04<00:03,  3.93it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  3.46it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.11it/s]Evaluating:  66%|██████▌   | 21/32 [00:05<00:03,  3.08it/s]Evaluating:  69%|██████▉   | 22/32 [00:05<00:02,  3.48it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  3.83it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:01,  4.12it/s]Evaluating:  78%|███████▊  | 25/32 [00:06<00:01,  4.33it/s]Evaluating:  81%|████████▏ | 26/32 [00:06<00:01,  4.21it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.84it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:01,  3.37it/s]Evaluating:  91%|█████████ | 29/32 [00:07<00:00,  3.06it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.22it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  3.61it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s]
10/09/2021 00:01:29 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/09/2021 00:01:29 - INFO - __main__ -     f1 = 0.48248749106504646
10/09/2021 00:01:29 - INFO - __main__ -     loss = 2.5883963108062744
10/09/2021 00:01:29 - INFO - __main__ -     precision = 0.44002607561929596
10/09/2021 00:01:29 - INFO - __main__ -     recall = 0.5340189873417721
10/09/2021 00:01:29 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/09/2021 00:01:30 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/09/2021 00:01:30 - INFO - __main__ -     Num examples = 1000
10/09/2021 00:01:30 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:12,  2.53it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:11,  2.52it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:09,  3.09it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:07,  3.64it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:06,  4.04it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:06,  4.33it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:05,  4.53it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:05,  4.42it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:05,  4.11it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.47it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.25it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.53it/s]Evaluating:  41%|████      | 13/32 [00:03<00:04,  3.87it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:04,  4.15it/s]Evaluating:  47%|████▋     | 15/32 [00:03<00:03,  4.38it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:03,  4.48it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:03,  4.15it/s]Evaluating:  56%|█████▋    | 18/32 [00:04<00:03,  3.73it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  3.35it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.05it/s]Evaluating:  66%|██████▌   | 21/32 [00:05<00:03,  2.87it/s]Evaluating:  69%|██████▉   | 22/32 [00:06<00:03,  2.86it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  3.28it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:02,  3.66it/s]Evaluating:  78%|███████▊  | 25/32 [00:06<00:01,  3.98it/s]Evaluating:  81%|████████▏ | 26/32 [00:07<00:01,  4.22it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  4.08it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:01,  3.76it/s]Evaluating:  91%|█████████ | 29/32 [00:07<00:00,  3.40it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.08it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  2.88it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.66it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.60it/s]
10/09/2021 00:01:39 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/09/2021 00:01:39 - INFO - __main__ -     f1 = 0.30158730158730157
10/09/2021 00:01:39 - INFO - __main__ -     loss = 3.7288988679647446
10/09/2021 00:01:39 - INFO - __main__ -     precision = 0.31309627059843886
10/09/2021 00:01:39 - INFO - __main__ -     recall = 0.2908944399677679
10/09/2021 00:01:39 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:01:52 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:01:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:02:07 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:02:09 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:02:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:02:09 - INFO - __main__ -   Seed = 52
10/09/2021 00:02:09 - INFO - root -   save model
10/09/2021 00:02:09 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:02:09 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:02:24 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:02:24 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:02:24 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/09/2021 00:02:24 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:02:24 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:02:24 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/09/2021 00:02:24 - INFO - __main__ -   Language = en
10/09/2021 00:02:24 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:02:25 - INFO - __main__ -   Language = hi
10/09/2021 00:02:25 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/09/2021 00:02:26 - INFO - __main__ -   Language = ar
10/09/2021 00:02:26 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/09/2021 00:02:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/09/2021 00:02:32 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/09/2021 00:02:32 - INFO - __main__ -     Num examples = 1000
10/09/2021 00:02:32 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.52it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.62it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:05,  5.71it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  5.75it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  5.77it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:04,  5.77it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  5.75it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:04,  5.72it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:04,  5.70it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  5.69it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  5.67it/s]Evaluating:  38%|███▊      | 12/32 [00:02<00:03,  5.64it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  5.63it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:03,  5.61it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:03,  5.60it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  5.60it/s]Evaluating:  53%|█████▎    | 17/32 [00:03<00:02,  5.60it/s]Evaluating:  56%|█████▋    | 18/32 [00:03<00:02,  5.60it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  5.60it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:02,  5.59it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  5.59it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  5.58it/s]Evaluating:  72%|███████▏  | 23/32 [00:04<00:01,  5.60it/s]Evaluating:  75%|███████▌  | 24/32 [00:04<00:01,  5.58it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  5.53it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:01,  5.05it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:01,  4.39it/s]Evaluating:  88%|████████▊ | 28/32 [00:05<00:00,  4.46it/s]Evaluating:  91%|█████████ | 29/32 [00:05<00:00,  4.15it/s]Evaluating:  94%|█████████▍| 30/32 [00:05<00:00,  4.12it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  4.01it/s]Evaluating: 100%|██████████| 32/32 [00:06<00:00,  4.54it/s]Evaluating: 100%|██████████| 32/32 [00:06<00:00,  5.21it/s]
10/09/2021 00:02:38 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/09/2021 00:02:38 - INFO - __main__ -     f1 = 0.5050412465627865
10/09/2021 00:02:38 - INFO - __main__ -     loss = 3.701935041695833
10/09/2021 00:02:38 - INFO - __main__ -     precision = 0.5041171088746569
10/09/2021 00:02:38 - INFO - __main__ -     recall = 0.5059687786960514
10/09/2021 00:02:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/09/2021 00:02:39 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/09/2021 00:02:39 - INFO - __main__ -     Num examples = 1000
10/09/2021 00:02:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.81it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.69it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:05,  5.68it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  5.64it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  5.62it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:04,  5.61it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  5.60it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:04,  5.59it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:04,  5.59it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  5.59it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  5.41it/s]Evaluating:  38%|███▊      | 12/32 [00:02<00:03,  5.55it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  5.65it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:03,  5.71it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  5.74it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  5.76it/s]Evaluating:  53%|█████▎    | 17/32 [00:03<00:02,  5.76it/s]Evaluating:  56%|█████▋    | 18/32 [00:03<00:02,  5.75it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  5.74it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:02,  5.69it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  5.66it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  5.63it/s]Evaluating:  72%|███████▏  | 23/32 [00:04<00:01,  5.62it/s]Evaluating:  75%|███████▌  | 24/32 [00:04<00:01,  5.62it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  5.61it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:01,  5.60it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  5.59it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  5.59it/s]Evaluating:  91%|█████████ | 29/32 [00:05<00:00,  5.59it/s]Evaluating:  94%|█████████▍| 30/32 [00:05<00:00,  5.56it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  5.36it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  5.78it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  5.64it/s]
10/09/2021 00:02:44 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/09/2021 00:02:44 - INFO - __main__ -     f1 = 0.5080964375674702
10/09/2021 00:02:44 - INFO - __main__ -     loss = 2.0826499983668327
10/09/2021 00:02:44 - INFO - __main__ -     precision = 0.466006600660066
10/09/2021 00:02:44 - INFO - __main__ -     recall = 0.5585443037974683
10/09/2021 00:02:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/09/2021 00:02:44 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/09/2021 00:02:44 - INFO - __main__ -     Num examples = 1000
10/09/2021 00:02:44 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:12,  2.52it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:08,  3.52it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:07,  4.09it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:08,  3.15it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:08,  3.08it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:09,  2.88it/s]Evaluating:  22%|██▏       | 7/32 [00:02<00:09,  2.75it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:08,  2.76it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:07,  3.21it/s]Evaluating:  31%|███▏      | 10/32 [00:03<00:06,  3.61it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:05,  3.96it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:04,  4.24it/s]Evaluating:  41%|████      | 13/32 [00:03<00:04,  4.39it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:04,  4.14it/s]Evaluating:  47%|████▋     | 15/32 [00:04<00:04,  3.75it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:04,  3.27it/s]Evaluating:  53%|█████▎    | 17/32 [00:05<00:04,  3.01it/s]Evaluating:  56%|█████▋    | 18/32 [00:05<00:04,  3.42it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  3.78it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:02,  4.09it/s]Evaluating:  66%|██████▌   | 21/32 [00:05<00:02,  4.33it/s]Evaluating:  69%|██████▉   | 22/32 [00:06<00:02,  4.31it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  3.94it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:02,  3.54it/s]Evaluating:  78%|███████▊  | 25/32 [00:07<00:02,  3.16it/s]Evaluating:  81%|████████▏ | 26/32 [00:07<00:01,  3.09it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.49it/s]Evaluating:  88%|████████▊ | 28/32 [00:07<00:01,  3.85it/s]Evaluating:  91%|█████████ | 29/32 [00:08<00:00,  4.14it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  4.35it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  4.24it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  4.49it/s]Evaluating: 100%|██████████| 32/32 [00:08<00:00,  3.65it/s]
10/09/2021 00:02:53 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/09/2021 00:02:53 - INFO - __main__ -     f1 = 0.23432494279176203
10/09/2021 00:02:53 - INFO - __main__ -     loss = 3.734093278646469
10/09/2021 00:02:53 - INFO - __main__ -     precision = 0.2711864406779661
10/09/2021 00:02:53 - INFO - __main__ -     recall = 0.2062852538275584
10/09/2021 00:02:53 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:03:05 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:03:05 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:03:21 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:06:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:06:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:06:41 - INFO - __main__ -   Seed = 12
10/09/2021 00:06:41 - INFO - root -   save model
10/09/2021 00:06:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:06:41 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 829, in main
    cache_dir=args.cache_dir,
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_auto.py", line 333, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_utils.py", line 389, in get_config_dict
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 955, in cached_path
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connection.py", line 421, in connect
    tls_in_tls=tls_in_tls,
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/util/ssl_.py", line 450, in ssl_wrap_socket
    sock, context, tls_in_tls, server_hostname=server_hostname
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/usr/lib/python3.7/ssl.py", line 423, in wrap_socket
    session=session
  File "/usr/lib/python3.7/ssl.py", line 870, in _create
    self.do_handshake()
  File "/usr/lib/python3.7/ssl.py", line 1139, in do_handshake
    self._sslobj.do_handshake()
KeyboardInterrupt
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:06:43 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:06:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:06:43 - INFO - __main__ -   Seed = 22
10/09/2021 00:06:43 - INFO - root -   save model
10/09/2021 00:06:43 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:06:43 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 829, in main
    cache_dir=args.cache_dir,
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_auto.py", line 333, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_utils.py", line 389, in get_config_dict
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 955, in cached_path
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 706, in urlopen
    chunked=chunked,
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 445, in _make_request
    six.raise_from(e, None)
  File "<string>", line 3, in raise_from
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 440, in _make_request
    httplib_response = conn.getresponse()
  File "/usr/lib/python3.7/http/client.py", line 1369, in getresponse
    response.begin()
  File "/usr/lib/python3.7/http/client.py", line 310, in begin
    version, status, reason = self._read_status()
  File "/usr/lib/python3.7/http/client.py", line 271, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
  File "/usr/lib/python3.7/ssl.py", line 1071, in recv_into
    return self.read(nbytes, buffer)
  File "/usr/lib/python3.7/ssl.py", line 929, in read
    return self._sslobj.read(len, buffer)
KeyboardInterrupt
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 33, in <module>
    from seqeval.metrics import precision_score, recall_score, f1_score
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/__init__.py", line 1, in <module>
    from seqeval.metrics.sequence_labeling import (accuracy_score,
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py", line 14, in <module>
    from seqeval.metrics.v1 import SCORES, _precision_recall_fscore_support
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/v1.py", line 5, in <module>
    from sklearn.exceptions import UndefinedMetricWarning
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/__init__.py", line 82, in <module>
    from .base import clone
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/base.py", line 17, in <module>
    from .utils import _IS_32BIT
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/utils/__init__.py", line 23, in <module>
    from .class_weight import compute_class_weight, compute_sample_weight
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/utils/class_weight.py", line 7, in <module>
    from .validation import _deprecate_positional_args
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/utils/validation.py", line 22, in <module>
    import joblib
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/joblib/__init__.py", line 120, in <module>
    from .parallel import Parallel
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/joblib/parallel.py", line 26, in <module>
    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/joblib/_parallel_backends.py", line 17, in <module>
    from .pool import MemmappingPool
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/joblib/pool.py", line 31, in <module>
    from ._memmapping_reducer import get_memmapping_reducers
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/joblib/_memmapping_reducer.py", line 37, in <module>
    from .externals.loky.backend import resource_tracker
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/joblib/externals/loky/__init__.py", line 13, in <module>
    from .reusable_executor import get_reusable_executor
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/joblib/externals/loky/reusable_executor.py", line 11, in <module>
    from .process_executor import ProcessPoolExecutor, EXTRA_QUEUED_CALLS
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py", line 111, in <module>
    from psutil import Process
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 963, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 906, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1280, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1252, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1391, in find_spec
  File "<frozen importlib._bootstrap_external>", line 59, in _path_join
  File "<frozen importlib._bootstrap_external>", line 59, in <listcomp>
KeyboardInterrupt
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/__init__.py", line 197, in <module>
    from torch._C import *  # noqa: F403
RuntimeError: KeyboardInterrupt: 
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/__init__.py", line 150, in <module>
    from . import core
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 99, in <module>
    from . import _add_newdocs_scalars
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/_add_newdocs_scalars.py", line 125, in <module>
    """)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/_add_newdocs_scalars.py", line 63, in add_newdoc_for_scalar_type
    for (alias_type, alias, doc) in possible_aliases if alias_type is o)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/_add_newdocs_scalars.py", line 63, in <genexpr>
    for (alias_type, alias, doc) in possible_aliases if alias_type is o)
  File "/usr/lib/python3.7/platform.py", line 1091, in system
    return uname().system
KeyboardInterrupt
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:12:24 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:12:24 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:12:24 - INFO - __main__ -   Seed = 12
10/09/2021 00:12:24 - INFO - root -   save model
10/09/2021 00:12:24 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:12:24 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:12:37 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:12:37 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:12:37 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/09/2021 00:12:37 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:12:37 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:12:37 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/09/2021 00:12:37 - INFO - __main__ -   Language = en
10/09/2021 00:12:37 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:12:39 - INFO - __main__ -   Language = is
10/09/2021 00:12:39 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/09/2021 00:12:41 - INFO - __main__ -   Language = de
10/09/2021 00:12:41 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
10/09/2021 00:12:42 - INFO - filelock -   Lock 139777489094864 acquired on /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137.323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907.lock
https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip not found in cache or force_download set to True, downloading to /home/abhijeet/.cache/torch/adapters/tmpsz16_gr2
Downloading:   0%|          | 0.00/27.5M [00:00<?, ?B/s]Downloading:   0%|          | 32.8k/27.5M [00:00<01:43, 265kB/s]Downloading:   0%|          | 59.4k/27.5M [00:00<01:58, 232kB/s]Downloading:   0%|          | 82.9k/27.5M [00:00<02:09, 212kB/s]Downloading:   0%|          | 131k/27.5M [00:00<01:38, 278kB/s] Downloading:   1%|          | 180k/27.5M [00:00<01:25, 318kB/s]Downloading:   1%|          | 246k/27.5M [00:00<01:10, 387kB/s]Downloading:   1%|          | 328k/27.5M [00:00<00:57, 472kB/s]Downloading:   1%|▏         | 393k/27.5M [00:01<00:55, 487kB/s]Downloading:   2%|▏         | 508k/27.5M [00:01<00:43, 618kB/s]Downloading:   2%|▏         | 623k/27.5M [00:01<00:37, 709kB/s]Downloading:   3%|▎         | 786k/27.5M [00:01<00:30, 887kB/s]Downloading:   3%|▎         | 950k/27.5M [00:01<00:26, 1.01MB/s]Downloading:   4%|▍         | 1.16M/27.5M [00:01<00:21, 1.22MB/s]Downloading:   5%|▌         | 1.41M/27.5M [00:01<00:18, 1.44MB/s]Downloading:   6%|▌         | 1.69M/27.5M [00:01<00:15, 1.67MB/s]Downloading:   7%|▋         | 2.02M/27.5M [00:02<00:13, 1.95MB/s]Downloading:   9%|▊         | 2.38M/27.5M [00:02<00:11, 2.22MB/s]Downloading:  10%|█         | 2.79M/27.5M [00:02<00:09, 2.53MB/s]Downloading:  12%|█▏        | 3.24M/27.5M [00:02<00:08, 2.86MB/s]Downloading:  14%|█▎        | 3.75M/27.5M [00:02<00:07, 3.21MB/s]Downloading:  16%|█▌        | 4.34M/27.5M [00:02<00:06, 3.65MB/s]Downloading:  18%|█▊        | 5.01M/27.5M [00:02<00:05, 4.15MB/s]Downloading:  21%|██        | 5.77M/27.5M [00:02<00:04, 4.69MB/s]Downloading:  24%|██▍       | 6.60M/27.5M [00:03<00:03, 5.27MB/s]Downloading:  28%|██▊       | 7.57M/27.5M [00:03<00:03, 5.99MB/s]Downloading:  31%|███▏      | 8.62M/27.5M [00:03<00:02, 6.68MB/s]Downloading:  36%|███▌      | 9.78M/27.5M [00:03<00:02, 7.44MB/s]Downloading:  40%|████      | 11.1M/27.5M [00:03<00:01, 8.29MB/s]Downloading:  46%|████▌     | 12.5M/27.5M [00:03<00:01, 9.23MB/s]Downloading:  51%|█████▏    | 14.1M/27.5M [00:03<00:01, 10.3MB/s]Downloading:  58%|█████▊    | 15.9M/27.5M [00:03<00:01, 11.4MB/s]Downloading:  65%|██████▍   | 17.8M/27.5M [00:04<00:00, 12.4MB/s]Downloading:  72%|███████▏  | 19.8M/27.5M [00:04<00:00, 13.6MB/s]Downloading:  80%|████████  | 22.1M/27.5M [00:04<00:00, 14.8MB/s]Downloading:  87%|████████▋ | 23.8M/27.5M [00:04<00:00, 14.4MB/s]Downloading:  94%|█████████▍| 25.9M/27.5M [00:04<00:00, 15.1MB/s]Downloading: 100%|██████████| 27.5M/27.5M [00:04<00:00, 5.96MB/s]
storing https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip in cache at /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137.323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907
creating metadata file for /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137.323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907
10/09/2021 00:12:47 - INFO - filelock -   Lock 139777489094864 released on /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137.323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907.lock
10/09/2021 00:12:47 - INFO - filelock -   Lock 139769726916816 acquired on /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137.323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907.lock
10/09/2021 00:12:47 - INFO - filelock -   Lock 139769726916816 released on /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137.323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907.lock
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 862, in main
    model.to(args.device)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 852, in to
    return self._apply(convert)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 530, in _apply
    module._apply(fn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 530, in _apply
    module._apply(fn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 530, in _apply
    module._apply(fn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 552, in _apply
    param_applied = fn(param)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/modules/module.py", line 850, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
KeyboardInterrupt
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:12:54 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:12:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:12:54 - INFO - __main__ -   Seed = 22
10/09/2021 00:12:54 - INFO - root -   save model
10/09/2021 00:12:54 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:12:54 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 842, in main
    use_fast=False,
  File "/home/abhijeet/rohan/emea/src/transformers/tokenization_auto.py", line 306, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_auto.py", line 333, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_utils.py", line 389, in get_config_dict
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 955, in cached_path
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connection.py", line 421, in connect
    tls_in_tls=tls_in_tls,
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/util/ssl_.py", line 450, in ssl_wrap_socket
    sock, context, tls_in_tls, server_hostname=server_hostname
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/usr/lib/python3.7/ssl.py", line 423, in wrap_socket
    session=session
  File "/usr/lib/python3.7/ssl.py", line 870, in _create
    self.do_handshake()
  File "/usr/lib/python3.7/ssl.py", line 1139, in do_handshake
    self._sslobj.do_handshake()
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/usr/lib/python3.7/logging/__init__.py", line 2036, in shutdown
    h.flush()
  File "/usr/lib/python3.7/logging/__init__.py", line 1009, in flush
    self.stream.flush()
KeyboardInterrupt
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:13:39 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:13:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:13:39 - INFO - __main__ -   Seed = 12
10/09/2021 00:13:39 - INFO - root -   save model
10/09/2021 00:13:39 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:13:39 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:13:52 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:13:52 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:13:52 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/09/2021 00:13:52 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:13:52 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:13:52 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/09/2021 00:13:52 - INFO - __main__ -   Language = en
10/09/2021 00:13:52 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:13:53 - INFO - __main__ -   Language = is
10/09/2021 00:13:53 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/09/2021 00:13:54 - INFO - __main__ -   Language = de
10/09/2021 00:13:54 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/09/2021 00:14:00 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/09/2021 00:14:00 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/09/2021 00:14:00 - INFO - __main__ -     Num examples = 100
10/09/2021 00:14:00 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  4.07it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  4.75it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  4.98it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  6.09it/s]
10/09/2021 00:14:01 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/09/2021 00:14:01 - INFO - __main__ -     f1 = 0.6966292134831462
10/09/2021 00:14:01 - INFO - __main__ -     loss = 0.3093734085559845
10/09/2021 00:14:01 - INFO - __main__ -     precision = 0.6326530612244898
10/09/2021 00:14:01 - INFO - __main__ -     recall = 0.775
10/09/2021 00:14:01 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/09/2021 00:14:02 - INFO - __main__ -   ***** Running evaluation  in no *****
10/09/2021 00:14:02 - INFO - __main__ -     Num examples = 10000
10/09/2021 00:14:02 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:24,  3.70it/s]Evaluating:   1%|          | 2/313 [00:00<01:08,  4.55it/s]Evaluating:   1%|          | 3/313 [00:00<01:03,  4.89it/s]Evaluating:   1%|▏         | 4/313 [00:00<01:01,  5.06it/s]Evaluating:   2%|▏         | 5/313 [00:01<00:59,  5.15it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:57,  5.30it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:56,  5.46it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:54,  5.56it/s]Evaluating:   3%|▎         | 9/313 [00:01<01:04,  4.71it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:13,  4.12it/s]Evaluating:   4%|▎         | 11/313 [00:02<01:27,  3.45it/s]Evaluating:   4%|▍         | 12/313 [00:02<01:35,  3.16it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:24,  3.54it/s]Evaluating:   4%|▍         | 14/313 [00:03<01:17,  3.85it/s]Evaluating:   5%|▍         | 15/313 [00:03<01:12,  4.11it/s]Evaluating:   5%|▌         | 16/313 [00:03<01:09,  4.29it/s]Evaluating:   5%|▌         | 17/313 [00:03<01:10,  4.20it/s]Evaluating:   6%|▌         | 18/313 [00:04<01:15,  3.90it/s]Evaluating:   6%|▌         | 19/313 [00:04<01:28,  3.32it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:34,  3.11it/s]Evaluating:   7%|▋         | 21/313 [00:05<01:26,  3.38it/s]Evaluating:   7%|▋         | 22/313 [00:05<01:18,  3.72it/s]Evaluating:   7%|▋         | 23/313 [00:05<01:12,  4.01it/s]Evaluating:   8%|▊         | 24/313 [00:05<01:08,  4.24it/s]Evaluating:   8%|▊         | 25/313 [00:06<01:06,  4.32it/s]Evaluating:   8%|▊         | 26/313 [00:06<01:10,  4.05it/s]Evaluating:   9%|▊         | 27/313 [00:06<01:20,  3.57it/s]Evaluating:   9%|▉         | 28/313 [00:07<01:30,  3.14it/s]Evaluating:   9%|▉         | 29/313 [00:07<01:29,  3.17it/s]Evaluating:  10%|▉         | 30/313 [00:07<01:20,  3.54it/s]Evaluating:  10%|▉         | 31/313 [00:07<01:13,  3.85it/s]Evaluating:  10%|█         | 32/313 [00:08<01:08,  4.10it/s]Evaluating:  11%|█         | 33/313 [00:08<01:05,  4.25it/s]Evaluating:  11%|█         | 34/313 [00:08<01:08,  4.10it/s]Evaluating:  11%|█         | 35/313 [00:08<01:14,  3.73it/s]Evaluating:  12%|█▏        | 36/313 [00:09<01:25,  3.22it/s]Evaluating:  12%|█▏        | 37/313 [00:09<01:29,  3.08it/s]Evaluating:  12%|█▏        | 38/313 [00:09<01:20,  3.41it/s]Evaluating:  12%|█▏        | 39/313 [00:10<01:13,  3.72it/s]Evaluating:  13%|█▎        | 40/313 [00:10<01:08,  4.00it/s]Evaluating:  13%|█▎        | 41/313 [00:10<01:04,  4.22it/s]Evaluating:  13%|█▎        | 42/313 [00:10<01:05,  4.11it/s]Evaluating:  14%|█▎        | 43/313 [00:11<01:11,  3.77it/s]Evaluating:  14%|█▍        | 44/313 [00:11<01:23,  3.24it/s]Evaluating:  14%|█▍        | 45/313 [00:11<01:30,  2.95it/s]Evaluating:  15%|█▍        | 46/313 [00:12<01:20,  3.31it/s]Evaluating:  15%|█▌        | 47/313 [00:12<01:12,  3.67it/s]Evaluating:  15%|█▌        | 48/313 [00:12<01:06,  3.96it/s]Evaluating:  16%|█▌        | 49/313 [00:12<01:03,  4.19it/s]Evaluating:  16%|█▌        | 50/313 [00:12<01:02,  4.24it/s]Evaluating:  16%|█▋        | 51/313 [00:13<01:06,  3.92it/s]Evaluating:  17%|█▋        | 52/313 [00:13<01:15,  3.47it/s]Evaluating:  17%|█▋        | 53/313 [00:14<01:24,  3.08it/s]Evaluating:  17%|█▋        | 54/313 [00:14<01:22,  3.14it/s]Evaluating:  18%|█▊        | 55/313 [00:14<01:13,  3.51it/s]Evaluating:  18%|█▊        | 56/313 [00:14<01:06,  3.84it/s]Evaluating:  18%|█▊        | 57/313 [00:14<01:02,  4.11it/s]Evaluating:  19%|█▊        | 58/313 [00:15<01:00,  4.19it/s]Evaluating:  19%|█▉        | 59/313 [00:15<01:04,  3.91it/s]Evaluating:  19%|█▉        | 60/313 [00:15<01:13,  3.45it/s]Evaluating:  19%|█▉        | 61/313 [00:16<01:22,  3.07it/s]Evaluating:  20%|█▉        | 62/313 [00:16<01:18,  3.22it/s]Evaluating:  20%|██        | 63/313 [00:16<01:09,  3.58it/s]Evaluating:  20%|██        | 64/313 [00:17<01:17,  3.22it/s]Evaluating:  21%|██        | 65/313 [00:17<01:20,  3.07it/s]Evaluating:  21%|██        | 66/313 [00:17<01:23,  2.95it/s]Evaluating:  21%|██▏       | 67/313 [00:18<01:16,  3.20it/s]Evaluating:  22%|██▏       | 68/313 [00:18<01:08,  3.57it/s]Evaluating:  22%|██▏       | 69/313 [00:18<01:03,  3.87it/s]Evaluating:  22%|██▏       | 70/313 [00:18<00:59,  4.11it/s]Evaluating:  23%|██▎       | 71/313 [00:18<00:57,  4.22it/s]Evaluating:  23%|██▎       | 72/313 [00:19<01:01,  3.95it/s]Evaluating:  23%|██▎       | 73/313 [00:19<01:09,  3.47it/s]Evaluating:  24%|██▎       | 74/313 [00:20<01:17,  3.07it/s]Evaluating:  24%|██▍       | 75/313 [00:20<01:13,  3.23it/s]Evaluating:  24%|██▍       | 76/313 [00:20<01:06,  3.58it/s]Evaluating:  25%|██▍       | 77/313 [00:20<01:00,  3.87it/s]Evaluating:  25%|██▍       | 78/313 [00:20<00:57,  4.09it/s]Evaluating:  25%|██▌       | 79/313 [00:21<00:57,  4.06it/s]Evaluating:  26%|██▌       | 80/313 [00:21<01:01,  3.77it/s]Evaluating:  26%|██▌       | 81/313 [00:21<01:11,  3.24it/s]Evaluating:  26%|██▌       | 82/313 [00:22<01:18,  2.94it/s]Evaluating:  27%|██▋       | 83/313 [00:22<01:09,  3.33it/s]Evaluating:  27%|██▋       | 84/313 [00:22<01:02,  3.67it/s]Evaluating:  27%|██▋       | 85/313 [00:22<00:57,  3.95it/s]Evaluating:  27%|██▋       | 86/313 [00:23<00:55,  4.10it/s]Evaluating:  28%|██▊       | 87/313 [00:23<00:58,  3.89it/s]Evaluating:  28%|██▊       | 88/313 [00:23<01:04,  3.48it/s]Evaluating:  28%|██▊       | 89/313 [00:24<01:12,  3.09it/s]Evaluating:  29%|██▉       | 90/313 [00:24<01:10,  3.16it/s]Evaluating:  29%|██▉       | 91/313 [00:24<01:03,  3.51it/s]Evaluating:  29%|██▉       | 92/313 [00:24<00:57,  3.82it/s]Evaluating:  30%|██▉       | 93/313 [00:25<00:53,  4.09it/s]Evaluating:  30%|███       | 94/313 [00:25<00:53,  4.09it/s]Evaluating:  30%|███       | 95/313 [00:25<00:56,  3.85it/s]Evaluating:  31%|███       | 96/313 [00:26<01:06,  3.28it/s]Evaluating:  31%|███       | 97/313 [00:26<01:07,  3.18it/s]Evaluating:  31%|███▏      | 98/313 [00:26<01:00,  3.54it/s]Evaluating:  32%|███▏      | 99/313 [00:26<00:55,  3.84it/s]Evaluating:  32%|███▏      | 100/313 [00:27<00:52,  4.09it/s]Evaluating:  32%|███▏      | 101/313 [00:27<00:49,  4.27it/s]Evaluating:  33%|███▎      | 102/313 [00:27<00:50,  4.16it/s]Evaluating:  33%|███▎      | 103/313 [00:27<00:55,  3.81it/s]Evaluating:  33%|███▎      | 104/313 [00:28<01:04,  3.25it/s]Evaluating:  34%|███▎      | 105/313 [00:28<01:07,  3.06it/s]Evaluating:  34%|███▍      | 106/313 [00:28<01:01,  3.34it/s]Evaluating:  34%|███▍      | 107/313 [00:29<00:56,  3.67it/s]Evaluating:  35%|███▍      | 108/313 [00:29<00:51,  3.95it/s]Evaluating:  35%|███▍      | 109/313 [00:29<00:48,  4.16it/s]Evaluating:  35%|███▌      | 110/313 [00:29<00:50,  4.06it/s]Evaluating:  35%|███▌      | 111/313 [00:30<00:54,  3.72it/s]Evaluating:  36%|███▌      | 112/313 [00:30<01:02,  3.20it/s]Evaluating:  36%|███▌      | 113/313 [00:30<01:08,  2.93it/s]Evaluating:  36%|███▋      | 114/313 [00:31<01:00,  3.31it/s]Evaluating:  37%|███▋      | 115/313 [00:31<00:54,  3.65it/s]Evaluating:  37%|███▋      | 116/313 [00:31<00:50,  3.93it/s]Evaluating:  37%|███▋      | 117/313 [00:31<00:47,  4.10it/s]Evaluating:  38%|███▊      | 118/313 [00:32<00:49,  3.92it/s]Evaluating:  38%|███▊      | 119/313 [00:32<00:55,  3.49it/s]Evaluating:  38%|███▊      | 120/313 [00:32<01:02,  3.09it/s]Evaluating:  39%|███▊      | 121/313 [00:33<01:00,  3.16it/s]Evaluating:  39%|███▉      | 122/313 [00:33<00:54,  3.52it/s]Evaluating:  39%|███▉      | 123/313 [00:33<00:49,  3.81it/s]Evaluating:  40%|███▉      | 124/313 [00:33<00:46,  4.05it/s]Evaluating:  40%|███▉      | 125/313 [00:33<00:47,  3.96it/s]Evaluating:  40%|████      | 126/313 [00:34<00:51,  3.64it/s]Evaluating:  41%|████      | 127/313 [00:34<00:58,  3.17it/s]Evaluating:  41%|████      | 128/313 [00:35<01:02,  2.94it/s]Evaluating:  41%|████      | 129/313 [00:35<00:55,  3.33it/s]Evaluating:  42%|████▏     | 130/313 [00:35<00:49,  3.67it/s]Evaluating:  42%|████▏     | 131/313 [00:35<00:46,  3.94it/s]Evaluating:  42%|████▏     | 132/313 [00:35<00:44,  4.06it/s]Evaluating:  42%|████▏     | 133/313 [00:36<00:47,  3.83it/s]Evaluating:  43%|████▎     | 134/313 [00:36<01:00,  2.97it/s]Evaluating:  43%|████▎     | 135/313 [00:37<01:01,  2.92it/s]Evaluating:  43%|████▎     | 136/313 [00:37<00:53,  3.30it/s]Evaluating:  44%|████▍     | 137/313 [00:37<00:48,  3.63it/s]Evaluating:  44%|████▍     | 138/313 [00:37<00:44,  3.92it/s]Evaluating:  44%|████▍     | 139/313 [00:37<00:42,  4.09it/s]Evaluating:  45%|████▍     | 140/313 [00:38<00:44,  3.92it/s]Evaluating:  45%|████▌     | 141/313 [00:38<00:49,  3.51it/s]Evaluating:  45%|████▌     | 142/313 [00:39<00:55,  3.10it/s]Evaluating:  46%|████▌     | 143/313 [00:39<00:53,  3.17it/s]Evaluating:  46%|████▌     | 144/313 [00:39<00:47,  3.52it/s]Evaluating:  46%|████▋     | 145/313 [00:39<00:43,  3.83it/s]Evaluating:  47%|████▋     | 146/313 [00:39<00:40,  4.08it/s]Evaluating:  47%|████▋     | 147/313 [00:40<00:39,  4.18it/s]Evaluating:  47%|████▋     | 148/313 [00:40<00:42,  3.90it/s]Evaluating:  48%|████▊     | 149/313 [00:40<00:47,  3.43it/s]Evaluating:  48%|████▊     | 150/313 [00:41<00:53,  3.05it/s]Evaluating:  48%|████▊     | 151/313 [00:41<00:50,  3.23it/s]Evaluating:  49%|████▊     | 152/313 [00:41<00:45,  3.56it/s]Evaluating:  49%|████▉     | 153/313 [00:41<00:41,  3.86it/s]Evaluating:  49%|████▉     | 154/313 [00:42<00:39,  4.01it/s]Evaluating:  50%|████▉     | 155/313 [00:42<00:41,  3.82it/s]Evaluating:  50%|████▉     | 156/313 [00:42<00:46,  3.38it/s]Evaluating:  50%|█████     | 157/313 [00:43<00:50,  3.07it/s]Evaluating:  50%|█████     | 158/313 [00:43<00:47,  3.28it/s]Evaluating:  51%|█████     | 159/313 [00:43<00:42,  3.62it/s]Evaluating:  51%|█████     | 160/313 [00:43<00:39,  3.90it/s]Evaluating:  51%|█████▏    | 161/313 [00:44<00:36,  4.12it/s]Evaluating:  52%|█████▏    | 162/313 [00:44<00:35,  4.26it/s]Evaluating:  52%|█████▏    | 163/313 [00:44<00:35,  4.19it/s]Evaluating:  52%|█████▏    | 164/313 [00:44<00:34,  4.35it/s]Evaluating:  53%|█████▎    | 165/313 [00:45<00:33,  4.47it/s]Evaluating:  53%|█████▎    | 166/313 [00:45<00:32,  4.57it/s]Evaluating:  53%|█████▎    | 167/313 [00:45<00:30,  4.77it/s]Evaluating:  54%|█████▎    | 168/313 [00:45<00:29,  4.92it/s]Evaluating:  54%|█████▍    | 169/313 [00:45<00:28,  5.04it/s]Evaluating:  54%|█████▍    | 170/313 [00:45<00:28,  5.11it/s]Evaluating:  55%|█████▍    | 171/313 [00:46<00:27,  5.17it/s]Evaluating:  55%|█████▍    | 172/313 [00:46<00:27,  5.21it/s]Evaluating:  55%|█████▌    | 173/313 [00:46<00:26,  5.23it/s]Evaluating:  56%|█████▌    | 174/313 [00:46<00:26,  5.27it/s]Evaluating:  56%|█████▌    | 175/313 [00:46<00:26,  5.29it/s]Evaluating:  56%|█████▌    | 176/313 [00:47<00:25,  5.29it/s]Evaluating:  57%|█████▋    | 177/313 [00:47<00:25,  5.32it/s]Evaluating:  57%|█████▋    | 178/313 [00:47<00:25,  5.36it/s]Evaluating:  57%|█████▋    | 179/313 [00:47<00:24,  5.39it/s]Evaluating:  58%|█████▊    | 180/313 [00:47<00:24,  5.39it/s]Evaluating:  58%|█████▊    | 181/313 [00:48<00:24,  5.41it/s]Evaluating:  58%|█████▊    | 182/313 [00:48<00:24,  5.40it/s]Evaluating:  58%|█████▊    | 183/313 [00:48<00:24,  5.37it/s]Evaluating:  59%|█████▉    | 184/313 [00:48<00:24,  5.35it/s]Evaluating:  59%|█████▉    | 185/313 [00:48<00:23,  5.34it/s]Evaluating:  59%|█████▉    | 186/313 [00:48<00:23,  5.33it/s]Evaluating:  60%|█████▉    | 187/313 [00:49<00:23,  5.34it/s]Evaluating:  60%|██████    | 188/313 [00:49<00:23,  5.38it/s]Evaluating:  60%|██████    | 189/313 [00:49<00:22,  5.43it/s]Evaluating:  61%|██████    | 190/313 [00:49<00:22,  5.43it/s]Evaluating:  61%|██████    | 191/313 [00:49<00:22,  5.43it/s]Evaluating:  61%|██████▏   | 192/313 [00:50<00:22,  5.43it/s]Evaluating:  62%|██████▏   | 193/313 [00:50<00:22,  5.39it/s]Evaluating:  62%|██████▏   | 194/313 [00:50<00:22,  5.36it/s]Evaluating:  62%|██████▏   | 195/313 [00:50<00:22,  5.34it/s]Evaluating:  63%|██████▎   | 196/313 [00:50<00:22,  5.14it/s]Evaluating:  63%|██████▎   | 197/313 [00:51<00:25,  4.51it/s]Evaluating:  63%|██████▎   | 198/313 [00:51<00:27,  4.25it/s]Evaluating:  64%|██████▎   | 199/313 [00:51<00:27,  4.08it/s]Evaluating:  64%|██████▍   | 200/313 [00:51<00:29,  3.88it/s]Evaluating:  64%|██████▍   | 201/313 [00:52<00:29,  3.81it/s]Evaluating:  65%|██████▍   | 202/313 [00:52<00:30,  3.62it/s]Evaluating:  65%|██████▍   | 203/313 [00:52<00:28,  3.83it/s]Evaluating:  65%|██████▌   | 204/313 [00:52<00:26,  4.17it/s]Evaluating:  65%|██████▌   | 205/313 [00:53<00:24,  4.45it/s]Evaluating:  66%|██████▌   | 206/313 [00:53<00:22,  4.66it/s]Evaluating:  66%|██████▌   | 207/313 [00:53<00:21,  4.83it/s]Evaluating:  66%|██████▋   | 208/313 [00:53<00:21,  4.95it/s]Evaluating:  67%|██████▋   | 209/313 [00:53<00:20,  5.05it/s]Evaluating:  67%|██████▋   | 210/313 [00:54<00:20,  5.13it/s]Evaluating:  67%|██████▋   | 211/313 [00:54<00:19,  5.20it/s]Evaluating:  68%|██████▊   | 212/313 [00:54<00:19,  5.27it/s]Evaluating:  68%|██████▊   | 213/313 [00:54<00:18,  5.35it/s]Evaluating:  68%|██████▊   | 214/313 [00:54<00:18,  5.39it/s]Evaluating:  69%|██████▊   | 215/313 [00:54<00:18,  5.38it/s]Evaluating:  69%|██████▉   | 216/313 [00:55<00:18,  5.15it/s]Evaluating:  69%|██████▉   | 217/313 [00:55<00:18,  5.05it/s]Evaluating:  70%|██████▉   | 218/313 [00:55<00:19,  4.98it/s]Evaluating:  70%|██████▉   | 219/313 [00:55<00:18,  5.00it/s]Evaluating:  70%|███████   | 220/313 [00:56<00:18,  5.04it/s]Evaluating:  71%|███████   | 221/313 [00:56<00:18,  5.07it/s]Evaluating:  71%|███████   | 222/313 [00:56<00:17,  5.13it/s]Evaluating:  71%|███████   | 223/313 [00:56<00:17,  5.16it/s]Evaluating:  72%|███████▏  | 224/313 [00:56<00:17,  5.18it/s]Evaluating:  72%|███████▏  | 225/313 [00:56<00:16,  5.21it/s]Evaluating:  72%|███████▏  | 226/313 [00:57<00:16,  5.25it/s]Evaluating:  73%|███████▎  | 227/313 [00:57<00:16,  5.29it/s]Evaluating:  73%|███████▎  | 228/313 [00:57<00:15,  5.35it/s]Evaluating:  73%|███████▎  | 229/313 [00:57<00:15,  5.41it/s]Evaluating:  73%|███████▎  | 230/313 [00:57<00:15,  5.40it/s]Evaluating:  74%|███████▍  | 231/313 [00:58<00:15,  5.37it/s]Evaluating:  74%|███████▍  | 232/313 [00:58<00:15,  5.22it/s]Evaluating:  74%|███████▍  | 233/313 [00:58<00:17,  4.65it/s]Evaluating:  75%|███████▍  | 234/313 [00:58<00:19,  3.97it/s]Evaluating:  75%|███████▌  | 235/313 [00:59<00:23,  3.33it/s]Evaluating:  75%|███████▌  | 236/313 [00:59<00:24,  3.09it/s]Evaluating:  76%|███████▌  | 237/313 [00:59<00:22,  3.45it/s]Evaluating:  76%|███████▌  | 238/313 [01:00<00:19,  3.75it/s]Evaluating:  76%|███████▋  | 239/313 [01:00<00:18,  3.98it/s]Evaluating:  77%|███████▋  | 240/313 [01:00<00:18,  3.89it/s]Evaluating:  77%|███████▋  | 241/313 [01:00<00:20,  3.53it/s]Evaluating:  77%|███████▋  | 242/313 [01:01<00:22,  3.11it/s]Evaluating:  78%|███████▊  | 243/313 [01:01<00:22,  3.12it/s]Evaluating:  78%|███████▊  | 244/313 [01:01<00:19,  3.48it/s]Evaluating:  78%|███████▊  | 245/313 [01:02<00:18,  3.77it/s]Evaluating:  79%|███████▊  | 246/313 [01:02<00:16,  4.01it/s]Evaluating:  79%|███████▉  | 247/313 [01:02<00:16,  3.95it/s]Evaluating:  79%|███████▉  | 248/313 [01:02<00:17,  3.62it/s]Evaluating:  80%|███████▉  | 249/313 [01:03<00:20,  3.15it/s]Evaluating:  80%|███████▉  | 250/313 [01:03<00:20,  3.09it/s]Evaluating:  80%|████████  | 251/313 [01:03<00:18,  3.44it/s]Evaluating:  81%|████████  | 252/313 [01:04<00:16,  3.77it/s]Evaluating:  81%|████████  | 253/313 [01:04<00:14,  4.07it/s]Evaluating:  81%|████████  | 254/313 [01:04<00:14,  4.07it/s]Evaluating:  81%|████████▏ | 255/313 [01:04<00:15,  3.79it/s]Evaluating:  82%|████████▏ | 256/313 [01:05<00:17,  3.27it/s]Evaluating:  82%|████████▏ | 257/313 [01:05<00:17,  3.12it/s]Evaluating:  82%|████████▏ | 258/313 [01:05<00:15,  3.47it/s]Evaluating:  83%|████████▎ | 259/313 [01:06<00:14,  3.77it/s]Evaluating:  83%|████████▎ | 260/313 [01:06<00:13,  4.00it/s]Evaluating:  83%|████████▎ | 261/313 [01:06<00:13,  3.93it/s]Evaluating:  84%|████████▎ | 262/313 [01:06<00:14,  3.61it/s]Evaluating:  84%|████████▍ | 263/313 [01:07<00:16,  3.10it/s]Evaluating:  84%|████████▍ | 264/313 [01:07<00:16,  3.02it/s]Evaluating:  85%|████████▍ | 265/313 [01:07<00:14,  3.38it/s]Evaluating:  85%|████████▍ | 266/313 [01:08<00:12,  3.70it/s]Evaluating:  85%|████████▌ | 267/313 [01:08<00:11,  3.91it/s]Evaluating:  86%|████████▌ | 268/313 [01:08<00:11,  3.77it/s]Evaluating:  86%|████████▌ | 269/313 [01:08<00:13,  3.38it/s]Evaluating:  86%|████████▋ | 270/313 [01:09<00:14,  3.02it/s]Evaluating:  87%|████████▋ | 271/313 [01:09<00:13,  3.17it/s]Evaluating:  87%|████████▋ | 272/313 [01:09<00:11,  3.52it/s]Evaluating:  87%|████████▋ | 273/313 [01:10<00:10,  3.82it/s]Evaluating:  88%|████████▊ | 274/313 [01:10<00:09,  3.99it/s]Evaluating:  88%|████████▊ | 275/313 [01:10<00:10,  3.74it/s]Evaluating:  88%|████████▊ | 276/313 [01:10<00:11,  3.31it/s]Evaluating:  88%|████████▊ | 277/313 [01:11<00:12,  2.98it/s]Evaluating:  89%|████████▉ | 278/313 [01:11<00:11,  3.17it/s]Evaluating:  89%|████████▉ | 279/313 [01:11<00:09,  3.51it/s]Evaluating:  89%|████████▉ | 280/313 [01:12<00:08,  3.81it/s]Evaluating:  90%|████████▉ | 281/313 [01:12<00:08,  3.99it/s]Evaluating:  90%|█████████ | 282/313 [01:12<00:08,  3.74it/s]Evaluating:  90%|█████████ | 283/313 [01:12<00:09,  3.33it/s]Evaluating:  91%|█████████ | 284/313 [01:13<00:09,  2.98it/s]Evaluating:  91%|█████████ | 285/313 [01:13<00:08,  3.14it/s]Evaluating:  91%|█████████▏| 286/313 [01:13<00:07,  3.49it/s]Evaluating:  92%|█████████▏| 287/313 [01:14<00:06,  3.80it/s]Evaluating:  92%|█████████▏| 288/313 [01:14<00:06,  4.01it/s]Evaluating:  92%|█████████▏| 289/313 [01:14<00:06,  3.86it/s]Evaluating:  93%|█████████▎| 290/313 [01:14<00:06,  3.44it/s]Evaluating:  93%|█████████▎| 291/313 [01:15<00:07,  3.13it/s]Evaluating:  93%|█████████▎| 292/313 [01:15<00:06,  3.30it/s]Evaluating:  94%|█████████▎| 293/313 [01:15<00:05,  3.63it/s]Evaluating:  94%|█████████▍| 294/313 [01:15<00:04,  3.93it/s]Evaluating:  94%|█████████▍| 295/313 [01:16<00:04,  4.09it/s]Evaluating:  95%|█████████▍| 296/313 [01:16<00:04,  3.89it/s]Evaluating:  95%|█████████▍| 297/313 [01:16<00:04,  3.44it/s]Evaluating:  95%|█████████▌| 298/313 [01:17<00:04,  3.08it/s]Evaluating:  96%|█████████▌| 299/313 [01:17<00:04,  3.22it/s]Evaluating:  96%|█████████▌| 300/313 [01:17<00:03,  3.51it/s]Evaluating:  96%|█████████▌| 301/313 [01:17<00:03,  3.78it/s]Evaluating:  96%|█████████▋| 302/313 [01:18<00:02,  3.85it/s]Evaluating:  97%|█████████▋| 303/313 [01:18<00:02,  3.58it/s]Evaluating:  97%|█████████▋| 304/313 [01:18<00:02,  3.12it/s]Evaluating:  97%|█████████▋| 305/313 [01:19<00:02,  2.95it/s]Evaluating:  98%|█████████▊| 306/313 [01:19<00:02,  3.30it/s]Evaluating:  98%|█████████▊| 307/313 [01:19<00:01,  3.58it/s]Evaluating:  98%|█████████▊| 308/313 [01:20<00:01,  3.70it/s]Evaluating:  99%|█████████▊| 309/313 [01:20<00:01,  3.53it/s]Evaluating:  99%|█████████▉| 310/313 [01:20<00:00,  3.08it/s]Evaluating:  99%|█████████▉| 311/313 [01:21<00:00,  2.86it/s]Evaluating: 100%|█████████▉| 312/313 [01:21<00:00,  3.22it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.93it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.84it/s]
10/09/2021 00:15:25 - INFO - __main__ -   ***** Evaluation result  in no *****
10/09/2021 00:15:25 - INFO - __main__ -     f1 = 0.7808477548724396
10/09/2021 00:15:25 - INFO - __main__ -     loss = 0.2472876763810365
10/09/2021 00:15:25 - INFO - __main__ -     precision = 0.7458752133510336
10/09/2021 00:15:25 - INFO - __main__ -     recall = 0.8192612137203166
10/09/2021 00:15:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/09/2021 00:15:27 - INFO - __main__ -   ***** Running evaluation  in da *****
10/09/2021 00:15:27 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:15:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:31,  3.42it/s]Evaluating:   1%|          | 2/313 [00:00<01:14,  4.15it/s]Evaluating:   1%|          | 3/313 [00:00<01:09,  4.45it/s]Evaluating:   1%|▏         | 4/313 [00:00<01:06,  4.62it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:05,  4.71it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:06,  4.61it/s]Evaluating:   2%|▏         | 7/313 [00:01<01:13,  4.16it/s]Evaluating:   3%|▎         | 8/313 [00:01<01:25,  3.58it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:36,  3.14it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:36,  3.15it/s]Evaluating:   4%|▎         | 11/313 [00:02<01:25,  3.53it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:18,  3.85it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:13,  4.11it/s]Evaluating:   4%|▍         | 14/313 [00:03<01:09,  4.30it/s]Evaluating:   5%|▍         | 15/313 [00:03<01:09,  4.26it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:13,  4.02it/s]Evaluating:   5%|▌         | 17/313 [00:04<01:27,  3.37it/s]Evaluating:   6%|▌         | 18/313 [00:04<01:31,  3.22it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:22,  3.55it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:15,  3.86it/s]Evaluating:   7%|▋         | 21/313 [00:05<01:10,  4.11it/s]Evaluating:   7%|▋         | 22/313 [00:05<01:07,  4.30it/s]Evaluating:   7%|▋         | 23/313 [00:05<01:06,  4.37it/s]Evaluating:   8%|▊         | 24/313 [00:06<01:10,  4.09it/s]Evaluating:   8%|▊         | 25/313 [00:06<01:19,  3.61it/s]Evaluating:   8%|▊         | 26/313 [00:06<01:30,  3.17it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:30,  3.15it/s]Evaluating:   9%|▉         | 28/313 [00:07<01:20,  3.53it/s]Evaluating:   9%|▉         | 29/313 [00:07<01:13,  3.84it/s]Evaluating:  10%|▉         | 30/313 [00:07<01:09,  4.10it/s]Evaluating:  10%|▉         | 31/313 [00:08<01:05,  4.29it/s]Evaluating:  10%|█         | 32/313 [00:08<01:06,  4.23it/s]Evaluating:  11%|█         | 33/313 [00:08<01:10,  3.94it/s]Evaluating:  11%|█         | 34/313 [00:08<01:23,  3.33it/s]Evaluating:  11%|█         | 35/313 [00:09<01:28,  3.14it/s]Evaluating:  12%|█▏        | 36/313 [00:09<01:20,  3.43it/s]Evaluating:  12%|█▏        | 37/313 [00:09<01:13,  3.76it/s]Evaluating:  12%|█▏        | 38/313 [00:09<01:08,  4.03it/s]Evaluating:  12%|█▏        | 39/313 [00:10<01:04,  4.25it/s]Evaluating:  13%|█▎        | 40/313 [00:10<01:03,  4.33it/s]Evaluating:  13%|█▎        | 41/313 [00:10<01:07,  4.04it/s]Evaluating:  13%|█▎        | 42/313 [00:11<01:15,  3.59it/s]Evaluating:  14%|█▎        | 43/313 [00:11<01:26,  3.14it/s]Evaluating:  14%|█▍        | 44/313 [00:11<01:27,  3.08it/s]Evaluating:  14%|█▍        | 45/313 [00:12<01:17,  3.46it/s]Evaluating:  15%|█▍        | 46/313 [00:12<01:29,  2.99it/s]Evaluating:  15%|█▌        | 47/313 [00:12<01:32,  2.87it/s]Evaluating:  15%|█▌        | 48/313 [00:13<01:28,  3.00it/s]Evaluating:  16%|█▌        | 49/313 [00:13<01:17,  3.41it/s]Evaluating:  16%|█▌        | 50/313 [00:13<01:10,  3.76it/s]Evaluating:  16%|█▋        | 51/313 [00:13<01:04,  4.04it/s]Evaluating:  17%|█▋        | 52/313 [00:13<01:01,  4.25it/s]Evaluating:  17%|█▋        | 53/313 [00:14<01:01,  4.20it/s]Evaluating:  17%|█▋        | 54/313 [00:14<01:07,  3.86it/s]Evaluating:  18%|█▊        | 55/313 [00:14<01:17,  3.35it/s]Evaluating:  18%|█▊        | 56/313 [00:15<01:23,  3.07it/s]Evaluating:  18%|█▊        | 57/313 [00:15<01:18,  3.25it/s]Evaluating:  19%|█▊        | 58/313 [00:15<01:10,  3.60it/s]Evaluating:  19%|█▉        | 59/313 [00:15<01:05,  3.90it/s]Evaluating:  19%|█▉        | 60/313 [00:16<01:01,  4.14it/s]Evaluating:  19%|█▉        | 61/313 [00:16<00:59,  4.25it/s]Evaluating:  20%|█▉        | 62/313 [00:16<01:02,  4.04it/s]Evaluating:  20%|██        | 63/313 [00:17<01:09,  3.61it/s]Evaluating:  20%|██        | 64/313 [00:17<01:23,  2.97it/s]Evaluating:  21%|██        | 65/313 [00:17<01:16,  3.26it/s]Evaluating:  21%|██        | 66/313 [00:17<01:08,  3.61it/s]Evaluating:  21%|██▏       | 67/313 [00:18<01:02,  3.91it/s]Evaluating:  22%|██▏       | 68/313 [00:18<00:59,  4.15it/s]Evaluating:  22%|██▏       | 69/313 [00:18<00:57,  4.23it/s]Evaluating:  22%|██▏       | 70/313 [00:18<01:01,  3.95it/s]Evaluating:  23%|██▎       | 71/313 [00:19<01:09,  3.48it/s]Evaluating:  23%|██▎       | 72/313 [00:19<01:17,  3.11it/s]Evaluating:  23%|██▎       | 73/313 [00:19<01:13,  3.25it/s]Evaluating:  24%|██▎       | 74/313 [00:20<01:06,  3.60it/s]Evaluating:  24%|██▍       | 75/313 [00:20<01:01,  3.89it/s]Evaluating:  24%|██▍       | 76/313 [00:20<00:57,  4.13it/s]Evaluating:  25%|██▍       | 77/313 [00:20<00:55,  4.23it/s]Evaluating:  25%|██▍       | 78/313 [00:21<00:58,  4.01it/s]Evaluating:  25%|██▌       | 79/313 [00:21<01:06,  3.54it/s]Evaluating:  26%|██▌       | 80/313 [00:21<01:14,  3.13it/s]Evaluating:  26%|██▌       | 81/313 [00:22<01:11,  3.24it/s]Evaluating:  26%|██▌       | 82/313 [00:22<01:17,  3.00it/s]Evaluating:  27%|██▋       | 83/313 [00:22<01:19,  2.88it/s]Evaluating:  27%|██▋       | 84/313 [00:23<01:18,  2.91it/s]Evaluating:  27%|██▋       | 85/313 [00:23<01:09,  3.26it/s]Evaluating:  27%|██▋       | 86/313 [00:23<01:02,  3.65it/s]Evaluating:  28%|██▊       | 87/313 [00:23<00:57,  3.96it/s]Evaluating:  28%|██▊       | 88/313 [00:24<00:52,  4.25it/s]Evaluating:  28%|██▊       | 89/313 [00:24<00:52,  4.27it/s]Evaluating:  29%|██▉       | 90/313 [00:24<00:55,  4.04it/s]Evaluating:  29%|██▉       | 91/313 [00:24<01:04,  3.44it/s]Evaluating:  29%|██▉       | 92/313 [00:25<01:08,  3.23it/s]Evaluating:  30%|██▉       | 93/313 [00:25<01:02,  3.51it/s]Evaluating:  30%|███       | 94/313 [00:25<00:57,  3.82it/s]Evaluating:  30%|███       | 95/313 [00:25<00:53,  4.07it/s]Evaluating:  31%|███       | 96/313 [00:26<00:50,  4.26it/s]Evaluating:  31%|███       | 97/313 [00:26<00:52,  4.15it/s]Evaluating:  31%|███▏      | 98/313 [00:26<00:56,  3.81it/s]Evaluating:  32%|███▏      | 99/313 [00:27<01:05,  3.26it/s]Evaluating:  32%|███▏      | 100/313 [00:27<01:12,  2.96it/s]Evaluating:  32%|███▏      | 101/313 [00:27<01:03,  3.34it/s]Evaluating:  33%|███▎      | 102/313 [00:27<00:57,  3.68it/s]Evaluating:  33%|███▎      | 103/313 [00:28<00:53,  3.96it/s]Evaluating:  33%|███▎      | 104/313 [00:28<00:50,  4.12it/s]Evaluating:  34%|███▎      | 105/313 [00:28<00:52,  3.95it/s]Evaluating:  34%|███▍      | 106/313 [00:28<00:58,  3.55it/s]Evaluating:  34%|███▍      | 107/313 [00:29<01:05,  3.13it/s]Evaluating:  35%|███▍      | 108/313 [00:29<01:03,  3.22it/s]Evaluating:  35%|███▍      | 109/313 [00:29<00:57,  3.58it/s]Evaluating:  35%|███▌      | 110/313 [00:30<00:52,  3.88it/s]Evaluating:  35%|███▌      | 111/313 [00:30<00:49,  4.11it/s]Evaluating:  36%|███▌      | 112/313 [00:30<00:47,  4.21it/s]Evaluating:  36%|███▌      | 113/313 [00:30<00:50,  3.95it/s]Evaluating:  36%|███▋      | 114/313 [00:31<00:57,  3.49it/s]Evaluating:  37%|███▋      | 115/313 [00:31<01:04,  3.09it/s]Evaluating:  37%|███▋      | 116/313 [00:31<01:00,  3.23it/s]Evaluating:  37%|███▋      | 117/313 [00:32<00:54,  3.58it/s]Evaluating:  38%|███▊      | 118/313 [00:32<00:51,  3.80it/s]Evaluating:  38%|███▊      | 119/313 [00:32<00:48,  4.03it/s]Evaluating:  38%|███▊      | 120/313 [00:32<00:47,  4.04it/s]Evaluating:  39%|███▊      | 121/313 [00:32<00:45,  4.24it/s]Evaluating:  39%|███▉      | 122/313 [00:33<00:43,  4.42it/s]Evaluating:  39%|███▉      | 123/313 [00:33<00:41,  4.53it/s]Evaluating:  40%|███▉      | 124/313 [00:33<00:40,  4.67it/s]Evaluating:  40%|███▉      | 125/313 [00:33<00:38,  4.86it/s]Evaluating:  40%|████      | 126/313 [00:33<00:37,  4.99it/s]Evaluating:  41%|████      | 127/313 [00:34<00:36,  5.10it/s]Evaluating:  41%|████      | 128/313 [00:34<00:35,  5.18it/s]Evaluating:  41%|████      | 129/313 [00:34<00:35,  5.22it/s]Evaluating:  42%|████▏     | 130/313 [00:34<00:34,  5.25it/s]Evaluating:  42%|████▏     | 131/313 [00:34<00:34,  5.28it/s]Evaluating:  42%|████▏     | 132/313 [00:35<00:34,  5.29it/s]Evaluating:  42%|████▏     | 133/313 [00:35<00:33,  5.31it/s]Evaluating:  43%|████▎     | 134/313 [00:35<00:33,  5.31it/s]Evaluating:  43%|████▎     | 135/313 [00:35<00:33,  5.32it/s]Evaluating:  43%|████▎     | 136/313 [00:35<00:33,  5.34it/s]Evaluating:  44%|████▍     | 137/313 [00:36<00:32,  5.36it/s]Evaluating:  44%|████▍     | 138/313 [00:36<00:32,  5.38it/s]Evaluating:  44%|████▍     | 139/313 [00:36<00:32,  5.39it/s]Evaluating:  45%|████▍     | 140/313 [00:36<00:32,  5.38it/s]Evaluating:  45%|████▌     | 141/313 [00:36<00:32,  5.36it/s]Evaluating:  45%|████▌     | 142/313 [00:36<00:31,  5.35it/s]Evaluating:  46%|████▌     | 143/313 [00:37<00:31,  5.34it/s]Evaluating:  46%|████▌     | 144/313 [00:37<00:31,  5.34it/s]Evaluating:  46%|████▋     | 145/313 [00:37<00:31,  5.36it/s]Evaluating:  47%|████▋     | 146/313 [00:37<00:31,  5.39it/s]Evaluating:  47%|████▋     | 147/313 [00:37<00:30,  5.39it/s]Evaluating:  47%|████▋     | 148/313 [00:38<00:30,  5.39it/s]Evaluating:  48%|████▊     | 149/313 [00:38<00:30,  5.38it/s]Evaluating:  48%|████▊     | 150/313 [00:38<00:30,  5.35it/s]Evaluating:  48%|████▊     | 151/313 [00:38<00:30,  5.34it/s]Evaluating:  49%|████▊     | 152/313 [00:38<00:30,  5.34it/s]Evaluating:  49%|████▉     | 153/313 [00:38<00:30,  5.29it/s]Evaluating:  49%|████▉     | 154/313 [00:39<00:33,  4.76it/s]Evaluating:  50%|████▉     | 155/313 [00:39<00:37,  4.21it/s]Evaluating:  50%|████▉     | 156/313 [00:39<00:38,  4.09it/s]Evaluating:  50%|█████     | 157/313 [00:40<00:40,  3.86it/s]Evaluating:  50%|█████     | 158/313 [00:40<00:40,  3.82it/s]Evaluating:  51%|█████     | 159/313 [00:40<00:42,  3.63it/s]Evaluating:  51%|█████     | 160/313 [00:40<00:39,  3.89it/s]Evaluating:  51%|█████▏    | 161/313 [00:41<00:36,  4.14it/s]Evaluating:  52%|█████▏    | 162/313 [00:41<00:34,  4.37it/s]Evaluating:  52%|█████▏    | 163/313 [00:41<00:32,  4.60it/s]Evaluating:  52%|█████▏    | 164/313 [00:41<00:31,  4.79it/s]Evaluating:  53%|█████▎    | 165/313 [00:41<00:30,  4.93it/s]Evaluating:  53%|█████▎    | 166/313 [00:42<00:29,  5.05it/s]Evaluating:  53%|█████▎    | 167/313 [00:42<00:36,  4.05it/s]Evaluating:  54%|█████▎    | 168/313 [00:42<00:33,  4.36it/s]Evaluating:  54%|█████▍    | 169/313 [00:42<00:31,  4.62it/s]Evaluating:  54%|█████▍    | 170/313 [00:42<00:29,  4.85it/s]Evaluating:  55%|█████▍    | 171/313 [00:43<00:28,  5.02it/s]Evaluating:  55%|█████▍    | 172/313 [00:43<00:27,  5.15it/s]Evaluating:  55%|█████▌    | 173/313 [00:43<00:26,  5.24it/s]Evaluating:  56%|█████▌    | 174/313 [00:43<00:26,  5.27it/s]Evaluating:  56%|█████▌    | 175/313 [00:43<00:26,  5.30it/s]Evaluating:  56%|█████▌    | 176/313 [00:44<00:25,  5.33it/s]Evaluating:  57%|█████▋    | 177/313 [00:44<00:25,  5.34it/s]Evaluating:  57%|█████▋    | 178/313 [00:44<00:25,  5.36it/s]Evaluating:  57%|█████▋    | 179/313 [00:44<00:24,  5.37it/s]Evaluating:  58%|█████▊    | 180/313 [00:44<00:24,  5.35it/s]Evaluating:  58%|█████▊    | 181/313 [00:45<00:24,  5.41it/s]Evaluating:  58%|█████▊    | 182/313 [00:45<00:24,  5.44it/s]Evaluating:  58%|█████▊    | 183/313 [00:45<00:23,  5.43it/s]Evaluating:  59%|█████▉    | 184/313 [00:45<00:23,  5.45it/s]Evaluating:  59%|█████▉    | 185/313 [00:45<00:23,  5.47it/s]Evaluating:  59%|█████▉    | 186/313 [00:45<00:23,  5.41it/s]Evaluating:  60%|█████▉    | 187/313 [00:46<00:25,  5.03it/s]Evaluating:  60%|██████    | 188/313 [00:46<00:28,  4.43it/s]Evaluating:  60%|██████    | 189/313 [00:46<00:34,  3.61it/s]Evaluating:  61%|██████    | 190/313 [00:47<00:37,  3.28it/s]Evaluating:  61%|██████    | 191/313 [00:47<00:34,  3.53it/s]Evaluating:  61%|██████▏   | 192/313 [00:47<00:31,  3.82it/s]Evaluating:  62%|██████▏   | 193/313 [00:47<00:29,  4.07it/s]Evaluating:  62%|██████▏   | 194/313 [00:48<00:27,  4.28it/s]Evaluating:  62%|██████▏   | 195/313 [00:48<00:28,  4.08it/s]Evaluating:  63%|██████▎   | 196/313 [00:48<00:32,  3.64it/s]Evaluating:  63%|██████▎   | 197/313 [00:49<00:36,  3.16it/s]Evaluating:  63%|██████▎   | 198/313 [00:49<00:36,  3.12it/s]Evaluating:  64%|██████▎   | 199/313 [00:49<00:32,  3.49it/s]Evaluating:  64%|██████▍   | 200/313 [00:49<00:29,  3.80it/s]Evaluating:  64%|██████▍   | 201/313 [00:50<00:27,  4.05it/s]Evaluating:  65%|██████▍   | 202/313 [00:50<00:27,  4.07it/s]Evaluating:  65%|██████▍   | 203/313 [00:50<00:28,  3.84it/s]Evaluating:  65%|██████▌   | 204/313 [00:51<00:33,  3.25it/s]Evaluating:  65%|██████▌   | 205/313 [00:51<00:34,  3.10it/s]Evaluating:  66%|██████▌   | 206/313 [00:51<00:31,  3.41it/s]Evaluating:  66%|██████▌   | 207/313 [00:51<00:28,  3.73it/s]Evaluating:  66%|██████▋   | 208/313 [00:52<00:26,  4.00it/s]Evaluating:  67%|██████▋   | 209/313 [00:52<00:28,  3.67it/s]Evaluating:  67%|██████▋   | 210/313 [00:52<00:30,  3.43it/s]Evaluating:  67%|██████▋   | 211/313 [00:53<00:33,  3.07it/s]Evaluating:  68%|██████▊   | 212/313 [00:53<00:32,  3.08it/s]Evaluating:  68%|██████▊   | 213/313 [00:53<00:28,  3.45it/s]Evaluating:  68%|██████▊   | 214/313 [00:53<00:26,  3.76it/s]Evaluating:  69%|██████▊   | 215/313 [00:54<00:24,  3.99it/s]Evaluating:  69%|██████▉   | 216/313 [00:54<00:24,  3.90it/s]Evaluating:  69%|██████▉   | 217/313 [00:54<00:27,  3.53it/s]Evaluating:  70%|██████▉   | 218/313 [00:55<00:30,  3.10it/s]Evaluating:  70%|██████▉   | 219/313 [00:55<00:30,  3.10it/s]Evaluating:  70%|███████   | 220/313 [00:55<00:26,  3.46it/s]Evaluating:  71%|███████   | 221/313 [00:55<00:24,  3.76it/s]Evaluating:  71%|███████   | 222/313 [00:56<00:22,  4.01it/s]Evaluating:  71%|███████   | 223/313 [00:56<00:21,  4.14it/s]Evaluating:  72%|███████▏  | 224/313 [00:56<00:23,  3.80it/s]Evaluating:  72%|███████▏  | 225/313 [00:56<00:26,  3.34it/s]Evaluating:  72%|███████▏  | 226/313 [00:57<00:29,  3.00it/s]Evaluating:  73%|███████▎  | 227/313 [00:57<00:27,  3.17it/s]Evaluating:  73%|███████▎  | 228/313 [00:57<00:24,  3.51it/s]Evaluating:  73%|███████▎  | 229/313 [00:58<00:21,  3.82it/s]Evaluating:  73%|███████▎  | 230/313 [00:58<00:20,  4.10it/s]Evaluating:  74%|███████▍  | 231/313 [00:58<00:20,  4.01it/s]Evaluating:  74%|███████▍  | 232/313 [00:58<00:22,  3.64it/s]Evaluating:  74%|███████▍  | 233/313 [00:59<00:25,  3.16it/s]Evaluating:  75%|███████▍  | 234/313 [00:59<00:25,  3.04it/s]Evaluating:  75%|███████▌  | 235/313 [00:59<00:22,  3.40it/s]Evaluating:  75%|███████▌  | 236/313 [01:00<00:20,  3.72it/s]Evaluating:  76%|███████▌  | 237/313 [01:00<00:18,  4.01it/s]Evaluating:  76%|███████▌  | 238/313 [01:00<00:18,  4.17it/s]Evaluating:  76%|███████▋  | 239/313 [01:00<00:18,  3.92it/s]Evaluating:  77%|███████▋  | 240/313 [01:01<00:21,  3.43it/s]Evaluating:  77%|███████▋  | 241/313 [01:01<00:23,  3.05it/s]Evaluating:  77%|███████▋  | 242/313 [01:01<00:21,  3.37it/s]Evaluating:  78%|███████▊  | 243/313 [01:01<00:18,  3.69it/s]Evaluating:  78%|███████▊  | 244/313 [01:02<00:17,  3.94it/s]Evaluating:  78%|███████▊  | 245/313 [01:02<00:16,  4.09it/s]Evaluating:  79%|███████▊  | 246/313 [01:02<00:17,  3.82it/s]Evaluating:  79%|███████▉  | 247/313 [01:03<00:19,  3.41it/s]Evaluating:  79%|███████▉  | 248/313 [01:03<00:21,  3.03it/s]Evaluating:  80%|███████▉  | 249/313 [01:03<00:20,  3.08it/s]Evaluating:  80%|███████▉  | 250/313 [01:04<00:18,  3.43it/s]Evaluating:  80%|████████  | 251/313 [01:04<00:16,  3.73it/s]Evaluating:  81%|████████  | 252/313 [01:04<00:15,  3.93it/s]Evaluating:  81%|████████  | 253/313 [01:04<00:15,  3.81it/s]Evaluating:  81%|████████  | 254/313 [01:05<00:17,  3.41it/s]Evaluating:  81%|████████▏ | 255/313 [01:05<00:18,  3.10it/s]Evaluating:  82%|████████▏ | 256/313 [01:05<00:17,  3.28it/s]Evaluating:  82%|████████▏ | 257/313 [01:05<00:15,  3.64it/s]Evaluating:  82%|████████▏ | 258/313 [01:06<00:13,  3.93it/s]Evaluating:  83%|████████▎ | 259/313 [01:06<00:13,  4.14it/s]Evaluating:  83%|████████▎ | 260/313 [01:06<00:13,  4.01it/s]Evaluating:  83%|████████▎ | 261/313 [01:06<00:14,  3.59it/s]Evaluating:  84%|████████▎ | 262/313 [01:07<00:16,  3.15it/s]Evaluating:  84%|████████▍ | 263/313 [01:07<00:15,  3.25it/s]Evaluating:  84%|████████▍ | 264/313 [01:07<00:13,  3.58it/s]Evaluating:  85%|████████▍ | 265/313 [01:08<00:12,  3.85it/s]Evaluating:  85%|████████▍ | 266/313 [01:08<00:11,  4.10it/s]Evaluating:  85%|████████▌ | 267/313 [01:08<00:11,  4.00it/s]Evaluating:  86%|████████▌ | 268/313 [01:08<00:12,  3.66it/s]Evaluating:  86%|████████▌ | 269/313 [01:09<00:13,  3.17it/s]Evaluating:  86%|████████▋ | 270/313 [01:09<00:14,  3.02it/s]Evaluating:  87%|████████▋ | 271/313 [01:09<00:12,  3.36it/s]Evaluating:  87%|████████▋ | 272/313 [01:10<00:11,  3.68it/s]Evaluating:  87%|████████▋ | 273/313 [01:10<00:10,  3.96it/s]Evaluating:  88%|████████▊ | 274/313 [01:10<00:09,  4.11it/s]Evaluating:  88%|████████▊ | 275/313 [01:10<00:09,  3.89it/s]Evaluating:  88%|████████▊ | 276/313 [01:11<00:10,  3.42it/s]Evaluating:  88%|████████▊ | 277/313 [01:11<00:11,  3.07it/s]Evaluating:  89%|████████▉ | 278/313 [01:11<00:10,  3.20it/s]Evaluating:  89%|████████▉ | 279/313 [01:12<00:09,  3.53it/s]Evaluating:  89%|████████▉ | 280/313 [01:12<00:08,  3.82it/s]Evaluating:  90%|████████▉ | 281/313 [01:12<00:08,  3.98it/s]Evaluating:  90%|█████████ | 282/313 [01:12<00:08,  3.76it/s]Evaluating:  90%|█████████ | 283/313 [01:13<00:09,  3.31it/s]Evaluating:  91%|█████████ | 284/313 [01:13<00:09,  3.03it/s]Evaluating:  91%|█████████ | 285/313 [01:13<00:08,  3.22it/s]Evaluating:  91%|█████████▏| 286/313 [01:14<00:07,  3.56it/s]Evaluating:  92%|█████████▏| 287/313 [01:14<00:06,  3.83it/s]Evaluating:  92%|█████████▏| 288/313 [01:14<00:06,  3.97it/s]Evaluating:  92%|█████████▏| 289/313 [01:14<00:06,  3.72it/s]Evaluating:  93%|█████████▎| 290/313 [01:15<00:07,  3.29it/s]Evaluating:  93%|█████████▎| 291/313 [01:15<00:07,  3.00it/s]Evaluating:  93%|█████████▎| 292/313 [01:15<00:06,  3.15it/s]Evaluating:  94%|█████████▎| 293/313 [01:16<00:05,  3.49it/s]Evaluating:  94%|█████████▍| 294/313 [01:16<00:05,  3.79it/s]Evaluating:  94%|█████████▍| 295/313 [01:16<00:04,  4.04it/s]Evaluating:  95%|█████████▍| 296/313 [01:16<00:04,  3.93it/s]Evaluating:  95%|█████████▍| 297/313 [01:17<00:04,  3.58it/s]Evaluating:  95%|█████████▌| 298/313 [01:17<00:04,  3.15it/s]Evaluating:  96%|█████████▌| 299/313 [01:17<00:04,  3.08it/s]Evaluating:  96%|█████████▌| 300/313 [01:18<00:03,  3.43it/s]Evaluating:  96%|█████████▌| 301/313 [01:18<00:03,  3.67it/s]Evaluating:  96%|█████████▋| 302/313 [01:18<00:03,  3.58it/s]Evaluating:  97%|█████████▋| 303/313 [01:19<00:03,  3.23it/s]Evaluating:  97%|█████████▋| 304/313 [01:19<00:03,  2.95it/s]Evaluating:  97%|█████████▋| 305/313 [01:19<00:02,  3.11it/s]Evaluating:  98%|█████████▊| 306/313 [01:19<00:02,  3.44it/s]Evaluating:  98%|█████████▊| 307/313 [01:20<00:01,  3.73it/s]Evaluating:  98%|█████████▊| 308/313 [01:20<00:01,  3.70it/s]Evaluating:  99%|█████████▊| 309/313 [01:20<00:01,  3.38it/s]Evaluating:  99%|█████████▉| 310/313 [01:21<00:00,  3.00it/s]Evaluating:  99%|█████████▉| 311/313 [01:21<00:00,  3.00it/s]Evaluating: 100%|█████████▉| 312/313 [01:21<00:00,  3.34it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  4.03it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.82it/s]
10/09/2021 00:16:50 - INFO - __main__ -   ***** Evaluation result  in da *****
10/09/2021 00:16:50 - INFO - __main__ -     f1 = 0.8187598805550677
10/09/2021 00:16:50 - INFO - __main__ -     loss = 0.1943110661646619
10/09/2021 00:16:50 - INFO - __main__ -     precision = 0.7891243989977653
10/09/2021 00:16:50 - INFO - __main__ -     recall = 0.8507081325740984
10/09/2021 00:16:50 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:17:03 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:17:03 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:17:21 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:17:23 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:17:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:17:23 - INFO - __main__ -   Seed = 22
10/09/2021 00:17:23 - INFO - root -   save model
10/09/2021 00:17:23 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:17:23 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:17:39 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:17:39 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:17:39 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/09/2021 00:17:39 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:17:39 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:17:39 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/09/2021 00:17:39 - INFO - __main__ -   Language = en
10/09/2021 00:17:39 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:17:41 - INFO - __main__ -   Language = is
10/09/2021 00:17:41 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/09/2021 00:17:43 - INFO - __main__ -   Language = de
10/09/2021 00:17:43 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/09/2021 00:17:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/09/2021 00:17:49 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/09/2021 00:17:49 - INFO - __main__ -     Num examples = 100
10/09/2021 00:17:49 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  5.88it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  5.86it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  5.92it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  6.86it/s]
10/09/2021 00:17:49 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/09/2021 00:17:49 - INFO - __main__ -     f1 = 0.6618181818181817
10/09/2021 00:17:49 - INFO - __main__ -     loss = 1.1078749932348728
10/09/2021 00:17:49 - INFO - __main__ -     precision = 0.5870967741935483
10/09/2021 00:17:49 - INFO - __main__ -     recall = 0.7583333333333333
10/09/2021 00:17:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/09/2021 00:17:51 - INFO - __main__ -   ***** Running evaluation  in no *****
10/09/2021 00:17:51 - INFO - __main__ -     Num examples = 10000
10/09/2021 00:17:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.04it/s]Evaluating:   1%|          | 2/313 [00:00<00:53,  5.76it/s]Evaluating:   1%|          | 3/313 [00:00<00:55,  5.63it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:55,  5.58it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:55,  5.55it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:55,  5.52it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:55,  5.51it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:55,  5.50it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:55,  5.49it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:55,  5.49it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:55,  5.48it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:54,  5.48it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:54,  5.48it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:54,  5.46it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:54,  5.46it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:55,  5.38it/s]Evaluating:   5%|▌         | 17/313 [00:03<01:00,  4.93it/s]Evaluating:   6%|▌         | 18/313 [00:03<01:09,  4.27it/s]Evaluating:   6%|▌         | 19/313 [00:03<01:22,  3.56it/s]Evaluating:   6%|▋         | 20/313 [00:04<01:31,  3.19it/s]Evaluating:   7%|▋         | 21/313 [00:04<01:27,  3.34it/s]Evaluating:   7%|▋         | 22/313 [00:04<01:18,  3.69it/s]Evaluating:   7%|▋         | 23/313 [00:04<01:13,  3.97it/s]Evaluating:   8%|▊         | 24/313 [00:05<01:08,  4.20it/s]Evaluating:   8%|▊         | 25/313 [00:05<01:06,  4.34it/s]Evaluating:   8%|▊         | 26/313 [00:05<01:09,  4.15it/s]Evaluating:   9%|▊         | 27/313 [00:05<01:16,  3.74it/s]Evaluating:   9%|▉         | 28/313 [00:06<01:28,  3.23it/s]Evaluating:   9%|▉         | 29/313 [00:06<01:31,  3.11it/s]Evaluating:  10%|▉         | 30/313 [00:06<01:21,  3.48it/s]Evaluating:  10%|▉         | 31/313 [00:07<01:14,  3.81it/s]Evaluating:  10%|█         | 32/313 [00:07<01:08,  4.08it/s]Evaluating:  11%|█         | 33/313 [00:07<01:05,  4.27it/s]Evaluating:  11%|█         | 34/313 [00:07<01:06,  4.20it/s]Evaluating:  11%|█         | 35/313 [00:08<01:11,  3.91it/s]Evaluating:  12%|█▏        | 36/313 [00:08<01:23,  3.32it/s]Evaluating:  12%|█▏        | 37/313 [00:08<01:27,  3.14it/s]Evaluating:  12%|█▏        | 38/313 [00:09<01:19,  3.45it/s]Evaluating:  12%|█▏        | 39/313 [00:09<01:12,  3.77it/s]Evaluating:  13%|█▎        | 40/313 [00:09<01:07,  4.05it/s]Evaluating:  13%|█▎        | 41/313 [00:09<01:03,  4.26it/s]Evaluating:  13%|█▎        | 42/313 [00:09<01:02,  4.34it/s]Evaluating:  14%|█▎        | 43/313 [00:10<01:06,  4.09it/s]Evaluating:  14%|█▍        | 44/313 [00:10<01:13,  3.64it/s]Evaluating:  14%|█▍        | 45/313 [00:10<01:24,  3.18it/s]Evaluating:  15%|█▍        | 46/313 [00:11<01:24,  3.16it/s]Evaluating:  15%|█▌        | 47/313 [00:11<01:15,  3.52it/s]Evaluating:  15%|█▌        | 48/313 [00:11<01:09,  3.84it/s]Evaluating:  16%|█▌        | 49/313 [00:11<01:04,  4.09it/s]Evaluating:  16%|█▌        | 50/313 [00:12<01:01,  4.25it/s]Evaluating:  16%|█▋        | 51/313 [00:12<01:03,  4.11it/s]Evaluating:  17%|█▋        | 52/313 [00:12<01:10,  3.73it/s]Evaluating:  17%|█▋        | 53/313 [00:13<01:20,  3.22it/s]Evaluating:  17%|█▋        | 54/313 [00:13<01:21,  3.18it/s]Evaluating:  18%|█▊        | 55/313 [00:13<01:12,  3.55it/s]Evaluating:  18%|█▊        | 56/313 [00:14<01:37,  2.64it/s]Evaluating:  18%|█▊        | 57/313 [00:14<01:32,  2.77it/s]Evaluating:  19%|█▊        | 58/313 [00:14<01:19,  3.19it/s]Evaluating:  19%|█▉        | 59/313 [00:14<01:10,  3.62it/s]Evaluating:  19%|█▉        | 60/313 [00:15<01:03,  3.99it/s]Evaluating:  19%|█▉        | 61/313 [00:15<00:58,  4.30it/s]Evaluating:  20%|█▉        | 62/313 [00:15<00:56,  4.48it/s]Evaluating:  20%|██        | 63/313 [00:15<00:57,  4.33it/s]Evaluating:  20%|██        | 64/313 [00:16<01:03,  3.90it/s]Evaluating:  21%|██        | 65/313 [00:16<01:13,  3.36it/s]Evaluating:  21%|██        | 66/313 [00:16<01:14,  3.33it/s]Evaluating:  21%|██▏       | 67/313 [00:16<01:06,  3.69it/s]Evaluating:  22%|██▏       | 68/313 [00:17<01:01,  3.99it/s]Evaluating:  22%|██▏       | 69/313 [00:17<00:57,  4.23it/s]Evaluating:  22%|██▏       | 70/313 [00:17<00:55,  4.38it/s]Evaluating:  23%|██▎       | 71/313 [00:17<00:57,  4.22it/s]Evaluating:  23%|██▎       | 72/313 [00:18<01:02,  3.83it/s]Evaluating:  23%|██▎       | 73/313 [00:18<01:13,  3.27it/s]Evaluating:  24%|██▎       | 74/313 [00:18<01:20,  2.97it/s]Evaluating:  24%|██▍       | 75/313 [00:19<01:11,  3.34it/s]Evaluating:  24%|██▍       | 76/313 [00:19<01:04,  3.68it/s]Evaluating:  25%|██▍       | 77/313 [00:19<00:59,  3.97it/s]Evaluating:  25%|██▍       | 78/313 [00:19<00:56,  4.19it/s]Evaluating:  25%|██▌       | 79/313 [00:20<00:56,  4.14it/s]Evaluating:  26%|██▌       | 80/313 [00:20<01:00,  3.84it/s]Evaluating:  26%|██▌       | 81/313 [00:20<01:11,  3.27it/s]Evaluating:  26%|██▌       | 82/313 [00:21<01:14,  3.09it/s]Evaluating:  27%|██▋       | 83/313 [00:21<01:07,  3.40it/s]Evaluating:  27%|██▋       | 84/313 [00:21<01:01,  3.73it/s]Evaluating:  27%|██▋       | 85/313 [00:21<00:57,  3.99it/s]Evaluating:  27%|██▋       | 86/313 [00:21<00:53,  4.20it/s]Evaluating:  28%|██▊       | 87/313 [00:22<00:54,  4.11it/s]Evaluating:  28%|██▊       | 88/313 [00:22<00:59,  3.79it/s]Evaluating:  28%|██▊       | 89/313 [00:22<01:08,  3.25it/s]Evaluating:  29%|██▉       | 90/313 [00:23<01:12,  3.06it/s]Evaluating:  29%|██▉       | 91/313 [00:23<01:06,  3.32it/s]Evaluating:  29%|██▉       | 92/313 [00:23<01:00,  3.67it/s]Evaluating:  30%|██▉       | 93/313 [00:23<00:55,  3.95it/s]Evaluating:  30%|███       | 94/313 [00:24<00:52,  4.18it/s]Evaluating:  30%|███       | 95/313 [00:24<00:51,  4.23it/s]Evaluating:  31%|███       | 96/313 [00:24<00:55,  3.90it/s]Evaluating:  31%|███       | 97/313 [00:25<01:02,  3.44it/s]Evaluating:  31%|███▏      | 98/313 [00:25<01:10,  3.06it/s]Evaluating:  32%|███▏      | 99/313 [00:25<01:08,  3.14it/s]Evaluating:  32%|███▏      | 100/313 [00:26<01:00,  3.50it/s]Evaluating:  32%|███▏      | 101/313 [00:26<00:55,  3.81it/s]Evaluating:  33%|███▎      | 102/313 [00:26<00:52,  4.05it/s]Evaluating:  33%|███▎      | 103/313 [00:26<00:52,  3.99it/s]Evaluating:  33%|███▎      | 104/313 [00:27<00:56,  3.67it/s]Evaluating:  34%|███▎      | 105/313 [00:27<01:05,  3.18it/s]Evaluating:  34%|███▍      | 106/313 [00:27<01:10,  2.93it/s]Evaluating:  34%|███▍      | 107/313 [00:28<01:02,  3.32it/s]Evaluating:  35%|███▍      | 108/313 [00:28<00:55,  3.66it/s]Evaluating:  35%|███▍      | 109/313 [00:28<00:51,  3.94it/s]Evaluating:  35%|███▌      | 110/313 [00:28<00:49,  4.11it/s]Evaluating:  35%|███▌      | 111/313 [00:29<00:55,  3.62it/s]Evaluating:  36%|███▌      | 112/313 [00:29<01:02,  3.22it/s]Evaluating:  36%|███▌      | 113/313 [00:29<01:08,  2.94it/s]Evaluating:  36%|███▋      | 114/313 [00:30<01:02,  3.20it/s]Evaluating:  37%|███▋      | 115/313 [00:30<00:55,  3.55it/s]Evaluating:  37%|███▋      | 116/313 [00:30<00:51,  3.85it/s]Evaluating:  37%|███▋      | 117/313 [00:30<00:47,  4.10it/s]Evaluating:  38%|███▊      | 118/313 [00:30<00:46,  4.17it/s]Evaluating:  38%|███▊      | 119/313 [00:31<00:50,  3.83it/s]Evaluating:  38%|███▊      | 120/313 [00:31<00:57,  3.34it/s]Evaluating:  39%|███▊      | 121/313 [00:32<01:03,  3.01it/s]Evaluating:  39%|███▉      | 122/313 [00:32<00:57,  3.33it/s]Evaluating:  39%|███▉      | 123/313 [00:32<00:51,  3.67it/s]Evaluating:  40%|███▉      | 124/313 [00:32<00:47,  3.94it/s]Evaluating:  40%|███▉      | 125/313 [00:32<00:45,  4.12it/s]Evaluating:  40%|████      | 126/313 [00:33<00:46,  4.02it/s]Evaluating:  41%|████      | 127/313 [00:33<00:50,  3.68it/s]Evaluating:  41%|████      | 128/313 [00:33<00:58,  3.18it/s]Evaluating:  41%|████      | 129/313 [00:34<01:02,  2.93it/s]Evaluating:  42%|████▏     | 130/313 [00:34<00:55,  3.31it/s]Evaluating:  42%|████▏     | 131/313 [00:34<00:49,  3.65it/s]Evaluating:  42%|████▏     | 132/313 [00:34<00:46,  3.93it/s]Evaluating:  42%|████▏     | 133/313 [00:35<00:43,  4.17it/s]Evaluating:  43%|████▎     | 134/313 [00:35<00:44,  4.03it/s]Evaluating:  43%|████▎     | 135/313 [00:35<00:48,  3.65it/s]Evaluating:  43%|████▎     | 136/313 [00:36<00:55,  3.17it/s]Evaluating:  44%|████▍     | 137/313 [00:36<00:59,  2.96it/s]Evaluating:  44%|████▍     | 138/313 [00:36<00:52,  3.35it/s]Evaluating:  44%|████▍     | 139/313 [00:36<00:47,  3.68it/s]Evaluating:  45%|████▍     | 140/313 [00:37<00:43,  3.96it/s]Evaluating:  45%|████▌     | 141/313 [00:37<00:41,  4.10it/s]Evaluating:  45%|████▌     | 142/313 [00:37<00:44,  3.82it/s]Evaluating:  46%|████▌     | 143/313 [00:38<00:49,  3.41it/s]Evaluating:  46%|████▌     | 144/313 [00:38<00:55,  3.04it/s]Evaluating:  46%|████▋     | 145/313 [00:38<00:53,  3.12it/s]Evaluating:  47%|████▋     | 146/313 [00:39<00:49,  3.37it/s]Evaluating:  47%|████▋     | 147/313 [00:39<00:45,  3.62it/s]Evaluating:  47%|████▋     | 148/313 [00:39<00:45,  3.59it/s]Evaluating:  48%|████▊     | 149/313 [00:39<00:51,  3.19it/s]Evaluating:  48%|████▊     | 150/313 [00:40<00:54,  3.00it/s]Evaluating:  48%|████▊     | 151/313 [00:40<00:49,  3.26it/s]Evaluating:  49%|████▊     | 152/313 [00:40<00:44,  3.60it/s]Evaluating:  49%|████▉     | 153/313 [00:40<00:41,  3.89it/s]Evaluating:  49%|████▉     | 154/313 [00:41<00:39,  4.06it/s]Evaluating:  50%|████▉     | 155/313 [00:41<00:40,  3.94it/s]Evaluating:  50%|████▉     | 156/313 [00:41<00:43,  3.58it/s]Evaluating:  50%|█████     | 157/313 [00:42<00:49,  3.14it/s]Evaluating:  50%|█████     | 158/313 [00:42<00:51,  3.03it/s]Evaluating:  51%|█████     | 159/313 [00:42<00:45,  3.41it/s]Evaluating:  51%|█████     | 160/313 [00:42<00:41,  3.72it/s]Evaluating:  51%|█████▏    | 161/313 [00:43<00:38,  3.99it/s]Evaluating:  52%|█████▏    | 162/313 [00:43<00:36,  4.10it/s]Evaluating:  52%|█████▏    | 163/313 [00:43<00:38,  3.88it/s]Evaluating:  52%|█████▏    | 164/313 [00:44<00:43,  3.41it/s]Evaluating:  53%|█████▎    | 165/313 [00:44<00:47,  3.11it/s]Evaluating:  53%|█████▎    | 166/313 [00:44<00:44,  3.29it/s]Evaluating:  53%|█████▎    | 167/313 [00:44<00:40,  3.63it/s]Evaluating:  54%|█████▎    | 168/313 [00:45<00:36,  3.93it/s]Evaluating:  54%|█████▍    | 169/313 [00:45<00:34,  4.16it/s]Evaluating:  54%|█████▍    | 170/313 [00:45<00:33,  4.31it/s]Evaluating:  55%|█████▍    | 171/313 [00:45<00:35,  4.00it/s]Evaluating:  55%|█████▍    | 172/313 [00:46<00:40,  3.48it/s]Evaluating:  55%|█████▌    | 173/313 [00:46<00:45,  3.08it/s]Evaluating:  56%|█████▌    | 174/313 [00:46<00:42,  3.28it/s]Evaluating:  56%|█████▌    | 175/313 [00:47<00:38,  3.61it/s]Evaluating:  56%|█████▌    | 176/313 [00:47<00:35,  3.89it/s]Evaluating:  57%|█████▋    | 177/313 [00:47<00:32,  4.13it/s]Evaluating:  57%|█████▋    | 178/313 [00:47<00:32,  4.17it/s]Evaluating:  57%|█████▋    | 179/313 [00:48<00:35,  3.80it/s]Evaluating:  58%|█████▊    | 180/313 [00:48<00:40,  3.31it/s]Evaluating:  58%|█████▊    | 181/313 [00:48<00:44,  2.98it/s]Evaluating:  58%|█████▊    | 182/313 [00:49<00:40,  3.26it/s]Evaluating:  58%|█████▊    | 183/313 [00:49<00:36,  3.60it/s]Evaluating:  59%|█████▉    | 184/313 [00:49<00:33,  3.88it/s]Evaluating:  59%|█████▉    | 185/313 [00:49<00:31,  4.07it/s]Evaluating:  59%|█████▉    | 186/313 [00:50<00:32,  3.91it/s]Evaluating:  60%|█████▉    | 187/313 [00:50<00:36,  3.48it/s]Evaluating:  60%|██████    | 188/313 [00:50<00:40,  3.10it/s]Evaluating:  60%|██████    | 189/313 [00:51<00:38,  3.23it/s]Evaluating:  61%|██████    | 190/313 [00:51<00:34,  3.53it/s]Evaluating:  61%|██████    | 191/313 [00:51<00:31,  3.84it/s]Evaluating:  61%|██████▏   | 192/313 [00:51<00:29,  4.17it/s]Evaluating:  62%|██████▏   | 193/313 [00:51<00:29,  4.12it/s]Evaluating:  62%|██████▏   | 194/313 [00:52<00:31,  3.83it/s]Evaluating:  62%|██████▏   | 195/313 [00:52<00:36,  3.26it/s]Evaluating:  63%|██████▎   | 196/313 [00:53<00:38,  3.02it/s]Evaluating:  63%|██████▎   | 197/313 [00:53<00:34,  3.39it/s]Evaluating:  63%|██████▎   | 198/313 [00:53<00:30,  3.71it/s]Evaluating:  64%|██████▎   | 199/313 [00:53<00:28,  3.97it/s]Evaluating:  64%|██████▍   | 200/313 [00:53<00:28,  4.03it/s]Evaluating:  64%|██████▍   | 201/313 [00:54<00:28,  3.91it/s]Evaluating:  65%|██████▍   | 202/313 [00:54<00:31,  3.49it/s]Evaluating:  65%|██████▍   | 203/313 [00:54<00:35,  3.12it/s]Evaluating:  65%|██████▌   | 204/313 [00:55<00:33,  3.24it/s]Evaluating:  65%|██████▌   | 205/313 [00:55<00:30,  3.58it/s]Evaluating:  66%|██████▌   | 206/313 [00:55<00:27,  3.86it/s]Evaluating:  66%|██████▌   | 207/313 [00:55<00:25,  4.08it/s]Evaluating:  66%|██████▋   | 208/313 [00:56<00:24,  4.20it/s]Evaluating:  67%|██████▋   | 209/313 [00:56<00:26,  3.87it/s]Evaluating:  67%|██████▋   | 210/313 [00:56<00:30,  3.38it/s]Evaluating:  67%|██████▋   | 211/313 [00:57<00:33,  3.06it/s]Evaluating:  68%|██████▊   | 212/313 [00:57<00:31,  3.20it/s]Evaluating:  68%|██████▊   | 213/313 [00:57<00:28,  3.55it/s]Evaluating:  68%|██████▊   | 214/313 [00:57<00:25,  3.83it/s]Evaluating:  69%|██████▊   | 215/313 [00:58<00:24,  4.06it/s]Evaluating:  69%|██████▉   | 216/313 [00:58<00:24,  4.00it/s]Evaluating:  69%|██████▉   | 217/313 [00:58<00:25,  3.70it/s]Evaluating:  70%|██████▉   | 218/313 [00:59<00:30,  3.07it/s]Evaluating:  70%|██████▉   | 219/313 [00:59<00:29,  3.19it/s]Evaluating:  70%|███████   | 220/313 [00:59<00:26,  3.54it/s]Evaluating:  71%|███████   | 221/313 [00:59<00:24,  3.83it/s]Evaluating:  71%|███████   | 222/313 [01:00<00:22,  4.07it/s]Evaluating:  71%|███████   | 223/313 [01:00<00:22,  3.98it/s]Evaluating:  72%|███████▏  | 224/313 [01:00<00:24,  3.63it/s]Evaluating:  72%|███████▏  | 225/313 [01:01<00:27,  3.16it/s]Evaluating:  72%|███████▏  | 226/313 [01:01<00:28,  3.02it/s]Evaluating:  73%|███████▎  | 227/313 [01:01<00:25,  3.37it/s]Evaluating:  73%|███████▎  | 228/313 [01:01<00:23,  3.69it/s]Evaluating:  73%|███████▎  | 229/313 [01:02<00:21,  3.95it/s]Evaluating:  73%|███████▎  | 230/313 [01:02<00:20,  4.09it/s]Evaluating:  74%|███████▍  | 231/313 [01:02<00:21,  3.85it/s]Evaluating:  74%|███████▍  | 232/313 [01:02<00:23,  3.39it/s]Evaluating:  74%|███████▍  | 233/313 [01:03<00:26,  3.04it/s]Evaluating:  75%|███████▍  | 234/313 [01:03<00:24,  3.29it/s]Evaluating:  75%|███████▌  | 235/313 [01:03<00:21,  3.62it/s]Evaluating:  75%|███████▌  | 236/313 [01:04<00:19,  3.89it/s]Evaluating:  76%|███████▌  | 237/313 [01:04<00:18,  4.03it/s]Evaluating:  76%|███████▌  | 238/313 [01:04<00:19,  3.80it/s]Evaluating:  76%|███████▋  | 239/313 [01:04<00:21,  3.38it/s]Evaluating:  77%|███████▋  | 240/313 [01:05<00:24,  3.03it/s]Evaluating:  77%|███████▋  | 241/313 [01:05<00:23,  3.13it/s]Evaluating:  77%|███████▋  | 242/313 [01:05<00:20,  3.48it/s]Evaluating:  78%|███████▊  | 243/313 [01:06<00:18,  3.78it/s]Evaluating:  78%|███████▊  | 244/313 [01:06<00:17,  4.02it/s]Evaluating:  78%|███████▊  | 245/313 [01:06<00:17,  3.94it/s]Evaluating:  79%|███████▊  | 246/313 [01:06<00:18,  3.61it/s]Evaluating:  79%|███████▉  | 247/313 [01:07<00:20,  3.14it/s]Evaluating:  79%|███████▉  | 248/313 [01:07<00:21,  3.09it/s]Evaluating:  80%|███████▉  | 249/313 [01:07<00:18,  3.45it/s]Evaluating:  80%|███████▉  | 250/313 [01:08<00:16,  3.75it/s]Evaluating:  80%|████████  | 251/313 [01:08<00:15,  3.99it/s]Evaluating:  81%|████████  | 252/313 [01:08<00:15,  3.93it/s]Evaluating:  81%|████████  | 253/313 [01:08<00:16,  3.61it/s]Evaluating:  81%|████████  | 254/313 [01:09<00:18,  3.14it/s]Evaluating:  81%|████████▏ | 255/313 [01:09<00:19,  3.01it/s]Evaluating:  82%|████████▏ | 256/313 [01:09<00:17,  3.35it/s]Evaluating:  82%|████████▏ | 257/313 [01:10<00:15,  3.67it/s]Evaluating:  82%|████████▏ | 258/313 [01:10<00:13,  3.95it/s]Evaluating:  83%|████████▎ | 259/313 [01:10<00:13,  4.14it/s]Evaluating:  83%|████████▎ | 260/313 [01:10<00:12,  4.24it/s]Evaluating:  83%|████████▎ | 261/313 [01:10<00:12,  4.31it/s]Evaluating:  84%|████████▎ | 262/313 [01:11<00:11,  4.45it/s]Evaluating:  84%|████████▍ | 263/313 [01:11<00:11,  4.54it/s]Evaluating:  84%|████████▍ | 264/313 [01:11<00:10,  4.74it/s]Evaluating:  85%|████████▍ | 265/313 [01:11<00:09,  4.91it/s]Evaluating:  85%|████████▍ | 266/313 [01:11<00:09,  5.03it/s]Evaluating:  85%|████████▌ | 267/313 [01:12<00:08,  5.12it/s]Evaluating:  86%|████████▌ | 268/313 [01:12<00:08,  5.18it/s]Evaluating:  86%|████████▌ | 269/313 [01:12<00:08,  5.23it/s]Evaluating:  86%|████████▋ | 270/313 [01:12<00:08,  5.23it/s]Evaluating:  87%|████████▋ | 271/313 [01:12<00:08,  5.24it/s]Evaluating:  87%|████████▋ | 272/313 [01:13<00:07,  5.25it/s]Evaluating:  87%|████████▋ | 273/313 [01:13<00:07,  5.28it/s]Evaluating:  88%|████████▊ | 274/313 [01:13<00:07,  5.30it/s]Evaluating:  88%|████████▊ | 275/313 [01:13<00:07,  5.33it/s]Evaluating:  88%|████████▊ | 276/313 [01:13<00:06,  5.38it/s]Evaluating:  88%|████████▊ | 277/313 [01:14<00:07,  5.13it/s]Evaluating:  89%|████████▉ | 278/313 [01:14<00:06,  5.18it/s]Evaluating:  89%|████████▉ | 279/313 [01:14<00:06,  5.16it/s]Evaluating:  89%|████████▉ | 280/313 [01:14<00:06,  5.16it/s]Evaluating:  90%|████████▉ | 281/313 [01:14<00:06,  5.17it/s]Evaluating:  90%|█████████ | 282/313 [01:15<00:06,  5.10it/s]Evaluating:  90%|█████████ | 283/313 [01:15<00:05,  5.08it/s]Evaluating:  91%|█████████ | 284/313 [01:15<00:05,  5.07it/s]Evaluating:  91%|█████████ | 285/313 [01:15<00:05,  5.07it/s]Evaluating:  91%|█████████▏| 286/313 [01:15<00:05,  5.09it/s]Evaluating:  92%|█████████▏| 287/313 [01:15<00:05,  5.08it/s]Evaluating:  92%|█████████▏| 288/313 [01:16<00:04,  5.08it/s]Evaluating:  92%|█████████▏| 289/313 [01:16<00:04,  5.07it/s]Evaluating:  93%|█████████▎| 290/313 [01:16<00:04,  5.08it/s]Evaluating:  93%|█████████▎| 291/313 [01:16<00:04,  5.10it/s]Evaluating:  93%|█████████▎| 292/313 [01:16<00:04,  5.14it/s]Evaluating:  94%|█████████▎| 293/313 [01:17<00:03,  5.21it/s]Evaluating:  94%|█████████▍| 294/313 [01:17<00:03,  5.28it/s]Evaluating:  94%|█████████▍| 295/313 [01:17<00:03,  5.32it/s]Evaluating:  95%|█████████▍| 296/313 [01:17<00:03,  4.85it/s]Evaluating:  95%|█████████▍| 297/313 [01:18<00:03,  4.19it/s]Evaluating:  95%|█████████▌| 298/313 [01:18<00:03,  3.93it/s]Evaluating:  96%|█████████▌| 299/313 [01:18<00:03,  3.85it/s]Evaluating:  96%|█████████▌| 300/313 [01:18<00:03,  3.66it/s]Evaluating:  96%|█████████▌| 301/313 [01:19<00:03,  3.65it/s]Evaluating:  96%|█████████▋| 302/313 [01:19<00:02,  3.87it/s]Evaluating:  97%|█████████▋| 303/313 [01:19<00:02,  4.20it/s]Evaluating:  97%|█████████▋| 304/313 [01:19<00:02,  4.48it/s]Evaluating:  97%|█████████▋| 305/313 [01:20<00:01,  4.70it/s]Evaluating:  98%|█████████▊| 306/313 [01:20<00:01,  4.86it/s]Evaluating:  98%|█████████▊| 307/313 [01:20<00:01,  4.96it/s]Evaluating:  98%|█████████▊| 308/313 [01:20<00:01,  5.00it/s]Evaluating:  99%|█████████▊| 309/313 [01:20<00:00,  4.97it/s]Evaluating:  99%|█████████▉| 310/313 [01:20<00:00,  4.97it/s]Evaluating:  99%|█████████▉| 311/313 [01:21<00:00,  4.99it/s]Evaluating: 100%|█████████▉| 312/313 [01:21<00:00,  5.00it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  5.76it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.84it/s]
10/09/2021 00:19:14 - INFO - __main__ -   ***** Evaluation result  in no *****
10/09/2021 00:19:14 - INFO - __main__ -     f1 = 0.7489557638546293
10/09/2021 00:19:14 - INFO - __main__ -     loss = 0.8889416224373796
10/09/2021 00:19:14 - INFO - __main__ -     precision = 0.7114915953258764
10/09/2021 00:19:14 - INFO - __main__ -     recall = 0.7905846410220803
10/09/2021 00:19:14 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/09/2021 00:19:15 - INFO - __main__ -   ***** Running evaluation  in da *****
10/09/2021 00:19:15 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:19:15 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:56,  5.50it/s]Evaluating:   1%|          | 2/313 [00:00<00:56,  5.49it/s]Evaluating:   1%|          | 3/313 [00:00<00:56,  5.47it/s]Evaluating:   1%|▏         | 4/313 [00:00<01:01,  5.04it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:11,  4.33it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:23,  3.66it/s]Evaluating:   2%|▏         | 7/313 [00:01<01:37,  3.15it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:34,  3.22it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:24,  3.60it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:17,  3.92it/s]Evaluating:   4%|▎         | 11/313 [00:02<01:12,  4.17it/s]Evaluating:   4%|▍         | 12/313 [00:02<01:09,  4.36it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:10,  4.27it/s]Evaluating:   4%|▍         | 14/313 [00:03<01:15,  3.96it/s]Evaluating:   5%|▍         | 15/313 [00:03<01:29,  3.34it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:34,  3.15it/s]Evaluating:   5%|▌         | 17/313 [00:04<01:26,  3.44it/s]Evaluating:   6%|▌         | 18/313 [00:04<01:36,  3.07it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:41,  2.91it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:43,  2.84it/s]Evaluating:   7%|▋         | 21/313 [00:05<01:33,  3.13it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:23,  3.50it/s]Evaluating:   7%|▋         | 23/313 [00:06<01:15,  3.83it/s]Evaluating:   8%|▊         | 24/313 [00:06<01:10,  4.09it/s]Evaluating:   8%|▊         | 25/313 [00:06<01:06,  4.31it/s]Evaluating:   8%|▊         | 26/313 [00:06<01:06,  4.31it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:12,  3.93it/s]Evaluating:   9%|▉         | 28/313 [00:07<01:23,  3.42it/s]Evaluating:   9%|▉         | 29/313 [00:08<01:31,  3.10it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:27,  3.23it/s]Evaluating:  10%|▉         | 31/313 [00:08<01:18,  3.59it/s]Evaluating:  10%|█         | 32/313 [00:08<01:12,  3.89it/s]Evaluating:  11%|█         | 33/313 [00:08<01:07,  4.14it/s]Evaluating:  11%|█         | 34/313 [00:09<01:04,  4.32it/s]Evaluating:  11%|█         | 35/313 [00:09<01:05,  4.26it/s]Evaluating:  12%|█▏        | 36/313 [00:09<01:31,  3.03it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:31,  3.03it/s]Evaluating:  12%|█▏        | 38/313 [00:10<01:20,  3.42it/s]Evaluating:  12%|█▏        | 39/313 [00:10<01:13,  3.75it/s]Evaluating:  13%|█▎        | 40/313 [00:10<01:07,  4.03it/s]Evaluating:  13%|█▎        | 41/313 [00:11<01:04,  4.24it/s]Evaluating:  13%|█▎        | 42/313 [00:11<01:03,  4.30it/s]Evaluating:  14%|█▎        | 43/313 [00:11<01:07,  4.02it/s]Evaluating:  14%|█▍        | 44/313 [00:11<01:15,  3.54it/s]Evaluating:  14%|█▍        | 45/313 [00:12<01:25,  3.13it/s]Evaluating:  15%|█▍        | 46/313 [00:12<01:22,  3.22it/s]Evaluating:  15%|█▌        | 47/313 [00:12<01:14,  3.58it/s]Evaluating:  15%|█▌        | 48/313 [00:13<01:08,  3.89it/s]Evaluating:  16%|█▌        | 49/313 [00:13<01:03,  4.14it/s]Evaluating:  16%|█▌        | 50/313 [00:13<01:01,  4.31it/s]Evaluating:  16%|█▋        | 51/313 [00:13<01:01,  4.24it/s]Evaluating:  17%|█▋        | 52/313 [00:14<01:06,  3.90it/s]Evaluating:  17%|█▋        | 53/313 [00:14<01:17,  3.34it/s]Evaluating:  17%|█▋        | 54/313 [00:14<01:24,  3.07it/s]Evaluating:  18%|█▊        | 55/313 [00:15<01:17,  3.32it/s]Evaluating:  18%|█▊        | 56/313 [00:15<01:10,  3.67it/s]Evaluating:  18%|█▊        | 57/313 [00:15<01:04,  3.96it/s]Evaluating:  19%|█▊        | 58/313 [00:15<01:00,  4.18it/s]Evaluating:  19%|█▉        | 59/313 [00:15<00:58,  4.32it/s]Evaluating:  19%|█▉        | 60/313 [00:16<01:00,  4.16it/s]Evaluating:  19%|█▉        | 61/313 [00:16<01:07,  3.75it/s]Evaluating:  20%|█▉        | 62/313 [00:16<01:17,  3.23it/s]Evaluating:  20%|██        | 63/313 [00:17<01:16,  3.25it/s]Evaluating:  20%|██        | 64/313 [00:17<01:09,  3.60it/s]Evaluating:  21%|██        | 65/313 [00:17<01:03,  3.90it/s]Evaluating:  21%|██        | 66/313 [00:17<00:59,  4.14it/s]Evaluating:  21%|██▏       | 67/313 [00:18<00:56,  4.33it/s]Evaluating:  22%|██▏       | 68/313 [00:18<00:57,  4.26it/s]Evaluating:  22%|██▏       | 69/313 [00:18<01:02,  3.89it/s]Evaluating:  22%|██▏       | 70/313 [00:18<01:13,  3.31it/s]Evaluating:  23%|██▎       | 71/313 [00:19<01:19,  3.06it/s]Evaluating:  23%|██▎       | 72/313 [00:19<01:13,  3.28it/s]Evaluating:  23%|██▎       | 73/313 [00:19<01:06,  3.63it/s]Evaluating:  24%|██▎       | 74/313 [00:20<01:00,  3.93it/s]Evaluating:  24%|██▍       | 75/313 [00:20<00:57,  4.16it/s]Evaluating:  24%|██▍       | 76/313 [00:20<00:55,  4.24it/s]Evaluating:  25%|██▍       | 77/313 [00:20<00:59,  3.97it/s]Evaluating:  25%|██▍       | 78/313 [00:21<01:07,  3.50it/s]Evaluating:  25%|██▌       | 79/313 [00:21<01:15,  3.10it/s]Evaluating:  26%|██▌       | 80/313 [00:21<01:13,  3.15it/s]Evaluating:  26%|██▌       | 81/313 [00:22<01:05,  3.52it/s]Evaluating:  26%|██▌       | 82/313 [00:22<01:00,  3.83it/s]Evaluating:  27%|██▋       | 83/313 [00:22<00:56,  4.08it/s]Evaluating:  27%|██▋       | 84/313 [00:22<00:54,  4.22it/s]Evaluating:  27%|██▋       | 85/313 [00:22<00:56,  4.02it/s]Evaluating:  27%|██▋       | 86/313 [00:23<01:03,  3.60it/s]Evaluating:  28%|██▊       | 87/313 [00:23<01:11,  3.15it/s]Evaluating:  28%|██▊       | 88/313 [00:24<01:11,  3.17it/s]Evaluating:  28%|██▊       | 89/313 [00:24<01:03,  3.51it/s]Evaluating:  29%|██▉       | 90/313 [00:24<00:58,  3.80it/s]Evaluating:  29%|██▉       | 91/313 [00:24<00:58,  3.80it/s]Evaluating:  29%|██▉       | 92/313 [00:24<01:00,  3.63it/s]Evaluating:  30%|██▉       | 93/313 [00:25<01:07,  3.27it/s]Evaluating:  30%|███       | 94/313 [00:25<01:13,  2.97it/s]Evaluating:  30%|███       | 95/313 [00:26<01:07,  3.21it/s]Evaluating:  31%|███       | 96/313 [00:26<01:00,  3.57it/s]Evaluating:  31%|███       | 97/313 [00:26<00:55,  3.87it/s]Evaluating:  31%|███▏      | 98/313 [00:26<00:52,  4.11it/s]Evaluating:  32%|███▏      | 99/313 [00:26<00:52,  4.11it/s]Evaluating:  32%|███▏      | 100/313 [00:27<00:55,  3.82it/s]Evaluating:  32%|███▏      | 101/313 [00:27<01:04,  3.27it/s]Evaluating:  33%|███▎      | 102/313 [00:27<01:09,  3.05it/s]Evaluating:  33%|███▎      | 103/313 [00:28<01:03,  3.30it/s]Evaluating:  33%|███▎      | 104/313 [00:28<00:57,  3.64it/s]Evaluating:  34%|███▎      | 105/313 [00:28<00:52,  3.93it/s]Evaluating:  34%|███▍      | 106/313 [00:28<00:49,  4.15it/s]Evaluating:  34%|███▍      | 107/313 [00:29<00:48,  4.27it/s]Evaluating:  35%|███▍      | 108/313 [00:29<00:52,  3.90it/s]Evaluating:  35%|███▍      | 109/313 [00:29<01:12,  2.81it/s]Evaluating:  35%|███▌      | 110/313 [00:30<01:07,  3.00it/s]Evaluating:  35%|███▌      | 111/313 [00:30<00:59,  3.37it/s]Evaluating:  36%|███▌      | 112/313 [00:30<00:54,  3.71it/s]Evaluating:  36%|███▌      | 113/313 [00:30<00:50,  3.99it/s]Evaluating:  36%|███▋      | 114/313 [00:31<00:49,  4.01it/s]Evaluating:  37%|███▋      | 115/313 [00:31<00:52,  3.77it/s]Evaluating:  37%|███▋      | 116/313 [00:31<01:00,  3.24it/s]Evaluating:  37%|███▋      | 117/313 [00:32<01:04,  3.05it/s]Evaluating:  38%|███▊      | 118/313 [00:32<00:58,  3.32it/s]Evaluating:  38%|███▊      | 119/313 [00:32<00:53,  3.66it/s]Evaluating:  38%|███▊      | 120/313 [00:32<00:49,  3.93it/s]Evaluating:  39%|███▊      | 121/313 [00:33<00:46,  4.16it/s]Evaluating:  39%|███▉      | 122/313 [00:33<00:45,  4.19it/s]Evaluating:  39%|███▉      | 123/313 [00:33<00:49,  3.80it/s]Evaluating:  40%|███▉      | 124/313 [00:34<00:56,  3.33it/s]Evaluating:  40%|███▉      | 125/313 [00:34<01:02,  2.99it/s]Evaluating:  40%|████      | 126/313 [00:34<01:10,  2.67it/s]Evaluating:  41%|████      | 127/313 [00:35<01:09,  2.67it/s]Evaluating:  41%|████      | 128/313 [00:35<01:06,  2.78it/s]Evaluating:  41%|████      | 129/313 [00:35<00:57,  3.21it/s]Evaluating:  42%|████▏     | 130/313 [00:36<00:51,  3.59it/s]Evaluating:  42%|████▏     | 131/313 [00:36<00:46,  3.91it/s]Evaluating:  42%|████▏     | 132/313 [00:36<00:43,  4.16it/s]Evaluating:  42%|████▏     | 133/313 [00:36<00:42,  4.26it/s]Evaluating:  43%|████▎     | 134/313 [00:36<00:45,  3.96it/s]Evaluating:  43%|████▎     | 135/313 [00:37<00:52,  3.41it/s]Evaluating:  43%|████▎     | 136/313 [00:37<00:56,  3.13it/s]Evaluating:  44%|████▍     | 137/313 [00:37<00:52,  3.34it/s]Evaluating:  44%|████▍     | 138/313 [00:38<00:47,  3.67it/s]Evaluating:  44%|████▍     | 139/313 [00:38<00:44,  3.94it/s]Evaluating:  45%|████▍     | 140/313 [00:38<00:41,  4.15it/s]Evaluating:  45%|████▌     | 141/313 [00:38<00:42,  4.08it/s]Evaluating:  45%|████▌     | 142/313 [00:39<00:45,  3.78it/s]Evaluating:  46%|████▌     | 143/313 [00:39<00:52,  3.24it/s]Evaluating:  46%|████▌     | 144/313 [00:39<00:54,  3.10it/s]Evaluating:  46%|████▋     | 145/313 [00:40<00:48,  3.47it/s]Evaluating:  47%|████▋     | 146/313 [00:40<00:44,  3.78it/s]Evaluating:  47%|████▋     | 147/313 [00:40<00:41,  4.03it/s]Evaluating:  47%|████▋     | 148/313 [00:40<00:41,  4.00it/s]Evaluating:  48%|████▊     | 149/313 [00:41<00:44,  3.72it/s]Evaluating:  48%|████▊     | 150/313 [00:41<00:50,  3.21it/s]Evaluating:  48%|████▊     | 151/313 [00:41<00:54,  2.98it/s]Evaluating:  49%|████▊     | 152/313 [00:42<00:48,  3.35it/s]Evaluating:  49%|████▉     | 153/313 [00:42<00:43,  3.69it/s]Evaluating:  49%|████▉     | 154/313 [00:42<00:40,  3.97it/s]Evaluating:  50%|████▉     | 155/313 [00:42<00:38,  4.13it/s]Evaluating:  50%|████▉     | 156/313 [00:43<00:40,  3.92it/s]Evaluating:  50%|█████     | 157/313 [00:43<00:44,  3.48it/s]Evaluating:  50%|█████     | 158/313 [00:43<00:50,  3.08it/s]Evaluating:  51%|█████     | 159/313 [00:44<00:48,  3.14it/s]Evaluating:  51%|█████     | 160/313 [00:44<00:43,  3.50it/s]Evaluating:  51%|█████▏    | 161/313 [00:44<00:39,  3.81it/s]Evaluating:  52%|█████▏    | 162/313 [00:44<00:37,  4.05it/s]Evaluating:  52%|█████▏    | 163/313 [00:44<00:36,  4.13it/s]Evaluating:  52%|█████▏    | 164/313 [00:45<00:38,  3.83it/s]Evaluating:  53%|█████▎    | 165/313 [00:45<00:44,  3.35it/s]Evaluating:  53%|█████▎    | 166/313 [00:46<00:48,  3.06it/s]Evaluating:  53%|█████▎    | 167/313 [00:46<00:45,  3.23it/s]Evaluating:  54%|█████▎    | 168/313 [00:46<00:40,  3.58it/s]Evaluating:  54%|█████▍    | 169/313 [00:46<00:37,  3.87it/s]Evaluating:  54%|█████▍    | 170/313 [00:46<00:34,  4.09it/s]Evaluating:  55%|█████▍    | 171/313 [00:47<00:35,  3.98it/s]Evaluating:  55%|█████▍    | 172/313 [00:47<00:38,  3.64it/s]Evaluating:  55%|█████▌    | 173/313 [00:47<00:44,  3.17it/s]Evaluating:  56%|█████▌    | 174/313 [00:48<00:47,  2.92it/s]Evaluating:  56%|█████▌    | 175/313 [00:48<00:41,  3.30it/s]Evaluating:  56%|█████▌    | 176/313 [00:48<00:37,  3.63it/s]Evaluating:  57%|█████▋    | 177/313 [00:49<00:34,  3.91it/s]Evaluating:  57%|█████▋    | 178/313 [00:49<00:33,  4.03it/s]Evaluating:  57%|█████▋    | 179/313 [00:49<00:35,  3.78it/s]Evaluating:  58%|█████▊    | 180/313 [00:49<00:42,  3.15it/s]Evaluating:  58%|█████▊    | 181/313 [00:50<00:45,  2.93it/s]Evaluating:  58%|█████▊    | 182/313 [00:50<00:39,  3.31it/s]Evaluating:  58%|█████▊    | 183/313 [00:50<00:35,  3.64it/s]Evaluating:  59%|█████▉    | 184/313 [00:51<00:32,  3.93it/s]Evaluating:  59%|█████▉    | 185/313 [00:51<00:31,  4.06it/s]Evaluating:  59%|█████▉    | 186/313 [00:51<00:33,  3.81it/s]Evaluating:  60%|█████▉    | 187/313 [00:51<00:37,  3.38it/s]Evaluating:  60%|██████    | 188/313 [00:52<00:41,  3.02it/s]Evaluating:  60%|██████    | 189/313 [00:52<00:39,  3.15it/s]Evaluating:  61%|██████    | 190/313 [00:52<00:34,  3.51it/s]Evaluating:  61%|██████    | 191/313 [00:53<00:32,  3.81it/s]Evaluating:  61%|██████▏   | 192/313 [00:53<00:30,  4.02it/s]Evaluating:  62%|██████▏   | 193/313 [00:53<00:30,  3.91it/s]Evaluating:  62%|██████▏   | 194/313 [00:53<00:33,  3.55it/s]Evaluating:  62%|██████▏   | 195/313 [00:54<00:37,  3.11it/s]Evaluating:  63%|██████▎   | 196/313 [00:54<00:38,  3.03it/s]Evaluating:  63%|██████▎   | 197/313 [00:54<00:34,  3.39it/s]Evaluating:  63%|██████▎   | 198/313 [00:55<00:30,  3.72it/s]Evaluating:  64%|██████▎   | 199/313 [00:55<00:28,  4.01it/s]Evaluating:  64%|██████▍   | 200/313 [00:55<00:27,  4.12it/s]Evaluating:  64%|██████▍   | 201/313 [00:55<00:29,  3.82it/s]Evaluating:  65%|██████▍   | 202/313 [00:56<00:33,  3.34it/s]Evaluating:  65%|██████▍   | 203/313 [00:56<00:36,  3.04it/s]Evaluating:  65%|██████▌   | 204/313 [00:56<00:34,  3.19it/s]Evaluating:  65%|██████▌   | 205/313 [00:57<00:30,  3.54it/s]Evaluating:  66%|██████▌   | 206/313 [00:57<00:27,  3.86it/s]Evaluating:  66%|██████▌   | 207/313 [00:57<00:25,  4.11it/s]Evaluating:  66%|██████▋   | 208/313 [00:57<00:25,  4.12it/s]Evaluating:  67%|██████▋   | 209/313 [00:58<00:27,  3.83it/s]Evaluating:  67%|██████▋   | 210/313 [00:58<00:31,  3.26it/s]Evaluating:  67%|██████▋   | 211/313 [00:58<00:31,  3.20it/s]Evaluating:  68%|██████▊   | 212/313 [00:58<00:28,  3.54it/s]Evaluating:  68%|██████▊   | 213/313 [00:59<00:26,  3.82it/s]Evaluating:  68%|██████▊   | 214/313 [00:59<00:24,  4.06it/s]Evaluating:  69%|██████▊   | 215/313 [00:59<00:24,  3.95it/s]Evaluating:  69%|██████▉   | 216/313 [00:59<00:26,  3.61it/s]Evaluating:  69%|██████▉   | 217/313 [01:00<00:30,  3.14it/s]Evaluating:  70%|██████▉   | 218/313 [01:00<00:32,  2.94it/s]Evaluating:  70%|██████▉   | 219/313 [01:00<00:28,  3.32it/s]Evaluating:  70%|███████   | 220/313 [01:01<00:25,  3.66it/s]Evaluating:  71%|███████   | 221/313 [01:01<00:23,  3.93it/s]Evaluating:  71%|███████   | 222/313 [01:01<00:23,  3.94it/s]Evaluating:  71%|███████   | 223/313 [01:01<00:24,  3.70it/s]Evaluating:  72%|███████▏  | 224/313 [01:02<00:27,  3.20it/s]Evaluating:  72%|███████▏  | 225/313 [01:02<00:28,  3.10it/s]Evaluating:  72%|███████▏  | 226/313 [01:02<00:25,  3.46it/s]Evaluating:  73%|███████▎  | 227/313 [01:03<00:22,  3.77it/s]Evaluating:  73%|███████▎  | 228/313 [01:03<00:21,  4.01it/s]Evaluating:  73%|███████▎  | 229/313 [01:03<00:20,  4.00it/s]Evaluating:  73%|███████▎  | 230/313 [01:03<00:22,  3.68it/s]Evaluating:  74%|███████▍  | 231/313 [01:04<00:25,  3.18it/s]Evaluating:  74%|███████▍  | 232/313 [01:04<00:27,  2.91it/s]Evaluating:  74%|███████▍  | 233/313 [01:04<00:24,  3.28it/s]Evaluating:  75%|███████▍  | 234/313 [01:05<00:21,  3.62it/s]Evaluating:  75%|███████▌  | 235/313 [01:05<00:20,  3.90it/s]Evaluating:  75%|███████▌  | 236/313 [01:05<00:19,  4.02it/s]Evaluating:  76%|███████▌  | 237/313 [01:05<00:20,  3.79it/s]Evaluating:  76%|███████▌  | 238/313 [01:06<00:22,  3.34it/s]Evaluating:  76%|███████▋  | 239/313 [01:06<00:24,  3.06it/s]Evaluating:  77%|███████▋  | 240/313 [01:06<00:22,  3.25it/s]Evaluating:  77%|███████▋  | 241/313 [01:07<00:20,  3.59it/s]Evaluating:  77%|███████▋  | 242/313 [01:07<00:18,  3.88it/s]Evaluating:  78%|███████▊  | 243/313 [01:07<00:17,  4.10it/s]Evaluating:  78%|███████▊  | 244/313 [01:07<00:16,  4.28it/s]Evaluating:  78%|███████▊  | 245/313 [01:08<00:15,  4.35it/s]Evaluating:  79%|███████▊  | 246/313 [01:08<00:14,  4.51it/s]Evaluating:  79%|███████▉  | 247/313 [01:08<00:14,  4.61it/s]Evaluating:  79%|███████▉  | 248/313 [01:08<00:13,  4.77it/s]Evaluating:  80%|███████▉  | 249/313 [01:08<00:13,  4.92it/s]Evaluating:  80%|███████▉  | 250/313 [01:09<00:12,  5.03it/s]Evaluating:  80%|████████  | 251/313 [01:09<00:12,  5.11it/s]Evaluating:  81%|████████  | 252/313 [01:09<00:11,  5.22it/s]Evaluating:  81%|████████  | 253/313 [01:09<00:11,  5.34it/s]Evaluating:  81%|████████  | 254/313 [01:09<00:11,  5.30it/s]Evaluating:  81%|████████▏ | 255/313 [01:09<00:10,  5.41it/s]Evaluating:  82%|████████▏ | 256/313 [01:10<00:10,  5.45it/s]Evaluating:  82%|████████▏ | 257/313 [01:10<00:10,  5.29it/s]Evaluating:  82%|████████▏ | 258/313 [01:10<00:10,  5.13it/s]Evaluating:  83%|████████▎ | 259/313 [01:10<00:10,  4.98it/s]Evaluating:  83%|████████▎ | 260/313 [01:10<00:10,  5.00it/s]Evaluating:  83%|████████▎ | 261/313 [01:11<00:10,  5.03it/s]Evaluating:  84%|████████▎ | 262/313 [01:11<00:10,  5.07it/s]Evaluating:  84%|████████▍ | 263/313 [01:11<00:09,  5.10it/s]Evaluating:  84%|████████▍ | 264/313 [01:11<00:09,  5.14it/s]Evaluating:  85%|████████▍ | 265/313 [01:11<00:09,  5.17it/s]Evaluating:  85%|████████▍ | 266/313 [01:12<00:09,  5.20it/s]Evaluating:  85%|████████▌ | 267/313 [01:12<00:08,  5.24it/s]Evaluating:  86%|████████▌ | 268/313 [01:12<00:08,  5.28it/s]Evaluating:  86%|████████▌ | 269/313 [01:12<00:08,  5.32it/s]Evaluating:  86%|████████▋ | 270/313 [01:12<00:08,  5.23it/s]Evaluating:  87%|████████▋ | 271/313 [01:13<00:07,  5.32it/s]Evaluating:  87%|████████▋ | 272/313 [01:13<00:07,  5.39it/s]Evaluating:  87%|████████▋ | 273/313 [01:13<00:07,  5.44it/s]Evaluating:  88%|████████▊ | 274/313 [01:13<00:07,  5.05it/s]Evaluating:  88%|████████▊ | 275/313 [01:13<00:08,  4.67it/s]Evaluating:  88%|████████▊ | 276/313 [01:14<00:08,  4.51it/s]Evaluating:  88%|████████▊ | 277/313 [01:14<00:08,  4.27it/s]Evaluating:  89%|████████▉ | 278/313 [01:14<00:09,  3.79it/s]Evaluating:  89%|████████▉ | 279/313 [01:14<00:08,  3.90it/s]Evaluating:  89%|████████▉ | 280/313 [01:15<00:09,  3.66it/s]Evaluating:  90%|████████▉ | 281/313 [01:15<00:08,  3.80it/s]Evaluating:  90%|█████████ | 282/313 [01:15<00:08,  3.59it/s]Evaluating:  90%|█████████ | 283/313 [01:16<00:08,  3.72it/s]Evaluating:  91%|█████████ | 284/313 [01:16<00:07,  3.94it/s]Evaluating:  91%|█████████ | 285/313 [01:16<00:06,  4.25it/s]Evaluating:  91%|█████████▏| 286/313 [01:16<00:05,  4.51it/s]Evaluating:  92%|█████████▏| 287/313 [01:16<00:05,  4.70it/s]Evaluating:  92%|█████████▏| 288/313 [01:17<00:05,  4.85it/s]Evaluating:  92%|█████████▏| 289/313 [01:17<00:04,  4.96it/s]Evaluating:  93%|█████████▎| 290/313 [01:17<00:04,  5.04it/s]Evaluating:  93%|█████████▎| 291/313 [01:17<00:04,  5.11it/s]Evaluating:  93%|█████████▎| 292/313 [01:17<00:04,  5.16it/s]Evaluating:  94%|█████████▎| 293/313 [01:17<00:03,  5.11it/s]Evaluating:  94%|█████████▍| 294/313 [01:18<00:03,  5.16it/s]Evaluating:  94%|█████████▍| 295/313 [01:18<00:03,  5.24it/s]Evaluating:  95%|█████████▍| 296/313 [01:18<00:03,  5.31it/s]Evaluating:  95%|█████████▍| 297/313 [01:18<00:03,  5.33it/s]Evaluating:  95%|█████████▌| 298/313 [01:18<00:02,  5.33it/s]Evaluating:  96%|█████████▌| 299/313 [01:19<00:02,  5.19it/s]Evaluating:  96%|█████████▌| 300/313 [01:19<00:02,  5.09it/s]Evaluating:  96%|█████████▌| 301/313 [01:19<00:02,  5.05it/s]Evaluating:  96%|█████████▋| 302/313 [01:19<00:02,  5.03it/s]Evaluating:  97%|█████████▋| 303/313 [01:19<00:01,  5.02it/s]Evaluating:  97%|█████████▋| 304/313 [01:20<00:01,  5.02it/s]Evaluating:  97%|█████████▋| 305/313 [01:20<00:01,  5.03it/s]Evaluating:  98%|█████████▊| 306/313 [01:20<00:01,  5.03it/s]Evaluating:  98%|█████████▊| 307/313 [01:20<00:01,  5.03it/s]Evaluating:  98%|█████████▊| 308/313 [01:20<00:00,  5.03it/s]Evaluating:  99%|█████████▊| 309/313 [01:21<00:00,  5.06it/s]Evaluating:  99%|█████████▉| 310/313 [01:21<00:00,  5.06it/s]Evaluating:  99%|█████████▉| 311/313 [01:21<00:00,  5.04it/s]Evaluating: 100%|█████████▉| 312/313 [01:21<00:00,  5.08it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  5.57it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.82it/s]
10/09/2021 00:20:38 - INFO - __main__ -   ***** Evaluation result  in da *****
10/09/2021 00:20:38 - INFO - __main__ -     f1 = 0.7922559865565046
10/09/2021 00:20:38 - INFO - __main__ -     loss = 0.7262937105692233
10/09/2021 00:20:38 - INFO - __main__ -     precision = 0.7611327862235975
10/09/2021 00:20:38 - INFO - __main__ -     recall = 0.8260329975178858
10/09/2021 00:20:38 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:20:52 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:20:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:21:12 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:21:14 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:21:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:21:14 - INFO - __main__ -   Seed = 32
10/09/2021 00:21:14 - INFO - root -   save model
10/09/2021 00:21:14 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:21:14 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:21:29 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:21:29 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:21:29 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/09/2021 00:21:29 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:21:29 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:21:29 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/09/2021 00:21:29 - INFO - __main__ -   Language = en
10/09/2021 00:21:29 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:21:30 - INFO - __main__ -   Language = is
10/09/2021 00:21:30 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/09/2021 00:21:31 - INFO - __main__ -   Language = de
10/09/2021 00:21:31 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/09/2021 00:21:37 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/09/2021 00:21:37 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/09/2021 00:21:37 - INFO - __main__ -     Num examples = 100
10/09/2021 00:21:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  5.78it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  5.56it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  5.51it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  6.99it/s]
10/09/2021 00:21:38 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/09/2021 00:21:38 - INFO - __main__ -     f1 = 0.652014652014652
10/09/2021 00:21:38 - INFO - __main__ -     loss = 1.4847109615802765
10/09/2021 00:21:38 - INFO - __main__ -     precision = 0.5816993464052288
10/09/2021 00:21:38 - INFO - __main__ -     recall = 0.7416666666666667
10/09/2021 00:21:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/09/2021 00:21:39 - INFO - __main__ -   ***** Running evaluation  in no *****
10/09/2021 00:21:39 - INFO - __main__ -     Num examples = 10000
10/09/2021 00:21:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:48,  2.87it/s]Evaluating:   1%|          | 2/313 [00:00<01:26,  3.58it/s]Evaluating:   1%|          | 3/313 [00:00<01:14,  4.14it/s]Evaluating:   1%|▏         | 4/313 [00:00<01:09,  4.45it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:06,  4.61it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:05,  4.70it/s]Evaluating:   2%|▏         | 7/313 [00:01<01:05,  4.68it/s]Evaluating:   3%|▎         | 8/313 [00:01<01:11,  4.24it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:23,  3.65it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:35,  3.19it/s]Evaluating:   4%|▎         | 11/313 [00:02<01:34,  3.21it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:24,  3.58it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:17,  3.88it/s]Evaluating:   4%|▍         | 14/313 [00:03<01:12,  4.15it/s]Evaluating:   5%|▍         | 15/313 [00:03<01:08,  4.35it/s]Evaluating:   5%|▌         | 16/313 [00:03<01:06,  4.50it/s]Evaluating:   5%|▌         | 17/313 [00:04<01:10,  4.19it/s]Evaluating:   6%|▌         | 18/313 [00:04<01:19,  3.72it/s]Evaluating:   6%|▌         | 19/313 [00:04<01:30,  3.25it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:31,  3.20it/s]Evaluating:   7%|▋         | 21/313 [00:05<01:21,  3.57it/s]Evaluating:   7%|▋         | 22/313 [00:05<01:15,  3.87it/s]Evaluating:   7%|▋         | 23/313 [00:05<01:10,  4.12it/s]Evaluating:   8%|▊         | 24/313 [00:06<01:06,  4.32it/s]Evaluating:   8%|▊         | 25/313 [00:06<01:07,  4.29it/s]Evaluating:   8%|▊         | 26/313 [00:06<01:12,  3.93it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:24,  3.38it/s]Evaluating:   9%|▉         | 28/313 [00:07<01:31,  3.12it/s]Evaluating:   9%|▉         | 29/313 [00:07<01:24,  3.35it/s]Evaluating:  10%|▉         | 30/313 [00:07<01:16,  3.69it/s]Evaluating:  10%|▉         | 31/313 [00:08<01:10,  3.97it/s]Evaluating:  10%|█         | 32/313 [00:08<01:06,  4.20it/s]Evaluating:  11%|█         | 33/313 [00:08<01:05,  4.30it/s]Evaluating:  11%|█         | 34/313 [00:08<01:08,  4.05it/s]Evaluating:  11%|█         | 35/313 [00:09<01:16,  3.62it/s]Evaluating:  12%|█▏        | 36/313 [00:09<01:27,  3.16it/s]Evaluating:  12%|█▏        | 37/313 [00:09<01:29,  3.07it/s]Evaluating:  12%|█▏        | 38/313 [00:10<01:20,  3.44it/s]Evaluating:  12%|█▏        | 39/313 [00:10<01:12,  3.76it/s]Evaluating:  13%|█▎        | 40/313 [00:10<01:07,  4.03it/s]Evaluating:  13%|█▎        | 41/313 [00:10<01:04,  4.21it/s]Evaluating:  13%|█▎        | 42/313 [00:11<01:06,  4.06it/s]Evaluating:  14%|█▎        | 43/313 [00:11<01:13,  3.69it/s]Evaluating:  14%|█▍        | 44/313 [00:11<01:24,  3.19it/s]Evaluating:  14%|█▍        | 45/313 [00:12<01:27,  3.07it/s]Evaluating:  15%|█▍        | 46/313 [00:12<01:17,  3.45it/s]Evaluating:  15%|█▌        | 47/313 [00:12<01:10,  3.79it/s]Evaluating:  15%|█▌        | 48/313 [00:12<01:05,  4.08it/s]Evaluating:  16%|█▌        | 49/313 [00:12<01:01,  4.28it/s]Evaluating:  16%|█▌        | 50/313 [00:13<01:02,  4.18it/s]Evaluating:  16%|█▋        | 51/313 [00:13<01:08,  3.85it/s]Evaluating:  17%|█▋        | 52/313 [00:13<01:19,  3.28it/s]Evaluating:  17%|█▋        | 53/313 [00:14<01:24,  3.09it/s]Evaluating:  17%|█▋        | 54/313 [00:14<01:15,  3.43it/s]Evaluating:  18%|█▊        | 55/313 [00:14<01:08,  3.76it/s]Evaluating:  18%|█▊        | 56/313 [00:14<01:03,  4.04it/s]Evaluating:  18%|█▊        | 57/313 [00:15<01:00,  4.27it/s]Evaluating:  19%|█▊        | 58/313 [00:15<00:59,  4.32it/s]Evaluating:  19%|█▉        | 59/313 [00:15<01:03,  4.00it/s]Evaluating:  19%|█▉        | 60/313 [00:15<01:11,  3.52it/s]Evaluating:  19%|█▉        | 61/313 [00:16<01:21,  3.11it/s]Evaluating:  20%|█▉        | 62/313 [00:16<01:17,  3.24it/s]Evaluating:  20%|██        | 63/313 [00:16<01:09,  3.60it/s]Evaluating:  20%|██        | 64/313 [00:17<01:03,  3.89it/s]Evaluating:  21%|██        | 65/313 [00:17<00:59,  4.13it/s]Evaluating:  21%|██        | 66/313 [00:17<00:58,  4.24it/s]Evaluating:  21%|██▏       | 67/313 [00:17<01:01,  3.98it/s]Evaluating:  22%|██▏       | 68/313 [00:18<01:09,  3.52it/s]Evaluating:  22%|██▏       | 69/313 [00:18<01:18,  3.11it/s]Evaluating:  22%|██▏       | 70/313 [00:18<01:17,  3.14it/s]Evaluating:  23%|██▎       | 71/313 [00:19<01:08,  3.51it/s]Evaluating:  23%|██▎       | 72/313 [00:19<01:03,  3.82it/s]Evaluating:  23%|██▎       | 73/313 [00:19<00:58,  4.08it/s]Evaluating:  24%|██▎       | 74/313 [00:19<00:56,  4.24it/s]Evaluating:  24%|██▍       | 75/313 [00:19<00:58,  4.10it/s]Evaluating:  24%|██▍       | 76/313 [00:20<01:03,  3.71it/s]Evaluating:  25%|██▍       | 77/313 [00:20<01:13,  3.22it/s]Evaluating:  25%|██▍       | 78/313 [00:21<01:15,  3.13it/s]Evaluating:  25%|██▌       | 79/313 [00:21<01:06,  3.50it/s]Evaluating:  26%|██▌       | 80/313 [00:21<01:01,  3.81it/s]Evaluating:  26%|██▌       | 81/313 [00:21<00:57,  4.07it/s]Evaluating:  26%|██▌       | 82/313 [00:21<00:54,  4.25it/s]Evaluating:  27%|██▋       | 83/313 [00:22<00:56,  4.10it/s]Evaluating:  27%|██▋       | 84/313 [00:22<01:01,  3.70it/s]Evaluating:  27%|██▋       | 85/313 [00:22<01:11,  3.20it/s]Evaluating:  27%|██▋       | 86/313 [00:23<01:13,  3.08it/s]Evaluating:  28%|██▊       | 87/313 [00:23<01:05,  3.43it/s]Evaluating:  28%|██▊       | 88/313 [00:23<00:59,  3.76it/s]Evaluating:  28%|██▊       | 89/313 [00:23<00:55,  4.03it/s]Evaluating:  29%|██▉       | 90/313 [00:24<00:52,  4.24it/s]Evaluating:  29%|██▉       | 91/313 [00:24<00:53,  4.14it/s]Evaluating:  29%|██▉       | 92/313 [00:24<00:57,  3.81it/s]Evaluating:  30%|██▉       | 93/313 [00:25<01:07,  3.25it/s]Evaluating:  30%|███       | 94/313 [00:25<01:11,  3.07it/s]Evaluating:  30%|███       | 95/313 [00:25<01:05,  3.35it/s]Evaluating:  31%|███       | 96/313 [00:25<00:58,  3.68it/s]Evaluating:  31%|███       | 97/313 [00:26<00:54,  3.97it/s]Evaluating:  31%|███▏      | 98/313 [00:26<00:51,  4.20it/s]Evaluating:  32%|███▏      | 99/313 [00:26<00:51,  4.18it/s]Evaluating:  32%|███▏      | 100/313 [00:26<01:05,  3.27it/s]Evaluating:  32%|███▏      | 101/313 [00:27<01:11,  2.98it/s]Evaluating:  33%|███▎      | 102/313 [00:27<01:08,  3.09it/s]Evaluating:  33%|███▎      | 103/313 [00:27<01:00,  3.46it/s]Evaluating:  33%|███▎      | 104/313 [00:28<00:55,  3.78it/s]Evaluating:  34%|███▎      | 105/313 [00:28<00:51,  4.01it/s]Evaluating:  34%|███▍      | 106/313 [00:28<00:50,  4.12it/s]Evaluating:  34%|███▍      | 107/313 [00:28<00:53,  3.85it/s]Evaluating:  35%|███▍      | 108/313 [00:29<00:59,  3.42it/s]Evaluating:  35%|███▍      | 109/313 [00:29<01:06,  3.05it/s]Evaluating:  35%|███▌      | 110/313 [00:29<01:02,  3.22it/s]Evaluating:  35%|███▌      | 111/313 [00:30<00:56,  3.58it/s]Evaluating:  36%|███▌      | 112/313 [00:30<00:51,  3.87it/s]Evaluating:  36%|███▌      | 113/313 [00:30<00:48,  4.09it/s]Evaluating:  36%|███▋      | 114/313 [00:30<00:49,  4.02it/s]Evaluating:  37%|███▋      | 115/313 [00:31<00:53,  3.72it/s]Evaluating:  37%|███▋      | 116/313 [00:31<01:01,  3.20it/s]Evaluating:  37%|███▋      | 117/313 [00:31<01:05,  3.00it/s]Evaluating:  38%|███▊      | 118/313 [00:32<00:57,  3.38it/s]Evaluating:  38%|███▊      | 119/313 [00:32<00:52,  3.71it/s]Evaluating:  38%|███▊      | 120/313 [00:32<00:48,  3.97it/s]Evaluating:  39%|███▊      | 121/313 [00:32<00:46,  4.12it/s]Evaluating:  39%|███▉      | 122/313 [00:33<00:48,  3.96it/s]Evaluating:  39%|███▉      | 123/313 [00:33<00:54,  3.51it/s]Evaluating:  40%|███▉      | 124/313 [00:33<00:59,  3.17it/s]Evaluating:  40%|███▉      | 125/313 [00:34<00:55,  3.38it/s]Evaluating:  40%|████      | 126/313 [00:34<00:50,  3.71it/s]Evaluating:  41%|████      | 127/313 [00:34<00:46,  3.97it/s]Evaluating:  41%|████      | 128/313 [00:34<00:44,  4.18it/s]Evaluating:  41%|████      | 129/313 [00:34<00:42,  4.32it/s]Evaluating:  42%|████▏     | 130/313 [00:35<00:42,  4.27it/s]Evaluating:  42%|████▏     | 131/313 [00:35<00:41,  4.41it/s]Evaluating:  42%|████▏     | 132/313 [00:35<00:40,  4.51it/s]Evaluating:  42%|████▏     | 133/313 [00:35<00:39,  4.59it/s]Evaluating:  43%|████▎     | 134/313 [00:35<00:37,  4.72it/s]Evaluating:  43%|████▎     | 135/313 [00:36<00:36,  4.88it/s]Evaluating:  43%|████▎     | 136/313 [00:36<00:35,  5.02it/s]Evaluating:  44%|████▍     | 137/313 [00:36<00:34,  5.10it/s]Evaluating:  44%|████▍     | 138/313 [00:36<00:33,  5.17it/s]Evaluating:  44%|████▍     | 139/313 [00:36<00:34,  5.04it/s]Evaluating:  45%|████▍     | 140/313 [00:37<00:33,  5.11it/s]Evaluating:  45%|████▌     | 141/313 [00:37<00:33,  5.18it/s]Evaluating:  45%|████▌     | 142/313 [00:37<00:32,  5.22it/s]Evaluating:  46%|████▌     | 143/313 [00:37<00:32,  5.27it/s]Evaluating:  46%|████▌     | 144/313 [00:37<00:31,  5.31it/s]Evaluating:  46%|████▋     | 145/313 [00:38<00:31,  5.34it/s]Evaluating:  47%|████▋     | 146/313 [00:38<00:31,  5.37it/s]Evaluating:  47%|████▋     | 147/313 [00:38<00:30,  5.39it/s]Evaluating:  47%|████▋     | 148/313 [00:38<00:30,  5.41it/s]Evaluating:  48%|████▊     | 149/313 [00:38<00:30,  5.40it/s]Evaluating:  48%|████▊     | 150/313 [00:38<00:30,  5.41it/s]Evaluating:  48%|████▊     | 151/313 [00:39<00:29,  5.41it/s]Evaluating:  49%|████▊     | 152/313 [00:39<00:29,  5.38it/s]Evaluating:  49%|████▉     | 153/313 [00:39<00:29,  5.36it/s]Evaluating:  49%|████▉     | 154/313 [00:39<00:29,  5.36it/s]Evaluating:  50%|████▉     | 155/313 [00:39<00:29,  5.38it/s]Evaluating:  50%|████▉     | 156/313 [00:40<00:28,  5.43it/s]Evaluating:  50%|█████     | 157/313 [00:40<00:28,  5.47it/s]Evaluating:  50%|█████     | 158/313 [00:40<00:28,  5.47it/s]Evaluating:  51%|█████     | 159/313 [00:40<00:28,  5.46it/s]Evaluating:  51%|█████     | 160/313 [00:40<00:28,  5.44it/s]Evaluating:  51%|█████▏    | 161/313 [00:40<00:28,  5.42it/s]Evaluating:  52%|█████▏    | 162/313 [00:41<00:28,  5.38it/s]Evaluating:  52%|█████▏    | 163/313 [00:41<00:28,  5.28it/s]Evaluating:  52%|█████▏    | 164/313 [00:41<00:31,  4.73it/s]Evaluating:  53%|█████▎    | 165/313 [00:41<00:35,  4.21it/s]Evaluating:  53%|█████▎    | 166/313 [00:42<00:35,  4.09it/s]Evaluating:  53%|█████▎    | 167/313 [00:42<00:37,  3.86it/s]Evaluating:  54%|█████▎    | 168/313 [00:42<00:38,  3.81it/s]Evaluating:  54%|█████▍    | 169/313 [00:43<00:39,  3.64it/s]Evaluating:  54%|█████▍    | 170/313 [00:43<00:36,  3.89it/s]Evaluating:  55%|█████▍    | 171/313 [00:43<00:34,  4.15it/s]Evaluating:  55%|█████▍    | 172/313 [00:43<00:31,  4.44it/s]Evaluating:  55%|█████▌    | 173/313 [00:43<00:30,  4.66it/s]Evaluating:  56%|█████▌    | 174/313 [00:44<00:28,  4.84it/s]Evaluating:  56%|█████▌    | 175/313 [00:44<00:27,  4.97it/s]Evaluating:  56%|█████▌    | 176/313 [00:44<00:27,  5.06it/s]Evaluating:  57%|█████▋    | 177/313 [00:44<00:26,  5.13it/s]Evaluating:  57%|█████▋    | 178/313 [00:44<00:26,  5.18it/s]Evaluating:  57%|█████▋    | 179/313 [00:44<00:25,  5.21it/s]Evaluating:  58%|█████▊    | 180/313 [00:45<00:25,  5.22it/s]Evaluating:  58%|█████▊    | 181/313 [00:45<00:25,  5.27it/s]Evaluating:  58%|█████▊    | 182/313 [00:45<00:24,  5.31it/s]Evaluating:  58%|█████▊    | 183/313 [00:45<00:24,  5.36it/s]Evaluating:  59%|█████▉    | 184/313 [00:45<00:23,  5.38it/s]Evaluating:  59%|█████▉    | 185/313 [00:46<00:23,  5.39it/s]Evaluating:  59%|█████▉    | 186/313 [00:46<00:23,  5.36it/s]Evaluating:  60%|█████▉    | 187/313 [00:46<00:23,  5.33it/s]Evaluating:  60%|██████    | 188/313 [00:46<00:23,  5.31it/s]Evaluating:  60%|██████    | 189/313 [00:46<00:23,  5.31it/s]Evaluating:  61%|██████    | 190/313 [00:47<00:23,  5.31it/s]Evaluating:  61%|██████    | 191/313 [00:47<00:22,  5.30it/s]Evaluating:  61%|██████▏   | 192/313 [00:47<00:24,  4.93it/s]Evaluating:  62%|██████▏   | 193/313 [00:47<00:23,  5.01it/s]Evaluating:  62%|██████▏   | 194/313 [00:47<00:23,  5.12it/s]Evaluating:  62%|██████▏   | 195/313 [00:48<00:22,  5.20it/s]Evaluating:  63%|██████▎   | 196/313 [00:48<00:22,  5.22it/s]Evaluating:  63%|██████▎   | 197/313 [00:48<00:22,  5.24it/s]Evaluating:  63%|██████▎   | 198/313 [00:48<00:21,  5.26it/s]Evaluating:  64%|██████▎   | 199/313 [00:48<00:22,  5.04it/s]Evaluating:  64%|██████▍   | 200/313 [00:49<00:25,  4.47it/s]Evaluating:  64%|██████▍   | 201/313 [00:49<00:29,  3.78it/s]Evaluating:  65%|██████▍   | 202/313 [00:49<00:34,  3.23it/s]Evaluating:  65%|██████▍   | 203/313 [00:50<00:34,  3.19it/s]Evaluating:  65%|██████▌   | 204/313 [00:50<00:30,  3.54it/s]Evaluating:  65%|██████▌   | 205/313 [00:50<00:28,  3.84it/s]Evaluating:  66%|██████▌   | 206/313 [00:50<00:26,  4.07it/s]Evaluating:  66%|██████▌   | 207/313 [00:51<00:26,  3.99it/s]Evaluating:  66%|██████▋   | 208/313 [00:51<00:28,  3.65it/s]Evaluating:  67%|██████▋   | 209/313 [00:51<00:32,  3.17it/s]Evaluating:  67%|██████▋   | 210/313 [00:52<00:34,  3.02it/s]Evaluating:  67%|██████▋   | 211/313 [00:52<00:30,  3.35it/s]Evaluating:  68%|██████▊   | 212/313 [00:52<00:27,  3.66it/s]Evaluating:  68%|██████▊   | 213/313 [00:52<00:25,  3.95it/s]Evaluating:  68%|██████▊   | 214/313 [00:53<00:24,  4.05it/s]Evaluating:  69%|██████▊   | 215/313 [00:53<00:26,  3.76it/s]Evaluating:  69%|██████▉   | 216/313 [00:53<00:29,  3.32it/s]Evaluating:  69%|██████▉   | 217/313 [00:54<00:32,  2.99it/s]Evaluating:  70%|██████▉   | 218/313 [00:54<00:29,  3.17it/s]Evaluating:  70%|██████▉   | 219/313 [00:54<00:26,  3.52it/s]Evaluating:  70%|███████   | 220/313 [00:54<00:24,  3.82it/s]Evaluating:  71%|███████   | 221/313 [00:55<00:23,  3.98it/s]Evaluating:  71%|███████   | 222/313 [00:55<00:24,  3.74it/s]Evaluating:  71%|███████   | 223/313 [00:55<00:27,  3.32it/s]Evaluating:  72%|███████▏  | 224/313 [00:56<00:29,  2.98it/s]Evaluating:  72%|███████▏  | 225/313 [00:56<00:28,  3.13it/s]Evaluating:  72%|███████▏  | 226/313 [00:56<00:24,  3.49it/s]Evaluating:  73%|███████▎  | 227/313 [00:56<00:22,  3.82it/s]Evaluating:  73%|███████▎  | 228/313 [00:57<00:21,  4.05it/s]Evaluating:  73%|███████▎  | 229/313 [00:57<00:21,  3.95it/s]Evaluating:  73%|███████▎  | 230/313 [00:57<00:23,  3.54it/s]Evaluating:  74%|███████▍  | 231/313 [00:58<00:26,  3.12it/s]Evaluating:  74%|███████▍  | 232/313 [00:58<00:25,  3.21it/s]Evaluating:  74%|███████▍  | 233/313 [00:58<00:22,  3.55it/s]Evaluating:  75%|███████▍  | 234/313 [00:58<00:20,  3.83it/s]Evaluating:  75%|███████▌  | 235/313 [00:59<00:19,  4.02it/s]Evaluating:  75%|███████▌  | 236/313 [00:59<00:19,  3.89it/s]Evaluating:  76%|███████▌  | 237/313 [00:59<00:21,  3.50it/s]Evaluating:  76%|███████▌  | 238/313 [01:00<00:24,  3.09it/s]Evaluating:  76%|███████▋  | 239/313 [01:00<00:23,  3.16it/s]Evaluating:  77%|███████▋  | 240/313 [01:00<00:20,  3.52it/s]Evaluating:  77%|███████▋  | 241/313 [01:00<00:18,  3.81it/s]Evaluating:  77%|███████▋  | 242/313 [01:01<00:17,  4.01it/s]Evaluating:  78%|███████▊  | 243/313 [01:01<00:18,  3.87it/s]Evaluating:  78%|███████▊  | 244/313 [01:01<00:19,  3.49it/s]Evaluating:  78%|███████▊  | 245/313 [01:02<00:22,  3.09it/s]Evaluating:  79%|███████▊  | 246/313 [01:02<00:21,  3.14it/s]Evaluating:  79%|███████▉  | 247/313 [01:02<00:18,  3.49it/s]Evaluating:  79%|███████▉  | 248/313 [01:02<00:17,  3.79it/s]Evaluating:  80%|███████▉  | 249/313 [01:03<00:15,  4.02it/s]Evaluating:  80%|███████▉  | 250/313 [01:03<00:16,  3.91it/s]Evaluating:  80%|████████  | 251/313 [01:03<00:17,  3.54it/s]Evaluating:  81%|████████  | 252/313 [01:04<00:19,  3.09it/s]Evaluating:  81%|████████  | 253/313 [01:04<00:19,  3.10it/s]Evaluating:  81%|████████  | 254/313 [01:04<00:16,  3.47it/s]Evaluating:  81%|████████▏ | 255/313 [01:04<00:15,  3.78it/s]Evaluating:  82%|████████▏ | 256/313 [01:05<00:14,  3.96it/s]Evaluating:  82%|████████▏ | 257/313 [01:05<00:14,  3.81it/s]Evaluating:  82%|████████▏ | 258/313 [01:05<00:16,  3.42it/s]Evaluating:  83%|████████▎ | 259/313 [01:06<00:17,  3.02it/s]Evaluating:  83%|████████▎ | 260/313 [01:06<00:17,  3.11it/s]Evaluating:  83%|████████▎ | 261/313 [01:06<00:15,  3.45it/s]Evaluating:  84%|████████▎ | 262/313 [01:06<00:13,  3.72it/s]Evaluating:  84%|████████▍ | 263/313 [01:07<00:13,  3.74it/s]Evaluating:  84%|████████▍ | 264/313 [01:07<00:13,  3.51it/s]Evaluating:  85%|████████▍ | 265/313 [01:07<00:15,  3.07it/s]Evaluating:  85%|████████▍ | 266/313 [01:08<00:15,  2.99it/s]Evaluating:  85%|████████▌ | 267/313 [01:08<00:13,  3.34it/s]Evaluating:  86%|████████▌ | 268/313 [01:08<00:12,  3.65it/s]Evaluating:  86%|████████▌ | 269/313 [01:08<00:11,  3.80it/s]Evaluating:  86%|████████▋ | 270/313 [01:09<00:12,  3.52it/s]Evaluating:  87%|████████▋ | 271/313 [01:09<00:13,  3.08it/s]Evaluating:  87%|████████▋ | 272/313 [01:10<00:14,  2.83it/s]Evaluating:  87%|████████▋ | 273/313 [01:10<00:12,  3.19it/s]Evaluating:  88%|████████▊ | 274/313 [01:10<00:11,  3.50it/s]Evaluating:  88%|████████▊ | 275/313 [01:10<00:10,  3.70it/s]Evaluating:  88%|████████▊ | 276/313 [01:11<00:10,  3.54it/s]Evaluating:  88%|████████▊ | 277/313 [01:11<00:11,  3.14it/s]Evaluating:  89%|████████▉ | 278/313 [01:11<00:11,  2.95it/s]Evaluating:  89%|████████▉ | 279/313 [01:12<00:10,  3.23it/s]Evaluating:  89%|████████▉ | 280/313 [01:12<00:09,  3.56it/s]Evaluating:  90%|████████▉ | 281/313 [01:12<00:08,  3.87it/s]Evaluating:  90%|█████████ | 282/313 [01:12<00:08,  3.83it/s]Evaluating:  90%|█████████ | 283/313 [01:13<00:08,  3.51it/s]Evaluating:  91%|█████████ | 284/313 [01:13<00:09,  3.08it/s]Evaluating:  91%|█████████ | 285/313 [01:13<00:08,  3.13it/s]Evaluating:  91%|█████████▏| 286/313 [01:14<00:07,  3.45it/s]Evaluating:  92%|█████████▏| 287/313 [01:14<00:06,  3.73it/s]Evaluating:  92%|█████████▏| 288/313 [01:14<00:06,  3.71it/s]Evaluating:  92%|█████████▏| 289/313 [01:14<00:07,  3.40it/s]Evaluating:  93%|█████████▎| 290/313 [01:15<00:07,  3.00it/s]Evaluating:  93%|█████████▎| 291/313 [01:15<00:07,  3.02it/s]Evaluating:  93%|█████████▎| 292/313 [01:15<00:06,  3.36it/s]Evaluating:  94%|█████████▎| 293/313 [01:16<00:05,  3.65it/s]Evaluating:  94%|█████████▍| 294/313 [01:16<00:05,  3.71it/s]Evaluating:  94%|█████████▍| 295/313 [01:16<00:05,  3.50it/s]Evaluating:  95%|█████████▍| 296/313 [01:17<00:05,  3.05it/s]Evaluating:  95%|█████████▍| 297/313 [01:17<00:05,  3.08it/s]Evaluating:  95%|█████████▌| 298/313 [01:17<00:04,  3.41it/s]Evaluating:  96%|█████████▌| 299/313 [01:17<00:03,  3.70it/s]Evaluating:  96%|█████████▌| 300/313 [01:18<00:03,  3.76it/s]Evaluating:  96%|█████████▌| 301/313 [01:18<00:03,  3.50it/s]Evaluating:  96%|█████████▋| 302/313 [01:18<00:03,  3.06it/s]Evaluating:  97%|█████████▋| 303/313 [01:19<00:03,  2.88it/s]Evaluating:  97%|█████████▋| 304/313 [01:19<00:02,  3.24it/s]Evaluating:  97%|█████████▋| 305/313 [01:19<00:02,  3.57it/s]Evaluating:  98%|█████████▊| 306/313 [01:19<00:01,  3.67it/s]Evaluating:  98%|█████████▊| 307/313 [01:20<00:01,  3.49it/s]Evaluating:  98%|█████████▊| 308/313 [01:20<00:01,  3.07it/s]Evaluating:  99%|█████████▊| 309/313 [01:21<00:01,  2.94it/s]Evaluating:  99%|█████████▉| 310/313 [01:21<00:00,  3.30it/s]Evaluating:  99%|█████████▉| 311/313 [01:21<00:00,  3.63it/s]Evaluating: 100%|█████████▉| 312/313 [01:21<00:00,  3.86it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  4.30it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.83it/s]
10/09/2021 00:23:02 - INFO - __main__ -   ***** Evaluation result  in no *****
10/09/2021 00:23:02 - INFO - __main__ -     f1 = 0.7386275019495712
10/09/2021 00:23:02 - INFO - __main__ -     loss = 0.9852417911203525
10/09/2021 00:23:02 - INFO - __main__ -     precision = 0.694149261023574
10/09/2021 00:23:02 - INFO - __main__ -     recall = 0.7891959450076378
10/09/2021 00:23:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/09/2021 00:23:04 - INFO - __main__ -   ***** Running evaluation  in da *****
10/09/2021 00:23:04 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:23:04 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:04,  4.84it/s]Evaluating:   1%|          | 2/313 [00:00<01:01,  5.03it/s]Evaluating:   1%|          | 3/313 [00:00<01:01,  5.02it/s]Evaluating:   1%|▏         | 4/313 [00:00<01:01,  5.00it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:02,  4.94it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:06,  4.58it/s]Evaluating:   2%|▏         | 7/313 [00:01<01:15,  4.08it/s]Evaluating:   3%|▎         | 8/313 [00:01<01:30,  3.37it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:38,  3.10it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:31,  3.32it/s]Evaluating:   4%|▎         | 11/313 [00:02<01:22,  3.67it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:15,  3.97it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:11,  4.20it/s]Evaluating:   4%|▍         | 14/313 [00:03<01:08,  4.38it/s]Evaluating:   5%|▍         | 15/313 [00:03<01:11,  4.20it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:18,  3.80it/s]Evaluating:   5%|▌         | 17/313 [00:04<01:30,  3.26it/s]Evaluating:   6%|▌         | 18/313 [00:04<01:39,  2.97it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:27,  3.35it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:52,  2.61it/s]Evaluating:   7%|▋         | 21/313 [00:06<01:53,  2.57it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:42,  2.85it/s]Evaluating:   7%|▋         | 23/313 [00:06<01:29,  3.25it/s]Evaluating:   8%|▊         | 24/313 [00:06<01:19,  3.61it/s]Evaluating:   8%|▊         | 25/313 [00:06<01:13,  3.91it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:09,  4.12it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:11,  4.02it/s]Evaluating:   9%|▉         | 28/313 [00:07<01:17,  3.69it/s]Evaluating:   9%|▉         | 29/313 [00:08<01:28,  3.20it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:32,  3.05it/s]Evaluating:  10%|▉         | 31/313 [00:08<01:23,  3.38it/s]Evaluating:  10%|█         | 32/313 [00:08<01:15,  3.71it/s]Evaluating:  11%|█         | 33/313 [00:09<01:10,  3.99it/s]Evaluating:  11%|█         | 34/313 [00:09<01:06,  4.22it/s]Evaluating:  11%|█         | 35/313 [00:09<01:04,  4.29it/s]Evaluating:  12%|█▏        | 36/313 [00:09<01:08,  4.04it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:17,  3.55it/s]Evaluating:  12%|█▏        | 38/313 [00:10<01:27,  3.16it/s]Evaluating:  12%|█▏        | 39/313 [00:10<01:23,  3.29it/s]Evaluating:  13%|█▎        | 40/313 [00:11<01:15,  3.64it/s]Evaluating:  13%|█▎        | 41/313 [00:11<01:09,  3.93it/s]Evaluating:  13%|█▎        | 42/313 [00:11<01:05,  4.17it/s]Evaluating:  14%|█▎        | 43/313 [00:11<01:02,  4.32it/s]Evaluating:  14%|█▍        | 44/313 [00:11<01:05,  4.12it/s]Evaluating:  14%|█▍        | 45/313 [00:12<01:12,  3.72it/s]Evaluating:  15%|█▍        | 46/313 [00:12<01:23,  3.20it/s]Evaluating:  15%|█▌        | 47/313 [00:13<01:29,  2.99it/s]Evaluating:  15%|█▌        | 48/313 [00:13<01:18,  3.37it/s]Evaluating:  16%|█▌        | 49/313 [00:13<01:11,  3.71it/s]Evaluating:  16%|█▌        | 50/313 [00:13<01:05,  3.99it/s]Evaluating:  16%|█▋        | 51/313 [00:13<01:02,  4.20it/s]Evaluating:  17%|█▋        | 52/313 [00:14<01:03,  4.12it/s]Evaluating:  17%|█▋        | 53/313 [00:14<01:08,  3.80it/s]Evaluating:  17%|█▋        | 54/313 [00:14<01:19,  3.25it/s]Evaluating:  18%|█▊        | 55/313 [00:15<01:26,  2.99it/s]Evaluating:  18%|█▊        | 56/313 [00:15<01:16,  3.37it/s]Evaluating:  18%|█▊        | 57/313 [00:15<01:08,  3.71it/s]Evaluating:  19%|█▊        | 58/313 [00:15<01:03,  4.00it/s]Evaluating:  19%|█▉        | 59/313 [00:16<01:00,  4.21it/s]Evaluating:  19%|█▉        | 60/313 [00:16<01:01,  4.14it/s]Evaluating:  19%|█▉        | 61/313 [00:16<01:06,  3.81it/s]Evaluating:  20%|█▉        | 62/313 [00:17<01:17,  3.25it/s]Evaluating:  20%|██        | 63/313 [00:17<01:23,  3.01it/s]Evaluating:  20%|██        | 64/313 [00:17<01:13,  3.39it/s]Evaluating:  21%|██        | 65/313 [00:17<01:06,  3.72it/s]Evaluating:  21%|██        | 66/313 [00:18<01:01,  4.00it/s]Evaluating:  21%|██▏       | 67/313 [00:18<00:58,  4.21it/s]Evaluating:  22%|██▏       | 68/313 [00:18<00:59,  4.09it/s]Evaluating:  22%|██▏       | 69/313 [00:18<01:05,  3.73it/s]Evaluating:  22%|██▏       | 70/313 [00:19<01:15,  3.22it/s]Evaluating:  23%|██▎       | 71/313 [00:19<01:22,  2.95it/s]Evaluating:  23%|██▎       | 72/313 [00:19<01:12,  3.33it/s]Evaluating:  23%|██▎       | 73/313 [00:20<01:05,  3.68it/s]Evaluating:  24%|██▎       | 74/313 [00:20<01:00,  3.96it/s]Evaluating:  24%|██▍       | 75/313 [00:20<00:57,  4.17it/s]Evaluating:  24%|██▍       | 76/313 [00:20<00:58,  4.08it/s]Evaluating:  25%|██▍       | 77/313 [00:21<01:03,  3.74it/s]Evaluating:  25%|██▍       | 78/313 [00:21<01:13,  3.22it/s]Evaluating:  25%|██▌       | 79/313 [00:21<01:16,  3.06it/s]Evaluating:  26%|██▌       | 80/313 [00:22<01:09,  3.37it/s]Evaluating:  26%|██▌       | 81/313 [00:22<01:02,  3.71it/s]Evaluating:  26%|██▌       | 82/313 [00:22<00:57,  3.99it/s]Evaluating:  27%|██▋       | 83/313 [00:22<00:54,  4.20it/s]Evaluating:  27%|██▋       | 84/313 [00:22<00:52,  4.33it/s]Evaluating:  27%|██▋       | 85/313 [00:23<00:53,  4.29it/s]Evaluating:  27%|██▋       | 86/313 [00:23<00:51,  4.41it/s]Evaluating:  28%|██▊       | 87/313 [00:23<00:49,  4.53it/s]Evaluating:  28%|██▊       | 88/313 [00:23<00:48,  4.61it/s]Evaluating:  28%|██▊       | 89/313 [00:24<00:47,  4.68it/s]Evaluating:  29%|██▉       | 90/313 [00:24<00:45,  4.86it/s]Evaluating:  29%|██▉       | 91/313 [00:24<00:44,  5.00it/s]Evaluating:  29%|██▉       | 92/313 [00:24<00:43,  5.10it/s]Evaluating:  30%|██▉       | 93/313 [00:24<00:42,  5.17it/s]Evaluating:  30%|███       | 94/313 [00:24<00:41,  5.23it/s]Evaluating:  30%|███       | 95/313 [00:25<00:41,  5.26it/s]Evaluating:  31%|███       | 96/313 [00:25<00:41,  5.28it/s]Evaluating:  31%|███       | 97/313 [00:25<00:40,  5.32it/s]Evaluating:  31%|███▏      | 98/313 [00:25<00:40,  5.36it/s]Evaluating:  32%|███▏      | 99/313 [00:25<00:39,  5.38it/s]Evaluating:  32%|███▏      | 100/313 [00:26<00:39,  5.38it/s]Evaluating:  32%|███▏      | 101/313 [00:26<00:39,  5.37it/s]Evaluating:  33%|███▎      | 102/313 [00:26<00:39,  5.36it/s]Evaluating:  33%|███▎      | 103/313 [00:26<00:39,  5.36it/s]Evaluating:  33%|███▎      | 104/313 [00:26<00:39,  5.35it/s]Evaluating:  34%|███▎      | 105/313 [00:27<00:38,  5.35it/s]Evaluating:  34%|███▍      | 106/313 [00:27<00:38,  5.35it/s]Evaluating:  34%|███▍      | 107/313 [00:27<00:38,  5.36it/s]Evaluating:  35%|███▍      | 108/313 [00:27<00:38,  5.37it/s]Evaluating:  35%|███▍      | 109/313 [00:27<00:37,  5.37it/s]Evaluating:  35%|███▌      | 110/313 [00:27<00:37,  5.40it/s]Evaluating:  35%|███▌      | 111/313 [00:28<00:37,  5.42it/s]Evaluating:  36%|███▌      | 112/313 [00:28<00:37,  5.42it/s]Evaluating:  36%|███▌      | 113/313 [00:28<00:36,  5.42it/s]Evaluating:  36%|███▋      | 114/313 [00:28<00:36,  5.40it/s]Evaluating:  37%|███▋      | 115/313 [00:28<00:36,  5.38it/s]Evaluating:  37%|███▋      | 116/313 [00:29<00:36,  5.37it/s]Evaluating:  37%|███▋      | 117/313 [00:29<00:36,  5.31it/s]Evaluating:  38%|███▊      | 118/313 [00:29<00:40,  4.81it/s]Evaluating:  38%|███▊      | 119/313 [00:29<00:46,  4.22it/s]Evaluating:  38%|███▊      | 120/313 [00:30<00:47,  4.11it/s]Evaluating:  39%|███▊      | 121/313 [00:30<00:49,  3.84it/s]Evaluating:  39%|███▉      | 122/313 [00:30<00:50,  3.81it/s]Evaluating:  39%|███▉      | 123/313 [00:30<00:52,  3.63it/s]Evaluating:  40%|███▉      | 124/313 [00:31<00:48,  3.89it/s]Evaluating:  40%|███▉      | 125/313 [00:31<00:45,  4.14it/s]Evaluating:  40%|████      | 126/313 [00:31<00:42,  4.43it/s]Evaluating:  41%|████      | 127/313 [00:31<00:39,  4.67it/s]Evaluating:  41%|████      | 128/313 [00:31<00:38,  4.85it/s]Evaluating:  41%|████      | 129/313 [00:32<00:36,  4.98it/s]Evaluating:  42%|████▏     | 130/313 [00:32<00:36,  5.08it/s]Evaluating:  42%|████▏     | 131/313 [00:32<00:35,  5.15it/s]Evaluating:  42%|████▏     | 132/313 [00:32<00:34,  5.22it/s]Evaluating:  42%|████▏     | 133/313 [00:32<00:34,  5.27it/s]Evaluating:  43%|████▎     | 134/313 [00:33<00:33,  5.30it/s]Evaluating:  43%|████▎     | 135/313 [00:33<00:33,  5.32it/s]Evaluating:  43%|████▎     | 136/313 [00:33<00:33,  5.33it/s]Evaluating:  44%|████▍     | 137/313 [00:33<00:33,  5.33it/s]Evaluating:  44%|████▍     | 138/313 [00:33<00:32,  5.32it/s]Evaluating:  44%|████▍     | 139/313 [00:33<00:32,  5.32it/s]Evaluating:  45%|████▍     | 140/313 [00:34<00:32,  5.32it/s]Evaluating:  45%|████▌     | 141/313 [00:34<00:32,  5.32it/s]Evaluating:  45%|████▌     | 142/313 [00:34<00:32,  5.32it/s]Evaluating:  46%|████▌     | 143/313 [00:34<00:31,  5.32it/s]Evaluating:  46%|████▌     | 144/313 [00:34<00:31,  5.33it/s]Evaluating:  46%|████▋     | 145/313 [00:35<00:31,  5.34it/s]Evaluating:  47%|████▋     | 146/313 [00:35<00:31,  5.31it/s]Evaluating:  47%|████▋     | 147/313 [00:35<00:30,  5.40it/s]Evaluating:  47%|████▋     | 148/313 [00:35<00:30,  5.35it/s]Evaluating:  48%|████▊     | 149/313 [00:35<00:31,  5.28it/s]Evaluating:  48%|████▊     | 150/313 [00:36<00:31,  5.18it/s]Evaluating:  48%|████▊     | 151/313 [00:36<00:34,  4.74it/s]Evaluating:  49%|████▊     | 152/313 [00:36<00:38,  4.19it/s]Evaluating:  49%|████▉     | 153/313 [00:37<00:46,  3.44it/s]Evaluating:  49%|████▉     | 154/313 [00:37<00:49,  3.19it/s]Evaluating:  50%|████▉     | 155/313 [00:37<00:45,  3.47it/s]Evaluating:  50%|████▉     | 156/313 [00:37<00:41,  3.79it/s]Evaluating:  50%|█████     | 157/313 [00:38<00:38,  4.04it/s]Evaluating:  50%|█████     | 158/313 [00:38<00:36,  4.23it/s]Evaluating:  51%|█████     | 159/313 [00:38<00:37,  4.10it/s]Evaluating:  51%|█████     | 160/313 [00:38<00:41,  3.73it/s]Evaluating:  51%|█████▏    | 161/313 [00:39<00:47,  3.21it/s]Evaluating:  52%|█████▏    | 162/313 [00:39<00:48,  3.12it/s]Evaluating:  52%|█████▏    | 163/313 [00:39<00:43,  3.47it/s]Evaluating:  52%|█████▏    | 164/313 [00:39<00:39,  3.78it/s]Evaluating:  53%|█████▎    | 165/313 [00:40<00:39,  3.73it/s]Evaluating:  53%|█████▎    | 166/313 [00:40<00:40,  3.61it/s]Evaluating:  53%|█████▎    | 167/313 [00:40<00:45,  3.18it/s]Evaluating:  54%|█████▎    | 168/313 [00:41<00:49,  2.91it/s]Evaluating:  54%|█████▍    | 169/313 [00:41<00:43,  3.29it/s]Evaluating:  54%|█████▍    | 170/313 [00:41<00:39,  3.62it/s]Evaluating:  55%|█████▍    | 171/313 [00:42<00:36,  3.90it/s]Evaluating:  55%|█████▍    | 172/313 [00:42<00:34,  4.07it/s]Evaluating:  55%|█████▌    | 173/313 [00:42<00:36,  3.89it/s]Evaluating:  56%|█████▌    | 174/313 [00:42<00:40,  3.46it/s]Evaluating:  56%|█████▌    | 175/313 [00:43<00:44,  3.07it/s]Evaluating:  56%|█████▌    | 176/313 [00:43<00:43,  3.15it/s]Evaluating:  57%|█████▋    | 177/313 [00:43<00:38,  3.52it/s]Evaluating:  57%|█████▋    | 178/313 [00:44<00:35,  3.83it/s]Evaluating:  57%|█████▋    | 179/313 [00:44<00:33,  4.06it/s]Evaluating:  58%|█████▊    | 180/313 [00:44<00:32,  4.04it/s]Evaluating:  58%|█████▊    | 181/313 [00:44<00:35,  3.75it/s]Evaluating:  58%|█████▊    | 182/313 [00:45<00:40,  3.23it/s]Evaluating:  58%|█████▊    | 183/313 [00:45<00:42,  3.07it/s]Evaluating:  59%|█████▉    | 184/313 [00:45<00:38,  3.38it/s]Evaluating:  59%|█████▉    | 185/313 [00:45<00:34,  3.70it/s]Evaluating:  59%|█████▉    | 186/313 [00:46<00:32,  3.96it/s]Evaluating:  60%|█████▉    | 187/313 [00:46<00:30,  4.09it/s]Evaluating:  60%|██████    | 188/313 [00:46<00:32,  3.83it/s]Evaluating:  60%|██████    | 189/313 [00:47<00:36,  3.41it/s]Evaluating:  61%|██████    | 190/313 [00:47<00:40,  3.04it/s]Evaluating:  61%|██████    | 191/313 [00:47<00:39,  3.08it/s]Evaluating:  61%|██████▏   | 192/313 [00:48<00:35,  3.45it/s]Evaluating:  62%|██████▏   | 193/313 [00:48<00:31,  3.76it/s]Evaluating:  62%|██████▏   | 194/313 [00:48<00:29,  4.02it/s]Evaluating:  62%|██████▏   | 195/313 [00:48<00:29,  3.95it/s]Evaluating:  63%|██████▎   | 196/313 [00:49<00:32,  3.63it/s]Evaluating:  63%|██████▎   | 197/313 [00:49<00:36,  3.15it/s]Evaluating:  63%|██████▎   | 198/313 [00:49<00:38,  3.02it/s]Evaluating:  64%|██████▎   | 199/313 [00:50<00:34,  3.33it/s]Evaluating:  64%|██████▍   | 200/313 [00:50<00:30,  3.66it/s]Evaluating:  64%|██████▍   | 201/313 [00:50<00:28,  3.93it/s]Evaluating:  65%|██████▍   | 202/313 [00:50<00:28,  3.92it/s]Evaluating:  65%|██████▍   | 203/313 [00:51<00:30,  3.64it/s]Evaluating:  65%|██████▌   | 204/313 [00:51<00:34,  3.15it/s]Evaluating:  65%|██████▌   | 205/313 [00:51<00:35,  3.07it/s]Evaluating:  66%|██████▌   | 206/313 [00:52<00:31,  3.43it/s]Evaluating:  66%|██████▌   | 207/313 [00:52<00:28,  3.74it/s]Evaluating:  66%|██████▋   | 208/313 [00:52<00:26,  3.97it/s]Evaluating:  67%|██████▋   | 209/313 [00:52<00:26,  3.93it/s]Evaluating:  67%|██████▋   | 210/313 [00:53<00:28,  3.64it/s]Evaluating:  67%|██████▋   | 211/313 [00:53<00:32,  3.16it/s]Evaluating:  68%|██████▊   | 212/313 [00:53<00:33,  3.05it/s]Evaluating:  68%|██████▊   | 213/313 [00:54<00:29,  3.41it/s]Evaluating:  68%|██████▊   | 214/313 [00:54<00:26,  3.72it/s]Evaluating:  69%|██████▊   | 215/313 [00:54<00:24,  3.98it/s]Evaluating:  69%|██████▉   | 216/313 [00:54<00:23,  4.11it/s]Evaluating:  69%|██████▉   | 217/313 [00:54<00:24,  3.87it/s]Evaluating:  70%|██████▉   | 218/313 [00:55<00:28,  3.39it/s]Evaluating:  70%|██████▉   | 219/313 [00:55<00:30,  3.10it/s]Evaluating:  70%|███████   | 220/313 [00:55<00:28,  3.30it/s]Evaluating:  71%|███████   | 221/313 [00:56<00:25,  3.65it/s]Evaluating:  71%|███████   | 222/313 [00:56<00:23,  3.93it/s]Evaluating:  71%|███████   | 223/313 [00:56<00:22,  4.07it/s]Evaluating:  72%|███████▏  | 224/313 [00:56<00:23,  3.85it/s]Evaluating:  72%|███████▏  | 225/313 [00:57<00:25,  3.43it/s]Evaluating:  72%|███████▏  | 226/313 [00:57<00:28,  3.05it/s]Evaluating:  73%|███████▎  | 227/313 [00:57<00:27,  3.17it/s]Evaluating:  73%|███████▎  | 228/313 [00:58<00:24,  3.52it/s]Evaluating:  73%|███████▎  | 229/313 [00:58<00:22,  3.81it/s]Evaluating:  73%|███████▎  | 230/313 [00:58<00:20,  3.98it/s]Evaluating:  74%|███████▍  | 231/313 [00:58<00:21,  3.80it/s]Evaluating:  74%|███████▍  | 232/313 [00:59<00:23,  3.39it/s]Evaluating:  74%|███████▍  | 233/313 [00:59<00:26,  3.03it/s]Evaluating:  75%|███████▍  | 234/313 [01:00<00:25,  3.11it/s]Evaluating:  75%|███████▌  | 235/313 [01:00<00:22,  3.46it/s]Evaluating:  75%|███████▌  | 236/313 [01:00<00:20,  3.76it/s]Evaluating:  76%|███████▌  | 237/313 [01:00<00:19,  3.96it/s]Evaluating:  76%|███████▌  | 238/313 [01:00<00:19,  3.82it/s]Evaluating:  76%|███████▋  | 239/313 [01:01<00:21,  3.43it/s]Evaluating:  77%|███████▋  | 240/313 [01:01<00:23,  3.05it/s]Evaluating:  77%|███████▋  | 241/313 [01:02<00:23,  3.06it/s]Evaluating:  77%|███████▋  | 242/313 [01:02<00:20,  3.42it/s]Evaluating:  78%|███████▊  | 243/313 [01:02<00:18,  3.73it/s]Evaluating:  78%|███████▊  | 244/313 [01:02<00:17,  3.95it/s]Evaluating:  78%|███████▊  | 245/313 [01:02<00:17,  3.81it/s]Evaluating:  79%|███████▊  | 246/313 [01:03<00:19,  3.44it/s]Evaluating:  79%|███████▉  | 247/313 [01:03<00:21,  3.05it/s]Evaluating:  79%|███████▉  | 248/313 [01:04<00:21,  3.08it/s]Evaluating:  80%|███████▉  | 249/313 [01:04<00:18,  3.44it/s]Evaluating:  80%|███████▉  | 250/313 [01:04<00:16,  3.75it/s]Evaluating:  80%|████████  | 251/313 [01:04<00:15,  3.96it/s]Evaluating:  81%|████████  | 252/313 [01:04<00:15,  3.84it/s]Evaluating:  81%|████████  | 253/313 [01:05<00:17,  3.43it/s]Evaluating:  81%|████████  | 254/313 [01:05<00:20,  2.91it/s]Evaluating:  81%|████████▏ | 255/313 [01:06<00:17,  3.28it/s]Evaluating:  82%|████████▏ | 256/313 [01:06<00:15,  3.63it/s]Evaluating:  82%|████████▏ | 257/313 [01:06<00:14,  3.86it/s]Evaluating:  82%|████████▏ | 258/313 [01:06<00:14,  3.77it/s]Evaluating:  83%|████████▎ | 259/313 [01:07<00:15,  3.41it/s]Evaluating:  83%|████████▎ | 260/313 [01:07<00:17,  3.06it/s]Evaluating:  83%|████████▎ | 261/313 [01:07<00:16,  3.20it/s]Evaluating:  84%|████████▎ | 262/313 [01:07<00:14,  3.53it/s]Evaluating:  84%|████████▍ | 263/313 [01:08<00:13,  3.82it/s]Evaluating:  84%|████████▍ | 264/313 [01:08<00:12,  3.99it/s]Evaluating:  85%|████████▍ | 265/313 [01:08<00:12,  3.77it/s]Evaluating:  85%|████████▍ | 266/313 [01:09<00:14,  3.34it/s]Evaluating:  85%|████████▌ | 267/313 [01:09<00:15,  3.01it/s]Evaluating:  86%|████████▌ | 268/313 [01:09<00:14,  3.13it/s]Evaluating:  86%|████████▌ | 269/313 [01:09<00:12,  3.48it/s]Evaluating:  86%|████████▋ | 270/313 [01:10<00:11,  3.78it/s]Evaluating:  87%|████████▋ | 271/313 [01:10<00:10,  3.95it/s]Evaluating:  87%|████████▋ | 272/313 [01:10<00:10,  3.78it/s]Evaluating:  87%|████████▋ | 273/313 [01:11<00:11,  3.36it/s]Evaluating:  88%|████████▊ | 274/313 [01:11<00:12,  3.05it/s]Evaluating:  88%|████████▊ | 275/313 [01:11<00:11,  3.23it/s]Evaluating:  88%|████████▊ | 276/313 [01:11<00:10,  3.57it/s]Evaluating:  88%|████████▊ | 277/313 [01:12<00:09,  3.84it/s]Evaluating:  89%|████████▉ | 278/313 [01:12<00:08,  3.98it/s]Evaluating:  89%|████████▉ | 279/313 [01:12<00:09,  3.73it/s]Evaluating:  89%|████████▉ | 280/313 [01:13<00:10,  3.29it/s]Evaluating:  90%|████████▉ | 281/313 [01:13<00:10,  3.01it/s]Evaluating:  90%|█████████ | 282/313 [01:13<00:09,  3.20it/s]Evaluating:  90%|█████████ | 283/313 [01:13<00:08,  3.53it/s]Evaluating:  91%|█████████ | 284/313 [01:14<00:07,  3.81it/s]Evaluating:  91%|█████████ | 285/313 [01:14<00:07,  3.95it/s]Evaluating:  91%|█████████▏| 286/313 [01:14<00:07,  3.70it/s]Evaluating:  92%|█████████▏| 287/313 [01:15<00:07,  3.27it/s]Evaluating:  92%|█████████▏| 288/313 [01:15<00:08,  2.94it/s]Evaluating:  92%|█████████▏| 289/313 [01:15<00:07,  3.23it/s]Evaluating:  93%|█████████▎| 290/313 [01:16<00:06,  3.56it/s]Evaluating:  93%|█████████▎| 291/313 [01:16<00:05,  3.83it/s]Evaluating:  93%|█████████▎| 292/313 [01:16<00:05,  3.95it/s]Evaluating:  94%|█████████▎| 293/313 [01:16<00:05,  3.62it/s]Evaluating:  94%|█████████▍| 294/313 [01:17<00:05,  3.18it/s]Evaluating:  94%|█████████▍| 295/313 [01:17<00:06,  2.88it/s]Evaluating:  95%|█████████▍| 296/313 [01:17<00:05,  3.19it/s]Evaluating:  95%|█████████▍| 297/313 [01:18<00:04,  3.50it/s]Evaluating:  95%|█████████▌| 298/313 [01:18<00:04,  3.69it/s]Evaluating:  96%|█████████▌| 299/313 [01:18<00:03,  3.53it/s]Evaluating:  96%|█████████▌| 300/313 [01:19<00:04,  3.10it/s]Evaluating:  96%|█████████▌| 301/313 [01:19<00:04,  2.92it/s]Evaluating:  96%|█████████▋| 302/313 [01:19<00:03,  3.20it/s]Evaluating:  97%|█████████▋| 303/313 [01:19<00:02,  3.52it/s]Evaluating:  97%|█████████▋| 304/313 [01:20<00:02,  3.75it/s]Evaluating:  97%|█████████▋| 305/313 [01:20<00:02,  3.19it/s]Evaluating:  98%|█████████▊| 306/313 [01:20<00:02,  2.89it/s]Evaluating:  98%|█████████▊| 307/313 [01:21<00:02,  2.89it/s]Evaluating:  98%|█████████▊| 308/313 [01:21<00:01,  3.25it/s]Evaluating:  99%|█████████▊| 309/313 [01:21<00:01,  3.55it/s]Evaluating:  99%|█████████▉| 310/313 [01:22<00:00,  3.56it/s]Evaluating:  99%|█████████▉| 311/313 [01:22<00:00,  3.28it/s]Evaluating: 100%|█████████▉| 312/313 [01:22<00:00,  2.95it/s]Evaluating: 100%|██████████| 313/313 [01:22<00:00,  3.39it/s]Evaluating: 100%|██████████| 313/313 [01:22<00:00,  3.77it/s]
10/09/2021 00:24:28 - INFO - __main__ -   ***** Evaluation result  in da *****
10/09/2021 00:24:28 - INFO - __main__ -     f1 = 0.7941665511985588
10/09/2021 00:24:28 - INFO - __main__ -     loss = 0.8323360935758097
10/09/2021 00:24:28 - INFO - __main__ -     precision = 0.7556361239288069
10/09/2021 00:24:28 - INFO - __main__ -     recall = 0.8368374945247481
10/09/2021 00:24:28 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:24:41 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:24:41 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:24:58 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:25:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:25:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:25:00 - INFO - __main__ -   Seed = 42
10/09/2021 00:25:00 - INFO - root -   save model
10/09/2021 00:25:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:25:00 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:25:14 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:25:14 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:25:14 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/09/2021 00:25:14 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:25:14 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:25:14 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/09/2021 00:25:14 - INFO - __main__ -   Language = en
10/09/2021 00:25:14 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:25:15 - INFO - __main__ -   Language = is
10/09/2021 00:25:15 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/09/2021 00:25:17 - INFO - __main__ -   Language = de
10/09/2021 00:25:17 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/09/2021 00:25:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/09/2021 00:25:22 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/09/2021 00:25:22 - INFO - __main__ -     Num examples = 100
10/09/2021 00:25:22 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  5.93it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  5.94it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  5.96it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  7.51it/s]
10/09/2021 00:25:22 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/09/2021 00:25:22 - INFO - __main__ -     f1 = 0.5971223021582734
10/09/2021 00:25:22 - INFO - __main__ -     loss = 2.087847262620926
10/09/2021 00:25:22 - INFO - __main__ -     precision = 0.5253164556962026
10/09/2021 00:25:22 - INFO - __main__ -     recall = 0.6916666666666667
10/09/2021 00:25:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/09/2021 00:25:24 - INFO - __main__ -   ***** Running evaluation  in no *****
10/09/2021 00:25:24 - INFO - __main__ -     Num examples = 10000
10/09/2021 00:25:24 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.01it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  5.99it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:51,  5.98it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:51,  5.97it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:51,  5.95it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:51,  5.95it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:51,  5.95it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:51,  5.95it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:51,  5.93it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:51,  5.89it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:51,  5.84it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:51,  5.80it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:51,  5.80it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:53,  5.53it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:59,  5.00it/s]Evaluating:   5%|▌         | 17/313 [00:03<01:08,  4.33it/s]Evaluating:   6%|▌         | 18/313 [00:03<01:21,  3.62it/s]Evaluating:   6%|▌         | 19/313 [00:03<01:24,  3.48it/s]Evaluating:   6%|▋         | 20/313 [00:04<01:16,  3.85it/s]Evaluating:   7%|▋         | 21/313 [00:04<01:10,  4.15it/s]Evaluating:   7%|▋         | 22/313 [00:04<01:06,  4.40it/s]Evaluating:   7%|▋         | 23/313 [00:04<01:03,  4.58it/s]Evaluating:   8%|▊         | 24/313 [00:04<01:02,  4.65it/s]Evaluating:   8%|▊         | 25/313 [00:05<01:05,  4.42it/s]Evaluating:   8%|▊         | 26/313 [00:05<01:13,  3.91it/s]Evaluating:   9%|▊         | 27/313 [00:05<01:22,  3.45it/s]Evaluating:   9%|▉         | 28/313 [00:06<01:18,  3.62it/s]Evaluating:   9%|▉         | 29/313 [00:06<01:12,  3.94it/s]Evaluating:  10%|▉         | 30/313 [00:06<01:07,  4.20it/s]Evaluating:  10%|▉         | 31/313 [00:06<01:04,  4.39it/s]Evaluating:  10%|█         | 32/313 [00:06<01:02,  4.53it/s]Evaluating:  11%|█         | 33/313 [00:07<01:04,  4.32it/s]Evaluating:  11%|█         | 34/313 [00:07<01:11,  3.89it/s]Evaluating:  11%|█         | 35/313 [00:07<01:23,  3.32it/s]Evaluating:  12%|█▏        | 36/313 [00:08<01:31,  3.02it/s]Evaluating:  12%|█▏        | 37/313 [00:08<01:21,  3.41it/s]Evaluating:  12%|█▏        | 38/313 [00:08<01:13,  3.74it/s]Evaluating:  12%|█▏        | 39/313 [00:08<01:08,  4.02it/s]Evaluating:  13%|█▎        | 40/313 [00:09<01:04,  4.24it/s]Evaluating:  13%|█▎        | 41/313 [00:09<01:05,  4.16it/s]Evaluating:  13%|█▎        | 42/313 [00:09<01:11,  3.81it/s]Evaluating:  14%|█▎        | 43/313 [00:09<01:22,  3.27it/s]Evaluating:  14%|█▍        | 44/313 [00:10<01:30,  2.96it/s]Evaluating:  14%|█▍        | 45/313 [00:10<01:20,  3.31it/s]Evaluating:  15%|█▍        | 46/313 [00:10<01:12,  3.66it/s]Evaluating:  15%|█▌        | 47/313 [00:11<01:07,  3.95it/s]Evaluating:  15%|█▌        | 48/313 [00:11<01:03,  4.19it/s]Evaluating:  16%|█▌        | 49/313 [00:11<01:01,  4.29it/s]Evaluating:  16%|█▌        | 50/313 [00:11<01:05,  4.04it/s]Evaluating:  16%|█▋        | 51/313 [00:12<01:13,  3.57it/s]Evaluating:  17%|█▋        | 52/313 [00:12<01:22,  3.16it/s]Evaluating:  17%|█▋        | 53/313 [00:12<01:20,  3.25it/s]Evaluating:  17%|█▋        | 54/313 [00:12<01:11,  3.61it/s]Evaluating:  18%|█▊        | 55/313 [00:13<01:06,  3.91it/s]Evaluating:  18%|█▊        | 56/313 [00:13<01:01,  4.15it/s]Evaluating:  18%|█▊        | 57/313 [00:13<00:59,  4.30it/s]Evaluating:  19%|█▊        | 58/313 [00:13<01:02,  4.08it/s]Evaluating:  19%|█▉        | 59/313 [00:14<01:09,  3.65it/s]Evaluating:  19%|█▉        | 60/313 [00:14<01:19,  3.18it/s]Evaluating:  19%|█▉        | 61/313 [00:15<01:23,  3.02it/s]Evaluating:  20%|█▉        | 62/313 [00:15<01:13,  3.40it/s]Evaluating:  20%|██        | 63/313 [00:15<01:06,  3.74it/s]Evaluating:  20%|██        | 64/313 [00:15<01:01,  4.03it/s]Evaluating:  21%|██        | 65/313 [00:15<00:58,  4.23it/s]Evaluating:  21%|██        | 66/313 [00:16<00:59,  4.14it/s]Evaluating:  21%|██▏       | 67/313 [00:16<01:04,  3.82it/s]Evaluating:  22%|██▏       | 68/313 [00:16<01:14,  3.27it/s]Evaluating:  22%|██▏       | 69/313 [00:17<01:19,  3.05it/s]Evaluating:  22%|██▏       | 70/313 [00:17<01:13,  3.31it/s]Evaluating:  23%|██▎       | 71/313 [00:17<01:06,  3.66it/s]Evaluating:  23%|██▎       | 72/313 [00:17<01:01,  3.95it/s]Evaluating:  23%|██▎       | 73/313 [00:18<00:57,  4.19it/s]Evaluating:  24%|██▎       | 74/313 [00:18<00:55,  4.29it/s]Evaluating:  24%|██▍       | 75/313 [00:18<00:58,  4.04it/s]Evaluating:  24%|██▍       | 76/313 [00:18<01:06,  3.55it/s]Evaluating:  25%|██▍       | 77/313 [00:19<01:14,  3.16it/s]Evaluating:  25%|██▍       | 78/313 [00:19<01:11,  3.27it/s]Evaluating:  25%|██▌       | 79/313 [00:19<01:04,  3.62it/s]Evaluating:  26%|██▌       | 80/313 [00:19<00:59,  3.92it/s]Evaluating:  26%|██▌       | 81/313 [00:20<00:55,  4.15it/s]Evaluating:  26%|██▌       | 82/313 [00:20<00:54,  4.25it/s]Evaluating:  27%|██▋       | 83/313 [00:20<00:57,  3.97it/s]Evaluating:  27%|██▋       | 84/313 [00:21<01:05,  3.51it/s]Evaluating:  27%|██▋       | 85/313 [00:21<01:13,  3.11it/s]Evaluating:  27%|██▋       | 86/313 [00:21<01:12,  3.13it/s]Evaluating:  28%|██▊       | 87/313 [00:22<01:04,  3.50it/s]Evaluating:  28%|██▊       | 88/313 [00:22<00:59,  3.81it/s]Evaluating:  28%|██▊       | 89/313 [00:22<00:55,  4.07it/s]Evaluating:  29%|██▉       | 90/313 [00:22<00:53,  4.16it/s]Evaluating:  29%|██▉       | 91/313 [00:22<00:56,  3.90it/s]Evaluating:  29%|██▉       | 92/313 [00:23<01:03,  3.46it/s]Evaluating:  30%|██▉       | 93/313 [00:23<01:11,  3.08it/s]Evaluating:  30%|███       | 94/313 [00:24<01:09,  3.13it/s]Evaluating:  30%|███       | 95/313 [00:24<01:02,  3.50it/s]Evaluating:  31%|███       | 96/313 [00:24<00:56,  3.82it/s]Evaluating:  31%|███       | 97/313 [00:24<00:53,  4.07it/s]Evaluating:  31%|███▏      | 98/313 [00:24<00:50,  4.22it/s]Evaluating:  32%|███▏      | 99/313 [00:25<00:53,  4.00it/s]Evaluating:  32%|███▏      | 100/313 [00:25<00:59,  3.56it/s]Evaluating:  32%|███▏      | 101/313 [00:25<01:07,  3.14it/s]Evaluating:  33%|███▎      | 102/313 [00:26<01:08,  3.09it/s]Evaluating:  33%|███▎      | 103/313 [00:26<01:00,  3.47it/s]Evaluating:  33%|███▎      | 104/313 [00:26<00:55,  3.78it/s]Evaluating:  34%|███▎      | 105/313 [00:26<00:51,  4.04it/s]Evaluating:  34%|███▍      | 106/313 [00:27<00:49,  4.18it/s]Evaluating:  34%|███▍      | 107/313 [00:27<00:51,  3.97it/s]Evaluating:  35%|███▍      | 108/313 [00:27<00:57,  3.54it/s]Evaluating:  35%|███▍      | 109/313 [00:28<01:05,  3.12it/s]Evaluating:  35%|███▌      | 110/313 [00:28<01:04,  3.13it/s]Evaluating:  35%|███▌      | 111/313 [00:28<00:57,  3.50it/s]Evaluating:  36%|███▌      | 112/313 [00:28<00:52,  3.81it/s]Evaluating:  36%|███▌      | 113/313 [00:29<00:49,  4.06it/s]Evaluating:  36%|███▋      | 114/313 [00:29<00:47,  4.19it/s]Evaluating:  37%|███▋      | 115/313 [00:29<00:49,  3.97it/s]Evaluating:  37%|███▋      | 116/313 [00:29<00:55,  3.53it/s]Evaluating:  37%|███▋      | 117/313 [00:30<01:02,  3.11it/s]Evaluating:  38%|███▊      | 118/313 [00:30<01:02,  3.11it/s]Evaluating:  38%|███▊      | 119/313 [00:30<00:55,  3.48it/s]Evaluating:  38%|███▊      | 120/313 [00:31<00:50,  3.79it/s]Evaluating:  39%|███▊      | 121/313 [00:31<00:47,  4.05it/s]Evaluating:  39%|███▉      | 122/313 [00:31<00:45,  4.21it/s]Evaluating:  39%|███▉      | 123/313 [00:31<00:47,  4.01it/s]Evaluating:  40%|███▉      | 124/313 [00:32<00:53,  3.56it/s]Evaluating:  40%|███▉      | 125/313 [00:32<01:00,  3.13it/s]Evaluating:  40%|████      | 126/313 [00:32<01:00,  3.11it/s]Evaluating:  41%|████      | 127/313 [00:33<00:53,  3.48it/s]Evaluating:  41%|████      | 128/313 [00:33<00:48,  3.79it/s]Evaluating:  41%|████      | 129/313 [00:33<00:45,  4.04it/s]Evaluating:  42%|████▏     | 130/313 [00:33<00:44,  4.13it/s]Evaluating:  42%|████▏     | 131/313 [00:34<00:47,  3.83it/s]Evaluating:  42%|████▏     | 132/313 [00:34<00:53,  3.39it/s]Evaluating:  42%|████▏     | 133/313 [00:34<00:59,  3.03it/s]Evaluating:  43%|████▎     | 134/313 [00:35<00:56,  3.16it/s]Evaluating:  43%|████▎     | 135/313 [00:35<00:50,  3.52it/s]Evaluating:  43%|████▎     | 136/313 [00:35<00:46,  3.82it/s]Evaluating:  44%|████▍     | 137/313 [00:35<00:43,  4.06it/s]Evaluating:  44%|████▍     | 138/313 [00:35<00:42,  4.07it/s]Evaluating:  44%|████▍     | 139/313 [00:36<00:46,  3.76it/s]Evaluating:  45%|████▍     | 140/313 [00:36<00:53,  3.23it/s]Evaluating:  45%|████▌     | 141/313 [00:37<00:58,  2.95it/s]Evaluating:  45%|████▌     | 142/313 [00:37<00:51,  3.33it/s]Evaluating:  46%|████▌     | 143/313 [00:37<00:46,  3.68it/s]Evaluating:  46%|████▌     | 144/313 [00:37<00:42,  3.97it/s]Evaluating:  46%|████▋     | 145/313 [00:37<00:42,  3.97it/s]Evaluating:  47%|████▋     | 146/313 [00:38<00:45,  3.71it/s]Evaluating:  47%|████▋     | 147/313 [00:38<00:51,  3.20it/s]Evaluating:  47%|████▋     | 148/313 [00:39<00:54,  3.02it/s]Evaluating:  48%|████▊     | 149/313 [00:39<00:49,  3.30it/s]Evaluating:  48%|████▊     | 150/313 [00:39<00:44,  3.63it/s]Evaluating:  48%|████▊     | 151/313 [00:39<00:41,  3.91it/s]Evaluating:  49%|████▊     | 152/313 [00:39<00:39,  4.13it/s]Evaluating:  49%|████▉     | 153/313 [00:40<00:39,  4.04it/s]Evaluating:  49%|████▉     | 154/313 [00:40<00:42,  3.70it/s]Evaluating:  50%|████▉     | 155/313 [00:40<00:49,  3.20it/s]Evaluating:  50%|████▉     | 156/313 [00:41<00:53,  2.91it/s]Evaluating:  50%|█████     | 157/313 [00:41<00:52,  2.99it/s]Evaluating:  50%|█████     | 158/313 [00:42<00:55,  2.79it/s]Evaluating:  51%|█████     | 159/313 [00:42<00:57,  2.67it/s]Evaluating:  51%|█████     | 160/313 [00:42<00:49,  3.07it/s]Evaluating:  51%|█████▏    | 161/313 [00:42<00:44,  3.43it/s]Evaluating:  52%|█████▏    | 162/313 [00:43<00:41,  3.67it/s]Evaluating:  52%|█████▏    | 163/313 [00:43<00:42,  3.54it/s]Evaluating:  52%|█████▏    | 164/313 [00:43<00:46,  3.23it/s]Evaluating:  53%|█████▎    | 165/313 [00:44<00:50,  2.92it/s]Evaluating:  53%|█████▎    | 166/313 [00:44<00:47,  3.07it/s]Evaluating:  53%|█████▎    | 167/313 [00:44<00:42,  3.42it/s]Evaluating:  54%|█████▎    | 168/313 [00:44<00:38,  3.74it/s]Evaluating:  54%|█████▍    | 169/313 [00:45<00:36,  3.92it/s]Evaluating:  54%|█████▍    | 170/313 [00:45<00:38,  3.71it/s]Evaluating:  55%|█████▍    | 171/313 [00:45<00:42,  3.33it/s]Evaluating:  55%|█████▍    | 172/313 [00:46<00:47,  3.00it/s]Evaluating:  55%|█████▌    | 173/313 [00:46<00:45,  3.09it/s]Evaluating:  56%|█████▌    | 174/313 [00:46<00:40,  3.45it/s]Evaluating:  56%|█████▌    | 175/313 [00:46<00:36,  3.76it/s]Evaluating:  56%|█████▌    | 176/313 [00:47<00:34,  4.00it/s]Evaluating:  57%|█████▋    | 177/313 [00:47<00:34,  3.92it/s]Evaluating:  57%|█████▋    | 178/313 [00:47<00:37,  3.56it/s]Evaluating:  57%|█████▋    | 179/313 [00:48<00:43,  3.11it/s]Evaluating:  58%|█████▊    | 180/313 [00:48<00:44,  3.02it/s]Evaluating:  58%|█████▊    | 181/313 [00:48<00:38,  3.39it/s]Evaluating:  58%|█████▊    | 182/313 [00:48<00:35,  3.71it/s]Evaluating:  58%|█████▊    | 183/313 [00:49<00:32,  3.97it/s]Evaluating:  59%|█████▉    | 184/313 [00:49<00:32,  3.94it/s]Evaluating:  59%|█████▉    | 185/313 [00:49<00:34,  3.66it/s]Evaluating:  59%|█████▉    | 186/313 [00:50<00:40,  3.17it/s]Evaluating:  60%|█████▉    | 187/313 [00:50<00:41,  3.06it/s]Evaluating:  60%|██████    | 188/313 [00:50<00:36,  3.42it/s]Evaluating:  60%|██████    | 189/313 [00:50<00:33,  3.73it/s]Evaluating:  61%|██████    | 190/313 [00:51<00:30,  3.99it/s]Evaluating:  61%|██████    | 191/313 [00:51<00:29,  4.12it/s]Evaluating:  61%|██████▏   | 192/313 [00:51<00:31,  3.88it/s]Evaluating:  62%|██████▏   | 193/313 [00:52<00:35,  3.42it/s]Evaluating:  62%|██████▏   | 194/313 [00:52<00:38,  3.07it/s]Evaluating:  62%|██████▏   | 195/313 [00:52<00:36,  3.20it/s]Evaluating:  63%|██████▎   | 196/313 [00:52<00:32,  3.55it/s]Evaluating:  63%|██████▎   | 197/313 [00:53<00:30,  3.85it/s]Evaluating:  63%|██████▎   | 198/313 [00:53<00:28,  4.08it/s]Evaluating:  64%|██████▎   | 199/313 [00:53<00:28,  4.01it/s]Evaluating:  64%|██████▍   | 200/313 [00:53<00:31,  3.62it/s]Evaluating:  64%|██████▍   | 201/313 [00:54<00:35,  3.13it/s]Evaluating:  65%|██████▍   | 202/313 [00:54<00:36,  3.05it/s]Evaluating:  65%|██████▍   | 203/313 [00:54<00:32,  3.42it/s]Evaluating:  65%|██████▌   | 204/313 [00:55<00:29,  3.74it/s]Evaluating:  65%|██████▌   | 205/313 [00:55<00:27,  3.98it/s]Evaluating:  66%|██████▌   | 206/313 [00:55<00:27,  3.89it/s]Evaluating:  66%|██████▌   | 207/313 [00:55<00:29,  3.57it/s]Evaluating:  66%|██████▋   | 208/313 [00:56<00:33,  3.12it/s]Evaluating:  67%|██████▋   | 209/313 [00:56<00:34,  3.04it/s]Evaluating:  67%|██████▋   | 210/313 [00:56<00:30,  3.41it/s]Evaluating:  67%|██████▋   | 211/313 [00:57<00:27,  3.72it/s]Evaluating:  68%|██████▊   | 212/313 [00:57<00:25,  3.98it/s]Evaluating:  68%|██████▊   | 213/313 [00:57<00:25,  3.88it/s]Evaluating:  68%|██████▊   | 214/313 [00:57<00:28,  3.53it/s]Evaluating:  69%|██████▊   | 215/313 [00:58<00:31,  3.10it/s]Evaluating:  69%|██████▉   | 216/313 [00:58<00:32,  2.98it/s]Evaluating:  69%|██████▉   | 217/313 [00:58<00:28,  3.35it/s]Evaluating:  70%|██████▉   | 218/313 [00:59<00:25,  3.68it/s]Evaluating:  70%|██████▉   | 219/313 [00:59<00:23,  3.96it/s]Evaluating:  70%|███████   | 220/313 [00:59<00:23,  3.95it/s]Evaluating:  71%|███████   | 221/313 [00:59<00:25,  3.68it/s]Evaluating:  71%|███████   | 222/313 [01:00<00:28,  3.18it/s]Evaluating:  71%|███████   | 223/313 [01:00<00:28,  3.14it/s]Evaluating:  72%|███████▏  | 224/313 [01:00<00:25,  3.49it/s]Evaluating:  72%|███████▏  | 225/313 [01:01<00:23,  3.79it/s]Evaluating:  72%|███████▏  | 226/313 [01:01<00:21,  4.04it/s]Evaluating:  73%|███████▎  | 227/313 [01:01<00:21,  4.04it/s]Evaluating:  73%|███████▎  | 228/313 [01:01<00:22,  3.75it/s]Evaluating:  73%|███████▎  | 229/313 [01:02<00:26,  3.22it/s]Evaluating:  73%|███████▎  | 230/313 [01:02<00:27,  3.06it/s]Evaluating:  74%|███████▍  | 231/313 [01:02<00:24,  3.38it/s]Evaluating:  74%|███████▍  | 232/313 [01:03<00:21,  3.70it/s]Evaluating:  74%|███████▍  | 233/313 [01:03<00:20,  3.98it/s]Evaluating:  75%|███████▍  | 234/313 [01:03<00:19,  4.08it/s]Evaluating:  75%|███████▌  | 235/313 [01:03<00:20,  3.77it/s]Evaluating:  75%|███████▌  | 236/313 [01:04<00:23,  3.33it/s]Evaluating:  76%|███████▌  | 237/313 [01:04<00:25,  2.99it/s]Evaluating:  76%|███████▌  | 238/313 [01:04<00:23,  3.15it/s]Evaluating:  76%|███████▋  | 239/313 [01:05<00:21,  3.51it/s]Evaluating:  77%|███████▋  | 240/313 [01:05<00:19,  3.83it/s]Evaluating:  77%|███████▋  | 241/313 [01:05<00:17,  4.04it/s]Evaluating:  77%|███████▋  | 242/313 [01:05<00:18,  3.93it/s]Evaluating:  78%|███████▊  | 243/313 [01:06<00:19,  3.52it/s]Evaluating:  78%|███████▊  | 244/313 [01:06<00:21,  3.15it/s]Evaluating:  78%|███████▊  | 245/313 [01:06<00:20,  3.25it/s]Evaluating:  79%|███████▊  | 246/313 [01:07<00:18,  3.55it/s]Evaluating:  79%|███████▉  | 247/313 [01:07<00:17,  3.85it/s]Evaluating:  79%|███████▉  | 248/313 [01:07<00:16,  3.88it/s]Evaluating:  80%|███████▉  | 249/313 [01:07<00:17,  3.61it/s]Evaluating:  80%|███████▉  | 250/313 [01:08<00:20,  3.14it/s]Evaluating:  80%|████████  | 251/313 [01:08<00:19,  3.10it/s]Evaluating:  81%|████████  | 252/313 [01:08<00:17,  3.45it/s]Evaluating:  81%|████████  | 253/313 [01:09<00:15,  3.77it/s]Evaluating:  81%|████████  | 254/313 [01:09<00:14,  4.03it/s]Evaluating:  81%|████████▏ | 255/313 [01:09<00:13,  4.21it/s]Evaluating:  82%|████████▏ | 256/313 [01:09<00:13,  4.24it/s]Evaluating:  82%|████████▏ | 257/313 [01:09<00:12,  4.40it/s]Evaluating:  82%|████████▏ | 258/313 [01:10<00:12,  4.46it/s]Evaluating:  83%|████████▎ | 259/313 [01:10<00:11,  4.64it/s]Evaluating:  83%|████████▎ | 260/313 [01:10<00:11,  4.77it/s]Evaluating:  83%|████████▎ | 261/313 [01:10<00:10,  4.87it/s]Evaluating:  84%|████████▎ | 262/313 [01:10<00:10,  4.94it/s]Evaluating:  84%|████████▍ | 263/313 [01:11<00:10,  5.00it/s]Evaluating:  84%|████████▍ | 264/313 [01:11<00:09,  5.04it/s]Evaluating:  85%|████████▍ | 265/313 [01:11<00:09,  5.05it/s]Evaluating:  85%|████████▍ | 266/313 [01:11<00:09,  5.13it/s]Evaluating:  85%|████████▌ | 267/313 [01:11<00:08,  5.23it/s]Evaluating:  86%|████████▌ | 268/313 [01:12<00:08,  5.29it/s]Evaluating:  86%|████████▌ | 269/313 [01:12<00:08,  5.28it/s]Evaluating:  86%|████████▋ | 270/313 [01:12<00:08,  5.26it/s]Evaluating:  87%|████████▋ | 271/313 [01:12<00:08,  5.22it/s]Evaluating:  87%|████████▋ | 272/313 [01:12<00:07,  5.17it/s]Evaluating:  87%|████████▋ | 273/313 [01:13<00:07,  5.14it/s]Evaluating:  88%|████████▊ | 274/313 [01:13<00:07,  5.11it/s]Evaluating:  88%|████████▊ | 275/313 [01:13<00:07,  5.11it/s]Evaluating:  88%|████████▊ | 276/313 [01:13<00:07,  5.10it/s]Evaluating:  88%|████████▊ | 277/313 [01:13<00:07,  5.10it/s]Evaluating:  89%|████████▉ | 278/313 [01:14<00:06,  5.09it/s]Evaluating:  89%|████████▉ | 279/313 [01:14<00:06,  5.09it/s]Evaluating:  89%|████████▉ | 280/313 [01:14<00:06,  5.10it/s]Evaluating:  90%|████████▉ | 281/313 [01:14<00:06,  5.12it/s]Evaluating:  90%|█████████ | 282/313 [01:14<00:06,  5.15it/s]Evaluating:  90%|█████████ | 283/313 [01:14<00:05,  5.19it/s]Evaluating:  91%|█████████ | 284/313 [01:15<00:05,  5.24it/s]Evaluating:  91%|█████████ | 285/313 [01:15<00:05,  5.17it/s]Evaluating:  91%|█████████▏| 286/313 [01:15<00:05,  4.56it/s]Evaluating:  92%|█████████▏| 287/313 [01:15<00:06,  4.22it/s]Evaluating:  92%|█████████▏| 288/313 [01:16<00:06,  3.99it/s]Evaluating:  92%|█████████▏| 289/313 [01:16<00:06,  3.89it/s]Evaluating:  93%|█████████▎| 290/313 [01:16<00:06,  3.70it/s]Evaluating:  93%|█████████▎| 291/313 [01:17<00:05,  3.68it/s]Evaluating:  93%|█████████▎| 292/313 [01:17<00:05,  3.89it/s]Evaluating:  94%|█████████▎| 293/313 [01:17<00:04,  4.18it/s]Evaluating:  94%|█████████▍| 294/313 [01:17<00:04,  4.46it/s]Evaluating:  94%|█████████▍| 295/313 [01:17<00:03,  4.71it/s]Evaluating:  95%|█████████▍| 296/313 [01:18<00:03,  4.91it/s]Evaluating:  95%|█████████▍| 297/313 [01:18<00:03,  5.03it/s]Evaluating:  95%|█████████▌| 298/313 [01:18<00:02,  5.07it/s]Evaluating:  96%|█████████▌| 299/313 [01:18<00:02,  5.06it/s]Evaluating:  96%|█████████▌| 300/313 [01:18<00:02,  5.03it/s]Evaluating:  96%|█████████▌| 301/313 [01:19<00:02,  4.95it/s]Evaluating:  96%|█████████▋| 302/313 [01:19<00:02,  4.90it/s]Evaluating:  97%|█████████▋| 303/313 [01:19<00:02,  4.91it/s]Evaluating:  97%|█████████▋| 304/313 [01:19<00:01,  4.93it/s]Evaluating:  97%|█████████▋| 305/313 [01:19<00:01,  4.95it/s]Evaluating:  98%|█████████▊| 306/313 [01:20<00:01,  4.96it/s]Evaluating:  98%|█████████▊| 307/313 [01:20<00:01,  4.99it/s]Evaluating:  98%|█████████▊| 308/313 [01:20<00:00,  5.02it/s]Evaluating:  99%|█████████▊| 309/313 [01:20<00:00,  5.04it/s]Evaluating:  99%|█████████▉| 310/313 [01:20<00:00,  5.05it/s]Evaluating:  99%|█████████▉| 311/313 [01:21<00:00,  5.08it/s]Evaluating: 100%|█████████▉| 312/313 [01:21<00:00,  5.08it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  5.82it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.85it/s]
10/09/2021 00:26:46 - INFO - __main__ -   ***** Evaluation result  in no *****
10/09/2021 00:26:46 - INFO - __main__ -     f1 = 0.7724115334207077
10/09/2021 00:26:46 - INFO - __main__ -     loss = 0.8747855552469199
10/09/2021 00:26:46 - INFO - __main__ -     precision = 0.7312942052363817
10/09/2021 00:26:46 - INFO - __main__ -     recall = 0.8184279961116512
10/09/2021 00:26:46 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/09/2021 00:26:48 - INFO - __main__ -   ***** Running evaluation  in da *****
10/09/2021 00:26:48 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:26:48 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<02:06,  2.46it/s]Evaluating:   1%|          | 2/313 [00:00<02:04,  2.50it/s]Evaluating:   1%|          | 3/313 [00:01<01:36,  3.21it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:23,  3.72it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:15,  4.07it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:11,  4.31it/s]Evaluating:   2%|▏         | 7/313 [00:01<01:09,  4.40it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:14,  4.11it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:29,  3.39it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:39,  3.03it/s]Evaluating:   4%|▎         | 11/313 [00:03<01:33,  3.24it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:23,  3.60it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:16,  3.92it/s]Evaluating:   4%|▍         | 14/313 [00:03<01:11,  4.16it/s]Evaluating:   5%|▍         | 15/313 [00:04<01:08,  4.33it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:10,  4.20it/s]Evaluating:   5%|▌         | 17/313 [00:04<01:17,  3.83it/s]Evaluating:   6%|▌         | 18/313 [00:04<01:30,  3.27it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:33,  3.13it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:24,  3.47it/s]Evaluating:   7%|▋         | 21/313 [00:05<01:16,  3.80it/s]Evaluating:   7%|▋         | 22/313 [00:05<01:11,  4.07it/s]Evaluating:   7%|▋         | 23/313 [00:06<01:07,  4.28it/s]Evaluating:   8%|▊         | 24/313 [00:06<01:06,  4.37it/s]Evaluating:   8%|▊         | 25/313 [00:06<01:10,  4.08it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:18,  3.66it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:29,  3.19it/s]Evaluating:   9%|▉         | 28/313 [00:07<01:33,  3.05it/s]Evaluating:   9%|▉         | 29/313 [00:07<01:22,  3.43it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:15,  3.76it/s]Evaluating:  10%|▉         | 31/313 [00:08<01:09,  4.04it/s]Evaluating:  10%|█         | 32/313 [00:08<01:06,  4.25it/s]Evaluating:  11%|█         | 33/313 [00:08<01:06,  4.18it/s]Evaluating:  11%|█         | 34/313 [00:09<01:12,  3.85it/s]Evaluating:  11%|█         | 35/313 [00:09<01:24,  3.29it/s]Evaluating:  12%|█▏        | 36/313 [00:09<01:30,  3.06it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:24,  3.28it/s]Evaluating:  12%|█▏        | 38/313 [00:10<01:15,  3.63it/s]Evaluating:  12%|█▏        | 39/313 [00:10<01:09,  3.94it/s]Evaluating:  13%|█▎        | 40/313 [00:10<01:05,  4.18it/s]Evaluating:  13%|█▎        | 41/313 [00:11<01:02,  4.32it/s]Evaluating:  13%|█▎        | 42/313 [00:11<01:05,  4.14it/s]Evaluating:  14%|█▎        | 43/313 [00:11<01:12,  3.73it/s]Evaluating:  14%|█▍        | 44/313 [00:12<01:23,  3.23it/s]Evaluating:  14%|█▍        | 45/313 [00:12<01:24,  3.16it/s]Evaluating:  15%|█▍        | 46/313 [00:12<01:15,  3.53it/s]Evaluating:  15%|█▌        | 47/313 [00:12<01:09,  3.84it/s]Evaluating:  15%|█▌        | 48/313 [00:12<01:04,  4.10it/s]Evaluating:  16%|█▌        | 49/313 [00:13<01:01,  4.30it/s]Evaluating:  16%|█▌        | 50/313 [00:13<01:02,  4.21it/s]Evaluating:  16%|█▋        | 51/313 [00:13<01:07,  3.88it/s]Evaluating:  17%|█▋        | 52/313 [00:14<01:19,  3.30it/s]Evaluating:  17%|█▋        | 53/313 [00:14<01:24,  3.06it/s]Evaluating:  17%|█▋        | 54/313 [00:14<01:18,  3.28it/s]Evaluating:  18%|█▊        | 55/313 [00:14<01:10,  3.64it/s]Evaluating:  18%|█▊        | 56/313 [00:15<01:05,  3.93it/s]Evaluating:  18%|█▊        | 57/313 [00:15<01:01,  4.17it/s]Evaluating:  19%|█▊        | 58/313 [00:15<00:59,  4.28it/s]Evaluating:  19%|█▉        | 59/313 [00:15<01:02,  4.03it/s]Evaluating:  19%|█▉        | 60/313 [00:16<01:10,  3.58it/s]Evaluating:  19%|█▉        | 61/313 [00:16<01:20,  3.14it/s]Evaluating:  20%|█▉        | 62/313 [00:16<01:19,  3.17it/s]Evaluating:  20%|██        | 63/313 [00:17<01:10,  3.54it/s]Evaluating:  20%|██        | 64/313 [00:17<01:04,  3.85it/s]Evaluating:  21%|██        | 65/313 [00:17<01:00,  4.10it/s]Evaluating:  21%|██        | 66/313 [00:17<00:58,  4.22it/s]Evaluating:  21%|██▏       | 67/313 [00:18<01:01,  3.99it/s]Evaluating:  22%|██▏       | 68/313 [00:18<01:08,  3.56it/s]Evaluating:  22%|██▏       | 69/313 [00:18<01:17,  3.13it/s]Evaluating:  22%|██▏       | 70/313 [00:19<01:18,  3.09it/s]Evaluating:  23%|██▎       | 71/313 [00:19<01:09,  3.46it/s]Evaluating:  23%|██▎       | 72/313 [00:19<01:03,  3.79it/s]Evaluating:  23%|██▎       | 73/313 [00:19<00:59,  4.05it/s]Evaluating:  24%|██▎       | 74/313 [00:20<00:56,  4.25it/s]Evaluating:  24%|██▍       | 75/313 [00:20<00:57,  4.16it/s]Evaluating:  24%|██▍       | 76/313 [00:20<01:01,  3.83it/s]Evaluating:  25%|██▍       | 77/313 [00:21<01:12,  3.27it/s]Evaluating:  25%|██▍       | 78/313 [00:21<01:16,  3.07it/s]Evaluating:  25%|██▌       | 79/313 [00:21<01:10,  3.32it/s]Evaluating:  26%|██▌       | 80/313 [00:21<01:03,  3.67it/s]Evaluating:  26%|██▌       | 81/313 [00:22<00:58,  3.96it/s]Evaluating:  26%|██▌       | 82/313 [00:22<00:55,  4.19it/s]Evaluating:  27%|██▋       | 83/313 [00:22<00:54,  4.26it/s]Evaluating:  27%|██▋       | 84/313 [00:22<00:57,  3.97it/s]Evaluating:  27%|██▋       | 85/313 [00:23<01:05,  3.50it/s]Evaluating:  27%|██▋       | 86/313 [00:23<01:12,  3.11it/s]Evaluating:  28%|██▊       | 87/313 [00:23<01:10,  3.22it/s]Evaluating:  28%|██▊       | 88/313 [00:24<01:02,  3.58it/s]Evaluating:  28%|██▊       | 89/313 [00:24<00:57,  3.88it/s]Evaluating:  29%|██▉       | 90/313 [00:24<00:53,  4.14it/s]Evaluating:  29%|██▉       | 91/313 [00:24<00:52,  4.23it/s]Evaluating:  29%|██▉       | 92/313 [00:24<00:55,  3.97it/s]Evaluating:  30%|██▉       | 93/313 [00:25<01:02,  3.51it/s]Evaluating:  30%|███       | 94/313 [00:25<01:10,  3.11it/s]Evaluating:  30%|███       | 95/313 [00:26<01:09,  3.14it/s]Evaluating:  31%|███       | 96/313 [00:26<01:02,  3.50it/s]Evaluating:  31%|███       | 97/313 [00:26<00:56,  3.81it/s]Evaluating:  31%|███▏      | 98/313 [00:26<00:52,  4.06it/s]Evaluating:  32%|███▏      | 99/313 [00:26<00:50,  4.20it/s]Evaluating:  32%|███▏      | 100/313 [00:27<00:53,  3.95it/s]Evaluating:  32%|███▏      | 101/313 [00:27<01:00,  3.51it/s]Evaluating:  33%|███▎      | 102/313 [00:27<01:08,  3.10it/s]Evaluating:  33%|███▎      | 103/313 [00:28<01:08,  3.07it/s]Evaluating:  33%|███▎      | 104/313 [00:28<01:00,  3.44it/s]Evaluating:  34%|███▎      | 105/313 [00:28<00:55,  3.76it/s]Evaluating:  34%|███▍      | 106/313 [00:28<00:51,  4.03it/s]Evaluating:  34%|███▍      | 107/313 [00:29<00:49,  4.12it/s]Evaluating:  35%|███▍      | 108/313 [00:29<00:53,  3.83it/s]Evaluating:  35%|███▍      | 109/313 [00:29<01:00,  3.40it/s]Evaluating:  35%|███▌      | 110/313 [00:30<01:06,  3.04it/s]Evaluating:  35%|███▌      | 111/313 [00:30<01:04,  3.13it/s]Evaluating:  36%|███▌      | 112/313 [00:30<00:57,  3.49it/s]Evaluating:  36%|███▌      | 113/313 [00:30<00:52,  3.81it/s]Evaluating:  36%|███▋      | 114/313 [00:31<00:48,  4.06it/s]Evaluating:  37%|███▋      | 115/313 [00:31<00:47,  4.15it/s]Evaluating:  37%|███▋      | 116/313 [00:31<00:50,  3.89it/s]Evaluating:  37%|███▋      | 117/313 [00:32<00:57,  3.43it/s]Evaluating:  38%|███▊      | 118/313 [00:32<01:03,  3.07it/s]Evaluating:  38%|███▊      | 119/313 [00:32<01:01,  3.15it/s]Evaluating:  38%|███▊      | 120/313 [00:32<00:54,  3.51it/s]Evaluating:  39%|███▊      | 121/313 [00:33<00:50,  3.82it/s]Evaluating:  39%|███▉      | 122/313 [00:33<00:46,  4.07it/s]Evaluating:  39%|███▉      | 123/313 [00:33<00:45,  4.18it/s]Evaluating:  40%|███▉      | 124/313 [00:33<00:48,  3.89it/s]Evaluating:  40%|███▉      | 125/313 [00:34<00:54,  3.44it/s]Evaluating:  40%|████      | 126/313 [00:34<01:01,  3.05it/s]Evaluating:  41%|████      | 127/313 [00:34<00:57,  3.24it/s]Evaluating:  41%|████      | 128/313 [00:35<00:51,  3.59it/s]Evaluating:  41%|████      | 129/313 [00:35<00:47,  3.88it/s]Evaluating:  42%|████▏     | 130/313 [00:35<00:44,  4.11it/s]Evaluating:  42%|████▏     | 131/313 [00:35<00:44,  4.07it/s]Evaluating:  42%|████▏     | 132/313 [00:36<00:47,  3.77it/s]Evaluating:  42%|████▏     | 133/313 [00:36<00:55,  3.24it/s]Evaluating:  43%|████▎     | 134/313 [00:36<00:57,  3.09it/s]Evaluating:  43%|████▎     | 135/313 [00:37<00:51,  3.43it/s]Evaluating:  43%|████▎     | 136/313 [00:37<00:47,  3.75it/s]Evaluating:  44%|████▍     | 137/313 [00:37<00:43,  4.03it/s]Evaluating:  44%|████▍     | 138/313 [00:37<00:41,  4.25it/s]Evaluating:  44%|████▍     | 139/313 [00:37<00:41,  4.22it/s]Evaluating:  45%|████▍     | 140/313 [00:38<00:43,  3.96it/s]Evaluating:  45%|████▌     | 141/313 [00:38<00:51,  3.33it/s]Evaluating:  45%|████▌     | 142/313 [00:38<00:52,  3.28it/s]Evaluating:  46%|████▌     | 143/313 [00:39<00:46,  3.63it/s]Evaluating:  46%|████▌     | 144/313 [00:39<00:43,  3.91it/s]Evaluating:  46%|████▋     | 145/313 [00:39<00:40,  4.15it/s]Evaluating:  47%|████▋     | 146/313 [00:39<00:39,  4.24it/s]Evaluating:  47%|████▋     | 147/313 [00:40<00:42,  3.93it/s]Evaluating:  47%|████▋     | 148/313 [00:40<00:48,  3.44it/s]Evaluating:  48%|████▊     | 149/313 [00:40<00:53,  3.08it/s]Evaluating:  48%|████▊     | 150/313 [00:41<00:51,  3.18it/s]Evaluating:  48%|████▊     | 151/313 [00:41<00:45,  3.54it/s]Evaluating:  49%|████▊     | 152/313 [00:41<00:41,  3.84it/s]Evaluating:  49%|████▉     | 153/313 [00:41<00:39,  4.07it/s]Evaluating:  49%|████▉     | 154/313 [00:42<00:39,  4.01it/s]Evaluating:  50%|████▉     | 155/313 [00:42<00:42,  3.69it/s]Evaluating:  50%|████▉     | 156/313 [00:42<00:49,  3.19it/s]Evaluating:  50%|█████     | 157/313 [00:43<00:51,  3.02it/s]Evaluating:  50%|█████     | 158/313 [00:43<00:46,  3.31it/s]Evaluating:  51%|█████     | 159/313 [00:43<00:42,  3.65it/s]Evaluating:  51%|█████     | 160/313 [00:43<00:38,  3.94it/s]Evaluating:  51%|█████▏    | 161/313 [00:44<00:36,  4.18it/s]Evaluating:  52%|█████▏    | 162/313 [00:44<00:37,  4.07it/s]Evaluating:  52%|█████▏    | 163/313 [00:44<00:40,  3.70it/s]Evaluating:  52%|█████▏    | 164/313 [00:45<00:46,  3.20it/s]Evaluating:  53%|█████▎    | 165/313 [00:45<00:47,  3.09it/s]Evaluating:  53%|█████▎    | 166/313 [00:45<00:42,  3.45it/s]Evaluating:  53%|█████▎    | 167/313 [00:45<00:38,  3.76it/s]Evaluating:  54%|█████▎    | 168/313 [00:45<00:36,  4.03it/s]Evaluating:  54%|█████▍    | 169/313 [00:46<00:34,  4.14it/s]Evaluating:  54%|█████▍    | 170/313 [00:46<00:37,  3.86it/s]Evaluating:  55%|█████▍    | 171/313 [00:46<00:41,  3.40it/s]Evaluating:  55%|█████▍    | 172/313 [00:47<00:46,  3.04it/s]Evaluating:  55%|█████▌    | 173/313 [00:47<00:43,  3.25it/s]Evaluating:  56%|█████▌    | 174/313 [00:47<00:38,  3.59it/s]Evaluating:  56%|█████▌    | 175/313 [00:47<00:35,  3.88it/s]Evaluating:  56%|█████▌    | 176/313 [00:48<00:33,  4.09it/s]Evaluating:  57%|█████▋    | 177/313 [00:48<00:34,  3.97it/s]Evaluating:  57%|█████▋    | 178/313 [00:48<00:37,  3.59it/s]Evaluating:  57%|█████▋    | 179/313 [00:49<00:42,  3.14it/s]Evaluating:  58%|█████▊    | 180/313 [00:49<00:41,  3.17it/s]Evaluating:  58%|█████▊    | 181/313 [00:49<00:37,  3.53it/s]Evaluating:  58%|█████▊    | 182/313 [00:49<00:34,  3.83it/s]Evaluating:  58%|█████▊    | 183/313 [00:50<00:32,  4.06it/s]Evaluating:  59%|█████▉    | 184/313 [00:50<00:32,  3.99it/s]Evaluating:  59%|█████▉    | 185/313 [00:50<00:34,  3.68it/s]Evaluating:  59%|█████▉    | 186/313 [00:51<00:43,  2.89it/s]Evaluating:  60%|█████▉    | 187/313 [00:51<00:40,  3.12it/s]Evaluating:  60%|██████    | 188/313 [00:51<00:35,  3.49it/s]Evaluating:  60%|██████    | 189/313 [00:51<00:32,  3.79it/s]Evaluating:  61%|██████    | 190/313 [00:52<00:30,  3.99it/s]Evaluating:  61%|██████    | 191/313 [00:52<00:31,  3.84it/s]Evaluating:  61%|██████▏   | 192/313 [00:52<00:35,  3.46it/s]Evaluating:  62%|██████▏   | 193/313 [00:53<00:39,  3.06it/s]Evaluating:  62%|██████▏   | 194/313 [00:53<00:39,  3.03it/s]Evaluating:  62%|██████▏   | 195/313 [00:53<00:34,  3.40it/s]Evaluating:  63%|██████▎   | 196/313 [00:53<00:31,  3.72it/s]Evaluating:  63%|██████▎   | 197/313 [00:54<00:29,  3.97it/s]Evaluating:  63%|██████▎   | 198/313 [00:54<00:28,  3.98it/s]Evaluating:  64%|██████▎   | 199/313 [00:54<00:30,  3.72it/s]Evaluating:  64%|██████▍   | 200/313 [00:55<00:35,  3.20it/s]Evaluating:  64%|██████▍   | 201/313 [00:55<00:35,  3.14it/s]Evaluating:  65%|██████▍   | 202/313 [00:55<00:31,  3.49it/s]Evaluating:  65%|██████▍   | 203/313 [00:55<00:28,  3.80it/s]Evaluating:  65%|██████▌   | 204/313 [00:56<00:26,  4.05it/s]Evaluating:  65%|██████▌   | 205/313 [00:56<00:26,  4.03it/s]Evaluating:  66%|██████▌   | 206/313 [00:56<00:28,  3.74it/s]Evaluating:  66%|██████▌   | 207/313 [00:57<00:33,  3.21it/s]Evaluating:  66%|██████▋   | 208/313 [00:57<00:34,  3.04it/s]Evaluating:  67%|██████▋   | 209/313 [00:57<00:31,  3.34it/s]Evaluating:  67%|██████▋   | 210/313 [00:57<00:28,  3.67it/s]Evaluating:  67%|██████▋   | 211/313 [00:58<00:25,  3.94it/s]Evaluating:  68%|██████▊   | 212/313 [00:58<00:25,  4.00it/s]Evaluating:  68%|██████▊   | 213/313 [00:58<00:27,  3.65it/s]Evaluating:  68%|██████▊   | 214/313 [00:59<00:30,  3.22it/s]Evaluating:  69%|██████▊   | 215/313 [00:59<00:33,  2.93it/s]Evaluating:  69%|██████▉   | 216/313 [00:59<00:29,  3.23it/s]Evaluating:  69%|██████▉   | 217/313 [00:59<00:26,  3.58it/s]Evaluating:  70%|██████▉   | 218/313 [01:00<00:24,  3.87it/s]Evaluating:  70%|██████▉   | 219/313 [01:00<00:23,  4.02it/s]Evaluating:  70%|███████   | 220/313 [01:00<00:24,  3.76it/s]Evaluating:  71%|███████   | 221/313 [01:01<00:27,  3.34it/s]Evaluating:  71%|███████   | 222/313 [01:01<00:30,  2.99it/s]Evaluating:  71%|███████   | 223/313 [01:01<00:28,  3.15it/s]Evaluating:  72%|███████▏  | 224/313 [01:01<00:25,  3.51it/s]Evaluating:  72%|███████▏  | 225/313 [01:02<00:23,  3.81it/s]Evaluating:  72%|███████▏  | 226/313 [01:02<00:21,  4.02it/s]Evaluating:  73%|███████▎  | 227/313 [01:02<00:22,  3.89it/s]Evaluating:  73%|███████▎  | 228/313 [01:03<00:24,  3.48it/s]Evaluating:  73%|███████▎  | 229/313 [01:03<00:27,  3.09it/s]Evaluating:  73%|███████▎  | 230/313 [01:03<00:26,  3.19it/s]Evaluating:  74%|███████▍  | 231/313 [01:03<00:23,  3.54it/s]Evaluating:  74%|███████▍  | 232/313 [01:04<00:21,  3.83it/s]Evaluating:  74%|███████▍  | 233/313 [01:04<00:19,  4.07it/s]Evaluating:  75%|███████▍  | 234/313 [01:04<00:18,  4.25it/s]Evaluating:  75%|███████▌  | 235/313 [01:04<00:18,  4.25it/s]Evaluating:  75%|███████▌  | 236/313 [01:05<00:17,  4.40it/s]Evaluating:  76%|███████▌  | 237/313 [01:05<00:16,  4.49it/s]Evaluating:  76%|███████▌  | 238/313 [01:05<00:16,  4.62it/s]Evaluating:  76%|███████▋  | 239/313 [01:05<00:15,  4.80it/s]Evaluating:  77%|███████▋  | 240/313 [01:05<00:14,  4.92it/s]Evaluating:  77%|███████▋  | 241/313 [01:05<00:14,  5.03it/s]Evaluating:  77%|███████▋  | 242/313 [01:06<00:13,  5.09it/s]Evaluating:  78%|███████▊  | 243/313 [01:06<00:13,  5.15it/s]Evaluating:  78%|███████▊  | 244/313 [01:06<00:13,  5.18it/s]Evaluating:  78%|███████▊  | 245/313 [01:06<00:13,  5.22it/s]Evaluating:  79%|███████▊  | 246/313 [01:06<00:12,  5.28it/s]Evaluating:  79%|███████▉  | 247/313 [01:07<00:12,  5.35it/s]Evaluating:  79%|███████▉  | 248/313 [01:07<00:11,  5.42it/s]Evaluating:  80%|███████▉  | 249/313 [01:07<00:11,  5.50it/s]Evaluating:  80%|███████▉  | 250/313 [01:07<00:12,  5.17it/s]Evaluating:  80%|████████  | 251/313 [01:07<00:12,  5.07it/s]Evaluating:  81%|████████  | 252/313 [01:08<00:12,  5.02it/s]Evaluating:  81%|████████  | 253/313 [01:08<00:11,  5.02it/s]Evaluating:  81%|████████  | 254/313 [01:08<00:11,  5.05it/s]Evaluating:  81%|████████▏ | 255/313 [01:08<00:11,  5.08it/s]Evaluating:  82%|████████▏ | 256/313 [01:08<00:11,  5.13it/s]Evaluating:  82%|████████▏ | 257/313 [01:09<00:10,  5.18it/s]Evaluating:  82%|████████▏ | 258/313 [01:09<00:10,  5.20it/s]Evaluating:  83%|████████▎ | 259/313 [01:09<00:10,  5.22it/s]Evaluating:  83%|████████▎ | 260/313 [01:09<00:10,  5.23it/s]Evaluating:  83%|████████▎ | 261/313 [01:09<00:09,  5.21it/s]Evaluating:  84%|████████▎ | 262/313 [01:10<00:09,  5.24it/s]Evaluating:  84%|████████▍ | 263/313 [01:10<00:09,  5.28it/s]Evaluating:  84%|████████▍ | 264/313 [01:10<00:09,  5.29it/s]Evaluating:  85%|████████▍ | 265/313 [01:10<00:09,  5.29it/s]Evaluating:  85%|████████▍ | 266/313 [01:10<00:08,  5.32it/s]Evaluating:  85%|████████▌ | 267/313 [01:10<00:09,  5.06it/s]Evaluating:  86%|████████▌ | 268/313 [01:11<00:10,  4.41it/s]Evaluating:  86%|████████▌ | 269/313 [01:11<00:09,  4.49it/s]Evaluating:  86%|████████▋ | 270/313 [01:11<00:09,  4.53it/s]Evaluating:  87%|████████▋ | 271/313 [01:11<00:10,  4.18it/s]Evaluating:  87%|████████▋ | 272/313 [01:12<00:09,  4.15it/s]Evaluating:  87%|████████▋ | 273/313 [01:12<00:10,  3.93it/s]Evaluating:  88%|████████▊ | 274/313 [01:12<00:10,  3.85it/s]Evaluating:  88%|████████▊ | 275/313 [01:13<00:09,  4.03it/s]Evaluating:  88%|████████▊ | 276/313 [01:13<00:08,  4.33it/s]Evaluating:  88%|████████▊ | 277/313 [01:13<00:10,  3.60it/s]Evaluating:  89%|████████▉ | 278/313 [01:13<00:08,  3.97it/s]Evaluating:  89%|████████▉ | 279/313 [01:13<00:07,  4.28it/s]Evaluating:  89%|████████▉ | 280/313 [01:14<00:07,  4.54it/s]Evaluating:  90%|████████▉ | 281/313 [01:14<00:06,  4.74it/s]Evaluating:  90%|█████████ | 282/313 [01:14<00:06,  4.90it/s]Evaluating:  90%|█████████ | 283/313 [01:14<00:05,  5.04it/s]Evaluating:  91%|█████████ | 284/313 [01:14<00:05,  5.18it/s]Evaluating:  91%|█████████ | 285/313 [01:15<00:05,  5.30it/s]Evaluating:  91%|█████████▏| 286/313 [01:15<00:05,  5.38it/s]Evaluating:  92%|█████████▏| 287/313 [01:15<00:04,  5.39it/s]Evaluating:  92%|█████████▏| 288/313 [01:15<00:04,  5.37it/s]Evaluating:  92%|█████████▏| 289/313 [01:15<00:04,  5.36it/s]Evaluating:  93%|█████████▎| 290/313 [01:16<00:04,  5.34it/s]Evaluating:  93%|█████████▎| 291/313 [01:16<00:04,  5.34it/s]Evaluating:  93%|█████████▎| 292/313 [01:16<00:03,  5.35it/s]Evaluating:  94%|█████████▎| 293/313 [01:16<00:03,  5.38it/s]Evaluating:  94%|█████████▍| 294/313 [01:16<00:03,  5.42it/s]Evaluating:  94%|█████████▍| 295/313 [01:16<00:03,  5.37it/s]Evaluating:  95%|█████████▍| 296/313 [01:17<00:03,  5.36it/s]Evaluating:  95%|█████████▍| 297/313 [01:17<00:02,  5.36it/s]Evaluating:  95%|█████████▌| 298/313 [01:17<00:02,  5.36it/s]Evaluating:  96%|█████████▌| 299/313 [01:17<00:02,  5.28it/s]Evaluating:  96%|█████████▌| 300/313 [01:17<00:02,  5.20it/s]Evaluating:  96%|█████████▌| 301/313 [01:18<00:02,  5.21it/s]Evaluating:  96%|█████████▋| 302/313 [01:18<00:02,  5.15it/s]Evaluating:  97%|█████████▋| 303/313 [01:18<00:01,  5.11it/s]Evaluating:  97%|█████████▋| 304/313 [01:18<00:01,  4.64it/s]Evaluating:  97%|█████████▋| 305/313 [01:19<00:02,  3.98it/s]Evaluating:  98%|█████████▊| 306/313 [01:19<00:02,  3.29it/s]Evaluating:  98%|█████████▊| 307/313 [01:19<00:01,  3.10it/s]Evaluating:  98%|█████████▊| 308/313 [01:20<00:01,  3.42it/s]Evaluating:  99%|█████████▊| 309/313 [01:20<00:01,  3.72it/s]Evaluating:  99%|█████████▉| 310/313 [01:20<00:00,  3.95it/s]Evaluating:  99%|█████████▉| 311/313 [01:20<00:00,  3.74it/s]Evaluating: 100%|█████████▉| 312/313 [01:21<00:00,  3.30it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.30it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.84it/s]
10/09/2021 00:28:11 - INFO - __main__ -   ***** Evaluation result  in da *****
10/09/2021 00:28:11 - INFO - __main__ -     f1 = 0.7924947109215135
10/09/2021 00:28:11 - INFO - __main__ -     loss = 0.8198957042381786
10/09/2021 00:28:11 - INFO - __main__ -     precision = 0.7548728113643872
10/09/2021 00:28:11 - INFO - __main__ -     recall = 0.834063366914878
10/09/2021 00:28:11 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:28:24 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:28:24 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:28:44 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:28:46 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:28:46 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:28:46 - INFO - __main__ -   Seed = 52
10/09/2021 00:28:46 - INFO - root -   save model
10/09/2021 00:28:46 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:28:46 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:29:19 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:29:19 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:29:19 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/09/2021 00:29:19 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:29:19 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:29:19 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/09/2021 00:29:19 - INFO - __main__ -   Language = en
10/09/2021 00:29:19 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:29:20 - INFO - __main__ -   Language = is
10/09/2021 00:29:20 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/09/2021 00:29:21 - INFO - __main__ -   Language = de
10/09/2021 00:29:21 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/09/2021 00:29:25 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/09/2021 00:29:25 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/09/2021 00:29:25 - INFO - __main__ -     Num examples = 100
10/09/2021 00:29:25 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  5.49it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  5.71it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  5.83it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  7.30it/s]
10/09/2021 00:29:26 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/09/2021 00:29:26 - INFO - __main__ -     f1 = 0.6296296296296297
10/09/2021 00:29:26 - INFO - __main__ -     loss = 1.4785407707095146
10/09/2021 00:29:26 - INFO - __main__ -     precision = 0.5666666666666667
10/09/2021 00:29:26 - INFO - __main__ -     recall = 0.7083333333333334
10/09/2021 00:29:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/09/2021 00:29:27 - INFO - __main__ -   ***** Running evaluation  in no *****
10/09/2021 00:29:27 - INFO - __main__ -     Num examples = 10000
10/09/2021 00:29:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.02it/s]Evaluating:   1%|          | 2/313 [00:00<00:52,  5.97it/s]Evaluating:   1%|          | 3/313 [00:00<00:52,  5.94it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:52,  5.93it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:52,  5.92it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:52,  5.85it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:52,  5.87it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:52,  5.86it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:51,  5.86it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:51,  5.85it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:51,  5.83it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:52,  5.78it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:52,  5.75it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:52,  5.73it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:52,  5.72it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:51,  5.72it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:51,  5.70it/s]Evaluating:   6%|▌         | 18/313 [00:03<00:51,  5.68it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:52,  5.64it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:52,  5.60it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:52,  5.57it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:52,  5.54it/s]Evaluating:   7%|▋         | 23/313 [00:04<00:52,  5.54it/s]Evaluating:   8%|▊         | 24/313 [00:04<00:52,  5.53it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:52,  5.49it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:52,  5.48it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:53,  5.32it/s]Evaluating:   9%|▉         | 28/313 [00:05<00:59,  4.80it/s]Evaluating:   9%|▉         | 29/313 [00:05<01:08,  4.14it/s]Evaluating:  10%|▉         | 30/313 [00:05<01:22,  3.44it/s]Evaluating:  10%|▉         | 31/313 [00:06<01:30,  3.11it/s]Evaluating:  10%|█         | 32/313 [00:06<01:20,  3.48it/s]Evaluating:  11%|█         | 33/313 [00:06<01:13,  3.79it/s]Evaluating:  11%|█         | 34/313 [00:06<01:08,  4.06it/s]Evaluating:  11%|█         | 35/313 [00:06<01:04,  4.28it/s]Evaluating:  12%|█▏        | 36/313 [00:07<01:05,  4.21it/s]Evaluating:  12%|█▏        | 37/313 [00:07<01:10,  3.89it/s]Evaluating:  12%|█▏        | 38/313 [00:07<01:22,  3.34it/s]Evaluating:  12%|█▏        | 39/313 [00:08<01:29,  3.06it/s]Evaluating:  13%|█▎        | 40/313 [00:08<01:19,  3.45it/s]Evaluating:  13%|█▎        | 41/313 [00:08<01:11,  3.79it/s]Evaluating:  13%|█▎        | 42/313 [00:09<01:34,  2.88it/s]Evaluating:  14%|█▎        | 43/313 [00:09<01:36,  2.80it/s]Evaluating:  14%|█▍        | 44/313 [00:09<01:27,  3.06it/s]Evaluating:  14%|█▍        | 45/313 [00:10<01:16,  3.48it/s]Evaluating:  15%|█▍        | 46/313 [00:10<01:09,  3.83it/s]Evaluating:  15%|█▌        | 47/313 [00:10<01:04,  4.12it/s]Evaluating:  15%|█▌        | 48/313 [00:10<01:01,  4.28it/s]Evaluating:  16%|█▌        | 49/313 [00:10<01:04,  4.09it/s]Evaluating:  16%|█▌        | 50/313 [00:11<01:11,  3.69it/s]Evaluating:  16%|█▋        | 51/313 [00:11<01:22,  3.19it/s]Evaluating:  17%|█▋        | 52/313 [00:12<01:24,  3.10it/s]Evaluating:  17%|█▋        | 53/313 [00:12<01:14,  3.48it/s]Evaluating:  17%|█▋        | 54/313 [00:12<01:08,  3.79it/s]Evaluating:  18%|█▊        | 55/313 [00:12<01:03,  4.06it/s]Evaluating:  18%|█▊        | 56/313 [00:12<01:00,  4.25it/s]Evaluating:  18%|█▊        | 57/313 [00:13<01:00,  4.21it/s]Evaluating:  19%|█▊        | 58/313 [00:13<01:05,  3.89it/s]Evaluating:  19%|█▉        | 59/313 [00:13<01:17,  3.29it/s]Evaluating:  19%|█▉        | 60/313 [00:14<01:22,  3.08it/s]Evaluating:  19%|█▉        | 61/313 [00:14<01:12,  3.45it/s]Evaluating:  20%|█▉        | 62/313 [00:14<01:06,  3.77it/s]Evaluating:  20%|██        | 63/313 [00:14<01:01,  4.03it/s]Evaluating:  20%|██        | 64/313 [00:15<00:58,  4.23it/s]Evaluating:  21%|██        | 65/313 [00:15<01:00,  4.13it/s]Evaluating:  21%|██        | 66/313 [00:15<01:05,  3.77it/s]Evaluating:  21%|██▏       | 67/313 [00:16<01:16,  3.23it/s]Evaluating:  22%|██▏       | 68/313 [00:16<01:23,  2.95it/s]Evaluating:  22%|██▏       | 69/313 [00:16<01:14,  3.26it/s]Evaluating:  22%|██▏       | 70/313 [00:16<01:07,  3.61it/s]Evaluating:  23%|██▎       | 71/313 [00:17<01:02,  3.89it/s]Evaluating:  23%|██▎       | 72/313 [00:17<00:58,  4.14it/s]Evaluating:  23%|██▎       | 73/313 [00:17<00:58,  4.10it/s]Evaluating:  24%|██▎       | 74/313 [00:17<01:02,  3.80it/s]Evaluating:  24%|██▍       | 75/313 [00:18<01:13,  3.25it/s]Evaluating:  24%|██▍       | 76/313 [00:18<01:16,  3.09it/s]Evaluating:  25%|██▍       | 77/313 [00:18<01:09,  3.41it/s]Evaluating:  25%|██▍       | 78/313 [00:19<01:02,  3.74it/s]Evaluating:  25%|██▌       | 79/313 [00:19<00:58,  4.00it/s]Evaluating:  26%|██▌       | 80/313 [00:19<00:55,  4.20it/s]Evaluating:  26%|██▌       | 81/313 [00:19<00:56,  4.12it/s]Evaluating:  26%|██▌       | 82/313 [00:20<01:00,  3.79it/s]Evaluating:  27%|██▋       | 83/313 [00:20<01:10,  3.24it/s]Evaluating:  27%|██▋       | 84/313 [00:20<01:14,  3.07it/s]Evaluating:  27%|██▋       | 85/313 [00:21<01:07,  3.37it/s]Evaluating:  27%|██▋       | 86/313 [00:21<01:01,  3.70it/s]Evaluating:  28%|██▊       | 87/313 [00:21<00:56,  3.98it/s]Evaluating:  28%|██▊       | 88/313 [00:21<00:53,  4.18it/s]Evaluating:  28%|██▊       | 89/313 [00:21<00:54,  4.12it/s]Evaluating:  29%|██▉       | 90/313 [00:22<00:58,  3.83it/s]Evaluating:  29%|██▉       | 91/313 [00:22<01:07,  3.27it/s]Evaluating:  29%|██▉       | 92/313 [00:23<01:11,  3.09it/s]Evaluating:  30%|██▉       | 93/313 [00:23<01:05,  3.37it/s]Evaluating:  30%|███       | 94/313 [00:23<00:59,  3.68it/s]Evaluating:  30%|███       | 95/313 [00:23<00:55,  3.94it/s]Evaluating:  31%|███       | 96/313 [00:23<00:51,  4.19it/s]Evaluating:  31%|███       | 97/313 [00:24<00:52,  4.09it/s]Evaluating:  31%|███▏      | 98/313 [00:24<00:57,  3.73it/s]Evaluating:  32%|███▏      | 99/313 [00:24<01:06,  3.21it/s]Evaluating:  32%|███▏      | 100/313 [00:25<01:09,  3.05it/s]Evaluating:  32%|███▏      | 101/313 [00:25<01:02,  3.38it/s]Evaluating:  33%|███▎      | 102/313 [00:25<00:56,  3.71it/s]Evaluating:  33%|███▎      | 103/313 [00:25<00:52,  3.98it/s]Evaluating:  33%|███▎      | 104/313 [00:26<00:50,  4.18it/s]Evaluating:  34%|███▎      | 105/313 [00:26<00:50,  4.10it/s]Evaluating:  34%|███▍      | 106/313 [00:26<00:55,  3.76it/s]Evaluating:  34%|███▍      | 107/313 [00:27<01:03,  3.22it/s]Evaluating:  35%|███▍      | 108/313 [00:27<01:09,  2.93it/s]Evaluating:  35%|███▍      | 109/313 [00:27<01:01,  3.30it/s]Evaluating:  35%|███▌      | 110/313 [00:27<00:55,  3.64it/s]Evaluating:  35%|███▌      | 111/313 [00:28<00:51,  3.92it/s]Evaluating:  36%|███▌      | 112/313 [00:28<00:48,  4.12it/s]Evaluating:  36%|███▌      | 113/313 [00:28<00:49,  4.02it/s]Evaluating:  36%|███▋      | 114/313 [00:28<00:54,  3.68it/s]Evaluating:  37%|███▋      | 115/313 [00:29<01:02,  3.18it/s]Evaluating:  37%|███▋      | 116/313 [00:29<01:04,  3.05it/s]Evaluating:  37%|███▋      | 117/313 [00:29<00:57,  3.42it/s]Evaluating:  38%|███▊      | 118/313 [00:30<00:52,  3.74it/s]Evaluating:  38%|███▊      | 119/313 [00:30<00:48,  4.01it/s]Evaluating:  38%|███▊      | 120/313 [00:30<00:45,  4.20it/s]Evaluating:  39%|███▊      | 121/313 [00:30<00:46,  4.09it/s]Evaluating:  39%|███▉      | 122/313 [00:31<00:51,  3.71it/s]Evaluating:  39%|███▉      | 123/313 [00:31<00:59,  3.19it/s]Evaluating:  40%|███▉      | 124/313 [00:31<01:02,  3.04it/s]Evaluating:  40%|███▉      | 125/313 [00:32<00:55,  3.36it/s]Evaluating:  40%|████      | 126/313 [00:32<00:50,  3.70it/s]Evaluating:  41%|████      | 127/313 [00:32<00:46,  3.97it/s]Evaluating:  41%|████      | 128/313 [00:32<00:44,  4.19it/s]Evaluating:  41%|████      | 129/313 [00:32<00:44,  4.13it/s]Evaluating:  42%|████▏     | 130/313 [00:33<00:48,  3.81it/s]Evaluating:  42%|████▏     | 131/313 [00:33<00:56,  3.24it/s]Evaluating:  42%|████▏     | 132/313 [00:34<01:00,  3.01it/s]Evaluating:  42%|████▏     | 133/313 [00:34<00:53,  3.38it/s]Evaluating:  43%|████▎     | 134/313 [00:34<00:48,  3.70it/s]Evaluating:  43%|████▎     | 135/313 [00:34<00:44,  3.97it/s]Evaluating:  43%|████▎     | 136/313 [00:34<00:42,  4.13it/s]Evaluating:  44%|████▍     | 137/313 [00:35<00:45,  3.88it/s]Evaluating:  44%|████▍     | 138/313 [00:35<00:50,  3.44it/s]Evaluating:  44%|████▍     | 139/313 [00:36<00:56,  3.06it/s]Evaluating:  45%|████▍     | 140/313 [00:36<00:56,  3.07it/s]Evaluating:  45%|████▌     | 141/313 [00:36<00:49,  3.45it/s]Evaluating:  45%|████▌     | 142/313 [00:36<00:45,  3.77it/s]Evaluating:  46%|████▌     | 143/313 [00:36<00:42,  3.98it/s]Evaluating:  46%|████▌     | 144/313 [00:37<00:42,  4.00it/s]Evaluating:  46%|████▋     | 145/313 [00:37<00:44,  3.77it/s]Evaluating:  47%|████▋     | 146/313 [00:37<00:51,  3.23it/s]Evaluating:  47%|████▋     | 147/313 [00:38<00:55,  2.98it/s]Evaluating:  47%|████▋     | 148/313 [00:38<00:49,  3.36it/s]Evaluating:  48%|████▊     | 149/313 [00:38<00:44,  3.69it/s]Evaluating:  48%|████▊     | 150/313 [00:38<00:40,  3.99it/s]Evaluating:  48%|████▊     | 151/313 [00:39<00:39,  4.11it/s]Evaluating:  49%|████▊     | 152/313 [00:39<00:42,  3.81it/s]Evaluating:  49%|████▉     | 153/313 [00:39<00:47,  3.37it/s]Evaluating:  49%|████▉     | 154/313 [00:40<00:52,  3.00it/s]Evaluating:  50%|████▉     | 155/313 [00:40<00:50,  3.13it/s]Evaluating:  50%|████▉     | 156/313 [00:40<00:44,  3.49it/s]Evaluating:  50%|█████     | 157/313 [00:40<00:41,  3.79it/s]Evaluating:  50%|█████     | 158/313 [00:41<00:38,  4.03it/s]Evaluating:  51%|█████     | 159/313 [00:41<00:38,  3.95it/s]Evaluating:  51%|█████     | 160/313 [00:41<00:42,  3.63it/s]Evaluating:  51%|█████▏    | 161/313 [00:42<00:48,  3.16it/s]Evaluating:  52%|█████▏    | 162/313 [00:42<00:49,  3.03it/s]Evaluating:  52%|█████▏    | 163/313 [00:42<00:44,  3.41it/s]Evaluating:  52%|█████▏    | 164/313 [00:43<00:54,  2.73it/s]Evaluating:  53%|█████▎    | 165/313 [00:43<00:56,  2.63it/s]Evaluating:  53%|█████▎    | 166/313 [00:44<00:52,  2.80it/s]Evaluating:  53%|█████▎    | 167/313 [00:44<00:45,  3.20it/s]Evaluating:  54%|█████▎    | 168/313 [00:44<00:40,  3.54it/s]Evaluating:  54%|█████▍    | 169/313 [00:44<00:37,  3.84it/s]Evaluating:  54%|█████▍    | 170/313 [00:44<00:35,  3.97it/s]Evaluating:  55%|█████▍    | 171/313 [00:45<00:37,  3.75it/s]Evaluating:  55%|█████▍    | 172/313 [00:45<00:42,  3.32it/s]Evaluating:  55%|█████▌    | 173/313 [00:45<00:46,  3.01it/s]Evaluating:  56%|█████▌    | 174/313 [00:46<00:44,  3.13it/s]Evaluating:  56%|█████▌    | 175/313 [00:46<00:39,  3.48it/s]Evaluating:  56%|█████▌    | 176/313 [00:46<00:36,  3.79it/s]Evaluating:  57%|█████▋    | 177/313 [00:46<00:33,  4.02it/s]Evaluating:  57%|█████▋    | 178/313 [00:47<00:33,  4.09it/s]Evaluating:  57%|█████▋    | 179/313 [00:47<00:35,  3.77it/s]Evaluating:  58%|█████▊    | 180/313 [00:47<00:40,  3.28it/s]Evaluating:  58%|█████▊    | 181/313 [00:48<00:43,  3.03it/s]Evaluating:  58%|█████▊    | 182/313 [00:48<00:40,  3.22it/s]Evaluating:  58%|█████▊    | 183/313 [00:48<00:36,  3.56it/s]Evaluating:  59%|█████▉    | 184/313 [00:48<00:33,  3.85it/s]Evaluating:  59%|█████▉    | 185/313 [00:49<00:31,  4.06it/s]Evaluating:  59%|█████▉    | 186/313 [00:49<00:32,  3.96it/s]Evaluating:  60%|█████▉    | 187/313 [00:49<00:34,  3.61it/s]Evaluating:  60%|██████    | 188/313 [00:50<00:39,  3.14it/s]Evaluating:  60%|██████    | 189/313 [00:50<00:40,  3.05it/s]Evaluating:  61%|██████    | 190/313 [00:50<00:36,  3.42it/s]Evaluating:  61%|██████    | 191/313 [00:50<00:32,  3.73it/s]Evaluating:  61%|██████▏   | 192/313 [00:51<00:30,  3.98it/s]Evaluating:  62%|██████▏   | 193/313 [00:51<00:29,  4.10it/s]Evaluating:  62%|██████▏   | 194/313 [00:51<00:30,  3.87it/s]Evaluating:  62%|██████▏   | 195/313 [00:52<00:34,  3.40it/s]Evaluating:  63%|██████▎   | 196/313 [00:52<00:38,  3.05it/s]Evaluating:  63%|██████▎   | 197/313 [00:52<00:36,  3.19it/s]Evaluating:  63%|██████▎   | 198/313 [00:53<00:51,  2.25it/s]Evaluating:  64%|██████▎   | 199/313 [00:53<00:44,  2.54it/s]Evaluating:  64%|██████▍   | 200/313 [00:53<00:38,  2.96it/s]Evaluating:  64%|██████▍   | 201/313 [00:54<00:33,  3.35it/s]Evaluating:  65%|██████▍   | 202/313 [00:54<00:30,  3.65it/s]Evaluating:  65%|██████▍   | 203/313 [00:54<00:29,  3.67it/s]Evaluating:  65%|██████▌   | 204/313 [00:54<00:32,  3.39it/s]Evaluating:  65%|██████▌   | 205/313 [00:55<00:35,  3.02it/s]Evaluating:  66%|██████▌   | 206/313 [00:55<00:34,  3.12it/s]Evaluating:  66%|██████▌   | 207/313 [00:55<00:30,  3.47it/s]Evaluating:  66%|██████▋   | 208/313 [00:56<00:27,  3.77it/s]Evaluating:  67%|██████▋   | 209/313 [00:56<00:25,  4.01it/s]Evaluating:  67%|██████▋   | 210/313 [00:56<00:25,  3.97it/s]Evaluating:  67%|██████▋   | 211/313 [00:56<00:27,  3.65it/s]Evaluating:  68%|██████▊   | 212/313 [00:57<00:31,  3.16it/s]Evaluating:  68%|██████▊   | 213/313 [00:57<00:33,  3.02it/s]Evaluating:  68%|██████▊   | 214/313 [00:57<00:29,  3.37it/s]Evaluating:  69%|██████▊   | 215/313 [00:58<00:26,  3.68it/s]Evaluating:  69%|██████▉   | 216/313 [00:58<00:24,  3.95it/s]Evaluating:  69%|██████▉   | 217/313 [00:58<00:23,  4.06it/s]Evaluating:  70%|██████▉   | 218/313 [00:58<00:24,  3.81it/s]Evaluating:  70%|██████▉   | 219/313 [00:59<00:28,  3.35it/s]Evaluating:  70%|███████   | 220/313 [00:59<00:30,  3.04it/s]Evaluating:  71%|███████   | 221/313 [00:59<00:28,  3.19it/s]Evaluating:  71%|███████   | 222/313 [01:00<00:25,  3.53it/s]Evaluating:  71%|███████   | 223/313 [01:00<00:23,  3.83it/s]Evaluating:  72%|███████▏  | 224/313 [01:00<00:22,  4.03it/s]Evaluating:  72%|███████▏  | 225/313 [01:00<00:22,  3.93it/s]Evaluating:  72%|███████▏  | 226/313 [01:01<00:24,  3.52it/s]Evaluating:  73%|███████▎  | 227/313 [01:01<00:27,  3.15it/s]Evaluating:  73%|███████▎  | 228/313 [01:01<00:25,  3.32it/s]Evaluating:  73%|███████▎  | 229/313 [01:02<00:23,  3.65it/s]Evaluating:  73%|███████▎  | 230/313 [01:02<00:21,  3.91it/s]Evaluating:  74%|███████▍  | 231/313 [01:02<00:20,  4.08it/s]Evaluating:  74%|███████▍  | 232/313 [01:02<00:20,  3.89it/s]Evaluating:  74%|███████▍  | 233/313 [01:03<00:26,  3.03it/s]Evaluating:  75%|███████▍  | 234/313 [01:03<00:27,  2.90it/s]Evaluating:  75%|███████▌  | 235/313 [01:03<00:23,  3.28it/s]Evaluating:  75%|███████▌  | 236/313 [01:04<00:21,  3.56it/s]Evaluating:  76%|███████▌  | 237/313 [01:04<00:20,  3.79it/s]Evaluating:  76%|███████▌  | 238/313 [01:04<00:20,  3.63it/s]Evaluating:  76%|███████▋  | 239/313 [01:05<00:22,  3.22it/s]Evaluating:  77%|███████▋  | 240/313 [01:05<00:24,  2.93it/s]Evaluating:  77%|███████▋  | 241/313 [01:05<00:22,  3.18it/s]Evaluating:  77%|███████▋  | 242/313 [01:05<00:20,  3.52it/s]Evaluating:  78%|███████▊  | 243/313 [01:06<00:18,  3.81it/s]Evaluating:  78%|███████▊  | 244/313 [01:06<00:17,  3.97it/s]Evaluating:  78%|███████▊  | 245/313 [01:06<00:18,  3.72it/s]Evaluating:  79%|███████▊  | 246/313 [01:07<00:20,  3.29it/s]Evaluating:  79%|███████▉  | 247/313 [01:07<00:22,  2.96it/s]Evaluating:  79%|███████▉  | 248/313 [01:07<00:20,  3.17it/s]Evaluating:  80%|███████▉  | 249/313 [01:07<00:18,  3.53it/s]Evaluating:  80%|███████▉  | 250/313 [01:08<00:16,  3.83it/s]Evaluating:  80%|████████  | 251/313 [01:08<00:15,  3.99it/s]Evaluating:  81%|████████  | 252/313 [01:08<00:16,  3.78it/s]Evaluating:  81%|████████  | 253/313 [01:09<00:18,  3.32it/s]Evaluating:  81%|████████  | 254/313 [01:09<00:19,  3.04it/s]Evaluating:  81%|████████▏ | 255/313 [01:09<00:17,  3.24it/s]Evaluating:  82%|████████▏ | 256/313 [01:09<00:15,  3.57it/s]Evaluating:  82%|████████▏ | 257/313 [01:10<00:14,  3.85it/s]Evaluating:  82%|████████▏ | 258/313 [01:10<00:13,  4.05it/s]Evaluating:  83%|████████▎ | 259/313 [01:10<00:13,  4.02it/s]Evaluating:  83%|████████▎ | 260/313 [01:10<00:12,  4.18it/s]Evaluating:  83%|████████▎ | 261/313 [01:11<00:11,  4.36it/s]Evaluating:  84%|████████▎ | 262/313 [01:11<00:11,  4.62it/s]Evaluating:  84%|████████▍ | 263/313 [01:11<00:10,  4.80it/s]Evaluating:  84%|████████▍ | 264/313 [01:11<00:09,  4.92it/s]Evaluating:  85%|████████▍ | 265/313 [01:11<00:09,  4.95it/s]Evaluating:  85%|████████▍ | 266/313 [01:11<00:09,  4.97it/s]Evaluating:  85%|████████▌ | 267/313 [01:12<00:09,  4.98it/s]Evaluating:  86%|████████▌ | 268/313 [01:12<00:08,  5.01it/s]Evaluating:  86%|████████▌ | 269/313 [01:12<00:08,  5.02it/s]Evaluating:  86%|████████▋ | 270/313 [01:12<00:08,  5.03it/s]Evaluating:  87%|████████▋ | 271/313 [01:13<00:09,  4.43it/s]Evaluating:  87%|████████▋ | 272/313 [01:13<00:08,  4.65it/s]Evaluating:  87%|████████▋ | 273/313 [01:13<00:08,  4.82it/s]Evaluating:  88%|████████▊ | 274/313 [01:13<00:07,  4.94it/s]Evaluating:  88%|████████▊ | 275/313 [01:13<00:07,  4.96it/s]Evaluating:  88%|████████▊ | 276/313 [01:14<00:07,  4.88it/s]Evaluating:  88%|████████▊ | 277/313 [01:14<00:07,  4.83it/s]Evaluating:  89%|████████▉ | 278/313 [01:14<00:07,  4.83it/s]Evaluating:  89%|████████▉ | 279/313 [01:14<00:06,  4.88it/s]Evaluating:  89%|████████▉ | 280/313 [01:14<00:06,  4.90it/s]Evaluating:  90%|████████▉ | 281/313 [01:15<00:06,  4.93it/s]Evaluating:  90%|█████████ | 282/313 [01:15<00:06,  4.96it/s]Evaluating:  90%|█████████ | 283/313 [01:15<00:06,  4.99it/s]Evaluating:  91%|█████████ | 284/313 [01:15<00:05,  5.03it/s]Evaluating:  91%|█████████ | 285/313 [01:15<00:05,  5.04it/s]Evaluating:  91%|█████████▏| 286/313 [01:16<00:05,  5.04it/s]Evaluating:  92%|█████████▏| 287/313 [01:16<00:05,  4.92it/s]Evaluating:  92%|█████████▏| 288/313 [01:16<00:05,  4.31it/s]Evaluating:  92%|█████████▏| 289/313 [01:16<00:05,  4.11it/s]Evaluating:  93%|█████████▎| 290/313 [01:17<00:05,  3.84it/s]Evaluating:  93%|█████████▎| 291/313 [01:17<00:05,  3.84it/s]Evaluating:  93%|█████████▎| 292/313 [01:17<00:05,  3.61it/s]Evaluating:  94%|█████████▎| 293/313 [01:17<00:05,  3.70it/s]Evaluating:  94%|█████████▍| 294/313 [01:18<00:04,  3.91it/s]Evaluating:  94%|█████████▍| 295/313 [01:18<00:04,  4.23it/s]Evaluating:  95%|█████████▍| 296/313 [01:18<00:03,  4.52it/s]Evaluating:  95%|█████████▍| 297/313 [01:18<00:03,  4.72it/s]Evaluating:  95%|█████████▌| 298/313 [01:18<00:03,  4.83it/s]Evaluating:  96%|█████████▌| 299/313 [01:19<00:02,  4.86it/s]Evaluating:  96%|█████████▌| 300/313 [01:19<00:02,  4.89it/s]Evaluating:  96%|█████████▌| 301/313 [01:19<00:02,  4.93it/s]Evaluating:  96%|█████████▋| 302/313 [01:19<00:02,  4.95it/s]Evaluating:  97%|█████████▋| 303/313 [01:19<00:02,  4.97it/s]Evaluating:  97%|█████████▋| 304/313 [01:20<00:01,  4.99it/s]Evaluating:  97%|█████████▋| 305/313 [01:20<00:01,  4.99it/s]Evaluating:  98%|█████████▊| 306/313 [01:20<00:01,  5.00it/s]Evaluating:  98%|█████████▊| 307/313 [01:20<00:01,  4.99it/s]Evaluating:  98%|█████████▊| 308/313 [01:20<00:01,  4.99it/s]Evaluating:  99%|█████████▊| 309/313 [01:21<00:00,  4.99it/s]Evaluating:  99%|█████████▉| 310/313 [01:21<00:00,  5.02it/s]Evaluating:  99%|█████████▉| 311/313 [01:21<00:00,  5.04it/s]Evaluating: 100%|█████████▉| 312/313 [01:21<00:00,  5.12it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  5.94it/s]Evaluating: 100%|██████████| 313/313 [01:21<00:00,  3.82it/s]
10/09/2021 00:30:51 - INFO - __main__ -   ***** Evaluation result  in no *****
10/09/2021 00:30:51 - INFO - __main__ -     f1 = 0.7114856792276147
10/09/2021 00:30:51 - INFO - __main__ -     loss = 1.1035757379029125
10/09/2021 00:30:51 - INFO - __main__ -     precision = 0.6649966801472807
10/09/2021 00:30:51 - INFO - __main__ -     recall = 0.7649631995556173
10/09/2021 00:30:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/09/2021 00:30:52 - INFO - __main__ -   ***** Running evaluation  in da *****
10/09/2021 00:30:52 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:30:52 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:29,  3.51it/s]Evaluating:   1%|          | 2/313 [00:00<01:14,  4.20it/s]Evaluating:   1%|          | 3/313 [00:00<01:09,  4.47it/s]Evaluating:   1%|▏         | 4/313 [00:00<01:06,  4.63it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:05,  4.71it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:06,  4.60it/s]Evaluating:   2%|▏         | 7/313 [00:01<01:14,  4.10it/s]Evaluating:   3%|▎         | 8/313 [00:01<01:26,  3.53it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:38,  3.09it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:36,  3.13it/s]Evaluating:   4%|▎         | 11/313 [00:02<01:26,  3.51it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:18,  3.83it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:13,  4.09it/s]Evaluating:   4%|▍         | 14/313 [00:03<01:09,  4.29it/s]Evaluating:   5%|▍         | 15/313 [00:03<01:10,  4.20it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:16,  3.89it/s]Evaluating:   5%|▌         | 17/313 [00:04<01:29,  3.30it/s]Evaluating:   6%|▌         | 18/313 [00:04<01:35,  3.07it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:28,  3.32it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:19,  3.66it/s]Evaluating:   7%|▋         | 21/313 [00:05<01:13,  3.95it/s]Evaluating:   7%|▋         | 22/313 [00:05<01:09,  4.18it/s]Evaluating:   7%|▋         | 23/313 [00:05<01:06,  4.33it/s]Evaluating:   8%|▊         | 24/313 [00:06<01:09,  4.13it/s]Evaluating:   8%|▊         | 25/313 [00:06<01:17,  3.71it/s]Evaluating:   8%|▊         | 26/313 [00:06<01:29,  3.21it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:35,  2.99it/s]Evaluating:   9%|▉         | 28/313 [00:07<01:24,  3.37it/s]Evaluating:   9%|▉         | 29/313 [00:07<01:16,  3.71it/s]Evaluating:  10%|▉         | 30/313 [00:07<01:10,  3.99it/s]Evaluating:  10%|▉         | 31/313 [00:08<01:17,  3.62it/s]Evaluating:  10%|█         | 32/313 [00:08<01:25,  3.30it/s]Evaluating:  11%|█         | 33/313 [00:09<01:29,  3.14it/s]Evaluating:  11%|█         | 34/313 [00:09<01:21,  3.41it/s]Evaluating:  11%|█         | 35/313 [00:09<01:14,  3.74it/s]Evaluating:  12%|█▏        | 36/313 [00:09<01:09,  4.01it/s]Evaluating:  12%|█▏        | 37/313 [00:09<01:05,  4.23it/s]Evaluating:  12%|█▏        | 38/313 [00:10<01:03,  4.34it/s]Evaluating:  12%|█▏        | 39/313 [00:10<01:06,  4.14it/s]Evaluating:  13%|█▎        | 40/313 [00:10<01:13,  3.72it/s]Evaluating:  13%|█▎        | 41/313 [00:11<01:24,  3.21it/s]Evaluating:  13%|█▎        | 42/313 [00:11<01:27,  3.11it/s]Evaluating:  14%|█▎        | 43/313 [00:11<01:17,  3.48it/s]Evaluating:  14%|█▍        | 44/313 [00:11<01:10,  3.80it/s]Evaluating:  14%|█▍        | 45/313 [00:12<01:06,  4.05it/s]Evaluating:  15%|█▍        | 46/313 [00:12<01:02,  4.25it/s]Evaluating:  15%|█▌        | 47/313 [00:12<01:03,  4.17it/s]Evaluating:  15%|█▌        | 48/313 [00:12<01:08,  3.85it/s]Evaluating:  16%|█▌        | 49/313 [00:13<01:20,  3.28it/s]Evaluating:  16%|█▌        | 50/313 [00:13<01:25,  3.06it/s]Evaluating:  16%|█▋        | 51/313 [00:13<01:19,  3.29it/s]Evaluating:  17%|█▋        | 52/313 [00:14<01:11,  3.64it/s]Evaluating:  17%|█▋        | 53/313 [00:14<01:06,  3.93it/s]Evaluating:  17%|█▋        | 54/313 [00:14<01:02,  4.16it/s]Evaluating:  18%|█▊        | 55/313 [00:14<01:00,  4.25it/s]Evaluating:  18%|█▊        | 56/313 [00:15<01:04,  4.01it/s]Evaluating:  18%|█▊        | 57/313 [00:15<01:12,  3.54it/s]Evaluating:  19%|█▊        | 58/313 [00:15<01:21,  3.14it/s]Evaluating:  19%|█▉        | 59/313 [00:16<01:17,  3.28it/s]Evaluating:  19%|█▉        | 60/313 [00:16<01:09,  3.62it/s]Evaluating:  19%|█▉        | 61/313 [00:16<01:04,  3.91it/s]Evaluating:  20%|█▉        | 62/313 [00:16<01:00,  4.15it/s]Evaluating:  20%|██        | 63/313 [00:16<00:58,  4.26it/s]Evaluating:  20%|██        | 64/313 [00:17<01:01,  4.03it/s]Evaluating:  21%|██        | 65/313 [00:17<01:09,  3.58it/s]Evaluating:  21%|██        | 66/313 [00:17<01:18,  3.13it/s]Evaluating:  21%|██▏       | 67/313 [00:18<01:18,  3.14it/s]Evaluating:  22%|██▏       | 68/313 [00:18<01:22,  2.98it/s]Evaluating:  22%|██▏       | 69/313 [00:18<01:23,  2.93it/s]Evaluating:  22%|██▏       | 70/313 [00:19<01:27,  2.76it/s]Evaluating:  23%|██▎       | 71/313 [00:19<01:25,  2.83it/s]Evaluating:  23%|██▎       | 72/313 [00:19<01:14,  3.23it/s]Evaluating:  23%|██▎       | 73/313 [00:20<01:06,  3.59it/s]Evaluating:  24%|██▎       | 74/313 [00:20<01:01,  3.88it/s]Evaluating:  24%|██▍       | 75/313 [00:20<00:58,  4.09it/s]Evaluating:  24%|██▍       | 76/313 [00:20<00:59,  3.97it/s]Evaluating:  25%|██▍       | 77/313 [00:21<01:05,  3.60it/s]Evaluating:  25%|██▍       | 78/313 [00:21<01:14,  3.14it/s]Evaluating:  25%|██▌       | 79/313 [00:21<01:17,  3.03it/s]Evaluating:  26%|██▌       | 80/313 [00:22<01:08,  3.38it/s]Evaluating:  26%|██▌       | 81/313 [00:22<01:02,  3.71it/s]Evaluating:  26%|██▌       | 82/313 [00:22<00:57,  3.99it/s]Evaluating:  27%|██▋       | 83/313 [00:22<00:54,  4.19it/s]Evaluating:  27%|██▋       | 84/313 [00:23<00:55,  4.14it/s]Evaluating:  27%|██▋       | 85/313 [00:23<01:22,  2.76it/s]Evaluating:  27%|██▋       | 86/313 [00:23<01:18,  2.89it/s]Evaluating:  28%|██▊       | 87/313 [00:24<01:08,  3.28it/s]Evaluating:  28%|██▊       | 88/313 [00:24<01:02,  3.63it/s]Evaluating:  28%|██▊       | 89/313 [00:24<00:57,  3.91it/s]Evaluating:  29%|██▉       | 90/313 [00:24<00:55,  4.04it/s]Evaluating:  29%|██▉       | 91/313 [00:25<00:58,  3.81it/s]Evaluating:  29%|██▉       | 92/313 [00:25<01:05,  3.39it/s]Evaluating:  30%|██▉       | 93/313 [00:25<01:12,  3.04it/s]Evaluating:  30%|███       | 94/313 [00:26<01:10,  3.11it/s]Evaluating:  30%|███       | 95/313 [00:26<01:02,  3.48it/s]Evaluating:  31%|███       | 96/313 [00:26<00:57,  3.79it/s]Evaluating:  31%|███       | 97/313 [00:26<00:53,  4.04it/s]Evaluating:  31%|███▏      | 98/313 [00:27<00:52,  4.13it/s]Evaluating:  32%|███▏      | 99/313 [00:27<00:55,  3.83it/s]Evaluating:  32%|███▏      | 100/313 [00:27<01:03,  3.35it/s]Evaluating:  32%|███▏      | 101/313 [00:28<01:09,  3.04it/s]Evaluating:  33%|███▎      | 102/313 [00:28<01:06,  3.19it/s]Evaluating:  33%|███▎      | 103/313 [00:28<00:59,  3.55it/s]Evaluating:  33%|███▎      | 104/313 [00:28<00:54,  3.85it/s]Evaluating:  34%|███▎      | 105/313 [00:29<00:50,  4.10it/s]Evaluating:  34%|███▍      | 106/313 [00:29<00:49,  4.18it/s]Evaluating:  34%|███▍      | 107/313 [00:29<00:53,  3.87it/s]Evaluating:  35%|███▍      | 108/313 [00:29<01:00,  3.42it/s]Evaluating:  35%|███▍      | 109/313 [00:30<01:07,  3.04it/s]Evaluating:  35%|███▌      | 110/313 [00:30<01:02,  3.24it/s]Evaluating:  35%|███▌      | 111/313 [00:30<00:56,  3.58it/s]Evaluating:  36%|███▌      | 112/313 [00:31<00:51,  3.88it/s]Evaluating:  36%|███▌      | 113/313 [00:31<00:48,  4.10it/s]Evaluating:  36%|███▋      | 114/313 [00:31<00:47,  4.19it/s]Evaluating:  37%|███▋      | 115/313 [00:31<00:51,  3.82it/s]Evaluating:  37%|███▋      | 116/313 [00:32<00:58,  3.36it/s]Evaluating:  37%|███▋      | 117/313 [00:32<01:05,  3.01it/s]Evaluating:  38%|███▊      | 118/313 [00:32<01:01,  3.17it/s]Evaluating:  38%|███▊      | 119/313 [00:33<00:55,  3.52it/s]Evaluating:  38%|███▊      | 120/313 [00:33<00:50,  3.85it/s]Evaluating:  39%|███▊      | 121/313 [00:33<00:46,  4.11it/s]Evaluating:  39%|███▉      | 122/313 [00:33<00:46,  4.12it/s]Evaluating:  39%|███▉      | 123/313 [00:34<00:48,  3.90it/s]Evaluating:  40%|███▉      | 124/313 [00:34<00:56,  3.32it/s]Evaluating:  40%|███▉      | 125/313 [00:34<01:00,  3.09it/s]Evaluating:  40%|████      | 126/313 [00:35<00:55,  3.35it/s]Evaluating:  41%|████      | 127/313 [00:35<00:50,  3.67it/s]Evaluating:  41%|████      | 128/313 [00:35<00:46,  3.94it/s]Evaluating:  41%|████      | 129/313 [00:35<00:44,  4.15it/s]Evaluating:  42%|████▏     | 130/313 [00:35<00:45,  4.06it/s]Evaluating:  42%|████▏     | 131/313 [00:36<00:48,  3.72it/s]Evaluating:  42%|████▏     | 132/313 [00:36<00:56,  3.20it/s]Evaluating:  42%|████▏     | 133/313 [00:37<00:59,  3.03it/s]Evaluating:  43%|████▎     | 134/313 [00:37<00:52,  3.40it/s]Evaluating:  43%|████▎     | 135/313 [00:37<00:47,  3.72it/s]Evaluating:  43%|████▎     | 136/313 [00:37<00:44,  3.98it/s]Evaluating:  44%|████▍     | 137/313 [00:37<00:42,  4.09it/s]Evaluating:  44%|████▍     | 138/313 [00:38<00:45,  3.82it/s]Evaluating:  44%|████▍     | 139/313 [00:38<00:53,  3.25it/s]Evaluating:  45%|████▍     | 140/313 [00:39<00:58,  2.94it/s]Evaluating:  45%|████▌     | 141/313 [00:39<00:52,  3.30it/s]Evaluating:  45%|████▌     | 142/313 [00:39<00:47,  3.63it/s]Evaluating:  46%|████▌     | 143/313 [00:39<00:43,  3.92it/s]Evaluating:  46%|████▌     | 144/313 [00:39<00:41,  4.10it/s]Evaluating:  46%|████▋     | 145/313 [00:40<00:42,  3.93it/s]Evaluating:  47%|████▋     | 146/313 [00:40<00:47,  3.53it/s]Evaluating:  47%|████▋     | 147/313 [00:40<00:53,  3.10it/s]Evaluating:  47%|████▋     | 148/313 [00:41<00:53,  3.11it/s]Evaluating:  48%|████▊     | 149/313 [00:41<00:47,  3.47it/s]Evaluating:  48%|████▊     | 150/313 [00:41<00:43,  3.78it/s]Evaluating:  48%|████▊     | 151/313 [00:41<00:40,  4.02it/s]Evaluating:  49%|████▊     | 152/313 [00:42<00:39,  4.12it/s]Evaluating:  49%|████▉     | 153/313 [00:42<00:41,  3.86it/s]Evaluating:  49%|████▉     | 154/313 [00:42<00:47,  3.34it/s]Evaluating:  50%|████▉     | 155/313 [00:43<00:51,  3.09it/s]Evaluating:  50%|████▉     | 156/313 [00:43<00:46,  3.35it/s]Evaluating:  50%|█████     | 157/313 [00:43<00:42,  3.67it/s]Evaluating:  50%|█████     | 158/313 [00:43<00:39,  3.94it/s]Evaluating:  51%|█████     | 159/313 [00:44<00:37,  4.15it/s]Evaluating:  51%|█████     | 160/313 [00:44<00:37,  4.03it/s]Evaluating:  51%|█████▏    | 161/313 [00:44<00:41,  3.69it/s]Evaluating:  52%|█████▏    | 162/313 [00:45<00:47,  3.18it/s]Evaluating:  52%|█████▏    | 163/313 [00:45<00:49,  3.02it/s]Evaluating:  52%|█████▏    | 164/313 [00:45<00:44,  3.33it/s]Evaluating:  53%|█████▎    | 165/313 [00:45<00:40,  3.66it/s]Evaluating:  53%|█████▎    | 166/313 [00:46<00:37,  3.93it/s]Evaluating:  53%|█████▎    | 167/313 [00:46<00:35,  4.09it/s]Evaluating:  54%|█████▎    | 168/313 [00:46<00:37,  3.88it/s]Evaluating:  54%|█████▍    | 169/313 [00:46<00:41,  3.46it/s]Evaluating:  54%|█████▍    | 170/313 [00:47<00:46,  3.06it/s]Evaluating:  55%|█████▍    | 171/313 [00:47<00:46,  3.04it/s]Evaluating:  55%|█████▍    | 172/313 [00:47<00:41,  3.41it/s]Evaluating:  55%|█████▌    | 173/313 [00:48<00:37,  3.73it/s]Evaluating:  56%|█████▌    | 174/313 [00:48<00:34,  3.98it/s]Evaluating:  56%|█████▌    | 175/313 [00:48<00:34,  3.96it/s]Evaluating:  56%|█████▌    | 176/313 [00:48<00:37,  3.66it/s]Evaluating:  57%|█████▋    | 177/313 [00:49<00:42,  3.17it/s]Evaluating:  57%|█████▋    | 178/313 [00:49<00:44,  3.06it/s]Evaluating:  57%|█████▋    | 179/313 [00:49<00:39,  3.42it/s]Evaluating:  58%|█████▊    | 180/313 [00:50<00:35,  3.73it/s]Evaluating:  58%|█████▊    | 181/313 [00:50<00:33,  3.99it/s]Evaluating:  58%|█████▊    | 182/313 [00:50<00:31,  4.10it/s]Evaluating:  58%|█████▊    | 183/313 [00:50<00:33,  3.83it/s]Evaluating:  59%|█████▉    | 184/313 [00:51<00:38,  3.37it/s]Evaluating:  59%|█████▉    | 185/313 [00:51<00:41,  3.06it/s]Evaluating:  59%|█████▉    | 186/313 [00:51<00:39,  3.22it/s]Evaluating:  60%|█████▉    | 187/313 [00:52<00:35,  3.55it/s]Evaluating:  60%|██████    | 188/313 [00:52<00:32,  3.85it/s]Evaluating:  60%|██████    | 189/313 [00:52<00:30,  4.08it/s]Evaluating:  61%|██████    | 190/313 [00:52<00:31,  3.96it/s]Evaluating:  61%|██████    | 191/313 [00:53<00:33,  3.59it/s]Evaluating:  61%|██████▏   | 192/313 [00:53<00:38,  3.13it/s]Evaluating:  62%|██████▏   | 193/313 [00:53<00:40,  2.99it/s]Evaluating:  62%|██████▏   | 194/313 [00:54<00:35,  3.36it/s]Evaluating:  62%|██████▏   | 195/313 [00:54<00:32,  3.67it/s]Evaluating:  63%|██████▎   | 196/313 [00:54<00:29,  3.95it/s]Evaluating:  63%|██████▎   | 197/313 [00:54<00:29,  3.92it/s]Evaluating:  63%|██████▎   | 198/313 [00:55<00:31,  3.61it/s]Evaluating:  64%|██████▎   | 199/313 [00:55<00:36,  3.14it/s]Evaluating:  64%|██████▍   | 200/313 [00:55<00:37,  3.01it/s]Evaluating:  64%|██████▍   | 201/313 [00:56<00:33,  3.36it/s]Evaluating:  65%|██████▍   | 202/313 [00:56<00:30,  3.68it/s]Evaluating:  65%|██████▍   | 203/313 [00:56<00:27,  3.94it/s]Evaluating:  65%|██████▌   | 204/313 [00:56<00:26,  4.05it/s]Evaluating:  65%|██████▌   | 205/313 [00:57<00:28,  3.76it/s]Evaluating:  66%|██████▌   | 206/313 [00:57<00:32,  3.33it/s]Evaluating:  66%|██████▌   | 207/313 [00:57<00:35,  2.98it/s]Evaluating:  66%|██████▋   | 208/313 [00:58<00:33,  3.15it/s]Evaluating:  67%|██████▋   | 209/313 [00:58<00:29,  3.50it/s]Evaluating:  67%|██████▋   | 210/313 [00:58<00:27,  3.79it/s]Evaluating:  67%|██████▋   | 211/313 [00:58<00:25,  3.97it/s]Evaluating:  68%|██████▊   | 212/313 [00:59<00:26,  3.81it/s]Evaluating:  68%|██████▊   | 213/313 [00:59<00:29,  3.38it/s]Evaluating:  68%|██████▊   | 214/313 [00:59<00:32,  3.09it/s]Evaluating:  69%|██████▊   | 215/313 [01:00<00:29,  3.31it/s]Evaluating:  69%|██████▉   | 216/313 [01:00<00:26,  3.63it/s]Evaluating:  69%|██████▉   | 217/313 [01:00<00:24,  3.90it/s]Evaluating:  70%|██████▉   | 218/313 [01:00<00:23,  4.02it/s]Evaluating:  70%|██████▉   | 219/313 [01:01<00:24,  3.76it/s]Evaluating:  70%|███████   | 220/313 [01:01<00:27,  3.34it/s]Evaluating:  71%|███████   | 221/313 [01:01<00:30,  2.98it/s]Evaluating:  71%|███████   | 222/313 [01:02<00:29,  3.13it/s]Evaluating:  71%|███████   | 223/313 [01:02<00:25,  3.47it/s]Evaluating:  72%|███████▏  | 224/313 [01:02<00:23,  3.77it/s]Evaluating:  72%|███████▏  | 225/313 [01:02<00:21,  4.00it/s]Evaluating:  72%|███████▏  | 226/313 [01:03<00:20,  4.15it/s]Evaluating:  73%|███████▎  | 227/313 [01:03<00:23,  3.69it/s]Evaluating:  73%|███████▎  | 228/313 [01:03<00:20,  4.05it/s]Evaluating:  73%|███████▎  | 229/313 [01:03<00:19,  4.35it/s]Evaluating:  73%|███████▎  | 230/313 [01:03<00:18,  4.59it/s]Evaluating:  74%|███████▍  | 231/313 [01:04<00:17,  4.77it/s]Evaluating:  74%|███████▍  | 232/313 [01:04<00:16,  4.91it/s]Evaluating:  74%|███████▍  | 233/313 [01:04<00:15,  5.03it/s]Evaluating:  75%|███████▍  | 234/313 [01:04<00:15,  5.14it/s]Evaluating:  75%|███████▌  | 235/313 [01:04<00:14,  5.24it/s]Evaluating:  75%|███████▌  | 236/313 [01:05<00:14,  5.35it/s]Evaluating:  76%|███████▌  | 237/313 [01:05<00:13,  5.43it/s]Evaluating:  76%|███████▌  | 238/313 [01:05<00:13,  5.44it/s]Evaluating:  76%|███████▋  | 239/313 [01:05<00:13,  5.42it/s]Evaluating:  77%|███████▋  | 240/313 [01:05<00:13,  5.42it/s]Evaluating:  77%|███████▋  | 241/313 [01:05<00:13,  5.40it/s]Evaluating:  77%|███████▋  | 242/313 [01:06<00:13,  5.35it/s]Evaluating:  78%|███████▊  | 243/313 [01:06<00:13,  5.32it/s]Evaluating:  78%|███████▊  | 244/313 [01:06<00:12,  5.32it/s]Evaluating:  78%|███████▊  | 245/313 [01:06<00:12,  5.31it/s]Evaluating:  79%|███████▊  | 246/313 [01:06<00:12,  5.32it/s]Evaluating:  79%|███████▉  | 247/313 [01:07<00:12,  5.34it/s]Evaluating:  79%|███████▉  | 248/313 [01:07<00:12,  5.37it/s]Evaluating:  80%|███████▉  | 249/313 [01:07<00:11,  5.41it/s]Evaluating:  80%|███████▉  | 250/313 [01:07<00:12,  5.16it/s]Evaluating:  80%|████████  | 251/313 [01:07<00:12,  4.91it/s]Evaluating:  81%|████████  | 252/313 [01:08<00:12,  4.81it/s]Evaluating:  81%|████████  | 253/313 [01:08<00:16,  3.56it/s]Evaluating:  81%|████████  | 254/313 [01:08<00:15,  3.90it/s]Evaluating:  81%|████████▏ | 255/313 [01:08<00:13,  4.20it/s]Evaluating:  82%|████████▏ | 256/313 [01:09<00:12,  4.46it/s]Evaluating:  82%|████████▏ | 257/313 [01:09<00:12,  4.45it/s]Evaluating:  82%|████████▏ | 258/313 [01:09<00:13,  4.03it/s]Evaluating:  83%|████████▎ | 259/313 [01:09<00:13,  4.01it/s]Evaluating:  83%|████████▎ | 260/313 [01:10<00:13,  3.82it/s]Evaluating:  83%|████████▎ | 261/313 [01:10<00:13,  3.83it/s]Evaluating:  84%|████████▎ | 262/313 [01:10<00:13,  3.70it/s]Evaluating:  84%|████████▍ | 263/313 [01:11<00:13,  3.66it/s]Evaluating:  84%|████████▍ | 264/313 [01:11<00:12,  3.90it/s]Evaluating:  85%|████████▍ | 265/313 [01:11<00:11,  4.22it/s]Evaluating:  85%|████████▍ | 266/313 [01:11<00:10,  4.51it/s]Evaluating:  85%|████████▌ | 267/313 [01:11<00:09,  4.75it/s]Evaluating:  86%|████████▌ | 268/313 [01:12<00:09,  4.95it/s]Evaluating:  86%|████████▌ | 269/313 [01:12<00:08,  5.10it/s]Evaluating:  86%|████████▋ | 270/313 [01:12<00:08,  5.24it/s]Evaluating:  87%|████████▋ | 271/313 [01:12<00:07,  5.32it/s]Evaluating:  87%|████████▋ | 272/313 [01:12<00:07,  5.36it/s]Evaluating:  87%|████████▋ | 273/313 [01:12<00:07,  5.30it/s]Evaluating:  88%|████████▊ | 274/313 [01:13<00:07,  5.26it/s]Evaluating:  88%|████████▊ | 275/313 [01:13<00:07,  5.23it/s]Evaluating:  88%|████████▊ | 276/313 [01:13<00:07,  5.24it/s]Evaluating:  88%|████████▊ | 277/313 [01:13<00:06,  5.24it/s]Evaluating:  89%|████████▉ | 278/313 [01:13<00:06,  5.26it/s]Evaluating:  89%|████████▉ | 279/313 [01:14<00:06,  5.27it/s]Evaluating:  89%|████████▉ | 280/313 [01:14<00:06,  5.28it/s]Evaluating:  90%|████████▉ | 281/313 [01:14<00:06,  5.30it/s]Evaluating:  90%|█████████ | 282/313 [01:14<00:05,  5.33it/s]Evaluating:  90%|█████████ | 283/313 [01:14<00:05,  5.36it/s]Evaluating:  91%|█████████ | 284/313 [01:15<00:05,  5.38it/s]Evaluating:  91%|█████████ | 285/313 [01:15<00:05,  4.78it/s]Evaluating:  91%|█████████▏| 286/313 [01:15<00:06,  4.24it/s]Evaluating:  92%|█████████▏| 287/313 [01:15<00:06,  4.32it/s]Evaluating:  92%|█████████▏| 288/313 [01:16<00:05,  4.33it/s]Evaluating:  92%|█████████▏| 289/313 [01:16<00:05,  4.08it/s]Evaluating:  93%|█████████▎| 290/313 [01:16<00:06,  3.66it/s]Evaluating:  93%|█████████▎| 291/313 [01:17<00:06,  3.15it/s]Evaluating:  93%|█████████▎| 292/313 [01:17<00:06,  3.08it/s]Evaluating:  94%|█████████▎| 293/313 [01:17<00:05,  3.40it/s]Evaluating:  94%|█████████▍| 294/313 [01:17<00:05,  3.69it/s]Evaluating:  94%|█████████▍| 295/313 [01:18<00:04,  3.86it/s]Evaluating:  95%|█████████▍| 296/313 [01:18<00:04,  3.63it/s]Evaluating:  95%|█████████▍| 297/313 [01:18<00:05,  3.17it/s]Evaluating:  95%|█████████▌| 298/313 [01:19<00:05,  2.90it/s]Evaluating:  96%|█████████▌| 299/313 [01:19<00:04,  3.25it/s]Evaluating:  96%|█████████▌| 300/313 [01:19<00:03,  3.55it/s]Evaluating:  96%|█████████▌| 301/313 [01:19<00:03,  3.72it/s]Evaluating:  96%|█████████▋| 302/313 [01:20<00:03,  3.59it/s]Evaluating:  97%|█████████▋| 303/313 [01:20<00:03,  3.10it/s]Evaluating:  97%|█████████▋| 304/313 [01:20<00:02,  3.01it/s]Evaluating:  97%|█████████▋| 305/313 [01:21<00:02,  3.35it/s]Evaluating:  98%|█████████▊| 306/313 [01:21<00:01,  3.65it/s]Evaluating:  98%|█████████▊| 307/313 [01:21<00:01,  3.76it/s]Evaluating:  98%|█████████▊| 308/313 [01:21<00:01,  3.61it/s]Evaluating:  99%|█████████▊| 309/313 [01:22<00:01,  3.16it/s]Evaluating:  99%|█████████▉| 310/313 [01:22<00:00,  3.10it/s]Evaluating:  99%|█████████▉| 311/313 [01:22<00:00,  3.43it/s]Evaluating: 100%|█████████▉| 312/313 [01:23<00:00,  3.72it/s]Evaluating: 100%|██████████| 313/313 [01:23<00:00,  4.30it/s]Evaluating: 100%|██████████| 313/313 [01:23<00:00,  3.75it/s]
10/09/2021 00:32:17 - INFO - __main__ -   ***** Evaluation result  in da *****
10/09/2021 00:32:17 - INFO - __main__ -     f1 = 0.8013201320132013
10/09/2021 00:32:17 - INFO - __main__ -     loss = 0.7536702552161658
10/09/2021 00:32:17 - INFO - __main__ -     precision = 0.7644329555246239
10/09/2021 00:32:17 - INFO - __main__ -     recall = 0.8419477295955614
10/09/2021 00:32:17 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:32:36 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:32:36 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:33:08 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:36:56 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:36:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:36:56 - INFO - __main__ -   Seed = 12
10/09/2021 00:36:56 - INFO - root -   save model
10/09/2021 00:36:56 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:36:56 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:37:24 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:37:24 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:37:24 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/09/2021 00:37:24 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:37:24 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:37:24 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/09/2021 00:37:24 - INFO - __main__ -   Language = en
10/09/2021 00:37:24 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:37:26 - INFO - __main__ -   Language = ru
10/09/2021 00:37:26 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/09/2021 00:37:33 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/09/2021 00:37:33 - INFO - __main__ -   ***** Running evaluation  in be *****
10/09/2021 00:37:33 - INFO - __main__ -     Num examples = 1001
10/09/2021 00:37:33 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.79it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.13it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.29it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.47it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.57it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.63it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.68it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.70it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.70it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.73it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.71it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.71it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.71it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.73it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.74it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.74it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.73it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.74it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.70it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.71it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.69it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.70it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.58it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.63it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.64it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.67it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.67it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.69it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.71it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.70it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.70it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.80it/s]
10/09/2021 00:37:38 - INFO - __main__ -   ***** Evaluation result  in be *****
10/09/2021 00:37:38 - INFO - __main__ -     f1 = 0.6764356435643565
10/09/2021 00:37:38 - INFO - __main__ -     loss = 0.3526336667127907
10/09/2021 00:37:38 - INFO - __main__ -     precision = 0.6529051987767585
10/09/2021 00:37:38 - INFO - __main__ -     recall = 0.7017255546425637
10/09/2021 00:37:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/09/2021 00:37:40 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/09/2021 00:37:40 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:37:40 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.80it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.75it/s]Evaluating:   1%|          | 3/313 [00:00<00:45,  6.77it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.74it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:47,  6.53it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:46,  6.60it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:46,  6.58it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:46,  6.62it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.64it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.66it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:45,  6.69it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:45,  6.68it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.68it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.69it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.71it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.70it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.70it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:44,  6.70it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.72it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.72it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.71it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.71it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.72it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.71it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.71it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.70it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.70it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.68it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.68it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.69it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.67it/s]Evaluating:  10%|█         | 32/313 [00:04<00:42,  6.57it/s]Evaluating:  11%|█         | 33/313 [00:04<00:42,  6.61it/s]Evaluating:  11%|█         | 34/313 [00:05<00:58,  4.79it/s]Evaluating:  11%|█         | 35/313 [00:05<00:53,  5.23it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:49,  5.60it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:47,  5.87it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:45,  6.08it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:43,  6.24it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:43,  6.34it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:42,  6.40it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:41,  6.48it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:41,  6.53it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:41,  6.56it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:41,  6.45it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:41,  6.49it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.55it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:40,  6.57it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:40,  6.59it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.61it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.61it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:39,  6.62it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:39,  6.63it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:39,  6.62it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:38,  6.62it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:38,  6.62it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.63it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.63it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:38,  6.61it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:38,  6.63it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.62it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:39,  6.43it/s]Evaluating:  20%|██        | 63/313 [00:09<00:38,  6.49it/s]Evaluating:  20%|██        | 64/313 [00:09<00:38,  6.53it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.55it/s]Evaluating:  21%|██        | 66/313 [00:10<00:42,  5.85it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:40,  6.06it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:39,  6.22it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:38,  6.32it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:38,  6.39it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:37,  6.45it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:37,  6.49it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:36,  6.51it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:36,  6.54it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:36,  6.56it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:36,  6.53it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:36,  6.52it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:35,  6.53it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:35,  6.55it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:35,  6.56it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:35,  6.57it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:35,  6.56it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:35,  6.56it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:34,  6.57it/s]Evaluating:  27%|██▋       | 85/313 [00:13<00:34,  6.55it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:34,  6.56it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:34,  6.56it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:34,  6.57it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:34,  6.51it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:34,  6.52it/s]Evaluating:  29%|██▉       | 91/313 [00:14<00:34,  6.51it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:33,  6.52it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.53it/s]Evaluating:  30%|███       | 94/313 [00:14<00:33,  6.52it/s]Evaluating:  30%|███       | 95/313 [00:14<00:33,  6.53it/s]Evaluating:  31%|███       | 96/313 [00:14<00:33,  6.52it/s]Evaluating:  31%|███       | 97/313 [00:14<00:33,  6.53it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:40,  5.33it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:37,  5.64it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:36,  5.88it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:34,  6.06it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:34,  6.19it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:33,  6.27it/s]Evaluating:  33%|███▎      | 104/313 [00:16<00:33,  6.33it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:32,  6.39it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:32,  6.43it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.46it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:31,  6.48it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:31,  6.51it/s]Evaluating:  35%|███▌      | 110/313 [00:17<00:31,  6.52it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:31,  6.51it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:30,  6.51it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:30,  6.50it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:30,  6.51it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:30,  6.52it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:30,  6.52it/s]Evaluating:  37%|███▋      | 117/313 [00:18<00:30,  6.53it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:29,  6.50it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:29,  6.48it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.48it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:29,  6.48it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:29,  6.46it/s]Evaluating:  39%|███▉      | 123/313 [00:19<00:29,  6.46it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:29,  6.48it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:29,  6.36it/s]Evaluating:  40%|████      | 126/313 [00:19<00:29,  6.24it/s]Evaluating:  41%|████      | 127/313 [00:19<00:29,  6.32it/s]Evaluating:  41%|████      | 128/313 [00:19<00:29,  6.36it/s]Evaluating:  41%|████      | 129/313 [00:19<00:28,  6.40it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:28,  6.43it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:28,  6.45it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:28,  6.46it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:27,  6.46it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:27,  6.47it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:27,  6.47it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:27,  6.47it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:27,  6.46it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:27,  6.45it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:26,  6.46it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.46it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:26,  6.46it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:26,  6.43it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:26,  6.43it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:27,  6.25it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:26,  6.30it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:26,  6.34it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:26,  6.37it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:25,  6.38it/s]Evaluating:  48%|████▊     | 149/313 [00:23<00:25,  6.40it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:25,  6.38it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:25,  6.36it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:25,  6.35it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:25,  6.34it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:25,  6.28it/s]Evaluating:  50%|████▉     | 155/313 [00:24<00:25,  6.26it/s]Evaluating:  50%|████▉     | 156/313 [00:24<00:25,  6.26it/s]Evaluating:  50%|█████     | 157/313 [00:24<00:24,  6.25it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:24,  6.23it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:24,  6.21it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:24,  6.18it/s]Evaluating:  51%|█████▏    | 161/313 [00:25<00:24,  6.17it/s]Evaluating:  52%|█████▏    | 162/313 [00:25<00:24,  6.17it/s]Evaluating:  52%|█████▏    | 163/313 [00:25<00:24,  6.18it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:24,  6.17it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:24,  6.05it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:24,  6.08it/s]Evaluating:  53%|█████▎    | 167/313 [00:26<00:23,  6.15it/s]Evaluating:  54%|█████▎    | 168/313 [00:26<00:23,  6.22it/s]Evaluating:  54%|█████▍    | 169/313 [00:26<00:23,  6.24it/s]Evaluating:  54%|█████▍    | 170/313 [00:26<00:22,  6.26it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:22,  6.30it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:22,  6.33it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:22,  6.27it/s]Evaluating:  56%|█████▌    | 174/313 [00:27<00:22,  6.22it/s]Evaluating:  56%|█████▌    | 175/313 [00:27<00:22,  6.19it/s]Evaluating:  56%|█████▌    | 176/313 [00:27<00:22,  6.19it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:21,  6.21it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:21,  6.22it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:21,  6.23it/s]Evaluating:  58%|█████▊    | 180/313 [00:28<00:21,  6.18it/s]Evaluating:  58%|█████▊    | 181/313 [00:28<00:21,  6.23it/s]Evaluating:  58%|█████▊    | 182/313 [00:28<00:20,  6.26it/s]Evaluating:  58%|█████▊    | 183/313 [00:28<00:20,  6.27it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:20,  6.28it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:20,  6.29it/s]Evaluating:  59%|█████▉    | 186/313 [00:29<00:20,  6.30it/s]Evaluating:  60%|█████▉    | 187/313 [00:29<00:19,  6.31it/s]Evaluating:  60%|██████    | 188/313 [00:29<00:19,  6.31it/s]Evaluating:  60%|██████    | 189/313 [00:29<00:19,  6.33it/s]Evaluating:  61%|██████    | 190/313 [00:29<00:19,  6.32it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:24,  5.07it/s]Evaluating:  61%|██████▏   | 192/313 [00:30<00:22,  5.29it/s]Evaluating:  62%|██████▏   | 193/313 [00:30<00:21,  5.48it/s]Evaluating:  62%|██████▏   | 194/313 [00:30<00:21,  5.64it/s]Evaluating:  62%|██████▏   | 195/313 [00:30<00:20,  5.77it/s]Evaluating:  63%|██████▎   | 196/313 [00:30<00:19,  5.87it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:19,  5.95it/s]Evaluating:  63%|██████▎   | 198/313 [00:31<00:19,  6.02it/s]Evaluating:  64%|██████▎   | 199/313 [00:31<00:18,  6.03it/s]Evaluating:  64%|██████▍   | 200/313 [00:31<00:18,  6.03it/s]Evaluating:  64%|██████▍   | 201/313 [00:31<00:18,  6.04it/s]Evaluating:  65%|██████▍   | 202/313 [00:31<00:18,  6.06it/s]Evaluating:  65%|██████▍   | 203/313 [00:31<00:18,  6.08it/s]Evaluating:  65%|██████▌   | 204/313 [00:32<00:18,  6.04it/s]Evaluating:  65%|██████▌   | 205/313 [00:32<00:17,  6.02it/s]Evaluating:  66%|██████▌   | 206/313 [00:32<00:17,  6.02it/s]Evaluating:  66%|██████▌   | 207/313 [00:32<00:17,  6.05it/s]Evaluating:  66%|██████▋   | 208/313 [00:32<00:17,  6.08it/s]Evaluating:  67%|██████▋   | 209/313 [00:32<00:17,  6.10it/s]Evaluating:  67%|██████▋   | 210/313 [00:33<00:16,  6.11it/s]Evaluating:  67%|██████▋   | 211/313 [00:33<00:16,  6.14it/s]Evaluating:  68%|██████▊   | 212/313 [00:33<00:16,  6.17it/s]Evaluating:  68%|██████▊   | 213/313 [00:33<00:16,  6.22it/s]Evaluating:  68%|██████▊   | 214/313 [00:33<00:15,  6.25it/s]Evaluating:  69%|██████▊   | 215/313 [00:33<00:15,  6.25it/s]Evaluating:  69%|██████▉   | 216/313 [00:34<00:15,  6.27it/s]Evaluating:  69%|██████▉   | 217/313 [00:34<00:15,  6.29it/s]Evaluating:  70%|██████▉   | 218/313 [00:34<00:15,  6.29it/s]Evaluating:  70%|██████▉   | 219/313 [00:34<00:15,  6.26it/s]Evaluating:  70%|███████   | 220/313 [00:34<00:14,  6.23it/s]Evaluating:  71%|███████   | 221/313 [00:35<00:23,  3.99it/s]Evaluating:  71%|███████   | 222/313 [00:35<00:20,  4.47it/s]Evaluating:  71%|███████   | 223/313 [00:35<00:18,  4.89it/s]Evaluating:  72%|███████▏  | 224/313 [00:35<00:16,  5.25it/s]Evaluating:  72%|███████▏  | 225/313 [00:35<00:15,  5.53it/s]Evaluating:  72%|███████▏  | 226/313 [00:35<00:15,  5.75it/s]Evaluating:  73%|███████▎  | 227/313 [00:36<00:14,  5.89it/s]Evaluating:  73%|███████▎  | 228/313 [00:36<00:14,  6.01it/s]Evaluating:  73%|███████▎  | 229/313 [00:36<00:13,  6.10it/s]Evaluating:  73%|███████▎  | 230/313 [00:36<00:13,  6.12it/s]Evaluating:  74%|███████▍  | 231/313 [00:36<00:13,  6.11it/s]Evaluating:  74%|███████▍  | 232/313 [00:36<00:13,  6.12it/s]Evaluating:  74%|███████▍  | 233/313 [00:37<00:13,  6.12it/s]Evaluating:  75%|███████▍  | 234/313 [00:37<00:12,  6.15it/s]Evaluating:  75%|███████▌  | 235/313 [00:37<00:12,  6.11it/s]Evaluating:  75%|███████▌  | 236/313 [00:37<00:12,  6.01it/s]Evaluating:  76%|███████▌  | 237/313 [00:37<00:12,  5.99it/s]Evaluating:  76%|███████▌  | 238/313 [00:37<00:12,  6.06it/s]Evaluating:  76%|███████▋  | 239/313 [00:38<00:12,  6.09it/s]Evaluating:  77%|███████▋  | 240/313 [00:38<00:11,  6.10it/s]Evaluating:  77%|███████▋  | 241/313 [00:38<00:11,  6.09it/s]Evaluating:  77%|███████▋  | 242/313 [00:38<00:12,  5.89it/s]Evaluating:  78%|███████▊  | 243/313 [00:38<00:11,  5.99it/s]Evaluating:  78%|███████▊  | 244/313 [00:38<00:11,  6.04it/s]Evaluating:  78%|███████▊  | 245/313 [00:39<00:11,  5.93it/s]Evaluating:  79%|███████▊  | 246/313 [00:39<00:11,  6.03it/s]Evaluating:  79%|███████▉  | 247/313 [00:39<00:10,  6.09it/s]Evaluating:  79%|███████▉  | 248/313 [00:39<00:10,  6.10it/s]Evaluating:  80%|███████▉  | 249/313 [00:39<00:10,  6.14it/s]Evaluating:  80%|███████▉  | 250/313 [00:39<00:10,  6.19it/s]Evaluating:  80%|████████  | 251/313 [00:40<00:09,  6.21it/s]Evaluating:  81%|████████  | 252/313 [00:40<00:09,  6.23it/s]Evaluating:  81%|████████  | 253/313 [00:40<00:09,  6.24it/s]Evaluating:  81%|████████  | 254/313 [00:40<00:09,  6.03it/s]Evaluating:  81%|████████▏ | 255/313 [00:40<00:09,  6.06it/s]Evaluating:  82%|████████▏ | 256/313 [00:40<00:09,  6.07it/s]Evaluating:  82%|████████▏ | 257/313 [00:41<00:09,  6.10it/s]Evaluating:  82%|████████▏ | 258/313 [00:41<00:08,  6.15it/s]Evaluating:  83%|████████▎ | 259/313 [00:41<00:08,  6.09it/s]Evaluating:  83%|████████▎ | 260/313 [00:41<00:08,  6.04it/s]Evaluating:  83%|████████▎ | 261/313 [00:41<00:08,  6.04it/s]Evaluating:  84%|████████▎ | 262/313 [00:41<00:08,  6.06it/s]Evaluating:  84%|████████▍ | 263/313 [00:42<00:08,  6.05it/s]Evaluating:  84%|████████▍ | 264/313 [00:42<00:08,  6.05it/s]Evaluating:  85%|████████▍ | 265/313 [00:42<00:07,  6.03it/s]Evaluating:  85%|████████▍ | 266/313 [00:42<00:07,  6.03it/s]Evaluating:  85%|████████▌ | 267/313 [00:42<00:07,  6.04it/s]Evaluating:  86%|████████▌ | 268/313 [00:42<00:07,  6.03it/s]Evaluating:  86%|████████▌ | 269/313 [00:43<00:07,  5.93it/s]Evaluating:  86%|████████▋ | 270/313 [00:43<00:07,  5.81it/s]Evaluating:  87%|████████▋ | 271/313 [00:43<00:07,  5.72it/s]Evaluating:  87%|████████▋ | 272/313 [00:43<00:07,  5.66it/s]Evaluating:  87%|████████▋ | 273/313 [00:43<00:07,  5.60it/s]Evaluating:  88%|████████▊ | 274/313 [00:43<00:06,  5.58it/s]Evaluating:  88%|████████▊ | 275/313 [00:44<00:06,  5.60it/s]Evaluating:  88%|████████▊ | 276/313 [00:44<00:06,  5.62it/s]Evaluating:  88%|████████▊ | 277/313 [00:44<00:06,  5.63it/s]Evaluating:  89%|████████▉ | 278/313 [00:44<00:06,  5.60it/s]Evaluating:  89%|████████▉ | 279/313 [00:44<00:06,  5.59it/s]Evaluating:  89%|████████▉ | 280/313 [00:44<00:05,  5.60it/s]Evaluating:  90%|████████▉ | 281/313 [00:45<00:05,  5.62it/s]Evaluating:  90%|█████████ | 282/313 [00:45<00:05,  5.64it/s]Evaluating:  90%|█████████ | 283/313 [00:45<00:05,  5.68it/s]Evaluating:  91%|█████████ | 284/313 [00:45<00:05,  5.73it/s]Evaluating:  91%|█████████ | 285/313 [00:45<00:04,  5.78it/s]Evaluating:  91%|█████████▏| 286/313 [00:46<00:04,  5.73it/s]Evaluating:  92%|█████████▏| 287/313 [00:46<00:04,  5.68it/s]Evaluating:  92%|█████████▏| 288/313 [00:46<00:04,  5.68it/s]Evaluating:  92%|█████████▏| 289/313 [00:46<00:04,  5.72it/s]Evaluating:  93%|█████████▎| 290/313 [00:46<00:03,  5.77it/s]Evaluating:  93%|█████████▎| 291/313 [00:46<00:03,  5.84it/s]Evaluating:  93%|█████████▎| 292/313 [00:47<00:03,  5.88it/s]Evaluating:  94%|█████████▎| 293/313 [00:47<00:03,  5.93it/s]Evaluating:  94%|█████████▍| 294/313 [00:47<00:03,  5.96it/s]Evaluating:  94%|█████████▍| 295/313 [00:47<00:03,  5.98it/s]Evaluating:  95%|█████████▍| 296/313 [00:47<00:02,  6.00it/s]Evaluating:  95%|█████████▍| 297/313 [00:47<00:02,  5.98it/s]Evaluating:  95%|█████████▌| 298/313 [00:48<00:02,  5.85it/s]Evaluating:  96%|█████████▌| 299/313 [00:48<00:02,  5.75it/s]Evaluating:  96%|█████████▌| 300/313 [00:48<00:02,  5.70it/s]Evaluating:  96%|█████████▌| 301/313 [00:48<00:02,  5.70it/s]Evaluating:  96%|█████████▋| 302/313 [00:48<00:01,  5.71it/s]Evaluating:  97%|█████████▋| 303/313 [00:48<00:01,  5.76it/s]Evaluating:  97%|█████████▋| 304/313 [00:49<00:01,  5.81it/s]Evaluating:  97%|█████████▋| 305/313 [00:49<00:01,  5.83it/s]Evaluating:  98%|█████████▊| 306/313 [00:49<00:01,  5.85it/s]Evaluating:  98%|█████████▊| 307/313 [00:49<00:01,  5.89it/s]Evaluating:  98%|█████████▊| 308/313 [00:49<00:00,  5.92it/s]Evaluating:  99%|█████████▊| 309/313 [00:49<00:00,  5.90it/s]Evaluating:  99%|█████████▉| 310/313 [00:50<00:00,  5.21it/s]Evaluating:  99%|█████████▉| 311/313 [00:50<00:00,  5.28it/s]Evaluating: 100%|█████████▉| 312/313 [00:50<00:00,  5.36it/s]Evaluating: 100%|██████████| 313/313 [00:50<00:00,  6.14it/s]Evaluating: 100%|██████████| 313/313 [00:50<00:00,  6.17it/s]
10/09/2021 00:38:32 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/09/2021 00:38:32 - INFO - __main__ -     f1 = 0.6315384346244007
10/09/2021 00:38:32 - INFO - __main__ -     loss = 0.42114250366680156
10/09/2021 00:38:32 - INFO - __main__ -     precision = 0.5952380952380952
10/09/2021 00:38:32 - INFO - __main__ -     recall = 0.6725538415679261
10/09/2021 00:38:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/09/2021 00:38:34 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/09/2021 00:38:34 - INFO - __main__ -     Num examples = 10004
10/09/2021 00:38:34 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.03it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.03it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  6.03it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:51,  6.01it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:51,  6.02it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:51,  6.02it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.02it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.02it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:52,  5.83it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:50,  6.00it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:49,  6.13it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:48,  6.20it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:48,  6.23it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:47,  6.29it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:47,  6.33it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:46,  6.33it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:46,  6.32it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:46,  6.32it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:46,  6.30it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:46,  6.26it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:46,  6.21it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:47,  6.17it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:47,  6.15it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:47,  6.12it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:47,  6.05it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  6.04it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  5.98it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  6.00it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:47,  6.02it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:46,  6.03it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:46,  6.03it/s]Evaluating:  10%|█         | 32/313 [00:05<00:46,  6.02it/s]Evaluating:  11%|█         | 33/313 [00:05<00:46,  6.01it/s]Evaluating:  11%|█         | 34/313 [00:05<00:46,  6.01it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  5.99it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:46,  5.97it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:46,  5.97it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:46,  5.98it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  5.96it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  5.97it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  5.97it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:45,  5.98it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:45,  5.98it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  5.98it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:44,  5.97it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  5.98it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  5.99it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:44,  5.97it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:44,  5.97it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:44,  5.98it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  6.00it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  6.02it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  6.01it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:43,  6.00it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:43,  5.99it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:42,  6.00it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  6.01it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  6.00it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  5.99it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:42,  5.98it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:42,  5.97it/s]Evaluating:  20%|██        | 63/313 [00:10<00:41,  5.96it/s]Evaluating:  20%|██        | 64/313 [00:10<00:41,  5.97it/s]Evaluating:  21%|██        | 65/313 [00:10<00:41,  5.97it/s]Evaluating:  21%|██        | 66/313 [00:10<00:41,  5.97it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:41,  5.97it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:41,  5.96it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:40,  5.96it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  5.96it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  5.97it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:40,  5.97it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  5.97it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:40,  5.97it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:39,  5.97it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:39,  5.97it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  5.97it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:39,  5.97it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.99it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:38,  5.98it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:38,  5.98it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:38,  6.00it/s]Evaluating:  27%|██▋       | 83/313 [00:13<00:38,  6.01it/s]Evaluating:  27%|██▋       | 84/313 [00:13<00:38,  6.03it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:37,  6.01it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:37,  6.02it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:37,  6.03it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:37,  6.07it/s]Evaluating:  28%|██▊       | 89/313 [00:14<00:36,  6.09it/s]Evaluating:  29%|██▉       | 90/313 [00:14<00:36,  6.08it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:36,  6.05it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:36,  6.03it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:36,  6.01it/s]Evaluating:  30%|███       | 94/313 [00:15<00:36,  6.01it/s]Evaluating:  30%|███       | 95/313 [00:15<00:36,  6.02it/s]Evaluating:  31%|███       | 96/313 [00:15<00:36,  6.00it/s]Evaluating:  31%|███       | 97/313 [00:16<00:35,  6.03it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:35,  6.05it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:35,  6.07it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:34,  6.09it/s]Evaluating:  32%|███▏      | 101/313 [00:16<00:34,  6.09it/s]Evaluating:  33%|███▎      | 102/313 [00:16<00:34,  6.09it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:34,  6.09it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:34,  6.10it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:34,  6.10it/s]Evaluating:  34%|███▍      | 106/313 [00:17<00:33,  6.11it/s]Evaluating:  34%|███▍      | 107/313 [00:17<00:33,  6.07it/s]Evaluating:  35%|███▍      | 108/313 [00:17<00:33,  6.14it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:32,  6.20it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:32,  6.23it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:32,  6.24it/s]Evaluating:  36%|███▌      | 112/313 [00:18<00:32,  6.26it/s]Evaluating:  36%|███▌      | 113/313 [00:18<00:31,  6.28it/s]Evaluating:  36%|███▋      | 114/313 [00:18<00:31,  6.30it/s]Evaluating:  37%|███▋      | 115/313 [00:18<00:31,  6.31it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:31,  6.32it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:31,  6.31it/s]Evaluating:  38%|███▊      | 118/313 [00:19<00:30,  6.33it/s]Evaluating:  38%|███▊      | 119/313 [00:19<00:30,  6.33it/s]Evaluating:  38%|███▊      | 120/313 [00:19<00:30,  6.34it/s]Evaluating:  39%|███▊      | 121/313 [00:19<00:30,  6.34it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:30,  6.34it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:30,  6.32it/s]Evaluating:  40%|███▉      | 124/313 [00:20<00:30,  6.29it/s]Evaluating:  40%|███▉      | 125/313 [00:20<00:30,  6.25it/s]Evaluating:  40%|████      | 126/313 [00:20<00:29,  6.23it/s]Evaluating:  41%|████      | 127/313 [00:20<00:29,  6.22it/s]Evaluating:  41%|████      | 128/313 [00:21<00:30,  6.17it/s]Evaluating:  41%|████      | 129/313 [00:21<00:30,  6.13it/s]Evaluating:  42%|████▏     | 130/313 [00:21<00:29,  6.13it/s]Evaluating:  42%|████▏     | 131/313 [00:21<00:29,  6.14it/s]Evaluating:  42%|████▏     | 132/313 [00:21<00:29,  6.16it/s]Evaluating:  42%|████▏     | 133/313 [00:21<00:29,  6.14it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:29,  6.11it/s]Evaluating:  43%|████▎     | 135/313 [00:22<00:29,  6.10it/s]Evaluating:  43%|████▎     | 136/313 [00:22<00:29,  6.09it/s]Evaluating:  44%|████▍     | 137/313 [00:22<00:28,  6.08it/s]Evaluating:  44%|████▍     | 138/313 [00:22<00:29,  6.02it/s]Evaluating:  44%|████▍     | 139/313 [00:22<00:29,  5.99it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:28,  5.98it/s]Evaluating:  45%|████▌     | 141/313 [00:23<00:28,  5.97it/s]Evaluating:  45%|████▌     | 142/313 [00:23<00:28,  5.98it/s]Evaluating:  46%|████▌     | 143/313 [00:23<00:28,  5.97it/s]Evaluating:  46%|████▌     | 144/313 [00:23<00:28,  5.95it/s]Evaluating:  46%|████▋     | 145/313 [00:23<00:28,  5.93it/s]Evaluating:  47%|████▋     | 146/313 [00:24<00:28,  5.92it/s]Evaluating:  47%|████▋     | 147/313 [00:24<00:27,  5.93it/s]Evaluating:  47%|████▋     | 148/313 [00:24<00:27,  5.95it/s]Evaluating:  48%|████▊     | 149/313 [00:24<00:27,  5.96it/s]Evaluating:  48%|████▊     | 150/313 [00:24<00:27,  5.95it/s]Evaluating:  48%|████▊     | 151/313 [00:24<00:27,  5.98it/s]Evaluating:  49%|████▊     | 152/313 [00:25<00:26,  6.00it/s]Evaluating:  49%|████▉     | 153/313 [00:25<00:26,  5.99it/s]Evaluating:  49%|████▉     | 154/313 [00:25<00:26,  6.03it/s]Evaluating:  50%|████▉     | 155/313 [00:25<00:26,  6.07it/s]Evaluating:  50%|████▉     | 156/313 [00:25<00:25,  6.11it/s]Evaluating:  50%|█████     | 157/313 [00:25<00:25,  6.13it/s]Evaluating:  50%|█████     | 158/313 [00:26<00:25,  6.16it/s]Evaluating:  51%|█████     | 159/313 [00:26<00:24,  6.21it/s]Evaluating:  51%|█████     | 160/313 [00:26<00:24,  6.24it/s]Evaluating:  51%|█████▏    | 161/313 [00:26<00:27,  5.44it/s]Evaluating:  52%|█████▏    | 162/313 [00:26<00:26,  5.70it/s]Evaluating:  52%|█████▏    | 163/313 [00:26<00:25,  5.91it/s]Evaluating:  52%|█████▏    | 164/313 [00:27<00:24,  6.06it/s]Evaluating:  53%|█████▎    | 165/313 [00:27<00:24,  6.16it/s]Evaluating:  53%|█████▎    | 166/313 [00:27<00:23,  6.22it/s]Evaluating:  53%|█████▎    | 167/313 [00:27<00:23,  6.27it/s]Evaluating:  54%|█████▎    | 168/313 [00:27<00:22,  6.32it/s]Evaluating:  54%|█████▍    | 169/313 [00:27<00:22,  6.35it/s]Evaluating:  54%|█████▍    | 170/313 [00:27<00:22,  6.36it/s]Evaluating:  55%|█████▍    | 171/313 [00:28<00:22,  6.35it/s]Evaluating:  55%|█████▍    | 172/313 [00:28<00:22,  6.36it/s]Evaluating:  55%|█████▌    | 173/313 [00:28<00:22,  6.34it/s]Evaluating:  56%|█████▌    | 174/313 [00:28<00:22,  6.31it/s]Evaluating:  56%|█████▌    | 175/313 [00:28<00:22,  6.27it/s]Evaluating:  56%|█████▌    | 176/313 [00:28<00:21,  6.24it/s]Evaluating:  57%|█████▋    | 177/313 [00:29<00:21,  6.20it/s]Evaluating:  57%|█████▋    | 178/313 [00:29<00:21,  6.18it/s]Evaluating:  57%|█████▋    | 179/313 [00:29<00:21,  6.12it/s]Evaluating:  58%|█████▊    | 180/313 [00:29<00:21,  6.07it/s]Evaluating:  58%|█████▊    | 181/313 [00:29<00:21,  6.02it/s]Evaluating:  58%|█████▊    | 182/313 [00:29<00:21,  6.00it/s]Evaluating:  58%|█████▊    | 183/313 [00:30<00:21,  5.97it/s]Evaluating:  59%|█████▉    | 184/313 [00:30<00:21,  5.93it/s]Evaluating:  59%|█████▉    | 185/313 [00:30<00:21,  5.92it/s]Evaluating:  59%|█████▉    | 186/313 [00:30<00:21,  5.85it/s]Evaluating:  60%|█████▉    | 187/313 [00:30<00:21,  5.89it/s]Evaluating:  60%|██████    | 188/313 [00:30<00:20,  5.95it/s]Evaluating:  60%|██████    | 189/313 [00:31<00:20,  6.00it/s]Evaluating:  61%|██████    | 190/313 [00:31<00:20,  5.99it/s]Evaluating:  61%|██████    | 191/313 [00:31<00:21,  5.67it/s]Evaluating:  61%|██████▏   | 192/313 [00:31<00:20,  5.84it/s]Evaluating:  62%|██████▏   | 193/313 [00:31<00:20,  5.95it/s]Evaluating:  62%|██████▏   | 194/313 [00:31<00:19,  6.04it/s]Evaluating:  62%|██████▏   | 195/313 [00:32<00:19,  6.11it/s]Evaluating:  63%|██████▎   | 196/313 [00:32<00:19,  6.15it/s]Evaluating:  63%|██████▎   | 197/313 [00:32<00:18,  6.17it/s]Evaluating:  63%|██████▎   | 198/313 [00:32<00:18,  6.19it/s]Evaluating:  64%|██████▎   | 199/313 [00:32<00:18,  6.20it/s]Evaluating:  64%|██████▍   | 200/313 [00:32<00:18,  6.21it/s]Evaluating:  64%|██████▍   | 201/313 [00:33<00:18,  6.22it/s]Evaluating:  65%|██████▍   | 202/313 [00:33<00:17,  6.24it/s]Evaluating:  65%|██████▍   | 203/313 [00:33<00:17,  6.27it/s]Evaluating:  65%|██████▌   | 204/313 [00:33<00:17,  6.31it/s]Evaluating:  65%|██████▌   | 205/313 [00:33<00:17,  6.32it/s]Evaluating:  66%|██████▌   | 206/313 [00:33<00:16,  6.30it/s]Evaluating:  66%|██████▌   | 207/313 [00:34<00:16,  6.31it/s]Evaluating:  66%|██████▋   | 208/313 [00:34<00:16,  6.33it/s]Evaluating:  67%|██████▋   | 209/313 [00:34<00:16,  6.28it/s]Evaluating:  67%|██████▋   | 210/313 [00:34<00:16,  6.24it/s]Evaluating:  67%|██████▋   | 211/313 [00:34<00:16,  6.22it/s]Evaluating:  68%|██████▊   | 212/313 [00:34<00:16,  6.22it/s]Evaluating:  68%|██████▊   | 213/313 [00:35<00:16,  6.22it/s]Evaluating:  68%|██████▊   | 214/313 [00:35<00:15,  6.24it/s]Evaluating:  69%|██████▊   | 215/313 [00:35<00:15,  6.26it/s]Evaluating:  69%|██████▉   | 216/313 [00:35<00:15,  6.24it/s]Evaluating:  69%|██████▉   | 217/313 [00:35<00:15,  6.27it/s]Evaluating:  70%|██████▉   | 218/313 [00:35<00:15,  6.29it/s]Evaluating:  70%|██████▉   | 219/313 [00:35<00:14,  6.31it/s]Evaluating:  70%|███████   | 220/313 [00:36<00:14,  6.32it/s]Evaluating:  71%|███████   | 221/313 [00:36<00:14,  6.33it/s]Evaluating:  71%|███████   | 222/313 [00:36<00:15,  5.78it/s]Evaluating:  71%|███████   | 223/313 [00:36<00:15,  5.81it/s]Evaluating:  72%|███████▏  | 224/313 [00:36<00:15,  5.81it/s]Evaluating:  72%|███████▏  | 225/313 [00:37<00:15,  5.82it/s]Evaluating:  72%|███████▏  | 226/313 [00:37<00:14,  5.85it/s]Evaluating:  73%|███████▎  | 227/313 [00:37<00:14,  5.90it/s]Evaluating:  73%|███████▎  | 228/313 [00:37<00:14,  5.97it/s]Evaluating:  73%|███████▎  | 229/313 [00:37<00:13,  6.03it/s]Evaluating:  73%|███████▎  | 230/313 [00:37<00:13,  6.08it/s]Evaluating:  74%|███████▍  | 231/313 [00:37<00:13,  6.12it/s]Evaluating:  74%|███████▍  | 232/313 [00:38<00:13,  6.16it/s]Evaluating:  74%|███████▍  | 233/313 [00:38<00:12,  6.17it/s]Evaluating:  75%|███████▍  | 234/313 [00:38<00:12,  6.19it/s]Evaluating:  75%|███████▌  | 235/313 [00:38<00:12,  6.19it/s]Evaluating:  75%|███████▌  | 236/313 [00:38<00:12,  6.21it/s]Evaluating:  76%|███████▌  | 237/313 [00:38<00:12,  6.24it/s]Evaluating:  76%|███████▌  | 238/313 [00:39<00:11,  6.27it/s]Evaluating:  76%|███████▋  | 239/313 [00:39<00:11,  6.28it/s]Evaluating:  77%|███████▋  | 240/313 [00:39<00:11,  6.28it/s]Evaluating:  77%|███████▋  | 241/313 [00:39<00:11,  6.29it/s]Evaluating:  77%|███████▋  | 242/313 [00:39<00:11,  6.30it/s]Evaluating:  78%|███████▊  | 243/313 [00:39<00:11,  6.24it/s]Evaluating:  78%|███████▊  | 244/313 [00:40<00:11,  6.19it/s]Evaluating:  78%|███████▊  | 245/313 [00:40<00:11,  6.17it/s]Evaluating:  79%|███████▊  | 246/313 [00:40<00:10,  6.15it/s]Evaluating:  79%|███████▉  | 247/313 [00:40<00:10,  6.12it/s]Evaluating:  79%|███████▉  | 248/313 [00:40<00:10,  6.07it/s]Evaluating:  80%|███████▉  | 249/313 [00:40<00:10,  6.04it/s]Evaluating:  80%|███████▉  | 250/313 [00:41<00:10,  6.04it/s]Evaluating:  80%|████████  | 251/313 [00:41<00:10,  6.04it/s]Evaluating:  81%|████████  | 252/313 [00:41<00:10,  6.02it/s]Evaluating:  81%|████████  | 253/313 [00:41<00:14,  4.21it/s]Evaluating:  81%|████████  | 254/313 [00:41<00:12,  4.58it/s]Evaluating:  81%|████████▏ | 255/313 [00:42<00:11,  4.91it/s]Evaluating:  82%|████████▏ | 256/313 [00:42<00:11,  5.17it/s]Evaluating:  82%|████████▏ | 257/313 [00:42<00:10,  5.37it/s]Evaluating:  82%|████████▏ | 258/313 [00:42<00:09,  5.54it/s]Evaluating:  83%|████████▎ | 259/313 [00:42<00:09,  5.64it/s]Evaluating:  83%|████████▎ | 260/313 [00:42<00:09,  5.68it/s]Evaluating:  83%|████████▎ | 261/313 [00:43<00:09,  5.71it/s]Evaluating:  84%|████████▎ | 262/313 [00:43<00:08,  5.74it/s]Evaluating:  84%|████████▍ | 263/313 [00:43<00:08,  5.78it/s]Evaluating:  84%|████████▍ | 264/313 [00:43<00:08,  5.83it/s]Evaluating:  85%|████████▍ | 265/313 [00:43<00:08,  5.86it/s]Evaluating:  85%|████████▍ | 266/313 [00:44<00:08,  5.83it/s]Evaluating:  85%|████████▌ | 267/313 [00:44<00:07,  5.82it/s]Evaluating:  86%|████████▌ | 268/313 [00:44<00:07,  5.82it/s]Evaluating:  86%|████████▌ | 269/313 [00:44<00:07,  5.83it/s]Evaluating:  86%|████████▋ | 270/313 [00:44<00:07,  5.84it/s]Evaluating:  87%|████████▋ | 271/313 [00:44<00:07,  5.84it/s]Evaluating:  87%|████████▋ | 272/313 [00:45<00:07,  5.82it/s]Evaluating:  87%|████████▋ | 273/313 [00:45<00:06,  5.82it/s]Evaluating:  88%|████████▊ | 274/313 [00:45<00:06,  5.82it/s]Evaluating:  88%|████████▊ | 275/313 [00:45<00:06,  5.84it/s]Evaluating:  88%|████████▊ | 276/313 [00:45<00:06,  5.85it/s]Evaluating:  88%|████████▊ | 277/313 [00:45<00:06,  5.86it/s]Evaluating:  89%|████████▉ | 278/313 [00:46<00:05,  5.89it/s]Evaluating:  89%|████████▉ | 279/313 [00:46<00:05,  5.92it/s]Evaluating:  89%|████████▉ | 280/313 [00:46<00:05,  5.95it/s]Evaluating:  90%|████████▉ | 281/313 [00:46<00:06,  4.93it/s]Evaluating:  90%|█████████ | 282/313 [00:46<00:05,  5.26it/s]Evaluating:  90%|█████████ | 283/313 [00:47<00:05,  5.53it/s]Evaluating:  91%|█████████ | 284/313 [00:47<00:05,  5.73it/s]Evaluating:  91%|█████████ | 285/313 [00:47<00:04,  5.89it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:04,  6.00it/s]Evaluating:  92%|█████████▏| 287/313 [00:47<00:04,  6.08it/s]Evaluating:  92%|█████████▏| 288/313 [00:47<00:04,  6.14it/s]Evaluating:  92%|█████████▏| 289/313 [00:47<00:03,  6.18it/s]Evaluating:  93%|█████████▎| 290/313 [00:48<00:03,  6.20it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:03,  6.17it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  6.13it/s]Evaluating:  94%|█████████▎| 293/313 [00:48<00:03,  6.16it/s]Evaluating:  94%|█████████▍| 294/313 [00:48<00:03,  6.10it/s]Evaluating:  94%|█████████▍| 295/313 [00:48<00:02,  6.08it/s]Evaluating:  95%|█████████▍| 296/313 [00:49<00:02,  6.05it/s]Evaluating:  95%|█████████▍| 297/313 [00:49<00:02,  6.05it/s]Evaluating:  95%|█████████▌| 298/313 [00:49<00:02,  6.05it/s]Evaluating:  96%|█████████▌| 299/313 [00:49<00:02,  6.03it/s]Evaluating:  96%|█████████▌| 300/313 [00:49<00:02,  6.02it/s]Evaluating:  96%|█████████▌| 301/313 [00:49<00:02,  5.90it/s]Evaluating:  96%|█████████▋| 302/313 [00:50<00:01,  5.83it/s]Evaluating:  97%|█████████▋| 303/313 [00:50<00:01,  5.83it/s]Evaluating:  97%|█████████▋| 304/313 [00:50<00:01,  5.87it/s]Evaluating:  97%|█████████▋| 305/313 [00:50<00:01,  5.91it/s]Evaluating:  98%|█████████▊| 306/313 [00:50<00:01,  5.92it/s]Evaluating:  98%|█████████▊| 307/313 [00:50<00:01,  5.82it/s]Evaluating:  98%|█████████▊| 308/313 [00:51<00:00,  5.78it/s]Evaluating:  99%|█████████▊| 309/313 [00:51<00:00,  5.79it/s]Evaluating:  99%|█████████▉| 310/313 [00:51<00:00,  5.83it/s]Evaluating:  99%|█████████▉| 311/313 [00:51<00:00,  5.89it/s]Evaluating: 100%|█████████▉| 312/313 [00:51<00:00,  5.93it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.52it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.03it/s]
10/09/2021 00:39:27 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/09/2021 00:39:27 - INFO - __main__ -     f1 = 0.7373676450877898
10/09/2021 00:39:27 - INFO - __main__ -     loss = 0.3216905269188622
10/09/2021 00:39:27 - INFO - __main__ -     precision = 0.6971424950896534
10/09/2021 00:39:27 - INFO - __main__ -     recall = 0.7825190242514757
10/09/2021 00:39:27 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:39:39 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:39:39 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:39:56 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:39:58 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:39:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:39:58 - INFO - __main__ -   Seed = 22
10/09/2021 00:39:58 - INFO - root -   save model
10/09/2021 00:39:58 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:39:58 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:40:12 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:40:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:40:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/09/2021 00:40:12 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:40:12 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:40:12 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/09/2021 00:40:12 - INFO - __main__ -   Language = en
10/09/2021 00:40:12 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:40:13 - INFO - __main__ -   Language = ru
10/09/2021 00:40:13 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/09/2021 00:40:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/09/2021 00:40:18 - INFO - __main__ -   ***** Running evaluation  in be *****
10/09/2021 00:40:18 - INFO - __main__ -     Num examples = 1001
10/09/2021 00:40:18 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.97it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.24it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.44it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.53it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.60it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.63it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.63it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.65it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.67it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.67it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.68it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.67it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.67it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.68it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.68it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.68it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.68it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.67it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.67it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.67it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.67it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.66it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.65it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.65it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.64it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.64it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.63it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.63it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.59it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  5.55it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  5.83it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.63it/s]
10/09/2021 00:40:23 - INFO - __main__ -   ***** Evaluation result  in be *****
10/09/2021 00:40:23 - INFO - __main__ -     f1 = 0.6443860307923394
10/09/2021 00:40:23 - INFO - __main__ -     loss = 1.257279735058546
10/09/2021 00:40:23 - INFO - __main__ -     precision = 0.5933609958506224
10/09/2021 00:40:23 - INFO - __main__ -     recall = 0.705012325390304
10/09/2021 00:40:23 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/09/2021 00:40:25 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/09/2021 00:40:25 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:40:25 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.71it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.67it/s]Evaluating:   1%|          | 3/313 [00:00<00:46,  6.67it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:46,  6.67it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:46,  6.67it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:46,  6.67it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.66it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.66it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.66it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.66it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:45,  6.65it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:45,  6.66it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:45,  6.65it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.65it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.64it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.64it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.64it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:44,  6.65it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:44,  6.64it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:44,  6.63it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:44,  6.61it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:44,  6.61it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.62it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.61it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:43,  6.61it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:43,  6.60it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:43,  6.60it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:43,  6.59it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:43,  6.57it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.58it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.59it/s]Evaluating:  10%|█         | 32/313 [00:04<00:42,  6.59it/s]Evaluating:  11%|█         | 33/313 [00:04<00:42,  6.60it/s]Evaluating:  11%|█         | 34/313 [00:05<00:42,  6.59it/s]Evaluating:  11%|█         | 35/313 [00:05<00:42,  6.58it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:42,  6.59it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.59it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.58it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:41,  6.59it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:41,  6.59it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:41,  6.56it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:41,  6.56it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:41,  6.57it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:41,  6.53it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.54it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:40,  6.56it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.56it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:40,  6.56it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:40,  6.56it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:40,  6.56it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.56it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:39,  6.56it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:39,  6.56it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:39,  6.54it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:39,  6.55it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:39,  6.56it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:39,  6.56it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.56it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:38,  6.56it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:38,  6.56it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.55it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.55it/s]Evaluating:  20%|██        | 63/313 [00:09<00:38,  6.52it/s]Evaluating:  20%|██        | 64/313 [00:09<00:38,  6.52it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.53it/s]Evaluating:  21%|██        | 66/313 [00:10<00:37,  6.54it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:37,  6.52it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:37,  6.49it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:37,  6.47it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:37,  6.48it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:37,  6.46it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:37,  6.47it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:37,  6.47it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:36,  6.49it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:36,  6.48it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:36,  6.47it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:36,  6.47it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:36,  6.49it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:36,  6.48it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:35,  6.48it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:35,  6.49it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:35,  6.49it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:35,  6.49it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:35,  6.49it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:35,  6.48it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:35,  6.48it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:34,  6.47it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:34,  6.47it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:34,  6.47it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:34,  6.46it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:34,  6.47it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:34,  6.46it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:34,  6.45it/s]Evaluating:  30%|███       | 94/313 [00:14<00:33,  6.45it/s]Evaluating:  30%|███       | 95/313 [00:14<00:33,  6.44it/s]Evaluating:  31%|███       | 96/313 [00:14<00:33,  6.40it/s]Evaluating:  31%|███       | 97/313 [00:14<00:34,  6.35it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:34,  6.30it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:34,  6.20it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:34,  6.17it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:34,  6.19it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:34,  6.18it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:34,  6.14it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:34,  6.14it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:33,  6.14it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:33,  6.12it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:33,  6.11it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:33,  6.10it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:33,  6.07it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:33,  6.04it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:33,  6.01it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:33,  5.98it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:33,  5.95it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:33,  5.93it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:33,  5.91it/s]Evaluating:  37%|███▋      | 116/313 [00:18<00:41,  4.72it/s]Evaluating:  37%|███▋      | 117/313 [00:18<00:39,  5.02it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:37,  5.25it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:35,  5.42it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:34,  5.55it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:34,  5.64it/s]Evaluating:  39%|███▉      | 122/313 [00:19<00:33,  5.71it/s]Evaluating:  39%|███▉      | 123/313 [00:19<00:32,  5.76it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:32,  5.79it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:32,  5.81it/s]Evaluating:  40%|████      | 126/313 [00:19<00:32,  5.83it/s]Evaluating:  41%|████      | 127/313 [00:19<00:31,  5.84it/s]Evaluating:  41%|████      | 128/313 [00:20<00:31,  5.85it/s]Evaluating:  41%|████      | 129/313 [00:20<00:31,  5.86it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:31,  5.86it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:31,  5.86it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:30,  5.86it/s]Evaluating:  42%|████▏     | 133/313 [00:21<00:30,  5.86it/s]Evaluating:  43%|████▎     | 134/313 [00:21<00:31,  5.75it/s]Evaluating:  43%|████▎     | 135/313 [00:21<00:30,  5.80it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:30,  5.85it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:29,  5.87it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:29,  5.86it/s]Evaluating:  44%|████▍     | 139/313 [00:22<00:29,  5.84it/s]Evaluating:  45%|████▍     | 140/313 [00:22<00:29,  5.85it/s]Evaluating:  45%|████▌     | 141/313 [00:22<00:29,  5.85it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:29,  5.86it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:28,  5.86it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:33,  5.00it/s]Evaluating:  46%|████▋     | 145/313 [00:23<00:32,  5.23it/s]Evaluating:  47%|████▋     | 146/313 [00:23<00:30,  5.40it/s]Evaluating:  47%|████▋     | 147/313 [00:23<00:30,  5.52it/s]Evaluating:  47%|████▋     | 148/313 [00:23<00:29,  5.62it/s]Evaluating:  48%|████▊     | 149/313 [00:23<00:28,  5.68it/s]Evaluating:  48%|████▊     | 150/313 [00:24<00:28,  5.73it/s]Evaluating:  48%|████▊     | 151/313 [00:24<00:28,  5.76it/s]Evaluating:  49%|████▊     | 152/313 [00:24<00:27,  5.79it/s]Evaluating:  49%|████▉     | 153/313 [00:24<00:27,  5.81it/s]Evaluating:  49%|████▉     | 154/313 [00:24<00:27,  5.81it/s]Evaluating:  50%|████▉     | 155/313 [00:24<00:27,  5.82it/s]Evaluating:  50%|████▉     | 156/313 [00:25<00:26,  5.82it/s]Evaluating:  50%|█████     | 157/313 [00:25<00:26,  5.86it/s]Evaluating:  50%|█████     | 158/313 [00:25<00:26,  5.86it/s]Evaluating:  51%|█████     | 159/313 [00:25<00:26,  5.85it/s]Evaluating:  51%|█████     | 160/313 [00:25<00:26,  5.84it/s]Evaluating:  51%|█████▏    | 161/313 [00:25<00:26,  5.84it/s]Evaluating:  52%|█████▏    | 162/313 [00:26<00:25,  5.83it/s]Evaluating:  52%|█████▏    | 163/313 [00:26<00:25,  5.83it/s]Evaluating:  52%|█████▏    | 164/313 [00:26<00:25,  5.84it/s]Evaluating:  53%|█████▎    | 165/313 [00:26<00:25,  5.83it/s]Evaluating:  53%|█████▎    | 166/313 [00:26<00:25,  5.83it/s]Evaluating:  53%|█████▎    | 167/313 [00:26<00:25,  5.83it/s]Evaluating:  54%|█████▎    | 168/313 [00:27<00:24,  5.82it/s]Evaluating:  54%|█████▍    | 169/313 [00:27<00:24,  5.82it/s]Evaluating:  54%|█████▍    | 170/313 [00:27<00:24,  5.82it/s]Evaluating:  55%|█████▍    | 171/313 [00:27<00:24,  5.82it/s]Evaluating:  55%|█████▍    | 172/313 [00:27<00:24,  5.82it/s]Evaluating:  55%|█████▌    | 173/313 [00:27<00:24,  5.82it/s]Evaluating:  56%|█████▌    | 174/313 [00:28<00:24,  5.70it/s]Evaluating:  56%|█████▌    | 175/313 [00:28<00:23,  5.76it/s]Evaluating:  56%|█████▌    | 176/313 [00:28<00:23,  5.81it/s]Evaluating:  57%|█████▋    | 177/313 [00:28<00:23,  5.83it/s]Evaluating:  57%|█████▋    | 178/313 [00:28<00:23,  5.82it/s]Evaluating:  57%|█████▋    | 179/313 [00:28<00:23,  5.82it/s]Evaluating:  58%|█████▊    | 180/313 [00:29<00:22,  5.79it/s]Evaluating:  58%|█████▊    | 181/313 [00:29<00:22,  5.79it/s]Evaluating:  58%|█████▊    | 182/313 [00:29<00:22,  5.79it/s]Evaluating:  58%|█████▊    | 183/313 [00:29<00:22,  5.80it/s]Evaluating:  59%|█████▉    | 184/313 [00:29<00:22,  5.80it/s]Evaluating:  59%|█████▉    | 185/313 [00:30<00:22,  5.80it/s]Evaluating:  59%|█████▉    | 186/313 [00:30<00:21,  5.80it/s]Evaluating:  60%|█████▉    | 187/313 [00:30<00:21,  5.83it/s]Evaluating:  60%|██████    | 188/313 [00:30<00:21,  5.85it/s]Evaluating:  60%|██████    | 189/313 [00:30<00:21,  5.86it/s]Evaluating:  61%|██████    | 190/313 [00:30<00:21,  5.84it/s]Evaluating:  61%|██████    | 191/313 [00:31<00:20,  5.83it/s]Evaluating:  61%|██████▏   | 192/313 [00:31<00:20,  5.82it/s]Evaluating:  62%|██████▏   | 193/313 [00:31<00:20,  5.82it/s]Evaluating:  62%|██████▏   | 194/313 [00:31<00:20,  5.85it/s]Evaluating:  62%|██████▏   | 195/313 [00:31<00:20,  5.87it/s]Evaluating:  63%|██████▎   | 196/313 [00:31<00:19,  5.87it/s]Evaluating:  63%|██████▎   | 197/313 [00:32<00:19,  5.90it/s]Evaluating:  63%|██████▎   | 198/313 [00:32<00:19,  5.92it/s]Evaluating:  64%|██████▎   | 199/313 [00:32<00:19,  5.94it/s]Evaluating:  64%|██████▍   | 200/313 [00:32<00:18,  5.97it/s]Evaluating:  64%|██████▍   | 201/313 [00:32<00:18,  5.99it/s]Evaluating:  65%|██████▍   | 202/313 [00:33<00:22,  4.84it/s]Evaluating:  65%|██████▍   | 203/313 [00:33<00:21,  5.10it/s]Evaluating:  65%|██████▌   | 204/313 [00:33<00:20,  5.29it/s]Evaluating:  65%|██████▌   | 205/313 [00:33<00:19,  5.45it/s]Evaluating:  66%|██████▌   | 206/313 [00:33<00:19,  5.54it/s]Evaluating:  66%|██████▌   | 207/313 [00:33<00:18,  5.63it/s]Evaluating:  66%|██████▋   | 208/313 [00:34<00:18,  5.69it/s]Evaluating:  67%|██████▋   | 209/313 [00:34<00:18,  5.74it/s]Evaluating:  67%|██████▋   | 210/313 [00:34<00:17,  5.76it/s]Evaluating:  67%|██████▋   | 211/313 [00:34<00:17,  5.76it/s]Evaluating:  68%|██████▊   | 212/313 [00:34<00:17,  5.76it/s]Evaluating:  68%|██████▊   | 213/313 [00:34<00:17,  5.77it/s]Evaluating:  68%|██████▊   | 214/313 [00:35<00:17,  5.78it/s]Evaluating:  69%|██████▊   | 215/313 [00:35<00:16,  5.80it/s]Evaluating:  69%|██████▉   | 216/313 [00:35<00:16,  5.82it/s]Evaluating:  69%|██████▉   | 217/313 [00:35<00:16,  5.78it/s]Evaluating:  70%|██████▉   | 218/313 [00:35<00:16,  5.78it/s]Evaluating:  70%|██████▉   | 219/313 [00:35<00:16,  5.78it/s]Evaluating:  70%|███████   | 220/313 [00:36<00:16,  5.78it/s]Evaluating:  71%|███████   | 221/313 [00:36<00:15,  5.80it/s]Evaluating:  71%|███████   | 222/313 [00:36<00:15,  5.83it/s]Evaluating:  71%|███████   | 223/313 [00:36<00:15,  5.72it/s]Evaluating:  72%|███████▏  | 224/313 [00:36<00:15,  5.82it/s]Evaluating:  72%|███████▏  | 225/313 [00:36<00:14,  5.88it/s]Evaluating:  72%|███████▏  | 226/313 [00:37<00:14,  5.89it/s]Evaluating:  73%|███████▎  | 227/313 [00:37<00:14,  5.92it/s]Evaluating:  73%|███████▎  | 228/313 [00:37<00:14,  5.96it/s]Evaluating:  73%|███████▎  | 229/313 [00:37<00:13,  6.01it/s]Evaluating:  73%|███████▎  | 230/313 [00:37<00:13,  5.93it/s]Evaluating:  74%|███████▍  | 231/313 [00:38<00:13,  5.88it/s]Evaluating:  74%|███████▍  | 232/313 [00:38<00:13,  5.87it/s]Evaluating:  74%|███████▍  | 233/313 [00:38<00:22,  3.57it/s]Evaluating:  75%|███████▍  | 234/313 [00:38<00:19,  4.03it/s]Evaluating:  75%|███████▌  | 235/313 [00:39<00:17,  4.43it/s]Evaluating:  75%|███████▌  | 236/313 [00:39<00:16,  4.76it/s]Evaluating:  76%|███████▌  | 237/313 [00:39<00:15,  5.04it/s]Evaluating:  76%|███████▌  | 238/313 [00:39<00:14,  5.27it/s]Evaluating:  76%|███████▋  | 239/313 [00:39<00:13,  5.44it/s]Evaluating:  77%|███████▋  | 240/313 [00:39<00:13,  5.54it/s]Evaluating:  77%|███████▋  | 241/313 [00:40<00:12,  5.60it/s]Evaluating:  77%|███████▋  | 242/313 [00:40<00:12,  5.65it/s]Evaluating:  78%|███████▊  | 243/313 [00:40<00:12,  5.68it/s]Evaluating:  78%|███████▊  | 244/313 [00:40<00:12,  5.71it/s]Evaluating:  78%|███████▊  | 245/313 [00:40<00:11,  5.73it/s]Evaluating:  79%|███████▊  | 246/313 [00:40<00:11,  5.76it/s]Evaluating:  79%|███████▉  | 247/313 [00:41<00:11,  5.76it/s]Evaluating:  79%|███████▉  | 248/313 [00:41<00:11,  5.78it/s]Evaluating:  80%|███████▉  | 249/313 [00:41<00:11,  5.77it/s]Evaluating:  80%|███████▉  | 250/313 [00:41<00:10,  5.77it/s]Evaluating:  80%|████████  | 251/313 [00:41<00:10,  5.81it/s]Evaluating:  81%|████████  | 252/313 [00:41<00:10,  5.87it/s]Evaluating:  81%|████████  | 253/313 [00:42<00:10,  5.91it/s]Evaluating:  81%|████████  | 254/313 [00:42<00:09,  5.91it/s]Evaluating:  81%|████████▏ | 255/313 [00:42<00:09,  5.91it/s]Evaluating:  82%|████████▏ | 256/313 [00:42<00:09,  5.92it/s]Evaluating:  82%|████████▏ | 257/313 [00:42<00:09,  5.94it/s]Evaluating:  82%|████████▏ | 258/313 [00:42<00:09,  5.95it/s]Evaluating:  83%|████████▎ | 259/313 [00:43<00:09,  5.92it/s]Evaluating:  83%|████████▎ | 260/313 [00:43<00:09,  5.87it/s]Evaluating:  83%|████████▎ | 261/313 [00:43<00:09,  5.69it/s]Evaluating:  84%|████████▎ | 262/313 [00:43<00:08,  5.80it/s]Evaluating:  84%|████████▍ | 263/313 [00:43<00:08,  5.86it/s]Evaluating:  84%|████████▍ | 264/313 [00:44<00:08,  5.92it/s]Evaluating:  85%|████████▍ | 265/313 [00:44<00:08,  5.98it/s]Evaluating:  85%|████████▍ | 266/313 [00:44<00:07,  6.02it/s]Evaluating:  85%|████████▌ | 267/313 [00:44<00:07,  6.04it/s]Evaluating:  86%|████████▌ | 268/313 [00:44<00:07,  6.06it/s]Evaluating:  86%|████████▌ | 269/313 [00:44<00:07,  6.10it/s]Evaluating:  86%|████████▋ | 270/313 [00:45<00:07,  6.13it/s]Evaluating:  87%|████████▋ | 271/313 [00:45<00:06,  6.14it/s]Evaluating:  87%|████████▋ | 272/313 [00:45<00:06,  6.15it/s]Evaluating:  87%|████████▋ | 273/313 [00:45<00:06,  6.16it/s]Evaluating:  88%|████████▊ | 274/313 [00:45<00:06,  6.17it/s]Evaluating:  88%|████████▊ | 275/313 [00:45<00:06,  6.14it/s]Evaluating:  88%|████████▊ | 276/313 [00:45<00:06,  6.11it/s]Evaluating:  88%|████████▊ | 277/313 [00:46<00:05,  6.11it/s]Evaluating:  89%|████████▉ | 278/313 [00:46<00:05,  6.13it/s]Evaluating:  89%|████████▉ | 279/313 [00:46<00:05,  5.92it/s]Evaluating:  89%|████████▉ | 280/313 [00:46<00:05,  5.74it/s]Evaluating:  90%|████████▉ | 281/313 [00:46<00:05,  5.61it/s]Evaluating:  90%|█████████ | 282/313 [00:47<00:05,  5.54it/s]Evaluating:  90%|█████████ | 283/313 [00:47<00:05,  5.44it/s]Evaluating:  91%|█████████ | 284/313 [00:47<00:05,  5.44it/s]Evaluating:  91%|█████████ | 285/313 [00:47<00:05,  5.46it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:04,  5.49it/s]Evaluating:  92%|█████████▏| 287/313 [00:47<00:04,  5.52it/s]Evaluating:  92%|█████████▏| 288/313 [00:48<00:04,  5.57it/s]Evaluating:  92%|█████████▏| 289/313 [00:48<00:04,  5.60it/s]Evaluating:  93%|█████████▎| 290/313 [00:48<00:04,  5.65it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:03,  5.68it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  5.73it/s]Evaluating:  94%|█████████▎| 293/313 [00:49<00:03,  5.77it/s]Evaluating:  94%|█████████▍| 294/313 [00:49<00:03,  5.82it/s]Evaluating:  94%|█████████▍| 295/313 [00:49<00:03,  5.84it/s]Evaluating:  95%|█████████▍| 296/313 [00:49<00:02,  5.74it/s]Evaluating:  95%|█████████▍| 297/313 [00:49<00:02,  5.70it/s]Evaluating:  95%|█████████▌| 298/313 [00:49<00:02,  5.73it/s]Evaluating:  96%|█████████▌| 299/313 [00:50<00:02,  5.76it/s]Evaluating:  96%|█████████▌| 300/313 [00:50<00:02,  5.81it/s]Evaluating:  96%|█████████▌| 301/313 [00:50<00:02,  5.68it/s]Evaluating:  96%|█████████▋| 302/313 [00:50<00:01,  5.59it/s]Evaluating:  97%|█████████▋| 303/313 [00:50<00:01,  5.52it/s]Evaluating:  97%|█████████▋| 304/313 [00:50<00:01,  5.47it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  5.45it/s]Evaluating:  98%|█████████▊| 306/313 [00:51<00:01,  5.44it/s]Evaluating:  98%|█████████▊| 307/313 [00:51<00:01,  5.47it/s]Evaluating:  98%|█████████▊| 308/313 [00:51<00:00,  5.51it/s]Evaluating:  99%|█████████▊| 309/313 [00:51<00:00,  5.54it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.59it/s]Evaluating:  99%|█████████▉| 311/313 [00:52<00:00,  5.64it/s]Evaluating: 100%|█████████▉| 312/313 [00:52<00:00,  5.59it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  6.36it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  5.96it/s]
10/09/2021 00:41:18 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/09/2021 00:41:18 - INFO - __main__ -     f1 = 0.5776857259269321
10/09/2021 00:41:18 - INFO - __main__ -     loss = 1.4773216947389487
10/09/2021 00:41:18 - INFO - __main__ -     precision = 0.5305931516247739
10/09/2021 00:41:18 - INFO - __main__ -     recall = 0.633951859303972
10/09/2021 00:41:19 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/09/2021 00:41:20 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/09/2021 00:41:20 - INFO - __main__ -     Num examples = 10004
10/09/2021 00:41:20 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:52,  6.00it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  5.99it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  5.99it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:51,  5.99it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:51,  5.99it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:51,  5.99it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:51,  5.99it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  5.99it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:50,  5.99it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:50,  5.99it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:50,  5.99it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:50,  5.99it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:50,  5.99it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.00it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  5.99it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  5.98it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:49,  5.98it/s]Evaluating:   6%|▌         | 18/313 [00:03<00:49,  5.98it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:49,  5.98it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  5.98it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  5.98it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  5.97it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:48,  5.95it/s]Evaluating:   8%|▊         | 24/313 [00:04<00:48,  5.96it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:48,  5.96it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:48,  5.97it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  5.97it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  5.97it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:47,  5.96it/s]Evaluating:  10%|▉         | 30/313 [00:05<00:47,  5.96it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:47,  5.96it/s]Evaluating:  10%|█         | 32/313 [00:05<00:47,  5.96it/s]Evaluating:  11%|█         | 33/313 [00:05<00:46,  5.97it/s]Evaluating:  11%|█         | 34/313 [00:05<00:47,  5.93it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  5.93it/s]Evaluating:  12%|█▏        | 36/313 [00:06<00:46,  5.94it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:46,  5.94it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:46,  5.94it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:46,  5.95it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  5.95it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  5.95it/s]Evaluating:  13%|█▎        | 42/313 [00:07<00:45,  5.95it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:45,  5.95it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:45,  5.94it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:45,  5.92it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:45,  5.93it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  5.93it/s]Evaluating:  15%|█▌        | 48/313 [00:08<00:50,  5.24it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:47,  5.57it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:45,  5.81it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  6.01it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:42,  6.15it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:41,  6.24it/s]Evaluating:  17%|█▋        | 54/313 [00:09<00:41,  6.28it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:40,  6.31it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:40,  6.33it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:40,  6.32it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:40,  6.30it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:40,  6.28it/s]Evaluating:  19%|█▉        | 60/313 [00:10<00:40,  6.27it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:40,  6.27it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:40,  6.23it/s]Evaluating:  20%|██        | 63/313 [00:10<00:40,  6.16it/s]Evaluating:  20%|██        | 64/313 [00:10<00:40,  6.11it/s]Evaluating:  21%|██        | 65/313 [00:10<00:40,  6.07it/s]Evaluating:  21%|██        | 66/313 [00:10<00:40,  6.03it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:41,  5.99it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:41,  5.97it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:40,  5.96it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  5.95it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  5.94it/s]Evaluating:  23%|██▎       | 72/313 [00:12<00:40,  5.94it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  5.93it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:40,  5.93it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:40,  5.93it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:40,  5.92it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  5.92it/s]Evaluating:  25%|██▍       | 78/313 [00:13<01:01,  3.81it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:54,  4.27it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:50,  4.65it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:46,  4.97it/s]Evaluating:  26%|██▌       | 82/313 [00:14<00:44,  5.22it/s]Evaluating:  27%|██▋       | 83/313 [00:14<00:42,  5.41it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:41,  5.55it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:40,  5.65it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:39,  5.73it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:39,  5.78it/s]Evaluating:  28%|██▊       | 88/313 [00:15<00:38,  5.81it/s]Evaluating:  28%|██▊       | 89/313 [00:15<00:38,  5.84it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:38,  5.86it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:37,  5.87it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:37,  5.88it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:37,  5.88it/s]Evaluating:  30%|███       | 94/313 [00:16<00:37,  5.88it/s]Evaluating:  30%|███       | 95/313 [00:16<00:37,  5.88it/s]Evaluating:  31%|███       | 96/313 [00:16<00:36,  5.88it/s]Evaluating:  31%|███       | 97/313 [00:16<00:36,  5.89it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:36,  5.89it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:36,  5.88it/s]Evaluating:  32%|███▏      | 100/313 [00:17<00:36,  5.88it/s]Evaluating:  32%|███▏      | 101/313 [00:17<00:36,  5.89it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:35,  5.88it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:35,  5.88it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:35,  5.89it/s]Evaluating:  34%|███▎      | 105/313 [00:18<00:42,  4.92it/s]Evaluating:  34%|███▍      | 106/313 [00:18<00:39,  5.18it/s]Evaluating:  34%|███▍      | 107/313 [00:18<00:38,  5.37it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:37,  5.51it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:36,  5.60it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:36,  5.62it/s]Evaluating:  35%|███▌      | 111/313 [00:19<00:35,  5.70it/s]Evaluating:  36%|███▌      | 112/313 [00:19<00:34,  5.76it/s]Evaluating:  36%|███▌      | 113/313 [00:19<00:34,  5.79it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:34,  5.82it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:33,  5.83it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:33,  5.84it/s]Evaluating:  37%|███▋      | 117/313 [00:20<00:33,  5.85it/s]Evaluating:  38%|███▊      | 118/313 [00:20<00:33,  5.86it/s]Evaluating:  38%|███▊      | 119/313 [00:20<00:33,  5.86it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:32,  5.86it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:32,  5.87it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:32,  5.85it/s]Evaluating:  39%|███▉      | 123/313 [00:21<00:32,  5.85it/s]Evaluating:  40%|███▉      | 124/313 [00:21<00:32,  5.85it/s]Evaluating:  40%|███▉      | 125/313 [00:21<00:32,  5.86it/s]Evaluating:  40%|████      | 126/313 [00:21<00:31,  5.87it/s]Evaluating:  41%|████      | 127/313 [00:21<00:31,  5.87it/s]Evaluating:  41%|████      | 128/313 [00:21<00:31,  5.86it/s]Evaluating:  41%|████      | 129/313 [00:22<00:31,  5.86it/s]Evaluating:  42%|████▏     | 130/313 [00:22<00:31,  5.86it/s]Evaluating:  42%|████▏     | 131/313 [00:22<00:31,  5.83it/s]Evaluating:  42%|████▏     | 132/313 [00:22<00:31,  5.82it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:30,  5.84it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:31,  5.69it/s]Evaluating:  43%|████▎     | 135/313 [00:23<00:30,  5.85it/s]Evaluating:  43%|████▎     | 136/313 [00:23<00:29,  5.94it/s]Evaluating:  44%|████▍     | 137/313 [00:23<00:29,  6.01it/s]Evaluating:  44%|████▍     | 138/313 [00:23<00:29,  6.02it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:29,  5.98it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:28,  6.01it/s]Evaluating:  45%|████▌     | 141/313 [00:24<00:28,  6.04it/s]Evaluating:  45%|████▌     | 142/313 [00:24<00:28,  6.00it/s]Evaluating:  46%|████▌     | 143/313 [00:24<00:28,  5.95it/s]Evaluating:  46%|████▌     | 144/313 [00:24<00:28,  5.91it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:28,  5.80it/s]Evaluating:  47%|████▋     | 146/313 [00:25<00:28,  5.79it/s]Evaluating:  47%|████▋     | 147/313 [00:25<00:28,  5.83it/s]Evaluating:  47%|████▋     | 148/313 [00:25<00:28,  5.85it/s]Evaluating:  48%|████▊     | 149/313 [00:25<00:27,  5.87it/s]Evaluating:  48%|████▊     | 150/313 [00:25<00:28,  5.82it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:27,  5.82it/s]Evaluating:  49%|████▊     | 152/313 [00:26<00:27,  5.84it/s]Evaluating:  49%|████▉     | 153/313 [00:26<00:27,  5.75it/s]Evaluating:  49%|████▉     | 154/313 [00:26<00:27,  5.75it/s]Evaluating:  50%|████▉     | 155/313 [00:26<00:27,  5.77it/s]Evaluating:  50%|████▉     | 156/313 [00:26<00:27,  5.78it/s]Evaluating:  50%|█████     | 157/313 [00:26<00:26,  5.82it/s]Evaluating:  50%|█████     | 158/313 [00:27<00:26,  5.84it/s]Evaluating:  51%|█████     | 159/313 [00:27<00:26,  5.87it/s]Evaluating:  51%|█████     | 160/313 [00:27<00:26,  5.88it/s]Evaluating:  51%|█████▏    | 161/313 [00:27<00:26,  5.79it/s]Evaluating:  52%|█████▏    | 162/313 [00:27<00:25,  5.84it/s]Evaluating:  52%|█████▏    | 163/313 [00:28<00:39,  3.81it/s]Evaluating:  52%|█████▏    | 164/313 [00:28<00:34,  4.26it/s]Evaluating:  53%|█████▎    | 165/313 [00:28<00:31,  4.66it/s]Evaluating:  53%|█████▎    | 166/313 [00:28<00:29,  5.01it/s]Evaluating:  53%|█████▎    | 167/313 [00:28<00:27,  5.31it/s]Evaluating:  54%|█████▎    | 168/313 [00:29<00:26,  5.53it/s]Evaluating:  54%|█████▍    | 169/313 [00:29<00:25,  5.65it/s]Evaluating:  54%|█████▍    | 170/313 [00:29<00:24,  5.73it/s]Evaluating:  55%|█████▍    | 171/313 [00:29<00:24,  5.82it/s]Evaluating:  55%|█████▍    | 172/313 [00:29<00:24,  5.78it/s]Evaluating:  55%|█████▌    | 173/313 [00:29<00:23,  5.86it/s]Evaluating:  56%|█████▌    | 174/313 [00:30<00:23,  5.97it/s]Evaluating:  56%|█████▌    | 175/313 [00:30<00:22,  6.06it/s]Evaluating:  56%|█████▌    | 176/313 [00:30<00:22,  6.11it/s]Evaluating:  57%|█████▋    | 177/313 [00:30<00:22,  6.15it/s]Evaluating:  57%|█████▋    | 178/313 [00:30<00:21,  6.17it/s]Evaluating:  57%|█████▋    | 179/313 [00:30<00:21,  6.19it/s]Evaluating:  58%|█████▊    | 180/313 [00:31<00:21,  6.16it/s]Evaluating:  58%|█████▊    | 181/313 [00:31<00:21,  6.15it/s]Evaluating:  58%|█████▊    | 182/313 [00:31<00:21,  6.13it/s]Evaluating:  58%|█████▊    | 183/313 [00:31<00:21,  6.13it/s]Evaluating:  59%|█████▉    | 184/313 [00:31<00:21,  6.08it/s]Evaluating:  59%|█████▉    | 185/313 [00:31<00:21,  5.89it/s]Evaluating:  59%|█████▉    | 186/313 [00:32<00:21,  5.90it/s]Evaluating:  60%|█████▉    | 187/313 [00:32<00:21,  5.97it/s]Evaluating:  60%|██████    | 188/313 [00:32<00:20,  6.05it/s]Evaluating:  60%|██████    | 189/313 [00:32<00:20,  6.11it/s]Evaluating:  61%|██████    | 190/313 [00:32<00:20,  6.14it/s]Evaluating:  61%|██████    | 191/313 [00:32<00:19,  6.17it/s]Evaluating:  61%|██████▏   | 192/313 [00:32<00:19,  6.19it/s]Evaluating:  62%|██████▏   | 193/313 [00:33<00:19,  6.23it/s]Evaluating:  62%|██████▏   | 194/313 [00:33<00:19,  6.26it/s]Evaluating:  62%|██████▏   | 195/313 [00:33<00:18,  6.27it/s]Evaluating:  63%|██████▎   | 196/313 [00:33<00:18,  6.29it/s]Evaluating:  63%|██████▎   | 197/313 [00:33<00:18,  6.30it/s]Evaluating:  63%|██████▎   | 198/313 [00:33<00:18,  6.28it/s]Evaluating:  64%|██████▎   | 199/313 [00:34<00:18,  6.26it/s]Evaluating:  64%|██████▍   | 200/313 [00:34<00:18,  6.06it/s]Evaluating:  64%|██████▍   | 201/313 [00:34<00:18,  6.06it/s]Evaluating:  65%|██████▍   | 202/313 [00:34<00:18,  6.07it/s]Evaluating:  65%|██████▍   | 203/313 [00:34<00:18,  6.09it/s]Evaluating:  65%|██████▌   | 204/313 [00:34<00:18,  6.04it/s]Evaluating:  65%|██████▌   | 205/313 [00:35<00:18,  5.99it/s]Evaluating:  66%|██████▌   | 206/313 [00:35<00:17,  5.97it/s]Evaluating:  66%|██████▌   | 207/313 [00:35<00:17,  5.93it/s]Evaluating:  66%|██████▋   | 208/313 [00:35<00:17,  5.90it/s]Evaluating:  67%|██████▋   | 209/313 [00:35<00:17,  5.91it/s]Evaluating:  67%|██████▋   | 210/313 [00:35<00:17,  5.86it/s]Evaluating:  67%|██████▋   | 211/313 [00:36<00:17,  5.84it/s]Evaluating:  68%|██████▊   | 212/313 [00:36<00:17,  5.86it/s]Evaluating:  68%|██████▊   | 213/313 [00:36<00:16,  5.90it/s]Evaluating:  68%|██████▊   | 214/313 [00:36<00:16,  5.93it/s]Evaluating:  69%|██████▊   | 215/313 [00:36<00:16,  5.97it/s]Evaluating:  69%|██████▉   | 216/313 [00:36<00:16,  6.00it/s]Evaluating:  69%|██████▉   | 217/313 [00:37<00:16,  6.00it/s]Evaluating:  70%|██████▉   | 218/313 [00:37<00:16,  5.93it/s]Evaluating:  70%|██████▉   | 219/313 [00:37<00:15,  5.96it/s]Evaluating:  70%|███████   | 220/313 [00:37<00:15,  6.05it/s]Evaluating:  71%|███████   | 221/313 [00:37<00:15,  6.09it/s]Evaluating:  71%|███████   | 222/313 [00:37<00:14,  6.12it/s]Evaluating:  71%|███████   | 223/313 [00:38<00:14,  6.17it/s]Evaluating:  72%|███████▏  | 224/313 [00:38<00:14,  6.20it/s]Evaluating:  72%|███████▏  | 225/313 [00:38<00:14,  6.23it/s]Evaluating:  72%|███████▏  | 226/313 [00:38<00:13,  6.24it/s]Evaluating:  73%|███████▎  | 227/313 [00:38<00:13,  6.24it/s]Evaluating:  73%|███████▎  | 228/313 [00:38<00:13,  6.25it/s]Evaluating:  73%|███████▎  | 229/313 [00:39<00:13,  6.26it/s]Evaluating:  73%|███████▎  | 230/313 [00:39<00:13,  6.22it/s]Evaluating:  74%|███████▍  | 231/313 [00:39<00:13,  6.17it/s]Evaluating:  74%|███████▍  | 232/313 [00:39<00:13,  6.14it/s]Evaluating:  74%|███████▍  | 233/313 [00:39<00:13,  6.11it/s]Evaluating:  75%|███████▍  | 234/313 [00:39<00:12,  6.11it/s]Evaluating:  75%|███████▌  | 235/313 [00:40<00:12,  6.11it/s]Evaluating:  75%|███████▌  | 236/313 [00:40<00:12,  6.11it/s]Evaluating:  76%|███████▌  | 237/313 [00:40<00:12,  6.12it/s]Evaluating:  76%|███████▌  | 238/313 [00:40<00:12,  6.11it/s]Evaluating:  76%|███████▋  | 239/313 [00:40<00:12,  6.10it/s]Evaluating:  77%|███████▋  | 240/313 [00:40<00:12,  5.91it/s]Evaluating:  77%|███████▋  | 241/313 [00:41<00:12,  5.94it/s]Evaluating:  77%|███████▋  | 242/313 [00:41<00:11,  6.02it/s]Evaluating:  78%|███████▊  | 243/313 [00:41<00:11,  6.06it/s]Evaluating:  78%|███████▊  | 244/313 [00:41<00:11,  6.12it/s]Evaluating:  78%|███████▊  | 245/313 [00:41<00:11,  6.10it/s]Evaluating:  79%|███████▊  | 246/313 [00:41<00:11,  6.07it/s]Evaluating:  79%|███████▉  | 247/313 [00:42<00:10,  6.06it/s]Evaluating:  79%|███████▉  | 248/313 [00:42<00:10,  6.05it/s]Evaluating:  80%|███████▉  | 249/313 [00:42<00:10,  6.05it/s]Evaluating:  80%|███████▉  | 250/313 [00:42<00:10,  6.08it/s]Evaluating:  80%|████████  | 251/313 [00:42<00:10,  6.14it/s]Evaluating:  81%|████████  | 252/313 [00:42<00:09,  6.18it/s]Evaluating:  81%|████████  | 253/313 [00:43<00:09,  6.21it/s]Evaluating:  81%|████████  | 254/313 [00:43<00:09,  6.22it/s]Evaluating:  81%|████████▏ | 255/313 [00:43<00:09,  6.19it/s]Evaluating:  82%|████████▏ | 256/313 [00:43<00:09,  6.16it/s]Evaluating:  82%|████████▏ | 257/313 [00:43<00:09,  6.14it/s]Evaluating:  82%|████████▏ | 258/313 [00:43<00:08,  6.16it/s]Evaluating:  83%|████████▎ | 259/313 [00:43<00:08,  6.16it/s]Evaluating:  83%|████████▎ | 260/313 [00:44<00:08,  6.10it/s]Evaluating:  83%|████████▎ | 261/313 [00:44<00:08,  6.06it/s]Evaluating:  84%|████████▎ | 262/313 [00:44<00:08,  6.05it/s]Evaluating:  84%|████████▍ | 263/313 [00:44<00:08,  6.05it/s]Evaluating:  84%|████████▍ | 264/313 [00:44<00:08,  6.07it/s]Evaluating:  85%|████████▍ | 265/313 [00:44<00:07,  6.03it/s]Evaluating:  85%|████████▍ | 266/313 [00:45<00:08,  5.82it/s]Evaluating:  85%|████████▌ | 267/313 [00:45<00:07,  5.85it/s]Evaluating:  86%|████████▌ | 268/313 [00:45<00:07,  5.92it/s]Evaluating:  86%|████████▌ | 269/313 [00:45<00:07,  5.95it/s]Evaluating:  86%|████████▋ | 270/313 [00:45<00:07,  5.98it/s]Evaluating:  87%|████████▋ | 271/313 [00:46<00:07,  5.99it/s]Evaluating:  87%|████████▋ | 272/313 [00:46<00:06,  6.01it/s]Evaluating:  87%|████████▋ | 273/313 [00:46<00:06,  6.01it/s]Evaluating:  88%|████████▊ | 274/313 [00:46<00:06,  6.07it/s]Evaluating:  88%|████████▊ | 275/313 [00:46<00:06,  6.11it/s]Evaluating:  88%|████████▊ | 276/313 [00:46<00:06,  6.06it/s]Evaluating:  88%|████████▊ | 277/313 [00:47<00:05,  6.03it/s]Evaluating:  89%|████████▉ | 278/313 [00:47<00:05,  6.02it/s]Evaluating:  89%|████████▉ | 279/313 [00:47<00:05,  6.03it/s]Evaluating:  89%|████████▉ | 280/313 [00:47<00:05,  6.04it/s]Evaluating:  90%|████████▉ | 281/313 [00:47<00:05,  6.00it/s]Evaluating:  90%|█████████ | 282/313 [00:47<00:05,  5.96it/s]Evaluating:  90%|█████████ | 283/313 [00:48<00:05,  5.92it/s]Evaluating:  91%|█████████ | 284/313 [00:48<00:04,  5.91it/s]Evaluating:  91%|█████████ | 285/313 [00:48<00:04,  5.93it/s]Evaluating:  91%|█████████▏| 286/313 [00:48<00:04,  5.94it/s]Evaluating:  92%|█████████▏| 287/313 [00:48<00:04,  5.91it/s]Evaluating:  92%|█████████▏| 288/313 [00:48<00:04,  5.89it/s]Evaluating:  92%|█████████▏| 289/313 [00:49<00:04,  5.88it/s]Evaluating:  93%|█████████▎| 290/313 [00:49<00:03,  5.89it/s]Evaluating:  93%|█████████▎| 291/313 [00:49<00:03,  5.87it/s]Evaluating:  93%|█████████▎| 292/313 [00:49<00:03,  5.86it/s]Evaluating:  94%|█████████▎| 293/313 [00:49<00:03,  5.72it/s]Evaluating:  94%|█████████▍| 294/313 [00:49<00:03,  5.67it/s]Evaluating:  94%|█████████▍| 295/313 [00:50<00:03,  5.64it/s]Evaluating:  95%|█████████▍| 296/313 [00:50<00:02,  5.68it/s]Evaluating:  95%|█████████▍| 297/313 [00:50<00:02,  5.74it/s]Evaluating:  95%|█████████▌| 298/313 [00:50<00:02,  5.81it/s]Evaluating:  96%|█████████▌| 299/313 [00:50<00:02,  5.85it/s]Evaluating:  96%|█████████▌| 300/313 [00:50<00:02,  5.78it/s]Evaluating:  96%|█████████▌| 301/313 [00:51<00:02,  5.71it/s]Evaluating:  96%|█████████▋| 302/313 [00:51<00:01,  5.70it/s]Evaluating:  97%|█████████▋| 303/313 [00:51<00:01,  5.64it/s]Evaluating:  97%|█████████▋| 304/313 [00:51<00:01,  5.66it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  5.73it/s]Evaluating:  98%|█████████▊| 306/313 [00:51<00:01,  5.78it/s]Evaluating:  98%|█████████▊| 307/313 [00:52<00:01,  5.69it/s]Evaluating:  98%|█████████▊| 308/313 [00:52<00:00,  5.63it/s]Evaluating:  99%|█████████▊| 309/313 [00:52<00:00,  5.60it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.61it/s]Evaluating:  99%|█████████▉| 311/313 [00:52<00:00,  5.66it/s]Evaluating: 100%|█████████▉| 312/313 [00:53<00:00,  5.17it/s]Evaluating: 100%|██████████| 313/313 [00:53<00:00,  5.85it/s]Evaluating: 100%|██████████| 313/313 [00:53<00:00,  5.88it/s]
10/09/2021 00:42:15 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/09/2021 00:42:15 - INFO - __main__ -     f1 = 0.6989767744031185
10/09/2021 00:42:15 - INFO - __main__ -     loss = 1.1602427635234767
10/09/2021 00:42:15 - INFO - __main__ -     precision = 0.643326955273858
10/09/2021 00:42:15 - INFO - __main__ -     recall = 0.7651660621577413
10/09/2021 00:42:15 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:42:29 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:42:29 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:42:46 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:42:48 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:42:48 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:42:48 - INFO - __main__ -   Seed = 32
10/09/2021 00:42:48 - INFO - root -   save model
10/09/2021 00:42:48 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:42:48 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:43:02 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:43:02 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:43:02 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/09/2021 00:43:02 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:43:02 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:43:02 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/09/2021 00:43:02 - INFO - __main__ -   Language = en
10/09/2021 00:43:02 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:43:04 - INFO - __main__ -   Language = ru
10/09/2021 00:43:04 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/09/2021 00:43:08 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/09/2021 00:43:09 - INFO - __main__ -   ***** Running evaluation  in be *****
10/09/2021 00:43:09 - INFO - __main__ -     Num examples = 1001
10/09/2021 00:43:09 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.48it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.85it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.22it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.42it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.53it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.60it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.64it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.66it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.68it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.71it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.71it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.70it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.70it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.70it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.70it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.70it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.69it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.69it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.69it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.69it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.69it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.69it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.69it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.70it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.69it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.69it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.65it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.65it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.66it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.66it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.66it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.77it/s]
10/09/2021 00:43:13 - INFO - __main__ -   ***** Evaluation result  in be *****
10/09/2021 00:43:13 - INFO - __main__ -     f1 = 0.6661665416354089
10/09/2021 00:43:13 - INFO - __main__ -     loss = 1.3825890184380114
10/09/2021 00:43:13 - INFO - __main__ -     precision = 0.6128364389233955
10/09/2021 00:43:13 - INFO - __main__ -     recall = 0.7296631059983566
10/09/2021 00:43:13 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/09/2021 00:43:15 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/09/2021 00:43:15 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:43:15 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.78it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.73it/s]Evaluating:   1%|          | 3/313 [00:00<00:46,  6.71it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:46,  6.70it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.70it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.70it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.70it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.71it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.71it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.70it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:45,  6.70it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:45,  6.68it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:45,  6.67it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.66it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.67it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.68it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.68it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:44,  6.67it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:44,  6.67it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.67it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:44,  6.63it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.64it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.63it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.63it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:44,  6.44it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:44,  6.50it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:43,  6.52it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:43,  6.53it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:43,  6.56it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:43,  6.58it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.59it/s]Evaluating:  10%|█         | 32/313 [00:04<00:42,  6.61it/s]Evaluating:  11%|█         | 33/313 [00:04<00:42,  6.60it/s]Evaluating:  11%|█         | 34/313 [00:05<00:42,  6.61it/s]Evaluating:  11%|█         | 35/313 [00:05<00:42,  6.61it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.62it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.61it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.62it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:41,  6.62it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:41,  6.61it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:41,  6.60it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:41,  6.58it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:41,  6.57it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.57it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.57it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:40,  6.56it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.54it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:40,  6.53it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:40,  6.53it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:40,  6.52it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:40,  6.53it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:39,  6.53it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:39,  6.52it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:39,  6.51it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:39,  6.53it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:39,  6.55it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:39,  6.55it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:53,  4.77it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:49,  5.18it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:45,  5.53it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:43,  5.80it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:42,  5.97it/s]Evaluating:  20%|██        | 63/313 [00:09<00:41,  6.08it/s]Evaluating:  20%|██        | 64/313 [00:09<00:40,  6.16it/s]Evaluating:  21%|██        | 65/313 [00:10<00:39,  6.23it/s]Evaluating:  21%|██        | 66/313 [00:10<00:39,  6.22it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:39,  6.24it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:39,  6.27it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:38,  6.28it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:38,  6.28it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:38,  6.28it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:38,  6.26it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:38,  6.22it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:38,  6.21it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:38,  6.15it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:38,  6.09it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:38,  6.06it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:38,  6.03it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:38,  6.01it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:38,  6.02it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:38,  6.03it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:38,  6.07it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:37,  6.08it/s]Evaluating:  27%|██▋       | 84/313 [00:13<00:37,  6.13it/s]Evaluating:  27%|██▋       | 85/313 [00:13<00:36,  6.17it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:36,  6.18it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:36,  6.15it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:36,  6.18it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:36,  6.20it/s]Evaluating:  29%|██▉       | 90/313 [00:14<00:36,  6.18it/s]Evaluating:  29%|██▉       | 91/313 [00:14<00:36,  6.15it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:36,  6.10it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:36,  6.07it/s]Evaluating:  30%|███       | 94/313 [00:14<00:36,  6.06it/s]Evaluating:  30%|███       | 95/313 [00:14<00:36,  6.06it/s]Evaluating:  31%|███       | 96/313 [00:15<00:36,  6.03it/s]Evaluating:  31%|███       | 97/313 [00:15<00:35,  6.00it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:35,  5.98it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:35,  5.97it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:35,  5.95it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:35,  5.95it/s]Evaluating:  33%|███▎      | 102/313 [00:16<00:35,  5.94it/s]Evaluating:  33%|███▎      | 103/313 [00:16<00:35,  5.94it/s]Evaluating:  33%|███▎      | 104/313 [00:16<00:35,  5.94it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:34,  5.94it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:34,  5.94it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:34,  5.94it/s]Evaluating:  35%|███▍      | 108/313 [00:17<00:34,  5.95it/s]Evaluating:  35%|███▍      | 109/313 [00:17<00:34,  5.94it/s]Evaluating:  35%|███▌      | 110/313 [00:17<00:34,  5.94it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:34,  5.93it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:33,  5.93it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:33,  5.93it/s]Evaluating:  36%|███▋      | 114/313 [00:18<00:33,  5.93it/s]Evaluating:  37%|███▋      | 115/313 [00:18<00:33,  5.93it/s]Evaluating:  37%|███▋      | 116/313 [00:18<00:33,  5.93it/s]Evaluating:  37%|███▋      | 117/313 [00:18<00:33,  5.93it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:32,  5.93it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:32,  5.93it/s]Evaluating:  38%|███▊      | 120/313 [00:19<00:32,  5.94it/s]Evaluating:  39%|███▊      | 121/313 [00:19<00:32,  5.93it/s]Evaluating:  39%|███▉      | 122/313 [00:19<00:32,  5.93it/s]Evaluating:  39%|███▉      | 123/313 [00:19<00:32,  5.93it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:31,  5.93it/s]Evaluating:  40%|███▉      | 125/313 [00:20<00:31,  5.93it/s]Evaluating:  40%|████      | 126/313 [00:20<00:31,  5.93it/s]Evaluating:  41%|████      | 127/313 [00:20<00:31,  5.91it/s]Evaluating:  41%|████      | 128/313 [00:20<00:31,  5.91it/s]Evaluating:  41%|████      | 129/313 [00:20<00:31,  5.91it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:30,  5.92it/s]Evaluating:  42%|████▏     | 131/313 [00:21<00:30,  5.92it/s]Evaluating:  42%|████▏     | 132/313 [00:21<00:30,  5.92it/s]Evaluating:  42%|████▏     | 133/313 [00:21<00:30,  5.91it/s]Evaluating:  43%|████▎     | 134/313 [00:21<00:30,  5.91it/s]Evaluating:  43%|████▎     | 135/313 [00:21<00:30,  5.91it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:29,  5.91it/s]Evaluating:  44%|████▍     | 137/313 [00:22<00:29,  5.91it/s]Evaluating:  44%|████▍     | 138/313 [00:22<00:29,  5.91it/s]Evaluating:  44%|████▍     | 139/313 [00:22<00:29,  5.91it/s]Evaluating:  45%|████▍     | 140/313 [00:22<00:29,  5.91it/s]Evaluating:  45%|████▌     | 141/313 [00:22<00:29,  5.91it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:28,  5.91it/s]Evaluating:  46%|████▌     | 143/313 [00:23<00:28,  5.90it/s]Evaluating:  46%|████▌     | 144/313 [00:23<00:28,  5.90it/s]Evaluating:  46%|████▋     | 145/313 [00:23<00:28,  5.92it/s]Evaluating:  47%|████▋     | 146/313 [00:23<00:28,  5.93it/s]Evaluating:  47%|████▋     | 147/313 [00:23<00:28,  5.91it/s]Evaluating:  47%|████▋     | 148/313 [00:23<00:27,  5.91it/s]Evaluating:  48%|████▊     | 149/313 [00:24<00:27,  5.90it/s]Evaluating:  48%|████▊     | 150/313 [00:24<00:27,  5.92it/s]Evaluating:  48%|████▊     | 151/313 [00:24<00:27,  5.92it/s]Evaluating:  49%|████▊     | 152/313 [00:24<00:27,  5.91it/s]Evaluating:  49%|████▉     | 153/313 [00:24<00:27,  5.90it/s]Evaluating:  49%|████▉     | 154/313 [00:24<00:26,  5.90it/s]Evaluating:  50%|████▉     | 155/313 [00:25<00:26,  5.89it/s]Evaluating:  50%|████▉     | 156/313 [00:25<00:26,  5.89it/s]Evaluating:  50%|█████     | 157/313 [00:25<00:26,  5.89it/s]Evaluating:  50%|█████     | 158/313 [00:25<00:26,  5.89it/s]Evaluating:  51%|█████     | 159/313 [00:25<00:26,  5.88it/s]Evaluating:  51%|█████     | 160/313 [00:25<00:25,  5.89it/s]Evaluating:  51%|█████▏    | 161/313 [00:26<00:25,  5.89it/s]Evaluating:  52%|█████▏    | 162/313 [00:26<00:25,  5.89it/s]Evaluating:  52%|█████▏    | 163/313 [00:26<00:25,  5.89it/s]Evaluating:  52%|█████▏    | 164/313 [00:26<00:25,  5.89it/s]Evaluating:  53%|█████▎    | 165/313 [00:26<00:25,  5.88it/s]Evaluating:  53%|█████▎    | 166/313 [00:26<00:24,  5.88it/s]Evaluating:  53%|█████▎    | 167/313 [00:27<00:24,  5.88it/s]Evaluating:  54%|█████▎    | 168/313 [00:27<00:24,  5.88it/s]Evaluating:  54%|█████▍    | 169/313 [00:27<00:24,  5.87it/s]Evaluating:  54%|█████▍    | 170/313 [00:27<00:24,  5.87it/s]Evaluating:  55%|█████▍    | 171/313 [00:27<00:24,  5.87it/s]Evaluating:  55%|█████▍    | 172/313 [00:27<00:24,  5.87it/s]Evaluating:  55%|█████▌    | 173/313 [00:28<00:23,  5.87it/s]Evaluating:  56%|█████▌    | 174/313 [00:28<00:23,  5.87it/s]Evaluating:  56%|█████▌    | 175/313 [00:28<00:23,  5.87it/s]Evaluating:  56%|█████▌    | 176/313 [00:28<00:23,  5.87it/s]Evaluating:  57%|█████▋    | 177/313 [00:28<00:23,  5.87it/s]Evaluating:  57%|█████▋    | 178/313 [00:28<00:23,  5.87it/s]Evaluating:  57%|█████▋    | 179/313 [00:29<00:25,  5.17it/s]Evaluating:  58%|█████▊    | 180/313 [00:29<00:24,  5.48it/s]Evaluating:  58%|█████▊    | 181/313 [00:29<00:23,  5.61it/s]Evaluating:  58%|█████▊    | 182/313 [00:29<00:22,  5.82it/s]Evaluating:  58%|█████▊    | 183/313 [00:29<00:21,  5.98it/s]Evaluating:  59%|█████▉    | 184/313 [00:30<00:21,  6.10it/s]Evaluating:  59%|█████▉    | 185/313 [00:30<00:20,  6.18it/s]Evaluating:  59%|█████▉    | 186/313 [00:30<00:20,  6.22it/s]Evaluating:  60%|█████▉    | 187/313 [00:30<00:20,  6.24it/s]Evaluating:  60%|██████    | 188/313 [00:30<00:19,  6.28it/s]Evaluating:  60%|██████    | 189/313 [00:30<00:19,  6.31it/s]Evaluating:  61%|██████    | 190/313 [00:30<00:19,  6.31it/s]Evaluating:  61%|██████    | 191/313 [00:31<00:19,  6.22it/s]Evaluating:  61%|██████▏   | 192/313 [00:31<00:19,  6.13it/s]Evaluating:  62%|██████▏   | 193/313 [00:31<00:19,  6.07it/s]Evaluating:  62%|██████▏   | 194/313 [00:31<00:19,  6.03it/s]Evaluating:  62%|██████▏   | 195/313 [00:31<00:19,  6.01it/s]Evaluating:  63%|██████▎   | 196/313 [00:31<00:19,  6.00it/s]Evaluating:  63%|██████▎   | 197/313 [00:32<00:19,  6.00it/s]Evaluating:  63%|██████▎   | 198/313 [00:32<00:19,  5.99it/s]Evaluating:  64%|██████▎   | 199/313 [00:32<00:19,  5.95it/s]Evaluating:  64%|██████▍   | 200/313 [00:32<00:19,  5.91it/s]Evaluating:  64%|██████▍   | 201/313 [00:32<00:19,  5.89it/s]Evaluating:  65%|██████▍   | 202/313 [00:33<00:18,  5.88it/s]Evaluating:  65%|██████▍   | 203/313 [00:33<00:18,  5.86it/s]Evaluating:  65%|██████▌   | 204/313 [00:33<00:18,  5.85it/s]Evaluating:  65%|██████▌   | 205/313 [00:33<00:18,  5.86it/s]Evaluating:  66%|██████▌   | 206/313 [00:33<00:18,  5.85it/s]Evaluating:  66%|██████▌   | 207/313 [00:33<00:18,  5.87it/s]Evaluating:  66%|██████▋   | 208/313 [00:34<00:17,  5.87it/s]Evaluating:  67%|██████▋   | 209/313 [00:34<00:17,  5.81it/s]Evaluating:  67%|██████▋   | 210/313 [00:34<00:17,  5.82it/s]Evaluating:  67%|██████▋   | 211/313 [00:34<00:17,  5.81it/s]Evaluating:  68%|██████▊   | 212/313 [00:34<00:17,  5.82it/s]Evaluating:  68%|██████▊   | 213/313 [00:34<00:17,  5.82it/s]Evaluating:  68%|██████▊   | 214/313 [00:35<00:16,  5.84it/s]Evaluating:  69%|██████▊   | 215/313 [00:35<00:16,  5.88it/s]Evaluating:  69%|██████▉   | 216/313 [00:35<00:16,  5.85it/s]Evaluating:  69%|██████▉   | 217/313 [00:35<00:16,  5.84it/s]Evaluating:  70%|██████▉   | 218/313 [00:35<00:16,  5.85it/s]Evaluating:  70%|██████▉   | 219/313 [00:35<00:16,  5.84it/s]Evaluating:  70%|███████   | 220/313 [00:36<00:15,  5.84it/s]Evaluating:  71%|███████   | 221/313 [00:36<00:15,  5.83it/s]Evaluating:  71%|███████   | 222/313 [00:36<00:15,  5.83it/s]Evaluating:  71%|███████   | 223/313 [00:36<00:15,  5.83it/s]Evaluating:  72%|███████▏  | 224/313 [00:36<00:15,  5.82it/s]Evaluating:  72%|███████▏  | 225/313 [00:36<00:15,  5.82it/s]Evaluating:  72%|███████▏  | 226/313 [00:37<00:14,  5.84it/s]Evaluating:  73%|███████▎  | 227/313 [00:37<00:14,  5.87it/s]Evaluating:  73%|███████▎  | 228/313 [00:37<00:14,  5.90it/s]Evaluating:  73%|███████▎  | 229/313 [00:37<00:14,  5.92it/s]Evaluating:  73%|███████▎  | 230/313 [00:37<00:14,  5.92it/s]Evaluating:  74%|███████▍  | 231/313 [00:37<00:13,  5.93it/s]Evaluating:  74%|███████▍  | 232/313 [00:38<00:13,  5.91it/s]Evaluating:  74%|███████▍  | 233/313 [00:38<00:13,  5.89it/s]Evaluating:  75%|███████▍  | 234/313 [00:38<00:13,  5.87it/s]Evaluating:  75%|███████▌  | 235/313 [00:38<00:13,  5.83it/s]Evaluating:  75%|███████▌  | 236/313 [00:38<00:13,  5.85it/s]Evaluating:  76%|███████▌  | 237/313 [00:38<00:13,  5.84it/s]Evaluating:  76%|███████▌  | 238/313 [00:39<00:12,  5.85it/s]Evaluating:  76%|███████▋  | 239/313 [00:39<00:12,  5.83it/s]Evaluating:  77%|███████▋  | 240/313 [00:39<00:12,  5.83it/s]Evaluating:  77%|███████▋  | 241/313 [00:39<00:12,  5.82it/s]Evaluating:  77%|███████▋  | 242/313 [00:39<00:12,  5.82it/s]Evaluating:  78%|███████▊  | 243/313 [00:40<00:12,  5.83it/s]Evaluating:  78%|███████▊  | 244/313 [00:40<00:11,  5.82it/s]Evaluating:  78%|███████▊  | 245/313 [00:40<00:11,  5.84it/s]Evaluating:  79%|███████▊  | 246/313 [00:40<00:11,  5.83it/s]Evaluating:  79%|███████▉  | 247/313 [00:40<00:11,  5.82it/s]Evaluating:  79%|███████▉  | 248/313 [00:40<00:11,  5.81it/s]Evaluating:  80%|███████▉  | 249/313 [00:41<00:10,  5.82it/s]Evaluating:  80%|███████▉  | 250/313 [00:41<00:10,  5.81it/s]Evaluating:  80%|████████  | 251/313 [00:41<00:10,  5.81it/s]Evaluating:  81%|████████  | 252/313 [00:41<00:10,  5.81it/s]Evaluating:  81%|████████  | 253/313 [00:41<00:10,  5.80it/s]Evaluating:  81%|████████  | 254/313 [00:41<00:10,  5.81it/s]Evaluating:  81%|████████▏ | 255/313 [00:42<00:09,  5.81it/s]Evaluating:  82%|████████▏ | 256/313 [00:42<00:09,  5.81it/s]Evaluating:  82%|████████▏ | 257/313 [00:42<00:09,  5.82it/s]Evaluating:  82%|████████▏ | 258/313 [00:42<00:09,  5.81it/s]Evaluating:  83%|████████▎ | 259/313 [00:42<00:09,  5.81it/s]Evaluating:  83%|████████▎ | 260/313 [00:42<00:09,  5.83it/s]Evaluating:  83%|████████▎ | 261/313 [00:43<00:08,  5.82it/s]Evaluating:  84%|████████▎ | 262/313 [00:43<00:08,  5.81it/s]Evaluating:  84%|████████▍ | 263/313 [00:43<00:08,  5.83it/s]Evaluating:  84%|████████▍ | 264/313 [00:43<00:08,  5.82it/s]Evaluating:  85%|████████▍ | 265/313 [00:43<00:08,  5.81it/s]Evaluating:  85%|████████▍ | 266/313 [00:43<00:08,  5.81it/s]Evaluating:  85%|████████▌ | 267/313 [00:44<00:07,  5.83it/s]Evaluating:  86%|████████▌ | 268/313 [00:44<00:12,  3.56it/s]Evaluating:  86%|████████▌ | 269/313 [00:44<00:10,  4.02it/s]Evaluating:  86%|████████▋ | 270/313 [00:45<00:09,  4.43it/s]Evaluating:  87%|████████▋ | 271/313 [00:45<00:08,  4.77it/s]Evaluating:  87%|████████▋ | 272/313 [00:45<00:08,  5.03it/s]Evaluating:  87%|████████▋ | 273/313 [00:45<00:07,  5.25it/s]Evaluating:  88%|████████▊ | 274/313 [00:45<00:07,  5.43it/s]Evaluating:  88%|████████▊ | 275/313 [00:45<00:06,  5.58it/s]Evaluating:  88%|████████▊ | 276/313 [00:46<00:06,  5.62it/s]Evaluating:  88%|████████▊ | 277/313 [00:46<00:06,  5.70it/s]Evaluating:  89%|████████▉ | 278/313 [00:46<00:06,  5.76it/s]Evaluating:  89%|████████▉ | 279/313 [00:46<00:05,  5.82it/s]Evaluating:  89%|████████▉ | 280/313 [00:46<00:05,  5.89it/s]Evaluating:  90%|████████▉ | 281/313 [00:46<00:05,  5.94it/s]Evaluating:  90%|█████████ | 282/313 [00:47<00:05,  5.96it/s]Evaluating:  90%|█████████ | 283/313 [00:47<00:05,  5.99it/s]Evaluating:  91%|█████████ | 284/313 [00:47<00:04,  5.99it/s]Evaluating:  91%|█████████ | 285/313 [00:47<00:04,  6.01it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:04,  6.01it/s]Evaluating:  92%|█████████▏| 287/313 [00:47<00:04,  6.01it/s]Evaluating:  92%|█████████▏| 288/313 [00:48<00:04,  6.02it/s]Evaluating:  92%|█████████▏| 289/313 [00:48<00:03,  6.02it/s]Evaluating:  93%|█████████▎| 290/313 [00:48<00:03,  6.02it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:03,  6.01it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  5.89it/s]Evaluating:  94%|█████████▎| 293/313 [00:48<00:03,  5.84it/s]Evaluating:  94%|█████████▍| 294/313 [00:49<00:03,  5.86it/s]Evaluating:  94%|█████████▍| 295/313 [00:49<00:03,  5.91it/s]Evaluating:  95%|█████████▍| 296/313 [00:49<00:02,  5.94it/s]Evaluating:  95%|█████████▍| 297/313 [00:49<00:03,  5.23it/s]Evaluating:  95%|█████████▌| 298/313 [00:49<00:03,  4.68it/s]Evaluating:  96%|█████████▌| 299/313 [00:50<00:03,  4.42it/s]Evaluating:  96%|█████████▌| 300/313 [00:50<00:02,  4.41it/s]Evaluating:  96%|█████████▌| 301/313 [00:50<00:02,  4.49it/s]Evaluating:  96%|█████████▋| 302/313 [00:50<00:02,  4.63it/s]Evaluating:  97%|█████████▋| 303/313 [00:51<00:02,  4.80it/s]Evaluating:  97%|█████████▋| 304/313 [00:51<00:01,  4.96it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  5.10it/s]Evaluating:  98%|█████████▊| 306/313 [00:51<00:01,  5.20it/s]Evaluating:  98%|█████████▊| 307/313 [00:51<00:01,  5.28it/s]Evaluating:  98%|█████████▊| 308/313 [00:51<00:00,  5.35it/s]Evaluating:  99%|█████████▊| 309/313 [00:52<00:00,  5.40it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.47it/s]Evaluating:  99%|█████████▉| 311/313 [00:52<00:00,  5.53it/s]Evaluating: 100%|█████████▉| 312/313 [00:52<00:00,  5.52it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  6.30it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  5.93it/s]
10/09/2021 00:44:09 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/09/2021 00:44:09 - INFO - __main__ -     f1 = 0.5799142682180036
10/09/2021 00:44:09 - INFO - __main__ -     loss = 1.9011212766360932
10/09/2021 00:44:09 - INFO - __main__ -     precision = 0.5335211267605634
10/09/2021 00:44:09 - INFO - __main__ -     recall = 0.6351441985244802
10/09/2021 00:44:09 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/09/2021 00:44:10 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/09/2021 00:44:10 - INFO - __main__ -     Num examples = 10004
10/09/2021 00:44:10 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:50,  6.07it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:50,  6.07it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:50,  6.06it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.06it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.06it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:50,  6.06it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:50,  6.06it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:49,  6.05it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:49,  6.04it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:49,  6.04it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.04it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  6.04it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  6.04it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:48,  6.04it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:48,  6.04it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.04it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.04it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  6.04it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:47,  6.04it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:47,  6.04it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:51,  5.57it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:49,  5.80it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  5.97it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:46,  6.10it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:45,  6.18it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:45,  6.22it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:45,  6.24it/s]Evaluating:  10%|█         | 32/313 [00:05<00:45,  6.23it/s]Evaluating:  11%|█         | 33/313 [00:05<00:45,  6.21it/s]Evaluating:  11%|█         | 34/313 [00:05<00:45,  6.16it/s]Evaluating:  11%|█         | 35/313 [00:05<00:45,  6.12it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:45,  6.09it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:45,  6.07it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  6.06it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.04it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  6.04it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.03it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:44,  6.02it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:44,  6.02it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  6.02it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:44,  6.02it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:43,  6.01it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:43,  6.01it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  6.01it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  6.01it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  6.01it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:43,  6.01it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:55,  4.67it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:51,  5.00it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:48,  5.27it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:46,  5.47it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:45,  5.61it/s]Evaluating:  19%|█▉        | 60/313 [00:10<00:44,  5.74it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:43,  5.83it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:42,  5.87it/s]Evaluating:  20%|██        | 63/313 [00:10<00:42,  5.91it/s]Evaluating:  20%|██        | 64/313 [00:10<00:41,  5.93it/s]Evaluating:  21%|██        | 65/313 [00:10<00:41,  5.95it/s]Evaluating:  21%|██        | 66/313 [00:11<00:41,  5.96it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:41,  5.93it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:41,  5.93it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:41,  5.94it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  5.95it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  5.96it/s]Evaluating:  23%|██▎       | 72/313 [00:12<00:40,  5.97it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  5.98it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:39,  5.98it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:39,  5.98it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:39,  5.98it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  5.98it/s]Evaluating:  25%|██▍       | 78/313 [00:13<00:39,  6.00it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.99it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:38,  5.98it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:38,  5.98it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:38,  5.98it/s]Evaluating:  27%|██▋       | 83/313 [00:13<00:38,  5.97it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:38,  5.97it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:38,  5.97it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:38,  5.96it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:37,  5.98it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:37,  6.01it/s]Evaluating:  28%|██▊       | 89/313 [00:14<00:37,  6.02it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:36,  6.05it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:36,  6.06it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:36,  6.06it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:36,  6.07it/s]Evaluating:  30%|███       | 94/313 [00:15<00:35,  6.09it/s]Evaluating:  30%|███       | 95/313 [00:15<00:35,  6.10it/s]Evaluating:  31%|███       | 96/313 [00:16<00:35,  6.11it/s]Evaluating:  31%|███       | 97/313 [00:16<00:35,  6.11it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:35,  6.09it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:35,  6.07it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:35,  6.05it/s]Evaluating:  32%|███▏      | 101/313 [00:16<00:35,  6.03it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:35,  6.00it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:35,  5.96it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:35,  5.95it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:34,  5.97it/s]Evaluating:  34%|███▍      | 106/313 [00:17<00:34,  5.95it/s]Evaluating:  34%|███▍      | 107/313 [00:17<00:34,  5.95it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:34,  5.94it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:34,  5.94it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:34,  5.93it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:34,  5.93it/s]Evaluating:  36%|███▌      | 112/313 [00:18<00:33,  5.93it/s]Evaluating:  36%|███▌      | 113/313 [00:18<00:33,  5.93it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:33,  5.93it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:33,  5.93it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:33,  5.93it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:33,  5.93it/s]Evaluating:  38%|███▊      | 118/313 [00:19<00:32,  5.93it/s]Evaluating:  38%|███▊      | 119/313 [00:19<00:32,  5.94it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:32,  5.93it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:32,  5.93it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:32,  5.92it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:32,  5.93it/s]Evaluating:  40%|███▉      | 124/313 [00:20<00:31,  5.93it/s]Evaluating:  40%|███▉      | 125/313 [00:20<00:31,  5.92it/s]Evaluating:  40%|████      | 126/313 [00:21<00:31,  5.92it/s]Evaluating:  41%|████      | 127/313 [00:21<00:31,  5.93it/s]Evaluating:  41%|████      | 128/313 [00:21<00:31,  5.93it/s]Evaluating:  41%|████      | 129/313 [00:21<00:31,  5.92it/s]Evaluating:  42%|████▏     | 130/313 [00:21<00:30,  5.92it/s]Evaluating:  42%|████▏     | 131/313 [00:21<00:30,  5.92it/s]Evaluating:  42%|████▏     | 132/313 [00:22<00:30,  5.92it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:30,  5.92it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:30,  5.92it/s]Evaluating:  43%|████▎     | 135/313 [00:22<00:30,  5.91it/s]Evaluating:  43%|████▎     | 136/313 [00:22<00:29,  5.91it/s]Evaluating:  44%|████▍     | 137/313 [00:22<00:29,  5.91it/s]Evaluating:  44%|████▍     | 138/313 [00:23<00:29,  5.90it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:29,  5.90it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:29,  5.91it/s]Evaluating:  45%|████▌     | 141/313 [00:23<00:29,  5.91it/s]Evaluating:  45%|████▌     | 142/313 [00:23<00:28,  5.90it/s]Evaluating:  46%|████▌     | 143/313 [00:24<00:28,  5.90it/s]Evaluating:  46%|████▌     | 144/313 [00:24<00:28,  5.90it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:28,  5.89it/s]Evaluating:  47%|████▋     | 146/313 [00:24<00:28,  5.89it/s]Evaluating:  47%|████▋     | 147/313 [00:24<00:28,  5.89it/s]Evaluating:  47%|████▋     | 148/313 [00:24<00:27,  5.89it/s]Evaluating:  48%|████▊     | 149/313 [00:25<00:27,  5.89it/s]Evaluating:  48%|████▊     | 150/313 [00:25<00:27,  5.90it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:27,  5.90it/s]Evaluating:  49%|████▊     | 152/313 [00:25<00:27,  5.90it/s]Evaluating:  49%|████▉     | 153/313 [00:25<00:27,  5.90it/s]Evaluating:  49%|████▉     | 154/313 [00:25<00:26,  5.90it/s]Evaluating:  50%|████▉     | 155/313 [00:26<00:26,  5.90it/s]Evaluating:  50%|████▉     | 156/313 [00:26<00:26,  5.90it/s]Evaluating:  50%|█████     | 157/313 [00:26<00:26,  5.89it/s]Evaluating:  50%|█████     | 158/313 [00:26<00:26,  5.89it/s]Evaluating:  51%|█████     | 159/313 [00:26<00:26,  5.89it/s]Evaluating:  51%|█████     | 160/313 [00:26<00:25,  5.89it/s]Evaluating:  51%|█████▏    | 161/313 [00:27<00:25,  5.88it/s]Evaluating:  52%|█████▏    | 162/313 [00:27<00:25,  5.89it/s]Evaluating:  52%|█████▏    | 163/313 [00:27<00:25,  5.88it/s]Evaluating:  52%|█████▏    | 164/313 [00:27<00:25,  5.89it/s]Evaluating:  53%|█████▎    | 165/313 [00:27<00:25,  5.89it/s]Evaluating:  53%|█████▎    | 166/313 [00:27<00:24,  5.88it/s]Evaluating:  53%|█████▎    | 167/313 [00:28<00:24,  5.88it/s]Evaluating:  54%|█████▎    | 168/313 [00:28<00:24,  5.88it/s]Evaluating:  54%|█████▍    | 169/313 [00:28<00:24,  5.87it/s]Evaluating:  54%|█████▍    | 170/313 [00:28<00:24,  5.87it/s]Evaluating:  55%|█████▍    | 171/313 [00:28<00:24,  5.87it/s]Evaluating:  55%|█████▍    | 172/313 [00:28<00:24,  5.87it/s]Evaluating:  55%|█████▌    | 173/313 [00:29<00:24,  5.75it/s]Evaluating:  56%|█████▌    | 174/313 [00:29<00:23,  5.83it/s]Evaluating:  56%|█████▌    | 175/313 [00:29<00:23,  5.89it/s]Evaluating:  56%|█████▌    | 176/313 [00:29<00:23,  5.90it/s]Evaluating:  57%|█████▋    | 177/313 [00:29<00:23,  5.88it/s]Evaluating:  57%|█████▋    | 178/313 [00:29<00:22,  5.87it/s]Evaluating:  57%|█████▋    | 179/313 [00:30<00:22,  5.87it/s]Evaluating:  58%|█████▊    | 180/313 [00:30<00:22,  5.88it/s]Evaluating:  58%|█████▊    | 181/313 [00:30<00:22,  5.88it/s]Evaluating:  58%|█████▊    | 182/313 [00:30<00:22,  5.89it/s]Evaluating:  58%|█████▊    | 183/313 [00:30<00:22,  5.88it/s]Evaluating:  59%|█████▉    | 184/313 [00:30<00:21,  5.87it/s]Evaluating:  59%|█████▉    | 185/313 [00:31<00:21,  5.87it/s]Evaluating:  59%|█████▉    | 186/313 [00:31<00:21,  5.88it/s]Evaluating:  60%|█████▉    | 187/313 [00:31<00:21,  5.89it/s]Evaluating:  60%|██████    | 188/313 [00:31<00:21,  5.88it/s]Evaluating:  60%|██████    | 189/313 [00:31<00:21,  5.87it/s]Evaluating:  61%|██████    | 190/313 [00:32<00:20,  5.86it/s]Evaluating:  61%|██████    | 191/313 [00:32<00:20,  5.87it/s]Evaluating:  61%|██████▏   | 192/313 [00:32<00:20,  5.86it/s]Evaluating:  62%|██████▏   | 193/313 [00:32<00:20,  5.86it/s]Evaluating:  62%|██████▏   | 194/313 [00:32<00:20,  5.86it/s]Evaluating:  62%|██████▏   | 195/313 [00:32<00:20,  5.85it/s]Evaluating:  63%|██████▎   | 196/313 [00:33<00:20,  5.85it/s]Evaluating:  63%|██████▎   | 197/313 [00:33<00:19,  5.87it/s]Evaluating:  63%|██████▎   | 198/313 [00:33<00:19,  5.89it/s]Evaluating:  64%|██████▎   | 199/313 [00:33<00:19,  5.92it/s]Evaluating:  64%|██████▍   | 200/313 [00:33<00:19,  5.95it/s]Evaluating:  64%|██████▍   | 201/313 [00:33<00:18,  5.93it/s]Evaluating:  65%|██████▍   | 202/313 [00:34<00:18,  5.90it/s]Evaluating:  65%|██████▍   | 203/313 [00:34<00:18,  5.90it/s]Evaluating:  65%|██████▌   | 204/313 [00:34<00:18,  5.88it/s]Evaluating:  65%|██████▌   | 205/313 [00:34<00:18,  5.89it/s]Evaluating:  66%|██████▌   | 206/313 [00:34<00:18,  5.87it/s]Evaluating:  66%|██████▌   | 207/313 [00:34<00:18,  5.86it/s]Evaluating:  66%|██████▋   | 208/313 [00:35<00:17,  5.85it/s]Evaluating:  67%|██████▋   | 209/313 [00:35<00:17,  5.86it/s]Evaluating:  67%|██████▋   | 210/313 [00:35<00:17,  5.85it/s]Evaluating:  67%|██████▋   | 211/313 [00:35<00:17,  5.87it/s]Evaluating:  68%|██████▊   | 212/313 [00:35<00:17,  5.88it/s]Evaluating:  68%|██████▊   | 213/313 [00:35<00:16,  5.89it/s]Evaluating:  68%|██████▊   | 214/313 [00:36<00:16,  5.92it/s]Evaluating:  69%|██████▊   | 215/313 [00:36<00:16,  5.96it/s]Evaluating:  69%|██████▉   | 216/313 [00:36<00:16,  5.94it/s]Evaluating:  69%|██████▉   | 217/313 [00:36<00:16,  5.96it/s]Evaluating:  70%|██████▉   | 218/313 [00:36<00:16,  5.92it/s]Evaluating:  70%|██████▉   | 219/313 [00:36<00:15,  5.89it/s]Evaluating:  70%|███████   | 220/313 [00:37<00:15,  5.88it/s]Evaluating:  71%|███████   | 221/313 [00:37<00:15,  5.86it/s]Evaluating:  71%|███████   | 222/313 [00:37<00:15,  5.85it/s]Evaluating:  71%|███████   | 223/313 [00:37<00:15,  5.84it/s]Evaluating:  72%|███████▏  | 224/313 [00:37<00:15,  5.84it/s]Evaluating:  72%|███████▏  | 225/313 [00:37<00:15,  5.83it/s]Evaluating:  72%|███████▏  | 226/313 [00:38<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 227/313 [00:38<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 228/313 [00:38<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 229/313 [00:38<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 230/313 [00:38<00:14,  5.82it/s]Evaluating:  74%|███████▍  | 231/313 [00:38<00:14,  5.82it/s]Evaluating:  74%|███████▍  | 232/313 [00:39<00:13,  5.82it/s]Evaluating:  74%|███████▍  | 233/313 [00:39<00:13,  5.85it/s]Evaluating:  75%|███████▍  | 234/313 [00:39<00:13,  5.86it/s]Evaluating:  75%|███████▌  | 235/313 [00:39<00:13,  5.85it/s]Evaluating:  75%|███████▌  | 236/313 [00:39<00:13,  5.84it/s]Evaluating:  76%|███████▌  | 237/313 [00:40<00:13,  5.84it/s]Evaluating:  76%|███████▌  | 238/313 [00:40<00:12,  5.85it/s]Evaluating:  76%|███████▋  | 239/313 [00:40<00:12,  5.87it/s]Evaluating:  77%|███████▋  | 240/313 [00:40<00:12,  5.90it/s]Evaluating:  77%|███████▋  | 241/313 [00:40<00:12,  5.93it/s]Evaluating:  77%|███████▋  | 242/313 [00:40<00:11,  5.95it/s]Evaluating:  78%|███████▊  | 243/313 [00:41<00:11,  5.97it/s]Evaluating:  78%|███████▊  | 244/313 [00:41<00:11,  6.00it/s]Evaluating:  78%|███████▊  | 245/313 [00:41<00:11,  6.02it/s]Evaluating:  79%|███████▊  | 246/313 [00:41<00:11,  6.04it/s]Evaluating:  79%|███████▉  | 247/313 [00:41<00:10,  6.02it/s]Evaluating:  79%|███████▉  | 248/313 [00:41<00:10,  6.01it/s]Evaluating:  80%|███████▉  | 249/313 [00:42<00:10,  6.01it/s]Evaluating:  80%|███████▉  | 250/313 [00:42<00:10,  6.02it/s]Evaluating:  80%|████████  | 251/313 [00:42<00:10,  6.04it/s]Evaluating:  81%|████████  | 252/313 [00:42<00:10,  6.07it/s]Evaluating:  81%|████████  | 253/313 [00:42<00:09,  6.09it/s]Evaluating:  81%|████████  | 254/313 [00:42<00:09,  6.11it/s]Evaluating:  81%|████████▏ | 255/313 [00:42<00:09,  6.13it/s]Evaluating:  82%|████████▏ | 256/313 [00:43<00:09,  6.15it/s]Evaluating:  82%|████████▏ | 257/313 [00:43<00:09,  6.19it/s]Evaluating:  82%|████████▏ | 258/313 [00:43<00:08,  6.19it/s]Evaluating:  83%|████████▎ | 259/313 [00:43<00:08,  6.20it/s]Evaluating:  83%|████████▎ | 260/313 [00:43<00:08,  6.20it/s]Evaluating:  83%|████████▎ | 261/313 [00:43<00:08,  6.22it/s]Evaluating:  84%|████████▎ | 262/313 [00:44<00:08,  5.70it/s]Evaluating:  84%|████████▍ | 263/313 [00:44<00:08,  5.77it/s]Evaluating:  84%|████████▍ | 264/313 [00:44<00:08,  5.86it/s]Evaluating:  85%|████████▍ | 265/313 [00:44<00:08,  5.96it/s]Evaluating:  85%|████████▍ | 266/313 [00:44<00:07,  6.04it/s]Evaluating:  85%|████████▌ | 267/313 [00:44<00:07,  6.11it/s]Evaluating:  86%|████████▌ | 268/313 [00:45<00:07,  6.08it/s]Evaluating:  86%|████████▌ | 269/313 [00:45<00:07,  6.06it/s]Evaluating:  86%|████████▋ | 270/313 [00:45<00:07,  6.05it/s]Evaluating:  87%|████████▋ | 271/313 [00:45<00:06,  6.08it/s]Evaluating:  87%|████████▋ | 272/313 [00:45<00:06,  6.10it/s]Evaluating:  87%|████████▋ | 273/313 [00:45<00:06,  6.10it/s]Evaluating:  88%|████████▊ | 274/313 [00:46<00:06,  6.10it/s]Evaluating:  88%|████████▊ | 275/313 [00:46<00:06,  6.12it/s]Evaluating:  88%|████████▊ | 276/313 [00:46<00:06,  6.14it/s]Evaluating:  88%|████████▊ | 277/313 [00:46<00:05,  6.15it/s]Evaluating:  89%|████████▉ | 278/313 [00:46<00:05,  6.13it/s]Evaluating:  89%|████████▉ | 279/313 [00:46<00:05,  6.12it/s]Evaluating:  89%|████████▉ | 280/313 [00:47<00:05,  6.13it/s]Evaluating:  90%|████████▉ | 281/313 [00:47<00:05,  6.11it/s]Evaluating:  90%|█████████ | 282/313 [00:47<00:05,  6.14it/s]Evaluating:  90%|█████████ | 283/313 [00:47<00:04,  6.13it/s]Evaluating:  91%|█████████ | 284/313 [00:47<00:04,  6.11it/s]Evaluating:  91%|█████████ | 285/313 [00:47<00:04,  6.13it/s]Evaluating:  91%|█████████▏| 286/313 [00:48<00:04,  6.17it/s]Evaluating:  92%|█████████▏| 287/313 [00:48<00:04,  6.20it/s]Evaluating:  92%|█████████▏| 288/313 [00:48<00:04,  6.23it/s]Evaluating:  92%|█████████▏| 289/313 [00:48<00:03,  6.25it/s]Evaluating:  93%|█████████▎| 290/313 [00:48<00:03,  6.26it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:03,  6.28it/s]Evaluating:  93%|█████████▎| 292/313 [00:49<00:03,  6.27it/s]Evaluating:  94%|█████████▎| 293/313 [00:49<00:03,  6.28it/s]Evaluating:  94%|█████████▍| 294/313 [00:49<00:03,  6.17it/s]Evaluating:  94%|█████████▍| 295/313 [00:49<00:02,  6.10it/s]Evaluating:  95%|█████████▍| 296/313 [00:49<00:02,  6.03it/s]Evaluating:  95%|█████████▍| 297/313 [00:49<00:02,  5.88it/s]Evaluating:  95%|█████████▌| 298/313 [00:50<00:02,  5.77it/s]Evaluating:  96%|█████████▌| 299/313 [00:50<00:02,  5.71it/s]Evaluating:  96%|█████████▌| 300/313 [00:50<00:02,  5.71it/s]Evaluating:  96%|█████████▌| 301/313 [00:50<00:02,  5.68it/s]Evaluating:  96%|█████████▋| 302/313 [00:50<00:01,  5.70it/s]Evaluating:  97%|█████████▋| 303/313 [00:50<00:01,  5.70it/s]Evaluating:  97%|█████████▋| 304/313 [00:51<00:01,  5.75it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  5.81it/s]Evaluating:  98%|█████████▊| 306/313 [00:51<00:01,  5.86it/s]Evaluating:  98%|█████████▊| 307/313 [00:51<00:01,  5.90it/s]Evaluating:  98%|█████████▊| 308/313 [00:51<00:00,  5.91it/s]Evaluating:  99%|█████████▊| 309/313 [00:51<00:00,  5.91it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.80it/s]Evaluating:  99%|█████████▉| 311/313 [00:52<00:00,  5.71it/s]Evaluating: 100%|█████████▉| 312/313 [00:52<00:00,  5.67it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  6.21it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  5.95it/s]
10/09/2021 00:45:04 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/09/2021 00:45:04 - INFO - __main__ -     f1 = 0.71266849888758
10/09/2021 00:45:04 - INFO - __main__ -     loss = 1.3063723503972013
10/09/2021 00:45:04 - INFO - __main__ -     precision = 0.659940616857541
10/09/2021 00:45:04 - INFO - __main__ -     recall = 0.7745537301756632
10/09/2021 00:45:04 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:45:17 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:45:17 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:45:34 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:45:36 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:45:36 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:45:36 - INFO - __main__ -   Seed = 42
10/09/2021 00:45:36 - INFO - root -   save model
10/09/2021 00:45:36 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:45:36 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:45:51 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:45:51 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:45:51 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/09/2021 00:45:51 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:45:51 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:45:51 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/09/2021 00:45:51 - INFO - __main__ -   Language = en
10/09/2021 00:45:51 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:45:52 - INFO - __main__ -   Language = ru
10/09/2021 00:45:52 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/09/2021 00:45:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/09/2021 00:45:57 - INFO - __main__ -   ***** Running evaluation  in be *****
10/09/2021 00:45:57 - INFO - __main__ -     Num examples = 1001
10/09/2021 00:45:57 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.92it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.22it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.45it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.54it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.61it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.65it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.66it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.67it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.69it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.70it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.69it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.69it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.70it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.70it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.69it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.67it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.67it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.67it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.64it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.65it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.65it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.64it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.64it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.65it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.65it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.66it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.66it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.66it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.66it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.66it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.66it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.78it/s]
10/09/2021 00:46:02 - INFO - __main__ -   ***** Evaluation result  in be *****
10/09/2021 00:46:02 - INFO - __main__ -     f1 = 0.6633165829145728
10/09/2021 00:46:02 - INFO - __main__ -     loss = 1.3505450412631035
10/09/2021 00:46:02 - INFO - __main__ -     precision = 0.6262773722627737
10/09/2021 00:46:02 - INFO - __main__ -     recall = 0.705012325390304
10/09/2021 00:46:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/09/2021 00:46:03 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/09/2021 00:46:03 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:46:03 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.75it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.70it/s]Evaluating:   1%|          | 3/313 [00:00<00:46,  6.72it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:46,  6.72it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.71it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.70it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.70it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.70it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.70it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.70it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:45,  6.69it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.70it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.69it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.69it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.68it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.68it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.66it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:44,  6.65it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:44,  6.66it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:44,  6.66it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.65it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.66it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.66it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.66it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:43,  6.66it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:43,  6.65it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.65it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.64it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.64it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.64it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.64it/s]Evaluating:  10%|█         | 32/313 [00:04<00:42,  6.63it/s]Evaluating:  11%|█         | 33/313 [00:04<00:42,  6.62it/s]Evaluating:  11%|█         | 34/313 [00:05<00:42,  6.62it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.63it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.63it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.63it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.62it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:41,  6.61it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:41,  6.61it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:41,  6.61it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.61it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.62it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.61it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.62it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:40,  6.61it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.61it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:40,  6.61it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:39,  6.61it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:39,  6.60it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:39,  6.60it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:39,  6.59it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:39,  6.58it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:39,  6.57it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:39,  6.56it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:39,  6.54it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:39,  6.52it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:39,  6.54it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:39,  6.48it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:38,  6.50it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.52it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.52it/s]Evaluating:  20%|██        | 63/313 [00:09<00:38,  6.53it/s]Evaluating:  20%|██        | 64/313 [00:09<00:38,  6.54it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.54it/s]Evaluating:  21%|██        | 66/313 [00:09<00:37,  6.53it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:37,  6.53it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:37,  6.53it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:37,  6.51it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:37,  6.52it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:37,  6.48it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:37,  6.46it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:37,  6.47it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:36,  6.48it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:36,  6.45it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:36,  6.43it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:36,  6.42it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:36,  6.40it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:36,  6.41it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:36,  6.41it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:36,  6.42it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:35,  6.45it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:35,  6.47it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:35,  6.47it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:35,  6.48it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:35,  6.43it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:35,  6.42it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:34,  6.45it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:34,  6.47it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:34,  6.48it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:34,  6.49it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:34,  6.49it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.49it/s]Evaluating:  30%|███       | 94/313 [00:14<00:33,  6.49it/s]Evaluating:  30%|███       | 95/313 [00:14<00:33,  6.49it/s]Evaluating:  31%|███       | 96/313 [00:14<00:33,  6.48it/s]Evaluating:  31%|███       | 97/313 [00:14<00:33,  6.48it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:33,  6.48it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.49it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:32,  6.49it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:32,  6.49it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.48it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:32,  6.47it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:32,  6.44it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:32,  6.40it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:35,  5.81it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:34,  5.99it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:33,  6.13it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:32,  6.23it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:32,  6.29it/s]Evaluating:  35%|███▌      | 111/313 [00:16<00:31,  6.32it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:31,  6.33it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:31,  6.33it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:31,  6.31it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:31,  6.29it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:31,  6.30it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:31,  6.28it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:31,  6.25it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:31,  6.20it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:31,  6.16it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:31,  6.17it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:30,  6.17it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:30,  6.14it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:31,  6.09it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:30,  6.06it/s]Evaluating:  40%|████      | 126/313 [00:19<00:30,  6.03it/s]Evaluating:  41%|████      | 127/313 [00:19<00:30,  6.01it/s]Evaluating:  41%|████      | 128/313 [00:19<00:30,  5.98it/s]Evaluating:  41%|████      | 129/313 [00:19<00:30,  5.94it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:30,  5.93it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:30,  5.97it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:30,  6.01it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:30,  5.97it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:30,  5.95it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:29,  5.95it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:29,  5.95it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:29,  5.94it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:29,  5.94it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:29,  5.92it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:29,  5.90it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:29,  5.89it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:28,  5.91it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:28,  5.92it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:28,  5.91it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:28,  5.86it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:28,  5.84it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:28,  5.87it/s]Evaluating:  47%|████▋     | 148/313 [00:23<00:28,  5.88it/s]Evaluating:  48%|████▊     | 149/313 [00:23<00:27,  5.91it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:27,  5.91it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:27,  5.89it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:27,  5.86it/s]Evaluating:  49%|████▉     | 153/313 [00:24<00:27,  5.78it/s]Evaluating:  49%|████▉     | 154/313 [00:24<00:27,  5.76it/s]Evaluating:  50%|████▉     | 155/313 [00:24<00:27,  5.85it/s]Evaluating:  50%|████▉     | 156/313 [00:24<00:26,  5.94it/s]Evaluating:  50%|█████     | 157/313 [00:24<00:25,  6.01it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:25,  6.05it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:25,  6.10it/s]Evaluating:  51%|█████     | 160/313 [00:25<00:24,  6.12it/s]Evaluating:  51%|█████▏    | 161/313 [00:25<00:25,  6.06it/s]Evaluating:  52%|█████▏    | 162/313 [00:25<00:25,  6.00it/s]Evaluating:  52%|█████▏    | 163/313 [00:25<00:25,  5.97it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:25,  5.96it/s]Evaluating:  53%|█████▎    | 165/313 [00:26<00:24,  5.96it/s]Evaluating:  53%|█████▎    | 166/313 [00:26<00:29,  5.03it/s]Evaluating:  53%|█████▎    | 167/313 [00:26<00:27,  5.30it/s]Evaluating:  54%|█████▎    | 168/313 [00:26<00:26,  5.48it/s]Evaluating:  54%|█████▍    | 169/313 [00:26<00:25,  5.62it/s]Evaluating:  54%|█████▍    | 170/313 [00:26<00:25,  5.69it/s]Evaluating:  55%|█████▍    | 171/313 [00:27<00:24,  5.74it/s]Evaluating:  55%|█████▍    | 172/313 [00:27<00:24,  5.81it/s]Evaluating:  55%|█████▌    | 173/313 [00:27<00:23,  5.89it/s]Evaluating:  56%|█████▌    | 174/313 [00:27<00:23,  5.96it/s]Evaluating:  56%|█████▌    | 175/313 [00:27<00:22,  6.03it/s]Evaluating:  56%|█████▌    | 176/313 [00:27<00:22,  6.07it/s]Evaluating:  57%|█████▋    | 177/313 [00:28<00:22,  6.10it/s]Evaluating:  57%|█████▋    | 178/313 [00:28<00:22,  6.11it/s]Evaluating:  57%|█████▋    | 179/313 [00:28<00:21,  6.13it/s]Evaluating:  58%|█████▊    | 180/313 [00:28<00:21,  6.14it/s]Evaluating:  58%|█████▊    | 181/313 [00:28<00:21,  6.18it/s]Evaluating:  58%|█████▊    | 182/313 [00:28<00:21,  6.20it/s]Evaluating:  58%|█████▊    | 183/313 [00:29<00:20,  6.22it/s]Evaluating:  59%|█████▉    | 184/313 [00:29<00:20,  6.24it/s]Evaluating:  59%|█████▉    | 185/313 [00:29<00:20,  6.26it/s]Evaluating:  59%|█████▉    | 186/313 [00:29<00:20,  6.27it/s]Evaluating:  60%|█████▉    | 187/313 [00:29<00:20,  6.26it/s]Evaluating:  60%|██████    | 188/313 [00:29<00:19,  6.26it/s]Evaluating:  60%|██████    | 189/313 [00:30<00:19,  6.22it/s]Evaluating:  61%|██████    | 190/313 [00:30<00:19,  6.23it/s]Evaluating:  61%|██████    | 191/313 [00:30<00:19,  6.26it/s]Evaluating:  61%|██████▏   | 192/313 [00:30<00:19,  6.25it/s]Evaluating:  62%|██████▏   | 193/313 [00:30<00:19,  6.23it/s]Evaluating:  62%|██████▏   | 194/313 [00:30<00:19,  6.19it/s]Evaluating:  62%|██████▏   | 195/313 [00:30<00:19,  6.19it/s]Evaluating:  63%|██████▎   | 196/313 [00:31<00:18,  6.19it/s]Evaluating:  63%|██████▎   | 197/313 [00:31<00:18,  6.17it/s]Evaluating:  63%|██████▎   | 198/313 [00:31<00:19,  6.01it/s]Evaluating:  64%|██████▎   | 199/313 [00:31<00:18,  6.03it/s]Evaluating:  64%|██████▍   | 200/313 [00:31<00:18,  6.06it/s]Evaluating:  64%|██████▍   | 201/313 [00:31<00:18,  6.01it/s]Evaluating:  65%|██████▍   | 202/313 [00:32<00:18,  6.00it/s]Evaluating:  65%|██████▍   | 203/313 [00:32<00:18,  5.98it/s]Evaluating:  65%|██████▌   | 204/313 [00:32<00:18,  5.96it/s]Evaluating:  65%|██████▌   | 205/313 [00:32<00:18,  5.97it/s]Evaluating:  66%|██████▌   | 206/313 [00:32<00:17,  5.97it/s]Evaluating:  66%|██████▌   | 207/313 [00:32<00:17,  5.98it/s]Evaluating:  66%|██████▋   | 208/313 [00:33<00:17,  5.98it/s]Evaluating:  67%|██████▋   | 209/313 [00:33<00:17,  5.95it/s]Evaluating:  67%|██████▋   | 210/313 [00:33<00:17,  5.91it/s]Evaluating:  67%|██████▋   | 211/313 [00:33<00:17,  5.90it/s]Evaluating:  68%|██████▊   | 212/313 [00:33<00:17,  5.91it/s]Evaluating:  68%|██████▊   | 213/313 [00:34<00:16,  5.94it/s]Evaluating:  68%|██████▊   | 214/313 [00:34<00:16,  5.96it/s]Evaluating:  69%|██████▊   | 215/313 [00:34<00:16,  5.92it/s]Evaluating:  69%|██████▉   | 216/313 [00:34<00:16,  5.88it/s]Evaluating:  69%|██████▉   | 217/313 [00:34<00:16,  5.88it/s]Evaluating:  70%|██████▉   | 218/313 [00:34<00:16,  5.87it/s]Evaluating:  70%|██████▉   | 219/313 [00:35<00:16,  5.85it/s]Evaluating:  70%|███████   | 220/313 [00:35<00:15,  5.86it/s]Evaluating:  71%|███████   | 221/313 [00:35<00:15,  5.87it/s]Evaluating:  71%|███████   | 222/313 [00:35<00:15,  5.85it/s]Evaluating:  71%|███████   | 223/313 [00:35<00:15,  5.84it/s]Evaluating:  72%|███████▏  | 224/313 [00:35<00:15,  5.83it/s]Evaluating:  72%|███████▏  | 225/313 [00:36<00:15,  5.82it/s]Evaluating:  72%|███████▏  | 226/313 [00:36<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 227/313 [00:36<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 228/313 [00:36<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 229/313 [00:36<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 230/313 [00:36<00:14,  5.85it/s]Evaluating:  74%|███████▍  | 231/313 [00:37<00:13,  5.86it/s]Evaluating:  74%|███████▍  | 232/313 [00:37<00:13,  5.87it/s]Evaluating:  74%|███████▍  | 233/313 [00:37<00:13,  5.91it/s]Evaluating:  75%|███████▍  | 234/313 [00:37<00:13,  5.93it/s]Evaluating:  75%|███████▌  | 235/313 [00:37<00:13,  5.97it/s]Evaluating:  75%|███████▌  | 236/313 [00:37<00:12,  5.96it/s]Evaluating:  76%|███████▌  | 237/313 [00:38<00:12,  6.00it/s]Evaluating:  76%|███████▌  | 238/313 [00:38<00:12,  6.02it/s]Evaluating:  76%|███████▋  | 239/313 [00:38<00:12,  5.97it/s]Evaluating:  77%|███████▋  | 240/313 [00:38<00:12,  5.91it/s]Evaluating:  77%|███████▋  | 241/313 [00:38<00:12,  5.92it/s]Evaluating:  77%|███████▋  | 242/313 [00:38<00:11,  5.95it/s]Evaluating:  78%|███████▊  | 243/313 [00:39<00:11,  5.99it/s]Evaluating:  78%|███████▊  | 244/313 [00:39<00:11,  5.99it/s]Evaluating:  78%|███████▊  | 245/313 [00:39<00:11,  5.99it/s]Evaluating:  79%|███████▊  | 246/313 [00:39<00:11,  6.00it/s]Evaluating:  79%|███████▉  | 247/313 [00:39<00:10,  6.04it/s]Evaluating:  79%|███████▉  | 248/313 [00:39<00:10,  6.08it/s]Evaluating:  80%|███████▉  | 249/313 [00:40<00:10,  6.13it/s]Evaluating:  80%|███████▉  | 250/313 [00:40<00:10,  6.18it/s]Evaluating:  80%|████████  | 251/313 [00:40<00:09,  6.22it/s]Evaluating:  81%|████████  | 252/313 [00:40<00:09,  6.24it/s]Evaluating:  81%|████████  | 253/313 [00:40<00:09,  6.04it/s]Evaluating:  81%|████████  | 254/313 [00:40<00:09,  6.09it/s]Evaluating:  81%|████████▏ | 255/313 [00:41<00:09,  6.14it/s]Evaluating:  82%|████████▏ | 256/313 [00:41<00:09,  6.15it/s]Evaluating:  82%|████████▏ | 257/313 [00:41<00:09,  6.11it/s]Evaluating:  82%|████████▏ | 258/313 [00:41<00:09,  6.01it/s]Evaluating:  83%|████████▎ | 259/313 [00:41<00:09,  5.95it/s]Evaluating:  83%|████████▎ | 260/313 [00:41<00:09,  5.87it/s]Evaluating:  83%|████████▎ | 261/313 [00:42<00:08,  5.82it/s]Evaluating:  84%|████████▎ | 262/313 [00:42<00:08,  5.77it/s]Evaluating:  84%|████████▍ | 263/313 [00:42<00:08,  5.75it/s]Evaluating:  84%|████████▍ | 264/313 [00:42<00:08,  5.73it/s]Evaluating:  85%|████████▍ | 265/313 [00:42<00:08,  5.73it/s]Evaluating:  85%|████████▍ | 266/313 [00:42<00:08,  5.71it/s]Evaluating:  85%|████████▌ | 267/313 [00:43<00:08,  5.72it/s]Evaluating:  86%|████████▌ | 268/313 [00:43<00:07,  5.76it/s]Evaluating:  86%|████████▌ | 269/313 [00:43<00:07,  5.81it/s]Evaluating:  86%|████████▋ | 270/313 [00:43<00:07,  5.85it/s]Evaluating:  87%|████████▋ | 271/313 [00:43<00:07,  5.90it/s]Evaluating:  87%|████████▋ | 272/313 [00:43<00:06,  5.95it/s]Evaluating:  87%|████████▋ | 273/313 [00:44<00:06,  5.97it/s]Evaluating:  88%|████████▊ | 274/313 [00:44<00:06,  5.99it/s]Evaluating:  88%|████████▊ | 275/313 [00:44<00:06,  5.98it/s]Evaluating:  88%|████████▊ | 276/313 [00:44<00:06,  5.97it/s]Evaluating:  88%|████████▊ | 277/313 [00:44<00:06,  5.87it/s]Evaluating:  89%|████████▉ | 278/313 [00:44<00:06,  5.77it/s]Evaluating:  89%|████████▉ | 279/313 [00:45<00:05,  5.71it/s]Evaluating:  89%|████████▉ | 280/313 [00:45<00:05,  5.71it/s]Evaluating:  90%|████████▉ | 281/313 [00:45<00:05,  5.77it/s]Evaluating:  90%|█████████ | 282/313 [00:45<00:05,  5.85it/s]Evaluating:  90%|█████████ | 283/313 [00:45<00:05,  5.87it/s]Evaluating:  91%|█████████ | 284/313 [00:46<00:04,  5.90it/s]Evaluating:  91%|█████████ | 285/313 [00:46<00:04,  5.90it/s]Evaluating:  91%|█████████▏| 286/313 [00:46<00:04,  5.86it/s]Evaluating:  92%|█████████▏| 287/313 [00:46<00:04,  5.82it/s]Evaluating:  92%|█████████▏| 288/313 [00:46<00:04,  5.80it/s]Evaluating:  92%|█████████▏| 289/313 [00:46<00:04,  5.80it/s]Evaluating:  93%|█████████▎| 290/313 [00:47<00:03,  5.80it/s]Evaluating:  93%|█████████▎| 291/313 [00:47<00:03,  5.76it/s]Evaluating:  93%|█████████▎| 292/313 [00:47<00:03,  5.75it/s]Evaluating:  94%|█████████▎| 293/313 [00:47<00:03,  5.78it/s]Evaluating:  94%|█████████▍| 294/313 [00:47<00:03,  5.83it/s]Evaluating:  94%|█████████▍| 295/313 [00:47<00:03,  5.86it/s]Evaluating:  95%|█████████▍| 296/313 [00:48<00:03,  5.23it/s]Evaluating:  95%|█████████▍| 297/313 [00:48<00:03,  4.81it/s]Evaluating:  95%|█████████▌| 298/313 [00:48<00:03,  4.47it/s]Evaluating:  96%|█████████▌| 299/313 [00:48<00:03,  4.47it/s]Evaluating:  96%|█████████▌| 300/313 [00:49<00:02,  4.62it/s]Evaluating:  96%|█████████▌| 301/313 [00:49<00:02,  4.75it/s]Evaluating:  96%|█████████▋| 302/313 [00:49<00:02,  4.88it/s]Evaluating:  97%|█████████▋| 303/313 [00:49<00:02,  5.00it/s]Evaluating:  97%|█████████▋| 304/313 [00:49<00:01,  5.11it/s]Evaluating:  97%|█████████▋| 305/313 [00:50<00:01,  5.20it/s]Evaluating:  98%|█████████▊| 306/313 [00:50<00:01,  5.28it/s]Evaluating:  98%|█████████▊| 307/313 [00:50<00:01,  5.35it/s]Evaluating:  98%|█████████▊| 308/313 [00:50<00:00,  5.40it/s]Evaluating:  99%|█████████▊| 309/313 [00:50<00:00,  5.43it/s]Evaluating:  99%|█████████▉| 310/313 [00:50<00:00,  5.44it/s]Evaluating:  99%|█████████▉| 311/313 [00:51<00:00,  5.46it/s]Evaluating: 100%|█████████▉| 312/313 [00:51<00:00,  5.50it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.30it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.09it/s]
10/09/2021 00:46:56 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/09/2021 00:46:56 - INFO - __main__ -     f1 = 0.60486706991138
10/09/2021 00:46:56 - INFO - __main__ -     loss = 1.4856300642505622
10/09/2021 00:46:56 - INFO - __main__ -     precision = 0.5726842911367117
10/09/2021 00:46:56 - INFO - __main__ -     recall = 0.640882331023176
10/09/2021 00:46:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/09/2021 00:46:57 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/09/2021 00:46:57 - INFO - __main__ -     Num examples = 10004
10/09/2021 00:46:57 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.04it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.04it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:51,  6.04it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:50,  6.04it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:50,  6.05it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.05it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.05it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:50,  6.05it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:50,  6.04it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:49,  6.04it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:49,  6.04it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:49,  6.04it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.04it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  6.04it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  6.04it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:49,  6.04it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:48,  6.04it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.04it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.04it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  6.04it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  6.04it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:48,  6.03it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:47,  6.03it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:47,  6.03it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  6.03it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  6.03it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  6.03it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:47,  6.03it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:46,  6.02it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:47,  6.00it/s]Evaluating:  10%|█         | 32/313 [00:05<00:46,  5.99it/s]Evaluating:  11%|█         | 33/313 [00:05<00:46,  6.00it/s]Evaluating:  11%|█         | 34/313 [00:05<00:46,  6.00it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  6.00it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:46,  6.01it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:45,  6.01it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  6.01it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.01it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  6.01it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.00it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:45,  6.01it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:54,  4.91it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:51,  5.19it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:49,  5.41it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:47,  5.58it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:46,  5.70it/s]Evaluating:  15%|█▌        | 48/313 [00:08<00:45,  5.79it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:45,  5.85it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:44,  5.89it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:44,  5.93it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  5.95it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  5.96it/s]Evaluating:  17%|█▋        | 54/313 [00:09<00:43,  5.96it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:43,  5.97it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:43,  5.97it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▉        | 60/313 [00:10<00:42,  5.98it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:42,  5.98it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:41,  5.98it/s]Evaluating:  20%|██        | 63/313 [00:10<00:41,  5.98it/s]Evaluating:  20%|██        | 64/313 [00:10<00:41,  5.98it/s]Evaluating:  21%|██        | 65/313 [00:10<00:41,  5.98it/s]Evaluating:  21%|██        | 66/313 [00:11<00:41,  5.97it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:41,  5.98it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:41,  5.97it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:40,  5.98it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  5.98it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  5.98it/s]Evaluating:  23%|██▎       | 72/313 [00:12<00:51,  4.71it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:47,  5.04it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:45,  5.30it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:43,  5.48it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:42,  5.62it/s]Evaluating:  25%|██▍       | 77/313 [00:13<00:41,  5.73it/s]Evaluating:  25%|██▍       | 78/313 [00:13<00:40,  5.82it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.88it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:39,  5.93it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:38,  5.96it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:38,  5.99it/s]Evaluating:  27%|██▋       | 83/313 [00:14<00:38,  6.02it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:37,  6.05it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:37,  6.07it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:37,  6.10it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:37,  6.10it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:36,  6.10it/s]Evaluating:  28%|██▊       | 89/313 [00:15<00:36,  6.08it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:36,  6.06it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:36,  6.06it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:36,  6.05it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:36,  6.03it/s]Evaluating:  30%|███       | 94/313 [00:15<00:36,  6.00it/s]Evaluating:  30%|███       | 95/313 [00:16<00:36,  5.99it/s]Evaluating:  31%|███       | 96/313 [00:16<00:36,  5.97it/s]Evaluating:  31%|███       | 97/313 [00:16<00:36,  5.96it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:36,  5.93it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:36,  5.93it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:35,  5.96it/s]Evaluating:  32%|███▏      | 101/313 [00:17<00:47,  4.44it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:43,  4.80it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:41,  5.10it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:39,  5.32it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:37,  5.49it/s]Evaluating:  34%|███▍      | 106/313 [00:18<00:36,  5.64it/s]Evaluating:  34%|███▍      | 107/313 [00:18<00:35,  5.73it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:35,  5.79it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:35,  5.83it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:34,  5.86it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:34,  5.90it/s]Evaluating:  36%|███▌      | 112/313 [00:19<00:33,  5.93it/s]Evaluating:  36%|███▌      | 113/313 [00:19<00:33,  5.96it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:33,  5.98it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:32,  6.01it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:32,  6.01it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:32,  6.01it/s]Evaluating:  38%|███▊      | 118/313 [00:20<00:32,  6.01it/s]Evaluating:  38%|███▊      | 119/313 [00:20<00:32,  5.98it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:32,  5.96it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:32,  5.97it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:31,  5.99it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:31,  5.98it/s]Evaluating:  40%|███▉      | 124/313 [00:21<00:31,  5.96it/s]Evaluating:  40%|███▉      | 125/313 [00:21<00:31,  5.98it/s]Evaluating:  40%|████      | 126/313 [00:21<00:31,  5.99it/s]Evaluating:  41%|████      | 127/313 [00:21<00:31,  6.00it/s]Evaluating:  41%|████      | 128/313 [00:21<00:30,  5.99it/s]Evaluating:  41%|████      | 129/313 [00:21<00:30,  5.99it/s]Evaluating:  42%|████▏     | 130/313 [00:22<00:37,  4.87it/s]Evaluating:  42%|████▏     | 131/313 [00:22<00:35,  5.14it/s]Evaluating:  42%|████▏     | 132/313 [00:22<00:33,  5.39it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:32,  5.55it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:31,  5.67it/s]Evaluating:  43%|████▎     | 135/313 [00:23<00:30,  5.79it/s]Evaluating:  43%|████▎     | 136/313 [00:23<00:30,  5.87it/s]Evaluating:  44%|████▍     | 137/313 [00:23<00:29,  5.91it/s]Evaluating:  44%|████▍     | 138/313 [00:23<00:29,  5.90it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:29,  5.90it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:29,  5.92it/s]Evaluating:  45%|████▌     | 141/313 [00:24<00:29,  5.93it/s]Evaluating:  45%|████▌     | 142/313 [00:24<00:28,  5.92it/s]Evaluating:  46%|████▌     | 143/313 [00:24<00:28,  5.93it/s]Evaluating:  46%|████▌     | 144/313 [00:24<00:28,  5.92it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:28,  5.91it/s]Evaluating:  47%|████▋     | 146/313 [00:24<00:28,  5.93it/s]Evaluating:  47%|████▋     | 147/313 [00:25<00:27,  5.94it/s]Evaluating:  47%|████▋     | 148/313 [00:25<00:27,  5.95it/s]Evaluating:  48%|████▊     | 149/313 [00:25<00:27,  5.93it/s]Evaluating:  48%|████▊     | 150/313 [00:25<00:27,  5.93it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:27,  5.91it/s]Evaluating:  49%|████▊     | 152/313 [00:25<00:27,  5.92it/s]Evaluating:  49%|████▉     | 153/313 [00:26<00:26,  5.93it/s]Evaluating:  49%|████▉     | 154/313 [00:26<00:26,  5.92it/s]Evaluating:  50%|████▉     | 155/313 [00:26<00:26,  5.91it/s]Evaluating:  50%|████▉     | 156/313 [00:26<00:26,  5.91it/s]Evaluating:  50%|█████     | 157/313 [00:26<00:26,  5.90it/s]Evaluating:  50%|█████     | 158/313 [00:26<00:26,  5.90it/s]Evaluating:  51%|█████     | 159/313 [00:27<00:26,  5.89it/s]Evaluating:  51%|█████     | 160/313 [00:27<00:25,  5.89it/s]Evaluating:  51%|█████▏    | 161/313 [00:27<00:25,  5.88it/s]Evaluating:  52%|█████▏    | 162/313 [00:27<00:25,  5.90it/s]Evaluating:  52%|█████▏    | 163/313 [00:27<00:25,  5.92it/s]Evaluating:  52%|█████▏    | 164/313 [00:27<00:24,  5.96it/s]Evaluating:  53%|█████▎    | 165/313 [00:28<00:24,  6.02it/s]Evaluating:  53%|█████▎    | 166/313 [00:28<00:24,  6.07it/s]Evaluating:  53%|█████▎    | 167/313 [00:28<00:23,  6.09it/s]Evaluating:  54%|█████▎    | 168/313 [00:28<00:23,  6.11it/s]Evaluating:  54%|█████▍    | 169/313 [00:28<00:23,  6.13it/s]Evaluating:  54%|█████▍    | 170/313 [00:28<00:23,  6.16it/s]Evaluating:  55%|█████▍    | 171/313 [00:29<00:23,  6.14it/s]Evaluating:  55%|█████▍    | 172/313 [00:29<00:23,  6.11it/s]Evaluating:  55%|█████▌    | 173/313 [00:29<00:23,  6.07it/s]Evaluating:  56%|█████▌    | 174/313 [00:29<00:22,  6.06it/s]Evaluating:  56%|█████▌    | 175/313 [00:29<00:22,  6.02it/s]Evaluating:  56%|█████▌    | 176/313 [00:29<00:22,  6.00it/s]Evaluating:  57%|█████▋    | 177/313 [00:30<00:22,  5.98it/s]Evaluating:  57%|█████▋    | 178/313 [00:30<00:22,  5.96it/s]Evaluating:  57%|█████▋    | 179/313 [00:30<00:22,  5.92it/s]Evaluating:  58%|█████▊    | 180/313 [00:30<00:22,  5.91it/s]Evaluating:  58%|█████▊    | 181/313 [00:30<00:22,  5.89it/s]Evaluating:  58%|█████▊    | 182/313 [00:30<00:22,  5.88it/s]Evaluating:  58%|█████▊    | 183/313 [00:31<00:22,  5.87it/s]Evaluating:  59%|█████▉    | 184/313 [00:31<00:21,  5.87it/s]Evaluating:  59%|█████▉    | 185/313 [00:31<00:21,  5.86it/s]Evaluating:  59%|█████▉    | 186/313 [00:31<00:21,  5.86it/s]Evaluating:  60%|█████▉    | 187/313 [00:31<00:21,  5.86it/s]Evaluating:  60%|██████    | 188/313 [00:31<00:21,  5.85it/s]Evaluating:  60%|██████    | 189/313 [00:32<00:22,  5.59it/s]Evaluating:  61%|██████    | 190/313 [00:32<00:21,  5.75it/s]Evaluating:  61%|██████    | 191/313 [00:32<00:20,  5.86it/s]Evaluating:  61%|██████▏   | 192/313 [00:32<00:20,  5.95it/s]Evaluating:  62%|██████▏   | 193/313 [00:32<00:19,  6.02it/s]Evaluating:  62%|██████▏   | 194/313 [00:32<00:19,  6.00it/s]Evaluating:  62%|██████▏   | 195/313 [00:33<00:19,  5.99it/s]Evaluating:  63%|██████▎   | 196/313 [00:33<00:19,  5.99it/s]Evaluating:  63%|██████▎   | 197/313 [00:33<00:19,  6.00it/s]Evaluating:  63%|██████▎   | 198/313 [00:33<00:19,  6.00it/s]Evaluating:  64%|██████▎   | 199/313 [00:33<00:19,  6.00it/s]Evaluating:  64%|██████▍   | 200/313 [00:33<00:18,  6.00it/s]Evaluating:  64%|██████▍   | 201/313 [00:34<00:18,  6.02it/s]Evaluating:  65%|██████▍   | 202/313 [00:34<00:18,  6.05it/s]Evaluating:  65%|██████▍   | 203/313 [00:34<00:18,  6.09it/s]Evaluating:  65%|██████▌   | 204/313 [00:34<00:17,  6.13it/s]Evaluating:  65%|██████▌   | 205/313 [00:34<00:17,  6.13it/s]Evaluating:  66%|██████▌   | 206/313 [00:34<00:17,  6.13it/s]Evaluating:  66%|██████▌   | 207/313 [00:35<00:17,  6.13it/s]Evaluating:  66%|██████▋   | 208/313 [00:35<00:17,  6.12it/s]Evaluating:  67%|██████▋   | 209/313 [00:35<00:17,  6.10it/s]Evaluating:  67%|██████▋   | 210/313 [00:35<00:17,  6.04it/s]Evaluating:  67%|██████▋   | 211/313 [00:35<00:17,  5.99it/s]Evaluating:  68%|██████▊   | 212/313 [00:35<00:16,  5.97it/s]Evaluating:  68%|██████▊   | 213/313 [00:36<00:16,  5.95it/s]Evaluating:  68%|██████▊   | 214/313 [00:36<00:16,  5.95it/s]Evaluating:  69%|██████▊   | 215/313 [00:36<00:16,  5.90it/s]Evaluating:  69%|██████▉   | 216/313 [00:36<00:16,  5.84it/s]Evaluating:  69%|██████▉   | 217/313 [00:36<00:16,  5.84it/s]Evaluating:  70%|██████▉   | 218/313 [00:37<00:16,  5.85it/s]Evaluating:  70%|██████▉   | 219/313 [00:37<00:16,  5.84it/s]Evaluating:  70%|███████   | 220/313 [00:37<00:15,  5.85it/s]Evaluating:  71%|███████   | 221/313 [00:37<00:15,  5.84it/s]Evaluating:  71%|███████   | 222/313 [00:37<00:15,  5.83it/s]Evaluating:  71%|███████   | 223/313 [00:37<00:15,  5.82it/s]Evaluating:  72%|███████▏  | 224/313 [00:38<00:15,  5.83it/s]Evaluating:  72%|███████▏  | 225/313 [00:38<00:15,  5.83it/s]Evaluating:  72%|███████▏  | 226/313 [00:38<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 227/313 [00:38<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 228/313 [00:38<00:14,  5.81it/s]Evaluating:  73%|███████▎  | 229/313 [00:38<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 230/313 [00:39<00:14,  5.84it/s]Evaluating:  74%|███████▍  | 231/313 [00:39<00:14,  5.85it/s]Evaluating:  74%|███████▍  | 232/313 [00:39<00:13,  5.86it/s]Evaluating:  74%|███████▍  | 233/313 [00:39<00:13,  5.86it/s]Evaluating:  75%|███████▍  | 234/313 [00:39<00:13,  5.84it/s]Evaluating:  75%|███████▌  | 235/313 [00:39<00:13,  5.83it/s]Evaluating:  75%|███████▌  | 236/313 [00:40<00:13,  5.82it/s]Evaluating:  76%|███████▌  | 237/313 [00:40<00:13,  5.83it/s]Evaluating:  76%|███████▌  | 238/313 [00:40<00:12,  5.82it/s]Evaluating:  76%|███████▋  | 239/313 [00:40<00:12,  5.82it/s]Evaluating:  77%|███████▋  | 240/313 [00:40<00:12,  5.81it/s]Evaluating:  77%|███████▋  | 241/313 [00:40<00:12,  5.81it/s]Evaluating:  77%|███████▋  | 242/313 [00:41<00:12,  5.81it/s]Evaluating:  78%|███████▊  | 243/313 [00:41<00:12,  5.83it/s]Evaluating:  78%|███████▊  | 244/313 [00:41<00:11,  5.84it/s]Evaluating:  78%|███████▊  | 245/313 [00:41<00:11,  5.86it/s]Evaluating:  79%|███████▊  | 246/313 [00:41<00:11,  5.84it/s]Evaluating:  79%|███████▉  | 247/313 [00:41<00:11,  5.83it/s]Evaluating:  79%|███████▉  | 248/313 [00:42<00:11,  5.83it/s]Evaluating:  80%|███████▉  | 249/313 [00:42<00:11,  5.66it/s]Evaluating:  80%|███████▉  | 250/313 [00:42<00:10,  5.76it/s]Evaluating:  80%|████████  | 251/313 [00:42<00:10,  5.79it/s]Evaluating:  81%|████████  | 252/313 [00:42<00:10,  5.79it/s]Evaluating:  81%|████████  | 253/313 [00:43<00:10,  5.78it/s]Evaluating:  81%|████████  | 254/313 [00:43<00:10,  5.78it/s]Evaluating:  81%|████████▏ | 255/313 [00:43<00:09,  5.81it/s]Evaluating:  82%|████████▏ | 256/313 [00:43<00:09,  5.82it/s]Evaluating:  82%|████████▏ | 257/313 [00:43<00:09,  5.83it/s]Evaluating:  82%|████████▏ | 258/313 [00:43<00:09,  5.86it/s]Evaluating:  83%|████████▎ | 259/313 [00:44<00:09,  5.86it/s]Evaluating:  83%|████████▎ | 260/313 [00:44<00:09,  5.85it/s]Evaluating:  83%|████████▎ | 261/313 [00:44<00:08,  5.86it/s]Evaluating:  84%|████████▎ | 262/313 [00:44<00:08,  5.88it/s]Evaluating:  84%|████████▍ | 263/313 [00:44<00:08,  5.87it/s]Evaluating:  84%|████████▍ | 264/313 [00:44<00:08,  5.84it/s]Evaluating:  85%|████████▍ | 265/313 [00:45<00:08,  5.82it/s]Evaluating:  85%|████████▍ | 266/313 [00:45<00:08,  5.80it/s]Evaluating:  85%|████████▌ | 267/313 [00:45<00:07,  5.79it/s]Evaluating:  86%|████████▌ | 268/313 [00:45<00:07,  5.78it/s]Evaluating:  86%|████████▌ | 269/313 [00:45<00:07,  5.78it/s]Evaluating:  86%|████████▋ | 270/313 [00:45<00:07,  5.60it/s]Evaluating:  87%|████████▋ | 271/313 [00:46<00:07,  5.65it/s]Evaluating:  87%|████████▋ | 272/313 [00:46<00:07,  5.68it/s]Evaluating:  87%|████████▋ | 273/313 [00:46<00:06,  5.72it/s]Evaluating:  88%|████████▊ | 274/313 [00:46<00:06,  5.76it/s]Evaluating:  88%|████████▊ | 275/313 [00:46<00:06,  5.79it/s]Evaluating:  88%|████████▊ | 276/313 [00:46<00:06,  5.82it/s]Evaluating:  88%|████████▊ | 277/313 [00:47<00:06,  5.88it/s]Evaluating:  89%|████████▉ | 278/313 [00:47<00:05,  5.89it/s]Evaluating:  89%|████████▉ | 279/313 [00:47<00:05,  5.91it/s]Evaluating:  89%|████████▉ | 280/313 [00:47<00:05,  5.95it/s]Evaluating:  90%|████████▉ | 281/313 [00:47<00:05,  5.96it/s]Evaluating:  90%|█████████ | 282/313 [00:47<00:05,  5.97it/s]Evaluating:  90%|█████████ | 283/313 [00:48<00:05,  5.91it/s]Evaluating:  91%|█████████ | 284/313 [00:48<00:04,  5.85it/s]Evaluating:  91%|█████████ | 285/313 [00:48<00:04,  5.82it/s]Evaluating:  91%|█████████▏| 286/313 [00:48<00:04,  5.79it/s]Evaluating:  92%|█████████▏| 287/313 [00:48<00:04,  5.79it/s]Evaluating:  92%|█████████▏| 288/313 [00:49<00:04,  5.76it/s]Evaluating:  92%|█████████▏| 289/313 [00:49<00:04,  5.78it/s]Evaluating:  93%|█████████▎| 290/313 [00:49<00:03,  5.80it/s]Evaluating:  93%|█████████▎| 291/313 [00:49<00:03,  5.81it/s]Evaluating:  93%|█████████▎| 292/313 [00:49<00:03,  5.84it/s]Evaluating:  94%|█████████▎| 293/313 [00:49<00:03,  5.88it/s]Evaluating:  94%|█████████▍| 294/313 [00:50<00:03,  5.83it/s]Evaluating:  94%|█████████▍| 295/313 [00:50<00:03,  5.77it/s]Evaluating:  95%|█████████▍| 296/313 [00:50<00:02,  5.69it/s]Evaluating:  95%|█████████▍| 297/313 [00:50<00:02,  5.65it/s]Evaluating:  95%|█████████▌| 298/313 [00:50<00:02,  5.63it/s]Evaluating:  96%|█████████▌| 299/313 [00:50<00:02,  5.66it/s]Evaluating:  96%|█████████▌| 300/313 [00:51<00:02,  5.70it/s]Evaluating:  96%|█████████▌| 301/313 [00:51<00:02,  5.76it/s]Evaluating:  96%|█████████▋| 302/313 [00:51<00:01,  5.82it/s]Evaluating:  97%|█████████▋| 303/313 [00:51<00:01,  5.87it/s]Evaluating:  97%|█████████▋| 304/313 [00:51<00:01,  5.91it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  5.94it/s]Evaluating:  98%|█████████▊| 306/313 [00:52<00:01,  5.95it/s]Evaluating:  98%|█████████▊| 307/313 [00:52<00:01,  5.63it/s]Evaluating:  98%|█████████▊| 308/313 [00:52<00:00,  5.73it/s]Evaluating:  99%|█████████▊| 309/313 [00:52<00:00,  5.80it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.84it/s]Evaluating:  99%|█████████▉| 311/313 [00:53<00:00,  5.88it/s]Evaluating: 100%|█████████▉| 312/313 [00:53<00:00,  5.92it/s]Evaluating: 100%|██████████| 313/313 [00:53<00:00,  6.51it/s]Evaluating: 100%|██████████| 313/313 [00:53<00:00,  5.87it/s]
10/09/2021 00:47:52 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/09/2021 00:47:52 - INFO - __main__ -     f1 = 0.7128349034542659
10/09/2021 00:47:52 - INFO - __main__ -     loss = 1.2952734954631366
10/09/2021 00:47:52 - INFO - __main__ -     precision = 0.6721476721476721
10/09/2021 00:47:52 - INFO - __main__ -     recall = 0.7587653794182491
10/09/2021 00:47:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:48:05 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:48:05 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:48:20 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/09/2021 00:48:22 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:48:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/09/2021 00:48:22 - INFO - __main__ -   Seed = 52
10/09/2021 00:48:22 - INFO - root -   save model
10/09/2021 00:48:22 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/09/2021 00:48:22 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:48:37 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:48:37 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/09/2021 00:48:37 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/09/2021 00:48:37 - INFO - root -   Trying to decide if add adapter
10/09/2021 00:48:37 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/09/2021 00:48:37 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/09/2021 00:48:37 - INFO - __main__ -   Language = en
10/09/2021 00:48:37 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/09/2021 00:48:38 - INFO - __main__ -   Language = ru
10/09/2021 00:48:38 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/09/2021 00:48:43 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/09/2021 00:48:44 - INFO - __main__ -   ***** Running evaluation  in be *****
10/09/2021 00:48:44 - INFO - __main__ -     Num examples = 1001
10/09/2021 00:48:44 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.98it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.24it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.45it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.56it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.62it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.65it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.67it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.68it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.68it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.69it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.69it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.70it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.68it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.68it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.66it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.65it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.65it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.66it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.66it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.67it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.66it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.66it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.67it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.67it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.66it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.66it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.66it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.66it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.65it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.65it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.64it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.78it/s]
10/09/2021 00:48:48 - INFO - __main__ -   ***** Evaluation result  in be *****
10/09/2021 00:48:48 - INFO - __main__ -     f1 = 0.6390395042602635
10/09/2021 00:48:48 - INFO - __main__ -     loss = 1.165598126128316
10/09/2021 00:48:48 - INFO - __main__ -     precision = 0.6043956043956044
10/09/2021 00:48:48 - INFO - __main__ -     recall = 0.6778964667214462
10/09/2021 00:48:48 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/09/2021 00:48:50 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/09/2021 00:48:50 - INFO - __main__ -     Num examples = 10001
10/09/2021 00:48:50 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.66it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.68it/s]Evaluating:   1%|          | 3/313 [00:00<00:46,  6.68it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:46,  6.68it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:46,  6.68it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.68it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.66it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.66it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.66it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.65it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:45,  6.65it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:45,  6.64it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:45,  6.65it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.65it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.65it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.65it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.65it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:44,  6.65it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:44,  6.65it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:44,  6.65it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.65it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.65it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.65it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.64it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:43,  6.64it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:43,  6.64it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:43,  6.64it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:43,  6.62it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.61it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.59it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.59it/s]Evaluating:  10%|█         | 32/313 [00:04<00:42,  6.59it/s]Evaluating:  11%|█         | 33/313 [00:04<00:42,  6.57it/s]Evaluating:  11%|█         | 34/313 [00:05<00:42,  6.56it/s]Evaluating:  11%|█         | 35/313 [00:05<00:56,  4.95it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:51,  5.34it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:48,  5.66it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:46,  5.91it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:44,  6.10it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:43,  6.23it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:42,  6.34it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:42,  6.41it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:41,  6.46it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:41,  6.49it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:41,  6.52it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:40,  6.54it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.55it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:40,  6.55it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:40,  6.52it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:40,  6.49it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:40,  6.49it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:40,  6.48it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:40,  6.49it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:39,  6.49it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:39,  6.50it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:39,  6.46it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:39,  6.47it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:39,  6.50it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:39,  6.51it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:39,  6.46it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:39,  6.43it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:39,  6.40it/s]Evaluating:  20%|██        | 63/313 [00:09<00:39,  6.35it/s]Evaluating:  20%|██        | 64/313 [00:09<00:39,  6.33it/s]Evaluating:  21%|██        | 65/313 [00:10<00:39,  6.32it/s]Evaluating:  21%|██        | 66/313 [00:10<00:39,  6.28it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:39,  6.27it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:39,  6.28it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:38,  6.29it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:38,  6.27it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:38,  6.23it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:38,  6.21it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:38,  6.18it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:39,  6.13it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:39,  6.07it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:39,  6.03it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  6.01it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:39,  5.99it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:39,  5.98it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:39,  5.97it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:38,  5.96it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:38,  5.96it/s]Evaluating:  27%|██▋       | 83/313 [00:13<00:38,  5.95it/s]Evaluating:  27%|██▋       | 84/313 [00:13<00:38,  5.94it/s]Evaluating:  27%|██▋       | 85/313 [00:13<00:38,  5.95it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:38,  5.89it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:38,  5.88it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:38,  5.91it/s]Evaluating:  28%|██▊       | 89/313 [00:14<00:37,  5.92it/s]Evaluating:  29%|██▉       | 90/313 [00:14<00:37,  5.93it/s]Evaluating:  29%|██▉       | 91/313 [00:14<00:37,  5.91it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:37,  5.92it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:37,  5.92it/s]Evaluating:  30%|███       | 94/313 [00:14<00:37,  5.91it/s]Evaluating:  30%|███       | 95/313 [00:15<00:36,  5.92it/s]Evaluating:  31%|███       | 96/313 [00:15<00:36,  5.92it/s]Evaluating:  31%|███       | 97/313 [00:15<00:36,  5.92it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:36,  5.93it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:36,  5.93it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:35,  5.93it/s]Evaluating:  32%|███▏      | 101/313 [00:16<00:35,  5.93it/s]Evaluating:  33%|███▎      | 102/313 [00:16<00:35,  5.93it/s]Evaluating:  33%|███▎      | 103/313 [00:16<00:35,  5.93it/s]Evaluating:  33%|███▎      | 104/313 [00:16<00:35,  5.93it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:35,  5.92it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:34,  5.92it/s]Evaluating:  34%|███▍      | 107/313 [00:17<00:34,  5.93it/s]Evaluating:  35%|███▍      | 108/313 [00:17<00:34,  5.92it/s]Evaluating:  35%|███▍      | 109/313 [00:17<00:34,  5.92it/s]Evaluating:  35%|███▌      | 110/313 [00:17<00:34,  5.92it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:34,  5.92it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:33,  5.92it/s]Evaluating:  36%|███▌      | 113/313 [00:18<00:33,  5.92it/s]Evaluating:  36%|███▋      | 114/313 [00:18<00:33,  5.92it/s]Evaluating:  37%|███▋      | 115/313 [00:18<00:33,  5.92it/s]Evaluating:  37%|███▋      | 116/313 [00:18<00:33,  5.92it/s]Evaluating:  37%|███▋      | 117/313 [00:18<00:33,  5.93it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:32,  5.92it/s]Evaluating:  38%|███▊      | 119/313 [00:19<00:32,  5.92it/s]Evaluating:  38%|███▊      | 120/313 [00:19<00:32,  5.91it/s]Evaluating:  39%|███▊      | 121/313 [00:19<00:32,  5.83it/s]Evaluating:  39%|███▉      | 122/313 [00:19<00:32,  5.84it/s]Evaluating:  39%|███▉      | 123/313 [00:19<00:32,  5.87it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:31,  5.91it/s]Evaluating:  40%|███▉      | 125/313 [00:20<00:31,  5.92it/s]Evaluating:  40%|████      | 126/313 [00:20<00:31,  5.91it/s]Evaluating:  41%|████      | 127/313 [00:20<00:31,  5.91it/s]Evaluating:  41%|████      | 128/313 [00:20<00:31,  5.90it/s]Evaluating:  41%|████      | 129/313 [00:20<00:31,  5.90it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:31,  5.88it/s]Evaluating:  42%|████▏     | 131/313 [00:21<00:30,  5.87it/s]Evaluating:  42%|████▏     | 132/313 [00:21<00:30,  5.88it/s]Evaluating:  42%|████▏     | 133/313 [00:21<00:30,  5.89it/s]Evaluating:  43%|████▎     | 134/313 [00:21<00:30,  5.89it/s]Evaluating:  43%|████▎     | 135/313 [00:21<00:30,  5.89it/s]Evaluating:  43%|████▎     | 136/313 [00:22<00:30,  5.89it/s]Evaluating:  44%|████▍     | 137/313 [00:22<00:29,  5.91it/s]Evaluating:  44%|████▍     | 138/313 [00:22<00:29,  5.90it/s]Evaluating:  44%|████▍     | 139/313 [00:22<00:29,  5.90it/s]Evaluating:  45%|████▍     | 140/313 [00:22<00:29,  5.90it/s]Evaluating:  45%|████▌     | 141/313 [00:22<00:29,  5.89it/s]Evaluating:  45%|████▌     | 142/313 [00:23<00:29,  5.89it/s]Evaluating:  46%|████▌     | 143/313 [00:23<00:28,  5.89it/s]Evaluating:  46%|████▌     | 144/313 [00:23<00:28,  5.89it/s]Evaluating:  46%|████▋     | 145/313 [00:23<00:28,  5.89it/s]Evaluating:  47%|████▋     | 146/313 [00:23<00:28,  5.88it/s]Evaluating:  47%|████▋     | 147/313 [00:23<00:28,  5.89it/s]Evaluating:  47%|████▋     | 148/313 [00:24<00:28,  5.88it/s]Evaluating:  48%|████▊     | 149/313 [00:24<00:27,  5.88it/s]Evaluating:  48%|████▊     | 150/313 [00:24<00:27,  5.88it/s]Evaluating:  48%|████▊     | 151/313 [00:24<00:27,  5.88it/s]Evaluating:  49%|████▊     | 152/313 [00:24<00:27,  5.90it/s]Evaluating:  49%|████▉     | 153/313 [00:24<00:27,  5.90it/s]Evaluating:  49%|████▉     | 154/313 [00:25<00:26,  5.90it/s]Evaluating:  50%|████▉     | 155/313 [00:25<00:26,  5.92it/s]Evaluating:  50%|████▉     | 156/313 [00:25<00:26,  5.95it/s]Evaluating:  50%|█████     | 157/313 [00:25<00:26,  5.96it/s]Evaluating:  50%|█████     | 158/313 [00:25<00:25,  5.97it/s]Evaluating:  51%|█████     | 159/313 [00:25<00:25,  5.97it/s]Evaluating:  51%|█████     | 160/313 [00:26<00:25,  5.96it/s]Evaluating:  51%|█████▏    | 161/313 [00:26<00:25,  5.94it/s]Evaluating:  52%|█████▏    | 162/313 [00:26<00:25,  5.94it/s]Evaluating:  52%|█████▏    | 163/313 [00:26<00:25,  5.95it/s]Evaluating:  52%|█████▏    | 164/313 [00:26<00:24,  5.97it/s]Evaluating:  53%|█████▎    | 165/313 [00:26<00:24,  5.98it/s]Evaluating:  53%|█████▎    | 166/313 [00:27<00:24,  5.94it/s]Evaluating:  53%|█████▎    | 167/313 [00:27<00:24,  5.92it/s]Evaluating:  54%|█████▎    | 168/313 [00:27<00:24,  5.90it/s]Evaluating:  54%|█████▍    | 169/313 [00:27<00:24,  5.89it/s]Evaluating:  54%|█████▍    | 170/313 [00:27<00:24,  5.89it/s]Evaluating:  55%|█████▍    | 171/313 [00:27<00:24,  5.89it/s]Evaluating:  55%|█████▍    | 172/313 [00:28<00:23,  5.88it/s]Evaluating:  55%|█████▌    | 173/313 [00:28<00:23,  5.87it/s]Evaluating:  56%|█████▌    | 174/313 [00:28<00:23,  5.88it/s]Evaluating:  56%|█████▌    | 175/313 [00:28<00:23,  5.91it/s]Evaluating:  56%|█████▌    | 176/313 [00:28<00:23,  5.94it/s]Evaluating:  57%|█████▋    | 177/313 [00:28<00:22,  5.95it/s]Evaluating:  57%|█████▋    | 178/313 [00:29<00:22,  5.95it/s]Evaluating:  57%|█████▋    | 179/313 [00:29<00:22,  5.95it/s]Evaluating:  58%|█████▊    | 180/313 [00:29<00:22,  5.95it/s]Evaluating:  58%|█████▊    | 181/313 [00:29<00:22,  5.97it/s]Evaluating:  58%|█████▊    | 182/313 [00:29<00:21,  5.99it/s]Evaluating:  58%|█████▊    | 183/313 [00:29<00:21,  6.00it/s]Evaluating:  59%|█████▉    | 184/313 [00:30<00:21,  5.98it/s]Evaluating:  59%|█████▉    | 185/313 [00:30<00:21,  5.97it/s]Evaluating:  59%|█████▉    | 186/313 [00:30<00:21,  5.96it/s]Evaluating:  60%|█████▉    | 187/313 [00:30<00:21,  5.95it/s]Evaluating:  60%|██████    | 188/313 [00:30<00:21,  5.92it/s]Evaluating:  60%|██████    | 189/313 [00:30<00:21,  5.90it/s]Evaluating:  61%|██████    | 190/313 [00:31<00:21,  5.75it/s]Evaluating:  61%|██████    | 191/313 [00:31<00:21,  5.80it/s]Evaluating:  61%|██████▏   | 192/313 [00:31<00:20,  5.86it/s]Evaluating:  62%|██████▏   | 193/313 [00:31<00:20,  5.91it/s]Evaluating:  62%|██████▏   | 194/313 [00:31<00:20,  5.94it/s]Evaluating:  62%|██████▏   | 195/313 [00:31<00:19,  5.94it/s]Evaluating:  63%|██████▎   | 196/313 [00:32<00:19,  5.93it/s]Evaluating:  63%|██████▎   | 197/313 [00:32<00:19,  5.92it/s]Evaluating:  63%|██████▎   | 198/313 [00:32<00:19,  5.91it/s]Evaluating:  64%|██████▎   | 199/313 [00:32<00:19,  5.91it/s]Evaluating:  64%|██████▍   | 200/313 [00:32<00:19,  5.90it/s]Evaluating:  64%|██████▍   | 201/313 [00:32<00:19,  5.88it/s]Evaluating:  65%|██████▍   | 202/313 [00:33<00:18,  5.86it/s]Evaluating:  65%|██████▍   | 203/313 [00:33<00:18,  5.85it/s]Evaluating:  65%|██████▌   | 204/313 [00:33<00:18,  5.84it/s]Evaluating:  65%|██████▌   | 205/313 [00:33<00:18,  5.86it/s]Evaluating:  66%|██████▌   | 206/313 [00:33<00:18,  5.84it/s]Evaluating:  66%|██████▌   | 207/313 [00:34<00:18,  5.81it/s]Evaluating:  66%|██████▋   | 208/313 [00:34<00:18,  5.81it/s]Evaluating:  67%|██████▋   | 209/313 [00:34<00:17,  5.82it/s]Evaluating:  67%|██████▋   | 210/313 [00:34<00:17,  5.82it/s]Evaluating:  67%|██████▋   | 211/313 [00:34<00:17,  5.82it/s]Evaluating:  68%|██████▊   | 212/313 [00:34<00:17,  5.82it/s]Evaluating:  68%|██████▊   | 213/313 [00:35<00:17,  5.82it/s]Evaluating:  68%|██████▊   | 214/313 [00:35<00:17,  5.81it/s]Evaluating:  69%|██████▊   | 215/313 [00:35<00:16,  5.83it/s]Evaluating:  69%|██████▉   | 216/313 [00:35<00:16,  5.82it/s]Evaluating:  69%|██████▉   | 217/313 [00:35<00:16,  5.79it/s]Evaluating:  70%|██████▉   | 218/313 [00:35<00:16,  5.80it/s]Evaluating:  70%|██████▉   | 219/313 [00:36<00:16,  5.83it/s]Evaluating:  70%|███████   | 220/313 [00:36<00:15,  5.82it/s]Evaluating:  71%|███████   | 221/313 [00:36<00:15,  5.84it/s]Evaluating:  71%|███████   | 222/313 [00:36<00:15,  5.86it/s]Evaluating:  71%|███████   | 223/313 [00:36<00:15,  5.87it/s]Evaluating:  72%|███████▏  | 224/313 [00:36<00:15,  5.87it/s]Evaluating:  72%|███████▏  | 225/313 [00:37<00:15,  5.85it/s]Evaluating:  72%|███████▏  | 226/313 [00:37<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 227/313 [00:37<00:14,  5.84it/s]Evaluating:  73%|███████▎  | 228/313 [00:37<00:14,  5.86it/s]Evaluating:  73%|███████▎  | 229/313 [00:37<00:14,  5.90it/s]Evaluating:  73%|███████▎  | 230/313 [00:37<00:14,  5.93it/s]Evaluating:  74%|███████▍  | 231/313 [00:38<00:13,  5.90it/s]Evaluating:  74%|███████▍  | 232/313 [00:38<00:13,  5.87it/s]Evaluating:  74%|███████▍  | 233/313 [00:38<00:13,  5.86it/s]Evaluating:  75%|███████▍  | 234/313 [00:38<00:13,  5.86it/s]Evaluating:  75%|███████▌  | 235/313 [00:38<00:13,  5.84it/s]Evaluating:  75%|███████▌  | 236/313 [00:38<00:13,  5.84it/s]Evaluating:  76%|███████▌  | 237/313 [00:39<00:13,  5.82it/s]Evaluating:  76%|███████▌  | 238/313 [00:39<00:12,  5.81it/s]Evaluating:  76%|███████▋  | 239/313 [00:39<00:12,  5.83it/s]Evaluating:  77%|███████▋  | 240/313 [00:39<00:12,  5.85it/s]Evaluating:  77%|███████▋  | 241/313 [00:39<00:12,  5.86it/s]Evaluating:  77%|███████▋  | 242/313 [00:40<00:12,  5.87it/s]Evaluating:  78%|███████▊  | 243/313 [00:40<00:11,  5.85it/s]Evaluating:  78%|███████▊  | 244/313 [00:40<00:11,  5.84it/s]Evaluating:  78%|███████▊  | 245/313 [00:40<00:11,  5.82it/s]Evaluating:  79%|███████▊  | 246/313 [00:40<00:11,  5.81it/s]Evaluating:  79%|███████▉  | 247/313 [00:40<00:11,  5.83it/s]Evaluating:  79%|███████▉  | 248/313 [00:41<00:11,  5.84it/s]Evaluating:  80%|███████▉  | 249/313 [00:41<00:10,  5.82it/s]Evaluating:  80%|███████▉  | 250/313 [00:41<00:10,  5.81it/s]Evaluating:  80%|████████  | 251/313 [00:41<00:10,  5.80it/s]Evaluating:  81%|████████  | 252/313 [00:41<00:10,  5.81it/s]Evaluating:  81%|████████  | 253/313 [00:41<00:10,  5.80it/s]Evaluating:  81%|████████  | 254/313 [00:42<00:10,  5.81it/s]Evaluating:  81%|████████▏ | 255/313 [00:42<00:10,  5.80it/s]Evaluating:  82%|████████▏ | 256/313 [00:42<00:09,  5.80it/s]Evaluating:  82%|████████▏ | 257/313 [00:42<00:09,  5.79it/s]Evaluating:  82%|████████▏ | 258/313 [00:42<00:09,  5.80it/s]Evaluating:  83%|████████▎ | 259/313 [00:42<00:09,  5.82it/s]Evaluating:  83%|████████▎ | 260/313 [00:43<00:09,  5.86it/s]Evaluating:  83%|████████▎ | 261/313 [00:43<00:08,  5.88it/s]Evaluating:  84%|████████▎ | 262/313 [00:43<00:08,  5.84it/s]Evaluating:  84%|████████▍ | 263/313 [00:43<00:08,  5.82it/s]Evaluating:  84%|████████▍ | 264/313 [00:43<00:08,  5.83it/s]Evaluating:  85%|████████▍ | 265/313 [00:43<00:08,  5.84it/s]Evaluating:  85%|████████▍ | 266/313 [00:44<00:08,  5.84it/s]Evaluating:  85%|████████▌ | 267/313 [00:44<00:07,  5.84it/s]Evaluating:  86%|████████▌ | 268/313 [00:44<00:07,  5.82it/s]Evaluating:  86%|████████▌ | 269/313 [00:44<00:07,  5.80it/s]Evaluating:  86%|████████▋ | 270/313 [00:44<00:07,  5.79it/s]Evaluating:  87%|████████▋ | 271/313 [00:45<00:07,  5.80it/s]Evaluating:  87%|████████▋ | 272/313 [00:45<00:07,  5.82it/s]Evaluating:  87%|████████▋ | 273/313 [00:45<00:06,  5.83it/s]Evaluating:  88%|████████▊ | 274/313 [00:45<00:06,  5.81it/s]Evaluating:  88%|████████▊ | 275/313 [00:45<00:06,  5.80it/s]Evaluating:  88%|████████▊ | 276/313 [00:45<00:06,  5.79it/s]Evaluating:  88%|████████▊ | 277/313 [00:46<00:06,  5.78it/s]Evaluating:  89%|████████▉ | 278/313 [00:46<00:06,  5.77it/s]Evaluating:  89%|████████▉ | 279/313 [00:46<00:05,  5.70it/s]Evaluating:  89%|████████▉ | 280/313 [00:46<00:05,  5.65it/s]Evaluating:  90%|████████▉ | 281/313 [00:46<00:05,  5.60it/s]Evaluating:  90%|█████████ | 282/313 [00:46<00:05,  5.59it/s]Evaluating:  90%|█████████ | 283/313 [00:47<00:05,  5.60it/s]Evaluating:  91%|█████████ | 284/313 [00:47<00:05,  5.61it/s]Evaluating:  91%|█████████ | 285/313 [00:47<00:04,  5.63it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:04,  5.67it/s]Evaluating:  92%|█████████▏| 287/313 [00:47<00:04,  5.75it/s]Evaluating:  92%|█████████▏| 288/313 [00:47<00:04,  5.83it/s]Evaluating:  92%|█████████▏| 289/313 [00:48<00:04,  5.89it/s]Evaluating:  93%|█████████▎| 290/313 [00:48<00:03,  5.93it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:03,  5.96it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  5.97it/s]Evaluating:  94%|█████████▎| 293/313 [00:48<00:03,  5.98it/s]Evaluating:  94%|█████████▍| 294/313 [00:48<00:03,  5.92it/s]Evaluating:  94%|█████████▍| 295/313 [00:49<00:03,  5.85it/s]Evaluating:  95%|█████████▍| 296/313 [00:49<00:02,  5.86it/s]Evaluating:  95%|█████████▍| 297/313 [00:49<00:02,  5.88it/s]Evaluating:  95%|█████████▌| 298/313 [00:49<00:02,  5.91it/s]Evaluating:  96%|█████████▌| 299/313 [00:49<00:02,  5.37it/s]Evaluating:  96%|█████████▌| 300/313 [00:50<00:02,  5.05it/s]Evaluating:  96%|█████████▌| 301/313 [00:50<00:02,  4.88it/s]Evaluating:  96%|█████████▋| 302/313 [00:50<00:02,  4.85it/s]Evaluating:  97%|█████████▋| 303/313 [00:50<00:02,  4.88it/s]Evaluating:  97%|█████████▋| 304/313 [00:50<00:01,  4.90it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  4.99it/s]Evaluating:  98%|█████████▊| 306/313 [00:51<00:01,  5.11it/s]Evaluating:  98%|█████████▊| 307/313 [00:51<00:01,  5.21it/s]Evaluating:  98%|█████████▊| 308/313 [00:51<00:00,  5.30it/s]Evaluating:  99%|█████████▊| 309/313 [00:51<00:00,  5.36it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.41it/s]Evaluating:  99%|█████████▉| 311/313 [00:52<00:00,  5.47it/s]Evaluating: 100%|█████████▉| 312/313 [00:52<00:00,  5.52it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  6.33it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  5.96it/s]
10/09/2021 00:49:44 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/09/2021 00:49:44 - INFO - __main__ -     f1 = 0.5524548887956358
10/09/2021 00:49:44 - INFO - __main__ -     loss = 1.7995321748736568
10/09/2021 00:49:44 - INFO - __main__ -     precision = 0.5204585886538842
10/09/2021 00:49:44 - INFO - __main__ -     recall = 0.5886429689246591
10/09/2021 00:49:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/09/2021 00:49:45 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/09/2021 00:49:45 - INFO - __main__ -     Num examples = 10004
10/09/2021 00:49:45 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.02it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.03it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  6.04it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:51,  6.03it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:51,  6.04it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:50,  6.04it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.04it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.04it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:50,  6.04it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:50,  6.04it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:50,  6.03it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:49,  6.03it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:49,  6.03it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.03it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  6.04it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  6.04it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:49,  6.03it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:48,  6.03it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.03it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.03it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  6.02it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  6.02it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:48,  6.02it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:48,  6.02it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:47,  6.02it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  6.02it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  6.02it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  5.99it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:47,  5.98it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:47,  6.00it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:47,  5.99it/s]Evaluating:  10%|█         | 32/313 [00:05<00:46,  6.00it/s]Evaluating:  11%|█         | 33/313 [00:05<00:46,  6.00it/s]Evaluating:  11%|█         | 34/313 [00:05<00:46,  6.00it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  6.00it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:46,  6.00it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:45,  6.00it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  6.00it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.00it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  5.99it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.00it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:45,  5.99it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:45,  6.00it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  6.00it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:44,  6.00it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  6.00it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  6.00it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:44,  6.00it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:44,  6.00it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:43,  5.99it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  5.99it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  5.99it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  5.99it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:43,  5.99it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:43,  5.99it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:42,  5.99it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  5.99it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  5.99it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  5.99it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:42,  5.99it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:56,  4.46it/s]Evaluating:  20%|██        | 63/313 [00:10<00:51,  4.83it/s]Evaluating:  20%|██        | 64/313 [00:10<00:48,  5.12it/s]Evaluating:  21%|██        | 65/313 [00:11<00:46,  5.35it/s]Evaluating:  21%|██        | 66/313 [00:11<00:44,  5.52it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:43,  5.65it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:42,  5.74it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:41,  5.81it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:41,  5.86it/s]Evaluating:  23%|██▎       | 71/313 [00:12<00:41,  5.89it/s]Evaluating:  23%|██▎       | 72/313 [00:12<00:40,  5.91it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  5.93it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:40,  5.94it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:40,  5.95it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:39,  5.96it/s]Evaluating:  25%|██▍       | 77/313 [00:13<00:39,  5.96it/s]Evaluating:  25%|██▍       | 78/313 [00:13<00:39,  5.96it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.96it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:39,  5.96it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:38,  5.96it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:38,  5.96it/s]Evaluating:  27%|██▋       | 83/313 [00:14<00:38,  5.96it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:38,  5.96it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:38,  5.96it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:38,  5.95it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:37,  5.95it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:37,  5.95it/s]Evaluating:  28%|██▊       | 89/313 [00:15<00:37,  5.95it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:37,  5.95it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:38,  5.73it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:37,  5.86it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:37,  5.94it/s]Evaluating:  30%|███       | 94/313 [00:15<00:36,  6.00it/s]Evaluating:  30%|███       | 95/313 [00:16<00:36,  6.02it/s]Evaluating:  31%|███       | 96/313 [00:16<00:36,  5.99it/s]Evaluating:  31%|███       | 97/313 [00:16<00:36,  5.97it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:36,  5.96it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:36,  5.94it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:35,  5.94it/s]Evaluating:  32%|███▏      | 101/313 [00:17<00:35,  5.94it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:35,  5.93it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:35,  5.93it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:35,  5.93it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:35,  5.92it/s]Evaluating:  34%|███▍      | 106/313 [00:17<00:34,  5.93it/s]Evaluating:  34%|███▍      | 107/313 [00:18<00:34,  5.93it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:34,  5.93it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:34,  5.93it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:34,  5.93it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:34,  5.93it/s]Evaluating:  36%|███▌      | 112/313 [00:18<00:33,  5.93it/s]Evaluating:  36%|███▌      | 113/313 [00:19<00:33,  5.92it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:33,  5.92it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:33,  5.92it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:33,  5.92it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:33,  5.94it/s]Evaluating:  38%|███▊      | 118/313 [00:19<00:32,  5.96it/s]Evaluating:  38%|███▊      | 119/313 [00:20<00:32,  5.95it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:32,  5.94it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:32,  5.93it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:32,  5.93it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:32,  5.92it/s]Evaluating:  40%|███▉      | 124/313 [00:20<00:31,  5.92it/s]Evaluating:  40%|███▉      | 125/313 [00:21<00:31,  5.92it/s]Evaluating:  40%|████      | 126/313 [00:21<00:31,  5.91it/s]Evaluating:  41%|████      | 127/313 [00:21<00:31,  5.91it/s]Evaluating:  41%|████      | 128/313 [00:21<00:31,  5.91it/s]Evaluating:  41%|████      | 129/313 [00:21<00:31,  5.91it/s]Evaluating:  42%|████▏     | 130/313 [00:21<00:30,  5.91it/s]Evaluating:  42%|████▏     | 131/313 [00:22<00:30,  5.91it/s]Evaluating:  42%|████▏     | 132/313 [00:22<00:30,  5.91it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:30,  5.92it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:30,  5.92it/s]Evaluating:  43%|████▎     | 135/313 [00:22<00:30,  5.91it/s]Evaluating:  43%|████▎     | 136/313 [00:22<00:29,  5.91it/s]Evaluating:  44%|████▍     | 137/313 [00:23<00:29,  5.91it/s]Evaluating:  44%|████▍     | 138/313 [00:23<00:29,  5.91it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:29,  5.91it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:29,  5.90it/s]Evaluating:  45%|████▌     | 141/313 [00:23<00:29,  5.90it/s]Evaluating:  45%|████▌     | 142/313 [00:23<00:28,  5.90it/s]Evaluating:  46%|████▌     | 143/313 [00:24<00:28,  5.89it/s]Evaluating:  46%|████▌     | 144/313 [00:24<00:28,  5.89it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:28,  5.89it/s]Evaluating:  47%|████▋     | 146/313 [00:24<00:28,  5.89it/s]Evaluating:  47%|████▋     | 147/313 [00:24<00:28,  5.89it/s]Evaluating:  47%|████▋     | 148/313 [00:25<00:28,  5.88it/s]Evaluating:  48%|████▊     | 149/313 [00:25<00:27,  5.89it/s]Evaluating:  48%|████▊     | 150/313 [00:25<00:27,  5.88it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:27,  5.89it/s]Evaluating:  49%|████▊     | 152/313 [00:25<00:27,  5.90it/s]Evaluating:  49%|████▉     | 153/313 [00:25<00:27,  5.89it/s]Evaluating:  49%|████▉     | 154/313 [00:26<00:27,  5.89it/s]Evaluating:  50%|████▉     | 155/313 [00:26<00:26,  5.89it/s]Evaluating:  50%|████▉     | 156/313 [00:26<00:26,  5.88it/s]Evaluating:  50%|█████     | 157/313 [00:26<00:26,  5.88it/s]Evaluating:  50%|█████     | 158/313 [00:26<00:26,  5.88it/s]Evaluating:  51%|█████     | 159/313 [00:26<00:26,  5.88it/s]Evaluating:  51%|█████     | 160/313 [00:27<00:26,  5.88it/s]Evaluating:  51%|█████▏    | 161/313 [00:27<00:25,  5.88it/s]Evaluating:  52%|█████▏    | 162/313 [00:27<00:25,  5.87it/s]Evaluating:  52%|█████▏    | 163/313 [00:27<00:25,  5.88it/s]Evaluating:  52%|█████▏    | 164/313 [00:27<00:25,  5.90it/s]Evaluating:  53%|█████▎    | 165/313 [00:27<00:25,  5.90it/s]Evaluating:  53%|█████▎    | 166/313 [00:28<00:24,  5.89it/s]Evaluating:  53%|█████▎    | 167/313 [00:28<00:24,  5.91it/s]Evaluating:  54%|█████▎    | 168/313 [00:28<00:24,  5.92it/s]Evaluating:  54%|█████▍    | 169/313 [00:28<00:24,  5.92it/s]Evaluating:  54%|█████▍    | 170/313 [00:28<00:24,  5.93it/s]Evaluating:  55%|█████▍    | 171/313 [00:28<00:23,  5.94it/s]Evaluating:  55%|█████▍    | 172/313 [00:29<00:23,  5.94it/s]Evaluating:  55%|█████▌    | 173/313 [00:29<00:23,  5.94it/s]Evaluating:  56%|█████▌    | 174/313 [00:29<00:23,  5.91it/s]Evaluating:  56%|█████▌    | 175/313 [00:29<00:23,  5.90it/s]Evaluating:  56%|█████▌    | 176/313 [00:29<00:23,  5.89it/s]Evaluating:  57%|█████▋    | 177/313 [00:29<00:23,  5.88it/s]Evaluating:  57%|█████▋    | 178/313 [00:30<00:23,  5.87it/s]Evaluating:  57%|█████▋    | 179/313 [00:30<00:22,  5.87it/s]Evaluating:  58%|█████▊    | 180/313 [00:30<00:22,  5.83it/s]Evaluating:  58%|█████▊    | 181/313 [00:30<00:22,  5.84it/s]Evaluating:  58%|█████▊    | 182/313 [00:30<00:22,  5.84it/s]Evaluating:  58%|█████▊    | 183/313 [00:30<00:22,  5.84it/s]Evaluating:  59%|█████▉    | 184/313 [00:31<00:22,  5.84it/s]Evaluating:  59%|█████▉    | 185/313 [00:31<00:21,  5.87it/s]Evaluating:  59%|█████▉    | 186/313 [00:31<00:21,  5.90it/s]Evaluating:  60%|█████▉    | 187/313 [00:31<00:21,  5.92it/s]Evaluating:  60%|██████    | 188/313 [00:31<00:21,  5.89it/s]Evaluating:  60%|██████    | 189/313 [00:31<00:21,  5.88it/s]Evaluating:  61%|██████    | 190/313 [00:32<00:20,  5.86it/s]Evaluating:  61%|██████    | 191/313 [00:32<00:20,  5.86it/s]Evaluating:  61%|██████▏   | 192/313 [00:32<00:20,  5.85it/s]Evaluating:  62%|██████▏   | 193/313 [00:32<00:20,  5.86it/s]Evaluating:  62%|██████▏   | 194/313 [00:32<00:20,  5.85it/s]Evaluating:  62%|██████▏   | 195/313 [00:32<00:20,  5.85it/s]Evaluating:  63%|██████▎   | 196/313 [00:33<00:20,  5.84it/s]Evaluating:  63%|██████▎   | 197/313 [00:33<00:19,  5.84it/s]Evaluating:  63%|██████▎   | 198/313 [00:33<00:19,  5.84it/s]Evaluating:  64%|██████▎   | 199/313 [00:33<00:19,  5.83it/s]Evaluating:  64%|██████▍   | 200/313 [00:33<00:19,  5.83it/s]Evaluating:  64%|██████▍   | 201/313 [00:34<00:19,  5.83it/s]Evaluating:  65%|██████▍   | 202/313 [00:34<00:18,  5.85it/s]Evaluating:  65%|██████▍   | 203/313 [00:34<00:18,  5.86it/s]Evaluating:  65%|██████▌   | 204/313 [00:34<00:18,  5.88it/s]Evaluating:  65%|██████▌   | 205/313 [00:34<00:18,  5.88it/s]Evaluating:  66%|██████▌   | 206/313 [00:34<00:18,  5.86it/s]Evaluating:  66%|██████▌   | 207/313 [00:35<00:18,  5.85it/s]Evaluating:  66%|██████▋   | 208/313 [00:35<00:17,  5.84it/s]Evaluating:  67%|██████▋   | 209/313 [00:35<00:17,  5.83it/s]Evaluating:  67%|██████▋   | 210/313 [00:35<00:17,  5.83it/s]Evaluating:  67%|██████▋   | 211/313 [00:35<00:17,  5.83it/s]Evaluating:  68%|██████▊   | 212/313 [00:35<00:17,  5.85it/s]Evaluating:  68%|██████▊   | 213/313 [00:36<00:17,  5.86it/s]Evaluating:  68%|██████▊   | 214/313 [00:36<00:16,  5.86it/s]Evaluating:  69%|██████▊   | 215/313 [00:36<00:16,  5.87it/s]Evaluating:  69%|██████▉   | 216/313 [00:36<00:16,  5.87it/s]Evaluating:  69%|██████▉   | 217/313 [00:36<00:16,  5.85it/s]Evaluating:  70%|██████▉   | 218/313 [00:36<00:16,  5.84it/s]Evaluating:  70%|██████▉   | 219/313 [00:37<00:16,  5.83it/s]Evaluating:  70%|███████   | 220/313 [00:37<00:15,  5.82it/s]Evaluating:  71%|███████   | 221/313 [00:37<00:15,  5.83it/s]Evaluating:  71%|███████   | 222/313 [00:37<00:15,  5.83it/s]Evaluating:  71%|███████   | 223/313 [00:37<00:15,  5.83it/s]Evaluating:  72%|███████▏  | 224/313 [00:37<00:15,  5.83it/s]Evaluating:  72%|███████▏  | 225/313 [00:38<00:15,  5.82it/s]Evaluating:  72%|███████▏  | 226/313 [00:38<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 227/313 [00:38<00:14,  5.81it/s]Evaluating:  73%|███████▎  | 228/313 [00:38<00:14,  5.81it/s]Evaluating:  73%|███████▎  | 229/313 [00:38<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 230/313 [00:38<00:14,  5.83it/s]Evaluating:  74%|███████▍  | 231/313 [00:39<00:14,  5.82it/s]Evaluating:  74%|███████▍  | 232/313 [00:39<00:13,  5.82it/s]Evaluating:  74%|███████▍  | 233/313 [00:39<00:13,  5.82it/s]Evaluating:  75%|███████▍  | 234/313 [00:39<00:13,  5.82it/s]Evaluating:  75%|███████▌  | 235/313 [00:39<00:13,  5.81it/s]Evaluating:  75%|███████▌  | 236/313 [00:40<00:13,  5.81it/s]Evaluating:  76%|███████▌  | 237/313 [00:40<00:13,  5.81it/s]Evaluating:  76%|███████▌  | 238/313 [00:40<00:12,  5.81it/s]Evaluating:  76%|███████▋  | 239/313 [00:40<00:12,  5.81it/s]Evaluating:  77%|███████▋  | 240/313 [00:40<00:12,  5.83it/s]Evaluating:  77%|███████▋  | 241/313 [00:40<00:12,  5.84it/s]Evaluating:  77%|███████▋  | 242/313 [00:41<00:12,  5.86it/s]Evaluating:  78%|███████▊  | 243/313 [00:41<00:11,  5.84it/s]Evaluating:  78%|███████▊  | 244/313 [00:41<00:11,  5.82it/s]Evaluating:  78%|███████▊  | 245/313 [00:41<00:11,  5.83it/s]Evaluating:  79%|███████▊  | 246/313 [00:41<00:11,  5.82it/s]Evaluating:  79%|███████▉  | 247/313 [00:41<00:11,  5.81it/s]Evaluating:  79%|███████▉  | 248/313 [00:42<00:11,  5.80it/s]Evaluating:  80%|███████▉  | 249/313 [00:42<00:11,  5.82it/s]Evaluating:  80%|███████▉  | 250/313 [00:42<00:10,  5.83it/s]Evaluating:  80%|████████  | 251/313 [00:42<00:10,  5.86it/s]Evaluating:  81%|████████  | 252/313 [00:42<00:10,  5.89it/s]Evaluating:  81%|████████  | 253/313 [00:42<00:10,  5.88it/s]Evaluating:  81%|████████  | 254/313 [00:43<00:10,  5.88it/s]Evaluating:  81%|████████▏ | 255/313 [00:43<00:09,  5.87it/s]Evaluating:  82%|████████▏ | 256/313 [00:43<00:09,  5.84it/s]Evaluating:  82%|████████▏ | 257/313 [00:43<00:09,  5.84it/s]Evaluating:  82%|████████▏ | 258/313 [00:43<00:09,  5.82it/s]Evaluating:  83%|████████▎ | 259/313 [00:43<00:09,  5.86it/s]Evaluating:  83%|████████▎ | 260/313 [00:44<00:09,  5.86it/s]Evaluating:  83%|████████▎ | 261/313 [00:44<00:08,  5.85it/s]Evaluating:  84%|████████▎ | 262/313 [00:44<00:08,  5.83it/s]Evaluating:  84%|████████▍ | 263/313 [00:44<00:08,  5.81it/s]Evaluating:  84%|████████▍ | 264/313 [00:44<00:08,  5.81it/s]Evaluating:  85%|████████▍ | 265/313 [00:44<00:08,  5.80it/s]Evaluating:  85%|████████▍ | 266/313 [00:45<00:08,  5.79it/s]Evaluating:  85%|████████▌ | 267/313 [00:45<00:08,  5.65it/s]Evaluating:  86%|████████▌ | 268/313 [00:45<00:07,  5.67it/s]Evaluating:  86%|████████▌ | 269/313 [00:45<00:07,  5.69it/s]Evaluating:  86%|████████▋ | 270/313 [00:45<00:07,  5.72it/s]Evaluating:  87%|████████▋ | 271/313 [00:46<00:07,  5.75it/s]Evaluating:  87%|████████▋ | 272/313 [00:46<00:07,  5.77it/s]Evaluating:  87%|████████▋ | 273/313 [00:46<00:06,  5.79it/s]Evaluating:  88%|████████▊ | 274/313 [00:46<00:06,  5.79it/s]Evaluating:  88%|████████▊ | 275/313 [00:46<00:06,  5.78it/s]Evaluating:  88%|████████▊ | 276/313 [00:46<00:06,  5.78it/s]Evaluating:  88%|████████▊ | 277/313 [00:47<00:06,  5.79it/s]Evaluating:  89%|████████▉ | 278/313 [00:47<00:06,  5.82it/s]Evaluating:  89%|████████▉ | 279/313 [00:47<00:05,  5.83it/s]Evaluating:  89%|████████▉ | 280/313 [00:47<00:05,  5.86it/s]Evaluating:  90%|████████▉ | 281/313 [00:47<00:05,  5.86it/s]Evaluating:  90%|█████████ | 282/313 [00:47<00:05,  5.86it/s]Evaluating:  90%|█████████ | 283/313 [00:48<00:05,  5.88it/s]Evaluating:  91%|█████████ | 284/313 [00:48<00:04,  5.90it/s]Evaluating:  91%|█████████ | 285/313 [00:48<00:04,  5.94it/s]Evaluating:  91%|█████████▏| 286/313 [00:48<00:04,  5.95it/s]Evaluating:  92%|█████████▏| 287/313 [00:48<00:04,  5.93it/s]Evaluating:  92%|█████████▏| 288/313 [00:48<00:04,  5.92it/s]Evaluating:  92%|█████████▏| 289/313 [00:49<00:04,  5.91it/s]Evaluating:  93%|█████████▎| 290/313 [00:49<00:03,  5.89it/s]Evaluating:  93%|█████████▎| 291/313 [00:49<00:03,  5.87it/s]Evaluating:  93%|█████████▎| 292/313 [00:49<00:03,  5.84it/s]Evaluating:  94%|█████████▎| 293/313 [00:49<00:03,  5.72it/s]Evaluating:  94%|█████████▍| 294/313 [00:49<00:03,  5.69it/s]Evaluating:  94%|█████████▍| 295/313 [00:50<00:03,  5.67it/s]Evaluating:  95%|█████████▍| 296/313 [00:50<00:02,  5.72it/s]Evaluating:  95%|█████████▍| 297/313 [00:50<00:02,  5.79it/s]Evaluating:  95%|█████████▌| 298/313 [00:50<00:02,  5.83it/s]Evaluating:  96%|█████████▌| 299/313 [00:50<00:02,  5.74it/s]Evaluating:  96%|█████████▌| 300/313 [00:51<00:02,  5.68it/s]Evaluating:  96%|█████████▌| 301/313 [00:51<00:02,  5.66it/s]Evaluating:  96%|█████████▋| 302/313 [00:51<00:01,  5.70it/s]Evaluating:  97%|█████████▋| 303/313 [00:51<00:01,  5.76it/s]Evaluating:  97%|█████████▋| 304/313 [00:51<00:01,  5.80it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  5.84it/s]Evaluating:  98%|█████████▊| 306/313 [00:52<00:01,  5.74it/s]Evaluating:  98%|█████████▊| 307/313 [00:52<00:01,  5.67it/s]Evaluating:  98%|█████████▊| 308/313 [00:52<00:00,  5.63it/s]Evaluating:  99%|█████████▊| 309/313 [00:52<00:00,  5.62it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.64it/s]Evaluating:  99%|█████████▉| 311/313 [00:52<00:00,  5.69it/s]Evaluating: 100%|█████████▉| 312/313 [00:53<00:00,  5.70it/s]Evaluating: 100%|██████████| 313/313 [00:53<00:00,  6.20it/s]Evaluating: 100%|██████████| 313/313 [00:53<00:00,  5.88it/s]
10/09/2021 00:50:40 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/09/2021 00:50:40 - INFO - __main__ -     f1 = 0.699784589892295
10/09/2021 00:50:40 - INFO - __main__ -     loss = 1.2068021286029023
10/09/2021 00:50:40 - INFO - __main__ -     precision = 0.6552066526002234
10/09/2021 00:50:40 - INFO - __main__ -     recall = 0.750871204039542
10/09/2021 00:50:40 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:50:52 - INFO - __main__ -   Using lang2id = None
10/09/2021 00:50:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/09/2021 00:51:09 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/10/2021 14:05:20 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_emea_s1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=1, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=0, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_emea_s1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=1, predict_save_prefix='')
10/10/2021 14:05:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 14:05:20 - INFO - __main__ -   Seed = 0
10/10/2021 14:05:20 - INFO - root -   save model
10/10/2021 14:05:20 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_emea_s1/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=1, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=0, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my_bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_emea_s1//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=1, predict_save_prefix='')
10/10/2021 14:05:20 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 14:05:38 - INFO - __main__ -   Using lang2id = None
10/10/2021 14:05:38 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 14:05:38 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/10/2021 14:05:38 - INFO - root -   Trying to decide if add adapter
10/10/2021 14:05:38 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 14:05:38 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/10/2021 14:05:38 - INFO - __main__ -   Language = en
10/10/2021 14:05:38 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/10/2021 14:05:41 - INFO - __main__ -   Language = hi
10/10/2021 14:05:41 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 861, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_tag.py", line 563, in setup_adapter
    load_as=language,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 1180, in load_adapter
    **kwargs,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 391, in load
    **kwargs,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_utils.py", line 437, in resolve_adapter_path
    adapter_name_or_path, adapter_type, model_name, adapter_config=adapter_config, version=version, **kwargs
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_utils.py", line 381, in pull_from_hub
    download_path = download_cached(file_entry["url"], checksum=checksum, checksum_algo=checksum_algo, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_utils.py", line 137, in download_cached
    output_path = get_from_cache(url, cache_dir=cache_dir, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connection.py", line 366, in connect
    self._tunnel()
  File "/usr/lib/python3.7/http/client.py", line 922, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.7/http/client.py", line 271, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
KeyboardInterrupt
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 40, in <module>
    from utils_tag import convert_examples_to_features
  File "/home/abhijeet/rohan/emea/third_party/utils_tag.py", line 24, in <module>
    from transformers import XLMTokenizer
  File "/home/abhijeet/rohan/emea/src/transformers/__init__.py", line 22, in <module>
    from .integrations import (  # isort:skip
  File "/home/abhijeet/rohan/emea/src/transformers/integrations.py", line 81, in <module>
    from .file_utils import is_torch_tpu_available  # noqa: E402
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 28, in <module>
    import requests
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/__init__.py", line 43, in <module>
    import urllib3
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/__init__.py", line 11, in <module>
    from . import exceptions
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/exceptions.py", line 168, in <module>
    class URLSchemeUnknown(LocationValueError):
KeyboardInterrupt
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/__init__.py", line 150, in <module>
    from . import core
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/__init__.py", line 72, in <module>
    from . import numeric
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/numeric.py", line 24, in <module>
    from . import shape_base
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/shape_base.py", line 12, in <module>
    from . import fromnumeric as _from_nx
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/fromnumeric.py", line 1719, in <module>
    def ravel(a, order='C'):
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/core/overrides.py", line 198, in decorator
    source, filename='<__array_function__ internals>', mode='exec')
KeyboardInterrupt
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 30, in <module>
    import numpy as np
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/__init__.py", line 112, in <module>
    from ._globals import (
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/numpy/_globals.py", line 57, in <module>
    class _NoValueType:
KeyboardInterrupt
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/__init__.py", line 197, in <module>
    from torch._C import *  # noqa: F403
RuntimeError: KeyboardInterrupt: 
PyTorch version 1.9.0+cu102 available.
10/10/2021 15:13:11 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:13:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:13:11 - INFO - __main__ -   Seed = 12
10/10/2021 15:13:11 - INFO - root -   save model
10/10/2021 15:13:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:13:11 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:13:27 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:13:27 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:13:27 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/10/2021 15:13:27 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:13:27 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:13:27 - INFO - root -   loading lang adpater hi/wiki@ukp
10/10/2021 15:13:27 - INFO - __main__ -   Language = hi
10/10/2021 15:13:27 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/10/2021 15:13:34 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/10/2021 15:13:34 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:13:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/10/2021 15:13:34 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/10/2021 15:13:34 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:13:34 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:08,  3.59it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:08,  3.52it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:07,  3.99it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:07,  3.61it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:07,  3.64it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:07,  3.67it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:06,  3.77it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:06,  3.74it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:06,  3.67it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.64it/s]Evaluating:  34%|███▍      | 11/32 [00:02<00:05,  3.69it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.61it/s]Evaluating:  41%|████      | 13/32 [00:03<00:05,  3.50it/s]Evaluating:  44%|████▍     | 14/32 [00:03<00:05,  3.44it/s]Evaluating:  47%|████▋     | 15/32 [00:04<00:04,  3.51it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:04,  3.44it/s]Evaluating:  53%|█████▎    | 17/32 [00:04<00:04,  3.39it/s]Evaluating:  56%|█████▋    | 18/32 [00:05<00:04,  3.37it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  3.47it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.42it/s]Evaluating:  66%|██████▌   | 21/32 [00:06<00:03,  3.09it/s]Evaluating:  69%|██████▉   | 22/32 [00:06<00:03,  3.25it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  3.27it/s]Evaluating:  75%|███████▌  | 24/32 [00:06<00:02,  3.30it/s]Evaluating:  78%|███████▊  | 25/32 [00:07<00:02,  3.31it/s]Evaluating:  81%|████████▏ | 26/32 [00:07<00:01,  3.39it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.37it/s]Evaluating:  88%|████████▊ | 28/32 [00:08<00:01,  3.35it/s]Evaluating:  91%|█████████ | 29/32 [00:08<00:00,  3.34it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.44it/s]Evaluating:  97%|█████████▋| 31/32 [00:08<00:00,  3.39it/s]Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.81it/s]Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.50it/s]
10/10/2021 15:13:43 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/10/2021 15:13:43 - INFO - __main__ -     f1 = 0.3344231667445119
10/10/2021 15:13:43 - INFO - __main__ -     loss = 1.3888210225850344
10/10/2021 15:13:43 - INFO - __main__ -     precision = 0.3403041825095057
10/10/2021 15:13:43 - INFO - __main__ -     recall = 0.3287419651056015
10/10/2021 15:13:43 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/10/2021 15:13:43 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:13:43 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/10/2021 15:13:43 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/10/2021 15:13:43 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:13:43 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:09,  3.42it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:08,  3.36it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:08,  3.40it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:08,  3.47it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:08,  3.02it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:08,  3.12it/s]Evaluating:  22%|██▏       | 7/32 [00:02<00:07,  3.27it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:07,  3.27it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:06,  3.29it/s]Evaluating:  31%|███▏      | 10/32 [00:03<00:06,  3.35it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.39it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.36it/s]Evaluating:  41%|████      | 13/32 [00:03<00:05,  3.35it/s]Evaluating:  44%|████▍     | 14/32 [00:04<00:05,  3.38it/s]Evaluating:  47%|████▋     | 15/32 [00:04<00:04,  3.42it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:04,  3.38it/s]Evaluating:  53%|█████▎    | 17/32 [00:05<00:04,  3.36it/s]Evaluating:  56%|█████▋    | 18/32 [00:05<00:04,  3.39it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  3.39it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.37it/s]Evaluating:  66%|██████▌   | 21/32 [00:06<00:03,  3.34it/s]Evaluating:  69%|██████▉   | 22/32 [00:06<00:03,  3.08it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  3.21it/s]Evaluating:  75%|███████▌  | 24/32 [00:07<00:02,  3.28it/s]Evaluating:  78%|███████▊  | 25/32 [00:07<00:02,  3.40it/s]Evaluating:  81%|████████▏ | 26/32 [00:07<00:01,  3.40it/s]Evaluating:  84%|████████▍ | 27/32 [00:08<00:01,  3.41it/s]Evaluating:  88%|████████▊ | 28/32 [00:08<00:01,  3.42it/s]Evaluating:  91%|█████████ | 29/32 [00:08<00:00,  3.51it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.47it/s]Evaluating:  97%|█████████▋| 31/32 [00:09<00:00,  3.42it/s]Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.91it/s]Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.39it/s]
10/10/2021 15:13:53 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/10/2021 15:13:53 - INFO - __main__ -     f1 = 0.5278676734987414
10/10/2021 15:13:53 - INFO - __main__ -     loss = 0.5681666410528123
10/10/2021 15:13:53 - INFO - __main__ -     precision = 0.4838497033618985
10/10/2021 15:13:53 - INFO - __main__ -     recall = 0.5806962025316456
10/10/2021 15:13:53 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/10/2021 15:13:53 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:13:53 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/10/2021 15:13:53 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/10/2021 15:13:53 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:13:53 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:09,  3.44it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:08,  3.36it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:08,  3.51it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:08,  3.43it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:07,  3.39it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:07,  3.32it/s]Evaluating:  22%|██▏       | 7/32 [00:02<00:07,  3.38it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:07,  3.36it/s]Evaluating:  28%|██▊       | 9/32 [00:02<00:06,  3.34it/s]Evaluating:  31%|███▏      | 10/32 [00:02<00:06,  3.38it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.42it/s]Evaluating:  38%|███▊      | 12/32 [00:03<00:05,  3.38it/s]Evaluating:  41%|████      | 13/32 [00:03<00:05,  3.36it/s]Evaluating:  44%|████▍     | 14/32 [00:04<00:05,  3.40it/s]Evaluating:  47%|████▋     | 15/32 [00:04<00:04,  3.45it/s]Evaluating:  50%|█████     | 16/32 [00:04<00:04,  3.40it/s]Evaluating:  53%|█████▎    | 17/32 [00:05<00:04,  3.37it/s]Evaluating:  56%|█████▋    | 18/32 [00:05<00:04,  3.43it/s]Evaluating:  59%|█████▉    | 19/32 [00:05<00:03,  3.42it/s]Evaluating:  62%|██████▎   | 20/32 [00:05<00:03,  3.39it/s]Evaluating:  66%|██████▌   | 21/32 [00:06<00:03,  3.37it/s]Evaluating:  69%|██████▉   | 22/32 [00:06<00:02,  3.44it/s]Evaluating:  72%|███████▏  | 23/32 [00:06<00:02,  3.39it/s]Evaluating:  75%|███████▌  | 24/32 [00:07<00:02,  3.36it/s]Evaluating:  78%|███████▊  | 25/32 [00:07<00:02,  3.38it/s]Evaluating:  81%|████████▏ | 26/32 [00:07<00:01,  3.41it/s]Evaluating:  84%|████████▍ | 27/32 [00:07<00:01,  3.38it/s]Evaluating:  88%|████████▊ | 28/32 [00:08<00:01,  3.37it/s]Evaluating:  91%|█████████ | 29/32 [00:08<00:00,  3.41it/s]Evaluating:  94%|█████████▍| 30/32 [00:08<00:00,  3.44it/s]Evaluating:  97%|█████████▋| 31/32 [00:09<00:00,  3.40it/s]Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.85it/s]Evaluating: 100%|██████████| 32/32 [00:09<00:00,  3.44it/s]
10/10/2021 15:14:02 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/10/2021 15:14:02 - INFO - __main__ -     f1 = 0.10427066001109261
10/10/2021 15:14:02 - INFO - __main__ -     loss = 1.4147008582949638
10/10/2021 15:14:02 - INFO - __main__ -     precision = 0.16725978647686832
10/10/2021 15:14:02 - INFO - __main__ -     recall = 0.0757453666398066
10/10/2021 15:14:02 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
PyTorch version 1.9.0+cu102 available.
10/10/2021 15:14:12 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:14:12 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:14:12 - INFO - __main__ -   Seed = 12
10/10/2021 15:14:12 - INFO - root -   save model
10/10/2021 15:14:12 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:14:12 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:14:30 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:14:30 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:14:35 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:14:35 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:14:35 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/10/2021 15:14:35 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:14:35 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:14:35 - INFO - root -   loading lang adpater is/wiki@ukp
10/10/2021 15:14:35 - INFO - __main__ -   Language = is
10/10/2021 15:14:35 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

10/10/2021 15:14:41 - INFO - __main__ -   Language adapter for fo not found, using is instead
10/10/2021 15:14:41 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:14:41 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/10/2021 15:14:41 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/10/2021 15:14:41 - INFO - __main__ -     Num examples = 100
10/10/2021 15:14:41 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  3.80it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  3.50it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  3.42it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.41it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.03it/s]
10/10/2021 15:14:42 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/10/2021 15:14:42 - INFO - __main__ -     f1 = 0.6420664206642066
10/10/2021 15:14:42 - INFO - __main__ -     loss = 0.35638488829135895
10/10/2021 15:14:42 - INFO - __main__ -     precision = 0.5761589403973509
10/10/2021 15:14:42 - INFO - __main__ -     recall = 0.725
10/10/2021 15:14:42 - INFO - __main__ -   Language adapter for no not found, using is instead
10/10/2021 15:14:42 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:14:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/10/2021 15:14:44 - INFO - __main__ -   ***** Running evaluation  in no *****
10/10/2021 15:14:44 - INFO - __main__ -     Num examples = 10000
10/10/2021 15:14:44 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:   0%|          | 1/313 [00:00<01:32,  3.38it/s]Evaluating:   1%|          | 2/313 [00:00<01:30,  3.46it/s]Evaluating:   1%|          | 3/313 [00:00<01:31,  3.39it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:30,  3.43it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:31,  3.38it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:29,  3.43it/s]Evaluating:   2%|▏         | 7/313 [00:02<01:29,  3.43it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:29,  3.40it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:30,  3.37it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:28,  3.42it/s]Evaluating:   4%|▎         | 11/313 [00:03<01:27,  3.44it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:28,  3.40it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:29,  3.36it/s]Evaluating:   4%|▍         | 14/313 [00:04<01:27,  3.42it/s]Evaluating:   5%|▍         | 15/313 [00:04<01:26,  3.44it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:27,  3.39it/s]Evaluating:   5%|▌         | 17/313 [00:05<01:28,  3.36it/s]Evaluating:   6%|▌         | 18/313 [00:05<01:26,  3.40it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:26,  3.41it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:26,  3.38it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:   7%|▋         | 21/313 [00:06<01:25,  3.40it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:25,  3.39it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   7%|▋         | 23/313 [00:06<01:25,  3.39it/s]Evaluating:   8%|▊         | 24/313 [00:07<01:26,  3.35it/s]Evaluating:   8%|▊         | 25/313 [00:07<01:24,  3.39it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:23,  3.45it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:22,  3.46it/s]Evaluating:   9%|▉         | 28/313 [00:08<01:23,  3.43it/s]Evaluating:   9%|▉         | 29/313 [00:08<01:22,  3.44it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:22,  3.41it/s]Evaluating:  10%|▉         | 31/313 [00:09<01:22,  3.41it/s]Evaluating:  10%|█         | 32/313 [00:09<01:23,  3.38it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  11%|█         | 33/313 [00:09<01:22,  3.41it/s]10/10/2021 15:14:54 - INFO - __main__ -   Using lang2id = None
Evaluating:  11%|█         | 34/313 [00:09<01:21,  3.44it/s]Evaluating:  11%|█         | 35/313 [00:10<01:21,  3.40it/s]Evaluating:  12%|█▏        | 36/313 [00:10<01:22,  3.37it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:21,  3.37it/s]Evaluating:  12%|█▏        | 38/313 [00:11<01:20,  3.40it/s]Evaluating:  12%|█▏        | 39/313 [00:11<01:21,  3.38it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  13%|█▎        | 40/313 [00:11<01:20,  3.41it/s]10/10/2021 15:14:56 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:14:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:14:56 - INFO - __main__ -   Seed = 22
10/10/2021 15:14:56 - INFO - root -   save model
10/10/2021 15:14:56 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:14:56 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  13%|█▎        | 41/313 [00:12<01:21,  3.36it/s]Evaluating:  13%|█▎        | 42/313 [00:12<01:20,  3.38it/s]Evaluating:  14%|█▎        | 43/313 [00:12<01:20,  3.35it/s]Evaluating:  14%|█▍        | 44/313 [00:13<01:29,  3.00it/s]Evaluating:  14%|█▍        | 45/313 [00:13<01:23,  3.19it/s]Evaluating:  15%|█▍        | 46/313 [00:13<01:21,  3.28it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  15%|█▌        | 47/313 [00:13<01:18,  3.38it/s]Evaluating:  15%|█▌        | 48/313 [00:14<01:17,  3.43it/s]Evaluating:  16%|█▌        | 49/313 [00:14<01:16,  3.45it/s]Evaluating:  16%|█▌        | 50/313 [00:14<01:15,  3.46it/s]Evaluating:  16%|█▋        | 51/313 [00:15<01:14,  3.50it/s]Evaluating:  17%|█▋        | 52/313 [00:15<01:13,  3.56it/s]Evaluating:  17%|█▋        | 53/313 [00:15<01:14,  3.49it/s]Evaluating:  17%|█▋        | 54/313 [00:15<01:15,  3.41it/s]Evaluating:  18%|█▊        | 55/313 [00:16<01:15,  3.43it/s]Evaluating:  18%|█▊        | 56/313 [00:16<01:14,  3.43it/s]Evaluating:  18%|█▊        | 57/313 [00:16<01:15,  3.38it/s]Evaluating:  19%|█▊        | 58/313 [00:17<01:16,  3.35it/s]Evaluating:  19%|█▉        | 59/313 [00:17<01:14,  3.39it/s]Evaluating:  19%|█▉        | 60/313 [00:17<01:14,  3.41it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  19%|█▉        | 61/313 [00:17<01:15,  3.33it/s]Evaluating:  20%|█▉        | 62/313 [00:18<01:16,  3.30it/s]Evaluating:  20%|██        | 63/313 [00:18<01:13,  3.40it/s]Evaluating:  20%|██        | 64/313 [00:18<01:14,  3.34it/s]Evaluating:  21%|██        | 65/313 [00:19<01:14,  3.32it/s]Evaluating:  21%|██        | 66/313 [00:19<01:14,  3.32it/s]Evaluating:  21%|██▏       | 67/313 [00:19<01:11,  3.44it/s]Evaluating:  22%|██▏       | 68/313 [00:20<01:12,  3.40it/s]Evaluating:  22%|██▏       | 69/313 [00:20<01:12,  3.36it/s]Evaluating:  22%|██▏       | 70/313 [00:20<01:12,  3.37it/s]Evaluating:  23%|██▎       | 71/313 [00:20<01:11,  3.39it/s]Evaluating:  23%|██▎       | 72/313 [00:21<01:12,  3.34it/s]Evaluating:  23%|██▎       | 73/313 [00:21<01:12,  3.33it/s]Evaluating:  24%|██▎       | 74/313 [00:21<01:10,  3.39it/s]Evaluating:  24%|██▍       | 75/313 [00:22<01:10,  3.37it/s]Evaluating:  24%|██▍       | 76/313 [00:22<01:11,  3.34it/s]Evaluating:  25%|██▍       | 77/313 [00:22<01:10,  3.33it/s]Evaluating:  25%|██▍       | 78/313 [00:23<01:08,  3.42it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  25%|██▌       | 79/313 [00:23<01:09,  3.39it/s]Evaluating:  26%|██▌       | 80/313 [00:23<01:09,  3.33it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  26%|██▌       | 81/313 [00:23<01:10,  3.31it/s]Evaluating:  26%|██▌       | 82/313 [00:24<01:07,  3.41it/s]Evaluating:  27%|██▋       | 83/313 [00:24<01:08,  3.37it/s]Evaluating:  27%|██▋       | 84/313 [00:24<01:08,  3.35it/s]Evaluating:  27%|██▋       | 85/313 [00:25<01:07,  3.37it/s]Evaluating:  27%|██▋       | 86/313 [00:25<01:07,  3.34it/s]Evaluating:  28%|██▊       | 87/313 [00:25<01:07,  3.32it/s]Evaluating:  28%|██▊       | 88/313 [00:26<01:07,  3.32it/s]Evaluating:  28%|██▊       | 89/313 [00:26<01:05,  3.42it/s]Evaluating:  29%|██▉       | 90/313 [00:26<01:06,  3.38it/s]Evaluating:  29%|██▉       | 91/313 [00:26<01:06,  3.35it/s]Evaluating:  29%|██▉       | 92/313 [00:27<01:06,  3.32it/s]Evaluating:  30%|██▉       | 93/313 [00:27<01:04,  3.43it/s]Evaluating:  30%|███       | 94/313 [00:27<01:05,  3.36it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  30%|███       | 95/313 [00:28<01:19,  2.74it/s]10/10/2021 15:15:12 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:15:12 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:15:12 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/10/2021 15:15:12 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:15:12 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:15:12 - INFO - root -   loading lang adpater hi/wiki@ukp
10/10/2021 15:15:12 - INFO - __main__ -   Language = hi
10/10/2021 15:15:12 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Evaluating:  31%|███       | 96/313 [00:28<01:15,  2.89it/s]Evaluating:  31%|███       | 97/313 [00:28<01:12,  2.99it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Evaluating:  31%|███▏      | 98/313 [00:29<01:09,  3.11it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
Evaluating:  32%|███▏      | 99/313 [00:29<01:05,  3.24it/s]Evaluating:  32%|███▏      | 100/313 [00:29<01:05,  3.26it/s]Evaluating:  32%|███▏      | 101/313 [00:30<01:04,  3.27it/s]Evaluating:  33%|███▎      | 102/313 [00:30<01:03,  3.32it/s]Evaluating:  33%|███▎      | 103/313 [00:30<01:02,  3.35it/s]Evaluating:  33%|███▎      | 104/313 [00:31<01:03,  3.31it/s]Evaluating:  34%|███▎      | 105/313 [00:31<01:03,  3.28it/s]Evaluating:  34%|███▍      | 106/313 [00:31<01:02,  3.31it/s]Evaluating:  34%|███▍      | 107/313 [00:31<01:01,  3.36it/s]Evaluating:  35%|███▍      | 108/313 [00:32<01:01,  3.34it/s]Evaluating:  35%|███▍      | 109/313 [00:32<01:01,  3.30it/s]Evaluating:  35%|███▌      | 110/313 [00:32<01:00,  3.36it/s]Evaluating:  35%|███▌      | 111/313 [00:33<01:00,  3.34it/s]Evaluating:  36%|███▌      | 112/313 [00:33<01:00,  3.31it/s]Evaluating:  36%|███▌      | 113/313 [00:33<01:00,  3.33it/s]Evaluating:  36%|███▋      | 114/313 [00:34<00:59,  3.36it/s]Evaluating:  37%|███▋      | 115/313 [00:34<01:04,  3.06it/s]Evaluating:  37%|███▋      | 116/313 [00:34<01:03,  3.12it/s]Evaluating:  37%|███▋      | 117/313 [00:34<01:00,  3.21it/s]Evaluating:  38%|███▊      | 118/313 [00:35<01:00,  3.22it/s]Evaluating:  38%|███▊      | 119/313 [00:35<01:00,  3.23it/s]Evaluating:  38%|███▊      | 120/313 [00:35<00:58,  3.28it/s]Evaluating:  39%|███▊      | 121/313 [00:36<00:57,  3.32it/s]Evaluating:  39%|███▉      | 122/313 [00:36<00:57,  3.30it/s]Evaluating:  39%|███▉      | 123/313 [00:36<00:57,  3.28it/s]Evaluating:  40%|███▉      | 124/313 [00:37<00:57,  3.29it/s]Evaluating:  40%|███▉      | 125/313 [00:37<00:56,  3.33it/s]10/10/2021 15:15:21 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/10/2021 15:15:21 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:15:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/10/2021 15:15:21 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/10/2021 15:15:21 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:15:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  40%|████      | 126/313 [00:37<00:57,  3.24it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.07it/s]Evaluating:  41%|████      | 127/313 [00:38<01:06,  2.80it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:13,  2.15it/s]Evaluating:  41%|████      | 128/313 [00:38<01:11,  2.59it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:13,  2.17it/s]Evaluating:  41%|████      | 129/313 [00:39<01:13,  2.50it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:13,  2.05it/s]Evaluating:  42%|████▏     | 130/313 [00:39<01:17,  2.37it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:12,  2.10it/s]Evaluating:  42%|████▏     | 131/313 [00:40<01:18,  2.31it/s]Evaluating:  42%|████▏     | 132/313 [00:40<01:19,  2.28it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:12,  2.13it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:11,  2.13it/s]Evaluating:  42%|████▏     | 133/313 [00:40<01:21,  2.22it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:11,  2.14it/s]Evaluating:  43%|████▎     | 134/313 [00:41<01:21,  2.19it/s]Evaluating:  28%|██▊       | 9/32 [00:04<00:10,  2.16it/s]Evaluating:  43%|████▎     | 135/313 [00:41<01:21,  2.19it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:10,  2.17it/s]Evaluating:  43%|████▎     | 136/313 [00:42<01:20,  2.20it/s]Evaluating:  34%|███▍      | 11/32 [00:05<00:09,  2.16it/s]Evaluating:  44%|████▍     | 137/313 [00:42<01:20,  2.18it/s]Evaluating:  38%|███▊      | 12/32 [00:05<00:09,  2.15it/s]Evaluating:  44%|████▍     | 138/313 [00:43<01:21,  2.16it/s]Evaluating:  41%|████      | 13/32 [00:06<00:08,  2.17it/s]Evaluating:  44%|████▍     | 139/313 [00:43<01:20,  2.15it/s]Evaluating:  44%|████▍     | 14/32 [00:06<00:08,  2.18it/s]Evaluating:  45%|████▍     | 140/313 [00:44<01:19,  2.17it/s]Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.16it/s]Evaluating:  45%|████▌     | 141/313 [00:44<01:19,  2.15it/s]Evaluating:  50%|█████     | 16/32 [00:07<00:07,  2.17it/s]Evaluating:  45%|████▌     | 142/313 [00:45<01:18,  2.17it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.17it/s]Evaluating:  46%|████▌     | 143/313 [00:45<01:18,  2.18it/s]Evaluating:  56%|█████▋    | 18/32 [00:08<00:06,  2.16it/s]Evaluating:  46%|████▌     | 144/313 [00:46<01:18,  2.17it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:06,  2.15it/s]Evaluating:  46%|████▋     | 145/313 [00:46<01:17,  2.15it/s]Evaluating:  62%|██████▎   | 20/32 [00:09<00:05,  2.17it/s]Evaluating:  47%|████▋     | 146/313 [00:46<01:17,  2.17it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:05,  2.17it/s]Evaluating:  47%|████▋     | 147/313 [00:47<01:16,  2.18it/s]Evaluating:  69%|██████▉   | 22/32 [00:10<00:04,  2.17it/s]Evaluating:  47%|████▋     | 148/313 [00:47<01:16,  2.16it/s]Evaluating:  72%|███████▏  | 23/32 [00:10<00:04,  2.16it/s]Evaluating:  48%|████▊     | 149/313 [00:48<01:16,  2.16it/s]Evaluating:  75%|███████▌  | 24/32 [00:11<00:03,  2.17it/s]Evaluating:  48%|████▊     | 150/313 [00:48<01:15,  2.15it/s]Evaluating:  78%|███████▊  | 25/32 [00:11<00:03,  2.18it/s]Evaluating:  48%|████▊     | 151/313 [00:49<01:14,  2.18it/s]Evaluating:  81%|████████▏ | 26/32 [00:12<00:03,  1.94it/s]Evaluating:  49%|████▊     | 152/313 [00:49<01:21,  1.98it/s]Evaluating:  84%|████████▍ | 27/32 [00:12<00:02,  1.99it/s]Evaluating:  49%|████▉     | 153/313 [00:50<01:19,  2.02it/s]Evaluating:  88%|████████▊ | 28/32 [00:13<00:01,  2.05it/s]Evaluating:  49%|████▉     | 154/313 [00:50<01:17,  2.06it/s]Evaluating:  91%|█████████ | 29/32 [00:13<00:01,  2.08it/s]Evaluating:  50%|████▉     | 155/313 [00:51<01:15,  2.08it/s]Evaluating:  94%|█████████▍| 30/32 [00:14<00:00,  2.10it/s]Evaluating:  50%|████▉     | 156/313 [00:51<01:14,  2.11it/s]Evaluating:  97%|█████████▋| 31/32 [00:14<00:00,  2.12it/s]Evaluating:  50%|█████     | 157/313 [00:52<01:13,  2.11it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.34it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.15it/s]Evaluating:  50%|█████     | 158/313 [00:52<01:07,  2.29it/s]
10/10/2021 15:15:36 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/10/2021 15:15:36 - INFO - __main__ -     f1 = 0.3197702249880326
10/10/2021 15:15:36 - INFO - __main__ -     loss = 5.357382453978062
10/10/2021 15:15:36 - INFO - __main__ -     precision = 0.334
10/10/2021 15:15:36 - INFO - __main__ -     recall = 0.30670339761248855
10/10/2021 15:15:36 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/10/2021 15:15:36 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:15:36 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/10/2021 15:15:37 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/10/2021 15:15:37 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:15:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  51%|█████     | 159/313 [00:52<01:01,  2.49it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.12it/s]Evaluating:  51%|█████     | 160/313 [00:53<01:04,  2.36it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:13,  2.17it/s]Evaluating:  51%|█████▏    | 161/313 [00:53<01:05,  2.32it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:13,  2.16it/s]Evaluating:  52%|█████▏    | 162/313 [00:54<01:06,  2.26it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:11,  2.41it/s]Evaluating:  52%|█████▏    | 163/313 [00:54<01:13,  2.04it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:11,  2.31it/s]Evaluating:  52%|█████▏    | 164/313 [00:55<01:11,  2.09it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:11,  2.27it/s]Evaluating:  53%|█████▎    | 165/313 [00:55<01:10,  2.10it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:11,  2.23it/s]Evaluating:  53%|█████▎    | 166/313 [00:56<01:09,  2.12it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:10,  2.22it/s]Evaluating:  53%|█████▎    | 167/313 [00:56<01:09,  2.12it/s]Evaluating:  28%|██▊       | 9/32 [00:04<00:10,  2.19it/s]Evaluating:  54%|█████▎    | 168/313 [00:57<01:07,  2.14it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:09,  2.21it/s]Evaluating:  54%|█████▍    | 169/313 [00:57<01:07,  2.13it/s]Evaluating:  34%|███▍      | 11/32 [00:04<00:09,  2.18it/s]Evaluating:  54%|█████▍    | 170/313 [00:58<01:06,  2.14it/s]Evaluating:  38%|███▊      | 12/32 [00:05<00:09,  2.20it/s]Evaluating:  55%|█████▍    | 171/313 [00:58<01:06,  2.13it/s]Evaluating:  41%|████      | 13/32 [00:05<00:08,  2.19it/s]Evaluating:  55%|█████▍    | 172/313 [00:59<01:05,  2.15it/s]Evaluating:  44%|████▍     | 14/32 [00:06<00:08,  2.18it/s]Evaluating:  55%|█████▌    | 173/313 [00:59<01:05,  2.14it/s]Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.19it/s]Evaluating:  56%|█████▌    | 174/313 [01:00<01:04,  2.14it/s]Evaluating:  50%|█████     | 16/32 [00:07<00:07,  2.18it/s]Evaluating:  56%|█████▌    | 175/313 [01:00<01:03,  2.16it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.19it/s]Evaluating:  56%|█████▌    | 176/313 [01:00<01:03,  2.14it/s]Evaluating:  56%|█████▋    | 18/32 [00:08<00:06,  2.17it/s]Evaluating:  57%|█████▋    | 177/313 [01:01<01:03,  2.16it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:05,  2.18it/s]Evaluating:  57%|█████▋    | 178/313 [01:01<01:02,  2.15it/s]Evaluating:  62%|██████▎   | 20/32 [00:09<00:05,  2.16it/s]Evaluating:  57%|█████▋    | 179/313 [01:02<01:02,  2.15it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:05,  2.18it/s]Evaluating:  58%|█████▊    | 180/313 [01:02<01:02,  2.13it/s]Evaluating:  69%|██████▉   | 22/32 [00:10<00:04,  2.16it/s]Evaluating:  58%|█████▊    | 181/313 [01:03<01:01,  2.15it/s]Evaluating:  72%|███████▏  | 23/32 [00:10<00:04,  2.17it/s]Evaluating:  58%|█████▊    | 182/313 [01:03<01:01,  2.13it/s]Evaluating:  75%|███████▌  | 24/32 [00:10<00:03,  2.16it/s]Evaluating:  78%|███████▊  | 25/32 [00:11<00:03,  2.17it/s]Evaluating:  58%|█████▊    | 183/313 [01:04<01:01,  2.12it/s]Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.19it/s]Evaluating:  59%|█████▉    | 184/313 [01:04<01:00,  2.14it/s]Evaluating:  84%|████████▍ | 27/32 [00:12<00:02,  2.17it/s]Evaluating:  59%|█████▉    | 185/313 [01:05<01:01,  2.10it/s]Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.20it/s]Evaluating:  59%|█████▉    | 186/313 [01:05<00:59,  2.12it/s]Evaluating:  91%|█████████ | 29/32 [00:13<00:01,  2.18it/s]Evaluating:  60%|█████▉    | 187/313 [01:06<00:59,  2.10it/s]Evaluating:  94%|█████████▍| 30/32 [00:13<00:00,  2.19it/s]Evaluating:  60%|██████    | 188/313 [01:06<00:58,  2.13it/s]Evaluating:  97%|█████████▋| 31/32 [00:14<00:00,  2.17it/s]Evaluating:  60%|██████    | 189/313 [01:07<00:57,  2.15it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.37it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.21it/s]Evaluating:  61%|██████    | 190/313 [01:07<00:52,  2.35it/s]
10/10/2021 15:15:51 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/10/2021 15:15:51 - INFO - __main__ -     f1 = 0.4896354538956397
10/10/2021 15:15:51 - INFO - __main__ -     loss = 1.9978054463863373
10/10/2021 15:15:51 - INFO - __main__ -     precision = 0.44654498044328556
10/10/2021 15:15:51 - INFO - __main__ -     recall = 0.5419303797468354
10/10/2021 15:15:51 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/10/2021 15:15:51 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:15:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/10/2021 15:15:51 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/10/2021 15:15:51 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:15:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  61%|██████    | 191/313 [01:07<00:47,  2.55it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:13,  2.23it/s]Evaluating:  61%|██████▏   | 192/313 [01:08<00:50,  2.40it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:13,  2.22it/s]Evaluating:  62%|██████▏   | 193/313 [01:08<00:51,  2.32it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:13,  2.18it/s]Evaluating:  62%|██████▏   | 194/313 [01:09<00:52,  2.25it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:12,  2.20it/s]Evaluating:  62%|██████▏   | 195/313 [01:09<00:53,  2.22it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:11,  2.44it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:11,  2.19it/s]Evaluating:  63%|██████▎   | 196/313 [01:10<01:05,  1.79it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:11,  2.19it/s]Evaluating:  63%|██████▎   | 197/313 [01:10<01:02,  1.86it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:10,  2.20it/s]Evaluating:  63%|██████▎   | 198/313 [01:11<00:59,  1.93it/s]PyTorch version 1.9.0+cu102 available.
10/10/2021 15:15:56 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:15:56 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:15:56 - INFO - __main__ -   Seed = 12
10/10/2021 15:15:56 - INFO - root -   save model
10/10/2021 15:15:56 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:15:56 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  28%|██▊       | 9/32 [00:04<00:10,  2.20it/s]Evaluating:  64%|██████▎   | 199/313 [01:11<00:56,  2.01it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:10,  2.18it/s]Evaluating:  64%|██████▍   | 200/313 [01:12<00:55,  2.02it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  34%|███▍      | 11/32 [00:04<00:09,  2.19it/s]Evaluating:  64%|██████▍   | 201/313 [01:12<00:54,  2.06it/s]Evaluating:  38%|███▊      | 12/32 [00:05<00:09,  2.18it/s]Evaluating:  65%|██████▍   | 202/313 [01:13<00:53,  2.07it/s]Evaluating:  41%|████      | 13/32 [00:05<00:08,  2.19it/s]Evaluating:  65%|██████▍   | 203/313 [01:13<00:52,  2.09it/s]Evaluating:  44%|████▍     | 14/32 [00:06<00:08,  2.19it/s]Evaluating:  65%|██████▌   | 204/313 [01:14<00:51,  2.10it/s]Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.20it/s]Evaluating:  65%|██████▌   | 205/313 [01:14<00:51,  2.11it/s]Evaluating:  50%|█████     | 16/32 [00:07<00:07,  2.22it/s]Evaluating:  66%|██████▌   | 206/313 [01:15<00:51,  2.06it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.21it/s]Evaluating:  66%|██████▌   | 207/313 [01:15<00:51,  2.07it/s]Evaluating:  56%|█████▋    | 18/32 [00:08<00:06,  2.21it/s]Evaluating:  66%|██████▋   | 208/313 [01:16<00:50,  2.08it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:05,  2.20it/s]Evaluating:  67%|██████▋   | 209/313 [01:16<00:49,  2.09it/s]Evaluating:  62%|██████▎   | 20/32 [00:09<00:05,  2.20it/s]Evaluating:  67%|██████▋   | 210/313 [01:17<00:48,  2.11it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:05,  2.18it/s]Evaluating:  67%|██████▋   | 211/313 [01:17<00:47,  2.13it/s]Evaluating:  69%|██████▉   | 22/32 [00:09<00:04,  2.19it/s]Evaluating:  68%|██████▊   | 212/313 [01:17<00:47,  2.13it/s]Evaluating:  72%|███████▏  | 23/32 [00:10<00:04,  2.17it/s]Evaluating:  68%|██████▊   | 213/313 [01:18<00:46,  2.15it/s]Evaluating:  75%|███████▌  | 24/32 [00:10<00:03,  2.18it/s]Evaluating:  68%|██████▊   | 214/313 [01:18<00:46,  2.13it/s]Evaluating:  78%|███████▊  | 25/32 [00:11<00:03,  2.16it/s]Evaluating:  69%|██████▊   | 215/313 [01:19<00:45,  2.15it/s]Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.18it/s]Evaluating:  69%|██████▉   | 216/313 [01:19<00:45,  2.12it/s]Evaluating:  84%|████████▍ | 27/32 [00:12<00:02,  2.17it/s]Evaluating:  69%|██████▉   | 217/313 [01:20<00:45,  2.13it/s]Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.19it/s]Evaluating:  70%|██████▉   | 218/313 [01:20<00:44,  2.12it/s]Evaluating:  91%|█████████ | 29/32 [00:13<00:01,  2.20it/s]Evaluating:  70%|██████▉   | 219/313 [01:21<00:43,  2.14it/s]Evaluating:  94%|█████████▍| 30/32 [00:13<00:00,  2.17it/s]Evaluating:  70%|███████   | 220/313 [01:21<00:43,  2.13it/s]Evaluating:  97%|█████████▋| 31/32 [00:14<00:00,  2.18it/s]Evaluating:  71%|███████   | 221/313 [01:22<00:40,  2.28it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.36it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.21it/s]
10/10/2021 15:16:06 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/10/2021 15:16:06 - INFO - __main__ -     f1 = 0.11822660098522168
10/10/2021 15:16:06 - INFO - __main__ -     loss = 4.587707541882992
10/10/2021 15:16:06 - INFO - __main__ -     precision = 0.1520912547528517
10/10/2021 15:16:06 - INFO - __main__ -     recall = 0.09669621273166801
10/10/2021 15:16:06 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  71%|███████   | 222/313 [01:22<00:36,  2.51it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  71%|███████   | 223/313 [01:22<00:33,  2.70it/s]Evaluating:  72%|███████▏  | 224/313 [01:22<00:30,  2.89it/s]Evaluating:  72%|███████▏  | 225/313 [01:23<00:29,  3.00it/s]Evaluating:  72%|███████▏  | 226/313 [01:23<00:27,  3.11it/s]Evaluating:  73%|███████▎  | 227/313 [01:23<00:27,  3.15it/s]Evaluating:  73%|███████▎  | 228/313 [01:24<00:26,  3.23it/s]Evaluating:  73%|███████▎  | 229/313 [01:24<00:25,  3.24it/s]Evaluating:  73%|███████▎  | 230/313 [01:24<00:25,  3.29it/s]Evaluating:  74%|███████▍  | 231/313 [01:25<00:25,  3.27it/s]Evaluating:  74%|███████▍  | 232/313 [01:25<00:24,  3.32it/s]Evaluating:  74%|███████▍  | 233/313 [01:25<00:23,  3.36it/s]Evaluating:  75%|███████▍  | 234/313 [01:25<00:23,  3.33it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  75%|███████▌  | 235/313 [01:26<00:23,  3.33it/s]Evaluating:  75%|███████▌  | 236/313 [01:26<00:23,  3.30it/s]Evaluating:  76%|███████▌  | 237/313 [01:26<00:22,  3.32it/s]Evaluating:  76%|███████▌  | 238/313 [01:27<00:23,  3.26it/s]Evaluating:  76%|███████▋  | 239/313 [01:27<00:22,  3.31it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  77%|███████▋  | 240/313 [01:27<00:22,  3.30it/s]Evaluating:  77%|███████▋  | 241/313 [01:28<00:21,  3.30it/s]Evaluating:  77%|███████▋  | 242/313 [01:28<00:24,  2.85it/s]Evaluating:  78%|███████▊  | 243/313 [01:28<00:23,  2.93it/s]Evaluating:  78%|███████▊  | 244/313 [01:29<00:23,  2.97it/s]Evaluating:  78%|███████▊  | 245/313 [01:29<00:24,  2.75it/s]Evaluating:  79%|███████▊  | 246/313 [01:29<00:24,  2.78it/s]Evaluating:  79%|███████▉  | 247/313 [01:30<00:24,  2.70it/s]Evaluating:  79%|███████▉  | 248/313 [01:30<00:23,  2.81it/s]Evaluating:  80%|███████▉  | 249/313 [01:31<00:22,  2.85it/s]Evaluating:  80%|███████▉  | 250/313 [01:31<00:22,  2.80it/s]Evaluating:  80%|████████  | 251/313 [01:31<00:20,  2.96it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  81%|████████  | 252/313 [01:31<00:20,  3.03it/s]Evaluating:  81%|████████  | 253/313 [01:32<00:21,  2.78it/s]Evaluating:  81%|████████  | 254/313 [01:32<00:20,  2.86it/s]Evaluating:  81%|████████▏ | 255/313 [01:33<00:23,  2.45it/s]Evaluating:  82%|████████▏ | 256/313 [01:33<00:21,  2.60it/s]Evaluating:  82%|████████▏ | 257/313 [01:33<00:20,  2.73it/s]Evaluating:  82%|████████▏ | 258/313 [01:34<00:20,  2.66it/s]Evaluating:  83%|████████▎ | 259/313 [01:34<00:19,  2.82it/s]Evaluating:  83%|████████▎ | 260/313 [01:35<00:19,  2.65it/s]Evaluating:  83%|████████▎ | 261/313 [01:35<00:18,  2.83it/s]Evaluating:  84%|████████▎ | 262/313 [01:35<00:17,  3.00it/s]Evaluating:  84%|████████▍ | 263/313 [01:35<00:16,  3.12it/s]Evaluating:  84%|████████▍ | 264/313 [01:36<00:16,  2.93it/s]Evaluating:  85%|████████▍ | 265/313 [01:36<00:15,  3.06it/s]Evaluating:  85%|████████▍ | 266/313 [01:37<00:16,  2.83it/s]Evaluating:  85%|████████▌ | 267/313 [01:37<00:15,  3.02it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  86%|████████▌ | 268/313 [01:37<00:15,  2.86it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  86%|████████▌ | 269/313 [01:38<00:14,  3.02it/s]Evaluating:  86%|████████▋ | 270/313 [01:38<00:13,  3.10it/s]Evaluating:  87%|████████▋ | 271/313 [01:38<00:15,  2.72it/s]Evaluating:  87%|████████▋ | 272/313 [01:39<00:14,  2.83it/s]Evaluating:  87%|████████▋ | 273/313 [01:39<00:15,  2.56it/s]Evaluating:  88%|████████▊ | 274/313 [01:39<00:14,  2.63it/s]Evaluating:  88%|████████▊ | 275/313 [01:40<00:15,  2.41it/s]Evaluating:  88%|████████▊ | 276/313 [01:40<00:14,  2.56it/s]Evaluating:  88%|████████▊ | 277/313 [01:41<00:13,  2.73it/s]Evaluating:  89%|████████▉ | 278/313 [01:41<00:13,  2.56it/s]Evaluating:  89%|████████▉ | 279/313 [01:41<00:12,  2.76it/s]Evaluating:  89%|████████▉ | 280/313 [01:42<00:12,  2.73it/s]Evaluating:  90%|████████▉ | 281/313 [01:42<00:11,  2.90it/s]Evaluating:  90%|█████████ | 282/313 [01:42<00:11,  2.69it/s]Evaluating:  90%|█████████ | 283/313 [01:43<00:10,  2.78it/s]Evaluating:  91%|█████████ | 284/313 [01:43<00:11,  2.62it/s]Evaluating:  91%|█████████ | 285/313 [01:44<00:10,  2.77it/s]Evaluating:  91%|█████████▏| 286/313 [01:44<00:09,  2.70it/s]Evaluating:  92%|█████████▏| 287/313 [01:44<00:08,  2.91it/s]Evaluating:  92%|█████████▏| 288/313 [01:45<00:09,  2.75it/s]Evaluating:  92%|█████████▏| 289/313 [01:45<00:08,  2.92it/s]Evaluating:  93%|█████████▎| 290/313 [01:45<00:07,  3.05it/s]Evaluating:  93%|█████████▎| 291/313 [01:46<00:07,  2.81it/s]Evaluating:  93%|█████████▎| 292/313 [01:46<00:07,  2.87it/s]Evaluating:  94%|█████████▎| 293/313 [01:46<00:06,  2.94it/s]Evaluating:  94%|█████████▍| 294/313 [01:47<00:06,  2.96it/s]Evaluating:  94%|█████████▍| 295/313 [01:47<00:06,  3.00it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  95%|█████████▍| 296/313 [01:47<00:06,  2.72it/s]10/10/2021 15:16:32 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:16:32 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:16:32 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/10/2021 15:16:32 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:16:32 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Evaluating:  95%|█████████▍| 297/313 [01:48<00:05,  2.82it/s]Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:16:32 - INFO - root -   loading lang adpater ru/wiki@ukp
10/10/2021 15:16:32 - INFO - __main__ -   Language = ru
10/10/2021 15:16:32 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Evaluating:  95%|█████████▌| 298/313 [01:48<00:05,  2.74it/s]Evaluating:  96%|█████████▌| 299/313 [01:48<00:04,  2.93it/s]Evaluating:  96%|█████████▌| 300/313 [01:49<00:04,  2.77it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
Evaluating:  96%|█████████▌| 301/313 [01:49<00:04,  2.84it/s]Evaluating:  96%|█████████▋| 302/313 [01:50<00:04,  2.53it/s]Evaluating:  97%|█████████▋| 303/313 [01:50<00:03,  2.69it/s]Evaluating:  97%|█████████▋| 304/313 [01:50<00:03,  2.45it/s]Evaluating:  97%|█████████▋| 305/313 [01:51<00:03,  2.61it/s]Evaluating:  98%|█████████▊| 306/313 [01:51<00:02,  2.47it/s]Evaluating:  98%|█████████▊| 307/313 [01:51<00:02,  2.62it/s]Evaluating:  98%|█████████▊| 308/313 [01:52<00:02,  2.50it/s]Evaluating:  99%|█████████▊| 309/313 [01:52<00:01,  2.64it/s]Evaluating:  99%|█████████▉| 310/313 [01:53<00:01,  2.49it/s]Evaluating:  99%|█████████▉| 311/313 [01:53<00:00,  2.66it/s]Evaluating: 100%|█████████▉| 312/313 [01:53<00:00,  2.52it/s]Evaluating: 100%|██████████| 313/313 [01:54<00:00,  2.65it/s]Evaluating: 100%|██████████| 313/313 [01:54<00:00,  2.74it/s]
10/10/2021 15:16:40 - INFO - __main__ -   ***** Evaluation result  in no *****
10/10/2021 15:16:40 - INFO - __main__ -     f1 = 0.7313532232285562
10/10/2021 15:16:40 - INFO - __main__ -     loss = 0.29067917434742657
10/10/2021 15:16:40 - INFO - __main__ -     precision = 0.7026231605886116
10/10/2021 15:16:40 - INFO - __main__ -     recall = 0.762532981530343
10/10/2021 15:16:40 - INFO - __main__ -   Language adapter for da not found, using is instead
10/10/2021 15:16:40 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:16:40 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/10/2021 15:16:40 - INFO - __main__ -   Language adapter for be not found, using ru instead
10/10/2021 15:16:40 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:16:40 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/10/2021 15:16:40 - INFO - __main__ -   ***** Running evaluation  in be *****
10/10/2021 15:16:40 - INFO - __main__ -     Num examples = 1001
10/10/2021 15:16:40 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:08,  3.48it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:08,  3.46it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:08,  3.44it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:08,  3.39it/s]10/10/2021 15:16:41 - INFO - __main__ -   ***** Running evaluation  in da *****
10/10/2021 15:16:41 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:16:41 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  16%|█▌        | 5/32 [00:01<00:09,  2.91it/s]Evaluating:   0%|          | 1/313 [00:00<02:20,  2.22it/s]10/10/2021 15:16:42 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:16:42 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  19%|█▉        | 6/32 [00:02<00:09,  2.64it/s]Evaluating:   1%|          | 2/313 [00:00<02:18,  2.24it/s]Evaluating:  22%|██▏       | 7/32 [00:02<00:10,  2.45it/s]Evaluating:   1%|          | 3/313 [00:01<02:21,  2.19it/s]Evaluating:  25%|██▌       | 8/32 [00:02<00:10,  2.34it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:21,  2.18it/s]Evaluating:  28%|██▊       | 9/32 [00:03<00:10,  2.30it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:20,  2.19it/s]Evaluating:  31%|███▏      | 10/32 [00:03<00:09,  2.25it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:19,  2.20it/s]Evaluating:  34%|███▍      | 11/32 [00:04<00:09,  2.22it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:20,  2.18it/s]Evaluating:  38%|███▊      | 12/32 [00:04<00:09,  2.22it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:19,  2.19it/s]Evaluating:  41%|████      | 13/32 [00:05<00:08,  2.19it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:19,  2.17it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  44%|████▍     | 14/32 [00:05<00:08,  2.20it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:18,  2.19it/s]Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.18it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:19,  2.17it/s]Evaluating:  50%|█████     | 16/32 [00:06<00:07,  2.19it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:17,  2.18it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.19it/s]Evaluating:   4%|▍         | 13/313 [00:05<02:16,  2.20it/s]Evaluating:  56%|█████▋    | 18/32 [00:07<00:06,  2.18it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:17,  2.18it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:06,  2.17it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:17,  2.17it/s]Evaluating:  62%|██████▎   | 20/32 [00:08<00:05,  2.18it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:16,  2.18it/s]Evaluating:  66%|██████▌   | 21/32 [00:08<00:05,  2.19it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:15,  2.19it/s]Evaluating:  69%|██████▉   | 22/32 [00:09<00:04,  2.17it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:15,  2.18it/s]Evaluating:  72%|███████▏  | 23/32 [00:09<00:04,  2.17it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:14,  2.19it/s]Evaluating:  75%|███████▌  | 24/32 [00:10<00:03,  2.17it/s]Evaluating:   6%|▋         | 20/313 [00:09<02:14,  2.18it/s]Evaluating:  78%|███████▊  | 25/32 [00:10<00:03,  2.19it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:12,  2.20it/s]Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.17it/s]Evaluating:   7%|▋         | 22/313 [00:10<02:13,  2.18it/s]Evaluating:  84%|████████▍ | 27/32 [00:11<00:02,  2.18it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:12,  2.19it/s]Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.17it/s]Evaluating:   8%|▊         | 24/313 [00:10<02:12,  2.18it/s]Evaluating:  91%|█████████ | 29/32 [00:12<00:01,  2.18it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:12,  2.18it/s]Evaluating:  94%|█████████▍| 30/32 [00:13<00:00,  2.17it/s]Evaluating:   8%|▊         | 26/313 [00:11<02:12,  2.17it/s]Evaluating:  97%|█████████▋| 31/32 [00:13<00:00,  2.18it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:11,  2.18it/s]Evaluating: 100%|██████████| 32/32 [00:13<00:00,  2.36it/s]Evaluating: 100%|██████████| 32/32 [00:13<00:00,  2.30it/s]Evaluating:   9%|▉         | 28/313 [00:12<02:00,  2.37it/s]
10/10/2021 15:16:54 - INFO - __main__ -   ***** Evaluation result  in be *****
10/10/2021 15:16:54 - INFO - __main__ -     f1 = 0.6314525810324129
10/10/2021 15:16:54 - INFO - __main__ -     loss = 0.37677945871837437
10/10/2021 15:16:54 - INFO - __main__ -     precision = 0.6154446177847114
10/10/2021 15:16:54 - INFO - __main__ -     recall = 0.6483155299917831
10/10/2021 15:16:54 - INFO - __main__ -   Language adapter for uk not found, using ru instead
10/10/2021 15:16:54 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:16:54 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
Evaluating:   9%|▉         | 29/313 [00:12<01:49,  2.59it/s]Evaluating:  10%|▉         | 30/313 [00:13<01:41,  2.79it/s]Evaluating:  10%|▉         | 31/313 [00:13<01:35,  2.95it/s]Evaluating:  10%|█         | 32/313 [00:13<01:29,  3.12it/s]Evaluating:  11%|█         | 33/313 [00:14<01:27,  3.19it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  11%|█         | 34/313 [00:14<01:25,  3.27it/s]Evaluating:  11%|█         | 35/313 [00:14<01:24,  3.28it/s]10/10/2021 15:16:56 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/10/2021 15:16:56 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:16:56 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  12%|█▏        | 36/313 [00:15<01:32,  2.99it/s]Evaluating:   0%|          | 1/313 [00:00<02:20,  2.23it/s]Evaluating:  12%|█▏        | 37/313 [00:15<01:43,  2.67it/s]Evaluating:   1%|          | 2/313 [00:00<02:23,  2.17it/s]Evaluating:  12%|█▏        | 38/313 [00:16<01:49,  2.52it/s]Evaluating:   1%|          | 3/313 [00:01<02:21,  2.19it/s]Evaluating:  12%|█▏        | 39/313 [00:16<01:54,  2.39it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:22,  2.17it/s]Evaluating:  13%|█▎        | 40/313 [00:16<01:57,  2.33it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:21,  2.18it/s]Evaluating:  13%|█▎        | 41/313 [00:17<01:57,  2.32it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:25,  2.10it/s]Evaluating:  13%|█▎        | 42/313 [00:17<01:58,  2.28it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:24,  2.11it/s]Evaluating:  14%|█▎        | 43/313 [00:18<02:00,  2.24it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:22,  2.15it/s]Evaluating:  14%|█▍        | 44/313 [00:18<02:00,  2.23it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:21,  2.15it/s]Evaluating:  14%|█▍        | 45/313 [00:19<02:00,  2.22it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:19,  2.17it/s]Evaluating:  15%|█▍        | 46/313 [00:19<02:01,  2.20it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:20,  2.16it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  15%|█▌        | 47/313 [00:20<02:00,  2.21it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:18,  2.18it/s]Evaluating:  15%|█▌        | 48/313 [00:20<02:01,  2.19it/s]Evaluating:   4%|▍         | 13/313 [00:06<02:18,  2.16it/s]Evaluating:  16%|█▌        | 49/313 [00:21<02:00,  2.19it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:17,  2.18it/s]Evaluating:  16%|█▌        | 50/313 [00:21<02:00,  2.18it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:17,  2.17it/s]Evaluating:  16%|█▋        | 51/313 [00:22<01:59,  2.19it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:15,  2.19it/s]Evaluating:  17%|█▋        | 52/313 [00:22<01:59,  2.18it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:15,  2.18it/s]Evaluating:  17%|█▋        | 53/313 [00:22<01:59,  2.18it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:15,  2.17it/s]Evaluating:  17%|█▋        | 54/313 [00:23<01:59,  2.17it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:13,  2.19it/s]Evaluating:  18%|█▊        | 55/313 [00:23<01:57,  2.19it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   6%|▋         | 20/313 [00:09<02:14,  2.18it/s]Evaluating:  18%|█▊        | 56/313 [00:24<01:58,  2.17it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:13,  2.18it/s]Evaluating:  18%|█▊        | 57/313 [00:24<01:56,  2.19it/s]Evaluating:   7%|▋         | 22/313 [00:10<02:14,  2.17it/s]Evaluating:  19%|█▊        | 58/313 [00:25<01:56,  2.19it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:13,  2.18it/s]Evaluating:  19%|█▉        | 59/313 [00:25<01:55,  2.19it/s]Evaluating:   8%|▊         | 24/313 [00:11<02:13,  2.16it/s]Evaluating:  19%|█▉        | 60/313 [00:26<01:55,  2.20it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:12,  2.18it/s]Evaluating:  19%|█▉        | 61/313 [00:26<01:55,  2.18it/s]Evaluating:   8%|▊         | 26/313 [00:11<02:12,  2.17it/s]Evaluating:  20%|█▉        | 62/313 [00:27<01:54,  2.20it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:10,  2.18it/s]Evaluating:  20%|██        | 63/313 [00:27<01:54,  2.18it/s]Evaluating:   9%|▉         | 28/313 [00:12<02:11,  2.17it/s]Evaluating:  20%|██        | 64/313 [00:27<01:53,  2.19it/s]Evaluating:   9%|▉         | 29/313 [00:13<02:10,  2.17it/s]Evaluating:  21%|██        | 65/313 [00:28<01:53,  2.18it/s]Evaluating:  10%|▉         | 30/313 [00:13<02:10,  2.17it/s]Evaluating:  21%|██        | 66/313 [00:28<01:52,  2.19it/s]Evaluating:  10%|▉         | 31/313 [00:14<02:09,  2.18it/s]Evaluating:  21%|██▏       | 67/313 [00:29<01:53,  2.18it/s]Evaluating:  10%|█         | 32/313 [00:14<02:08,  2.19it/s]Evaluating:  22%|██▏       | 68/313 [00:29<01:52,  2.18it/s]Evaluating:  11%|█         | 33/313 [00:15<02:08,  2.17it/s]Evaluating:  22%|██▏       | 69/313 [00:30<01:52,  2.17it/s]Evaluating:  11%|█         | 34/313 [00:15<02:07,  2.19it/s]Evaluating:  22%|██▏       | 70/313 [00:30<01:51,  2.19it/s]Evaluating:  11%|█         | 35/313 [00:16<02:08,  2.17it/s]Evaluating:  23%|██▎       | 71/313 [00:31<01:51,  2.17it/s]Evaluating:  12%|█▏        | 36/313 [00:16<02:07,  2.18it/s]Evaluating:  23%|██▎       | 72/313 [00:31<01:50,  2.18it/s]Evaluating:  12%|█▏        | 37/313 [00:17<02:07,  2.16it/s]Evaluating:  23%|██▎       | 73/313 [00:32<01:50,  2.18it/s]Evaluating:  12%|█▏        | 38/313 [00:17<02:05,  2.19it/s]Evaluating:  24%|██▎       | 74/313 [00:32<01:49,  2.17it/s]Evaluating:  12%|█▏        | 39/313 [00:17<02:06,  2.17it/s]Evaluating:  24%|██▍       | 75/313 [00:33<01:48,  2.18it/s]Evaluating:  13%|█▎        | 40/313 [00:18<02:05,  2.18it/s]Evaluating:  24%|██▍       | 76/313 [00:33<01:49,  2.17it/s]Evaluating:  13%|█▎        | 41/313 [00:18<02:05,  2.16it/s]Evaluating:  25%|██▍       | 77/313 [00:33<01:48,  2.18it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  13%|█▎        | 42/313 [00:19<02:04,  2.18it/s]10/10/2021 15:17:16 - INFO - __main__ -   Using lang2id = None
Evaluating:  25%|██▍       | 78/313 [00:34<01:48,  2.17it/s]Evaluating:  14%|█▎        | 43/313 [00:19<02:03,  2.18it/s]Evaluating:  25%|██▌       | 79/313 [00:34<01:47,  2.17it/s]Evaluating:  14%|█▍        | 44/313 [00:20<02:04,  2.17it/s]Evaluating:  26%|██▌       | 80/313 [00:35<01:47,  2.16it/s]Evaluating:  14%|█▍        | 45/313 [00:20<02:02,  2.19it/s]Evaluating:  26%|██▌       | 81/313 [00:35<01:46,  2.18it/s]Evaluating:  15%|█▍        | 46/313 [00:21<02:02,  2.17it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  26%|██▌       | 82/313 [00:36<01:47,  2.16it/s]10/10/2021 15:17:18 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:17:18 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:17:18 - INFO - __main__ -   Seed = 32
10/10/2021 15:17:18 - INFO - root -   save model
10/10/2021 15:17:18 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:17:18 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  15%|█▌        | 47/313 [00:21<02:01,  2.19it/s]Evaluating:  27%|██▋       | 83/313 [00:36<01:46,  2.17it/s]Evaluating:  15%|█▌        | 48/313 [00:22<02:01,  2.17it/s]Evaluating:  27%|██▋       | 84/313 [00:37<01:46,  2.15it/s]Evaluating:  16%|█▌        | 49/313 [00:22<01:54,  2.31it/s]Evaluating:  27%|██▋       | 85/313 [00:37<01:50,  2.05it/s]Evaluating:  16%|█▌        | 50/313 [00:22<01:55,  2.27it/s]Evaluating:  27%|██▋       | 86/313 [00:38<01:48,  2.09it/s]Evaluating:  16%|█▋        | 51/313 [00:23<01:56,  2.25it/s]Evaluating:  28%|██▊       | 87/313 [00:38<01:47,  2.11it/s]Evaluating:  17%|█▋        | 52/313 [00:23<01:57,  2.22it/s]Evaluating:  28%|██▊       | 88/313 [00:39<01:45,  2.14it/s]Evaluating:  17%|█▋        | 53/313 [00:24<01:57,  2.21it/s]Evaluating:  28%|██▊       | 89/313 [00:39<01:44,  2.14it/s]Evaluating:  17%|█▋        | 54/313 [00:24<01:58,  2.19it/s]Evaluating:  29%|██▉       | 90/313 [00:40<01:42,  2.17it/s]Evaluating:  18%|█▊        | 55/313 [00:25<01:57,  2.19it/s]Evaluating:  29%|██▉       | 91/313 [00:40<01:42,  2.16it/s]Evaluating:  18%|█▊        | 56/313 [00:25<01:57,  2.18it/s]Evaluating:  29%|██▉       | 92/313 [00:40<01:41,  2.18it/s]Evaluating:  18%|█▊        | 57/313 [00:26<01:57,  2.18it/s]Evaluating:  30%|██▉       | 93/313 [00:41<01:40,  2.19it/s]Evaluating:  19%|█▊        | 58/313 [00:26<01:56,  2.18it/s]Evaluating:  30%|███       | 94/313 [00:41<01:40,  2.19it/s]Evaluating:  19%|█▉        | 59/313 [00:27<01:57,  2.16it/s]Evaluating:  30%|███       | 95/313 [00:42<01:40,  2.17it/s]Evaluating:  19%|█▉        | 60/313 [00:27<01:57,  2.15it/s]Evaluating:  31%|███       | 96/313 [00:42<01:39,  2.19it/s]Evaluating:  19%|█▉        | 61/313 [00:28<01:56,  2.16it/s]Evaluating:  31%|███       | 97/313 [00:43<01:38,  2.20it/s]Evaluating:  20%|█▉        | 62/313 [00:28<01:55,  2.18it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  31%|███▏      | 98/313 [00:43<01:38,  2.17it/s]Evaluating:  20%|██        | 63/313 [00:28<01:55,  2.16it/s]Evaluating:  32%|███▏      | 99/313 [00:44<01:38,  2.17it/s]Evaluating:  20%|██        | 64/313 [00:29<01:54,  2.18it/s]Evaluating:  32%|███▏      | 100/313 [00:44<01:38,  2.17it/s]Evaluating:  21%|██        | 65/313 [00:29<01:54,  2.17it/s]Evaluating:  32%|███▏      | 101/313 [00:45<01:37,  2.18it/s]Evaluating:  21%|██        | 66/313 [00:30<01:52,  2.19it/s]Evaluating:  33%|███▎      | 102/313 [00:45<01:37,  2.16it/s]Evaluating:  21%|██▏       | 67/313 [00:30<01:52,  2.18it/s]Evaluating:  33%|███▎      | 103/313 [00:45<01:36,  2.18it/s]Evaluating:  22%|██▏       | 68/313 [00:31<01:51,  2.20it/s]Evaluating:  33%|███▎      | 104/313 [00:46<01:36,  2.16it/s]Evaluating:  22%|██▏       | 69/313 [00:31<01:51,  2.19it/s]Evaluating:  34%|███▎      | 105/313 [00:46<01:35,  2.17it/s]Evaluating:  22%|██▏       | 70/313 [00:32<01:50,  2.19it/s]Evaluating:  34%|███▍      | 106/313 [00:47<01:36,  2.15it/s]Evaluating:  23%|██▎       | 71/313 [00:32<01:51,  2.18it/s]Evaluating:  34%|███▍      | 107/313 [00:47<01:35,  2.17it/s]Evaluating:  23%|██▎       | 72/313 [00:33<01:49,  2.19it/s]Evaluating:  35%|███▍      | 108/313 [00:48<01:34,  2.18it/s]Evaluating:  23%|██▎       | 73/313 [00:33<01:49,  2.20it/s]Evaluating:  35%|███▍      | 109/313 [00:48<01:34,  2.16it/s]Evaluating:  24%|██▎       | 74/313 [00:33<01:49,  2.17it/s]Evaluating:  35%|███▌      | 110/313 [00:49<01:34,  2.15it/s]Evaluating:  24%|██▍       | 75/313 [00:34<01:50,  2.16it/s]Evaluating:  35%|███▌      | 111/313 [00:49<01:33,  2.17it/s]Evaluating:  24%|██▍       | 76/313 [00:34<01:48,  2.18it/s]Evaluating:  36%|███▌      | 112/313 [00:50<01:32,  2.18it/s]Evaluating:  25%|██▍       | 77/313 [00:35<01:48,  2.18it/s]Evaluating:  36%|███▌      | 113/313 [00:50<01:32,  2.16it/s]Evaluating:  25%|██▍       | 78/313 [00:35<01:48,  2.17it/s]Evaluating:  36%|███▋      | 114/313 [00:51<01:31,  2.18it/s]Evaluating:  25%|██▌       | 79/313 [00:36<01:47,  2.18it/s]Evaluating:  37%|███▋      | 115/313 [00:51<01:31,  2.17it/s]Evaluating:  26%|██▌       | 80/313 [00:36<01:46,  2.18it/s]Evaluating:  37%|███▋      | 116/313 [00:51<01:30,  2.18it/s]Evaluating:  26%|██▌       | 81/313 [00:37<01:45,  2.19it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  37%|███▋      | 117/313 [00:52<01:30,  2.17it/s]Evaluating:  26%|██▌       | 82/313 [00:37<01:46,  2.18it/s]Evaluating:  38%|███▊      | 118/313 [00:52<01:29,  2.18it/s]Evaluating:  27%|██▋       | 83/313 [00:38<01:45,  2.18it/s]Evaluating:  38%|███▊      | 119/313 [00:53<01:28,  2.19it/s]Evaluating:  27%|██▋       | 84/313 [00:38<01:44,  2.19it/s]Evaluating:  38%|███▊      | 120/313 [00:53<01:28,  2.17it/s]Evaluating:  27%|██▋       | 85/313 [00:38<01:44,  2.17it/s]Evaluating:  39%|███▊      | 121/313 [00:54<01:28,  2.16it/s]Evaluating:  27%|██▋       | 86/313 [00:39<01:45,  2.16it/s]Evaluating:  39%|███▉      | 122/313 [00:54<01:27,  2.17it/s]Evaluating:  28%|██▊       | 87/313 [00:39<01:43,  2.18it/s]Evaluating:  39%|███▉      | 123/313 [00:55<01:26,  2.18it/s]Evaluating:  28%|██▊       | 88/313 [00:40<01:42,  2.19it/s]Evaluating:  40%|███▉      | 124/313 [00:55<01:27,  2.16it/s]Evaluating:  28%|██▊       | 89/313 [00:40<01:43,  2.17it/s]Evaluating:  40%|███▉      | 125/313 [00:56<01:26,  2.16it/s]Evaluating:  29%|██▉       | 90/313 [00:41<01:42,  2.18it/s]Evaluating:  40%|████      | 126/313 [00:56<01:26,  2.16it/s]Evaluating:  29%|██▉       | 91/313 [00:41<01:42,  2.16it/s]Evaluating:  41%|████      | 127/313 [00:57<01:25,  2.18it/s]Evaluating:  29%|██▉       | 92/313 [00:42<01:41,  2.18it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Evaluating:  41%|████      | 128/313 [00:57<01:25,  2.16it/s]Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  30%|██▉       | 93/313 [00:42<01:41,  2.17it/s]Evaluating:  41%|████      | 129/313 [00:57<01:24,  2.19it/s]Evaluating:  30%|███       | 94/313 [00:43<01:39,  2.20it/s]Evaluating:  42%|████▏     | 130/313 [00:58<01:23,  2.19it/s]Evaluating:  30%|███       | 95/313 [00:43<01:39,  2.20it/s]Evaluating:  42%|████▏     | 131/313 [00:58<01:23,  2.18it/s]Evaluating:  31%|███       | 96/313 [00:44<01:39,  2.17it/s]Evaluating:  42%|████▏     | 132/313 [00:59<01:23,  2.16it/s]Evaluating:  31%|███       | 97/313 [00:44<01:40,  2.15it/s]Evaluating:  42%|████▏     | 133/313 [00:59<01:22,  2.18it/s]Evaluating:  31%|███▏      | 98/313 [00:44<01:38,  2.17it/s]Evaluating:  43%|████▎     | 134/313 [01:00<01:21,  2.18it/s]Evaluating:  32%|███▏      | 99/313 [00:45<01:38,  2.18it/s]Evaluating:  43%|████▎     | 135/313 [01:00<01:22,  2.16it/s]Evaluating:  32%|███▏      | 100/313 [00:45<01:38,  2.17it/s]Evaluating:  43%|████▎     | 136/313 [01:01<01:21,  2.16it/s]Evaluating:  32%|███▏      | 101/313 [00:46<01:37,  2.17it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  44%|████▍     | 137/313 [01:01<01:21,  2.17it/s]Evaluating:  33%|███▎      | 102/313 [00:46<01:37,  2.16it/s]Evaluating:  44%|████▍     | 138/313 [01:02<01:20,  2.17it/s]Evaluating:  33%|███▎      | 103/313 [00:47<01:36,  2.18it/s]Evaluating:  44%|████▍     | 139/313 [01:02<01:20,  2.16it/s]Evaluating:  33%|███▎      | 104/313 [00:47<01:36,  2.16it/s]Evaluating:  45%|████▍     | 140/313 [01:03<01:19,  2.18it/s]Evaluating:  34%|███▎      | 105/313 [00:48<01:35,  2.17it/s]Evaluating:  45%|████▌     | 141/313 [01:03<01:18,  2.19it/s]Evaluating:  34%|███▍      | 106/313 [00:48<01:34,  2.18it/s]Evaluating:  45%|████▌     | 142/313 [01:03<01:18,  2.17it/s]Evaluating:  34%|███▍      | 107/313 [00:49<01:35,  2.17it/s]Evaluating:  46%|████▌     | 143/313 [01:04<01:18,  2.16it/s]Evaluating:  35%|███▍      | 108/313 [00:49<01:35,  2.15it/s]Evaluating:  46%|████▌     | 144/313 [01:04<01:17,  2.17it/s]Evaluating:  35%|███▍      | 109/313 [00:50<01:33,  2.17it/s]Evaluating:  46%|████▋     | 145/313 [01:05<01:16,  2.18it/s]Evaluating:  35%|███▌      | 110/313 [00:50<01:32,  2.18it/s]Evaluating:  47%|████▋     | 146/313 [01:05<01:17,  2.17it/s]Evaluating:  35%|███▌      | 111/313 [00:50<01:33,  2.16it/s]Evaluating:  47%|████▋     | 147/313 [01:06<01:16,  2.16it/s]Evaluating:  36%|███▌      | 112/313 [00:51<01:32,  2.17it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:17:48 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:17:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:17:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/10/2021 15:17:48 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:17:48 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:17:48 - INFO - root -   loading lang adpater hi/wiki@ukp
10/10/2021 15:17:48 - INFO - __main__ -   Language = hi
10/10/2021 15:17:48 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Evaluating:  47%|████▋     | 148/313 [01:06<01:16,  2.17it/s]Evaluating:  36%|███▌      | 113/313 [00:51<01:32,  2.17it/s]Evaluating:  48%|████▊     | 149/313 [01:07<01:15,  2.18it/s]Evaluating:  36%|███▋      | 114/313 [00:52<01:31,  2.18it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
Evaluating:  48%|████▊     | 150/313 [01:07<01:21,  2.00it/s]Evaluating:  37%|███▋      | 115/313 [00:52<01:38,  2.01it/s]Evaluating:  48%|████▊     | 151/313 [01:08<01:17,  2.08it/s]Evaluating:  37%|███▋      | 116/313 [00:53<01:35,  2.07it/s]Evaluating:  49%|████▊     | 152/313 [01:08<01:16,  2.09it/s]Evaluating:  37%|███▋      | 117/313 [00:53<01:33,  2.09it/s]Evaluating:  49%|████▉     | 153/313 [01:09<01:16,  2.10it/s]Evaluating:  38%|███▊      | 118/313 [00:54<01:33,  2.10it/s]Evaluating:  49%|████▉     | 154/313 [01:09<01:14,  2.13it/s]Evaluating:  38%|███▊      | 119/313 [00:54<01:31,  2.12it/s]Evaluating:  50%|████▉     | 155/313 [01:10<01:13,  2.16it/s]Evaluating:  38%|███▊      | 120/313 [00:55<01:29,  2.15it/s]Evaluating:  50%|████▉     | 156/313 [01:10<01:13,  2.14it/s]Evaluating:  39%|███▊      | 121/313 [00:55<01:29,  2.14it/s]Evaluating:  50%|█████     | 157/313 [01:10<01:13,  2.14it/s]Evaluating:  39%|███▉      | 122/313 [00:56<01:29,  2.13it/s]Evaluating:  50%|█████     | 158/313 [01:11<01:11,  2.15it/s]Evaluating:  39%|███▉      | 123/313 [00:56<01:28,  2.15it/s]Evaluating:  51%|█████     | 159/313 [01:11<01:11,  2.16it/s]Evaluating:  40%|███▉      | 124/313 [00:57<01:27,  2.16it/s]Evaluating:  51%|█████     | 160/313 [01:12<01:11,  2.13it/s]Evaluating:  40%|███▉      | 125/313 [00:57<01:27,  2.15it/s]Evaluating:  51%|█████▏    | 161/313 [01:12<01:11,  2.13it/s]Evaluating:  40%|████      | 126/313 [00:58<01:27,  2.14it/s]Evaluating:  52%|█████▏    | 162/313 [01:13<01:09,  2.17it/s]Evaluating:  41%|████      | 127/313 [00:58<01:25,  2.17it/s]Evaluating:  52%|█████▏    | 163/313 [01:13<01:09,  2.16it/s]Evaluating:  41%|████      | 128/313 [00:58<01:25,  2.16it/s]Evaluating:  52%|█████▏    | 164/313 [01:14<01:09,  2.15it/s]Evaluating:  41%|████      | 129/313 [00:59<01:25,  2.14it/s]Evaluating:  53%|█████▎    | 165/313 [01:14<01:08,  2.16it/s]Evaluating:  42%|████▏     | 130/313 [00:59<01:24,  2.16it/s]Evaluating:  53%|█████▎    | 166/313 [01:15<01:07,  2.17it/s]Evaluating:  42%|████▏     | 131/313 [01:00<01:23,  2.17it/s]Evaluating:  53%|█████▎    | 167/313 [01:15<01:07,  2.16it/s]Evaluating:  42%|████▏     | 132/313 [01:00<01:24,  2.15it/s]Evaluating:  54%|█████▎    | 168/313 [01:16<01:07,  2.14it/s]Evaluating:  42%|████▏     | 133/313 [01:01<01:24,  2.14it/s]Evaluating:  54%|█████▍    | 169/313 [01:16<01:06,  2.16it/s]Evaluating:  43%|████▎     | 134/313 [01:01<01:23,  2.16it/s]Evaluating:  54%|█████▍    | 170/313 [01:16<01:05,  2.17it/s]Evaluating:  43%|████▎     | 135/313 [01:02<01:22,  2.17it/s]Evaluating:  55%|█████▍    | 171/313 [01:17<01:06,  2.15it/s]Evaluating:  43%|████▎     | 136/313 [01:02<01:22,  2.14it/s]Evaluating:  55%|█████▍    | 172/313 [01:17<01:05,  2.15it/s]Evaluating:  44%|████▍     | 137/313 [01:03<01:22,  2.13it/s]Evaluating:  55%|█████▌    | 173/313 [01:18<01:04,  2.18it/s]10/10/2021 15:18:00 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/10/2021 15:18:00 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:18:00 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
Evaluating:  44%|████▍     | 138/313 [01:03<01:20,  2.17it/s]10/10/2021 15:18:00 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/10/2021 15:18:00 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:18:00 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  56%|█████▌    | 174/313 [01:18<01:08,  2.04it/s]Evaluating:  44%|████▍     | 139/313 [01:04<01:26,  2.01it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:19,  1.57it/s]Evaluating:  56%|█████▌    | 175/313 [01:19<01:13,  1.87it/s]Evaluating:  45%|████▍     | 140/313 [01:04<01:32,  1.86it/s]Evaluating:   6%|▋         | 2/32 [00:01<00:18,  1.58it/s]Evaluating:  56%|█████▌    | 176/313 [01:20<01:17,  1.78it/s]Evaluating:  45%|████▌     | 141/313 [01:05<01:36,  1.79it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:18,  1.60it/s]Evaluating:  57%|█████▋    | 177/313 [01:20<01:18,  1.73it/s]Evaluating:  45%|████▌     | 142/313 [01:06<01:38,  1.73it/s]Evaluating:  12%|█▎        | 4/32 [00:02<00:17,  1.62it/s]Evaluating:  57%|█████▋    | 178/313 [01:21<01:20,  1.68it/s]Evaluating:  46%|████▌     | 143/313 [01:06<01:40,  1.69it/s]Evaluating:  16%|█▌        | 5/32 [00:03<00:16,  1.61it/s]Evaluating:  57%|█████▋    | 179/313 [01:22<01:21,  1.65it/s]Evaluating:  46%|████▌     | 144/313 [01:07<01:42,  1.65it/s]Evaluating:  19%|█▉        | 6/32 [00:03<00:16,  1.61it/s]Evaluating:  58%|█████▊    | 180/313 [01:22<01:21,  1.64it/s]Evaluating:  46%|████▋     | 145/313 [01:07<01:42,  1.64it/s]Evaluating:  22%|██▏       | 7/32 [00:04<00:15,  1.63it/s]Evaluating:  58%|█████▊    | 181/313 [01:23<01:20,  1.63it/s]Evaluating:  47%|████▋     | 146/313 [01:08<01:42,  1.63it/s]Evaluating:  25%|██▌       | 8/32 [00:04<00:14,  1.62it/s]Evaluating:  58%|█████▊    | 182/313 [01:23<01:21,  1.61it/s]Evaluating:  47%|████▋     | 147/313 [01:09<01:42,  1.61it/s]Evaluating:  28%|██▊       | 9/32 [00:05<00:14,  1.61it/s]Evaluating:  58%|█████▊    | 183/313 [01:24<01:21,  1.60it/s]Evaluating:  47%|████▋     | 148/313 [01:09<01:43,  1.60it/s]Evaluating:  31%|███▏      | 10/32 [00:06<00:13,  1.61it/s]Evaluating:  59%|█████▉    | 184/313 [01:25<01:19,  1.62it/s]Evaluating:  48%|████▊     | 149/313 [01:10<01:41,  1.62it/s]Evaluating:  34%|███▍      | 11/32 [00:06<00:12,  1.62it/s]Evaluating:  59%|█████▉    | 185/313 [01:25<01:19,  1.61it/s]Evaluating:  48%|████▊     | 150/313 [01:11<01:41,  1.61it/s]Evaluating:  38%|███▊      | 12/32 [00:07<00:12,  1.61it/s]Evaluating:  59%|█████▉    | 186/313 [01:26<01:19,  1.60it/s]Evaluating:  48%|████▊     | 151/313 [01:11<01:41,  1.60it/s]Evaluating:  41%|████      | 13/32 [00:08<00:11,  1.60it/s]Evaluating:  60%|█████▉    | 187/313 [01:27<01:18,  1.60it/s]Evaluating:  49%|████▊     | 152/313 [01:12<01:40,  1.60it/s]Evaluating:  44%|████▍     | 14/32 [00:08<00:11,  1.61it/s]Evaluating:  60%|██████    | 188/313 [01:27<01:17,  1.61it/s]Evaluating:  49%|████▉     | 153/313 [01:12<01:39,  1.61it/s]Evaluating:  47%|████▋     | 15/32 [00:09<00:10,  1.61it/s]Evaluating:  60%|██████    | 189/313 [01:28<01:17,  1.60it/s]Evaluating:  49%|████▉     | 154/313 [01:13<01:39,  1.60it/s]Evaluating:  50%|█████     | 16/32 [00:09<00:09,  1.61it/s]Evaluating:  61%|██████    | 190/313 [01:28<01:17,  1.59it/s]Evaluating:  50%|████▉     | 155/313 [01:14<01:39,  1.59it/s]Evaluating:  53%|█████▎    | 17/32 [00:10<00:09,  1.60it/s]Evaluating:  61%|██████    | 191/313 [01:29<01:16,  1.60it/s]Evaluating:  50%|████▉     | 156/313 [01:14<01:38,  1.60it/s]Evaluating:  56%|█████▋    | 18/32 [00:11<00:08,  1.62it/s]Evaluating:  61%|██████▏   | 192/313 [01:30<01:15,  1.60it/s]Evaluating:  50%|█████     | 157/313 [01:15<01:37,  1.60it/s]Evaluating:  59%|█████▉    | 19/32 [00:11<00:08,  1.62it/s]Evaluating:  62%|██████▏   | 193/313 [01:30<01:15,  1.59it/s]Evaluating:  50%|█████     | 158/313 [01:16<01:37,  1.59it/s]Evaluating:  62%|██████▎   | 20/32 [00:12<00:07,  1.61it/s]Evaluating:  62%|██████▏   | 194/313 [01:31<01:14,  1.59it/s]Evaluating:  51%|█████     | 159/313 [01:16<01:37,  1.59it/s]Evaluating:  66%|██████▌   | 21/32 [00:13<00:06,  1.61it/s]Evaluating:  62%|██████▏   | 195/313 [01:32<01:14,  1.59it/s]Evaluating:  51%|█████     | 160/313 [01:17<01:35,  1.61it/s]Evaluating:  69%|██████▉   | 22/32 [00:13<00:06,  1.64it/s]Evaluating:  51%|█████▏    | 161/313 [01:17<01:33,  1.62it/s]Evaluating:  63%|██████▎   | 196/313 [01:32<01:12,  1.60it/s]Evaluating:  72%|███████▏  | 23/32 [00:14<00:05,  1.57it/s]Evaluating:  63%|██████▎   | 197/313 [01:33<01:12,  1.60it/s]Evaluating:  52%|█████▏    | 162/313 [01:18<01:34,  1.61it/s]Evaluating:  75%|███████▌  | 24/32 [00:14<00:05,  1.58it/s]Evaluating:  52%|█████▏    | 163/313 [01:19<01:33,  1.61it/s]Evaluating:  63%|██████▎   | 198/313 [01:34<01:12,  1.58it/s]Evaluating:  78%|███████▊  | 25/32 [00:15<00:04,  1.61it/s]Evaluating:  52%|█████▏    | 164/313 [01:19<01:31,  1.62it/s]Evaluating:  64%|██████▎   | 199/313 [01:34<01:11,  1.59it/s]Evaluating:  81%|████████▏ | 26/32 [00:16<00:03,  1.62it/s]Evaluating:  53%|█████▎    | 165/313 [01:20<01:32,  1.61it/s]Evaluating:  64%|██████▍   | 200/313 [01:35<01:11,  1.59it/s]Evaluating:  84%|████████▍ | 27/32 [00:16<00:03,  1.61it/s]Evaluating:  53%|█████▎    | 166/313 [01:21<01:31,  1.60it/s]Evaluating:  64%|██████▍   | 201/313 [01:35<01:10,  1.59it/s]Evaluating:  88%|████████▊ | 28/32 [00:17<00:02,  1.61it/s]Evaluating:  53%|█████▎    | 167/313 [01:21<01:30,  1.61it/s]Evaluating:  65%|██████▍   | 202/313 [01:36<01:09,  1.60it/s]Evaluating:  91%|█████████ | 29/32 [00:17<00:01,  1.63it/s]Evaluating:  54%|█████▎    | 168/313 [01:22<01:30,  1.61it/s]Evaluating:  65%|██████▍   | 203/313 [01:37<01:08,  1.60it/s]Evaluating:  94%|█████████▍| 30/32 [00:18<00:01,  1.62it/s]Evaluating:  54%|█████▍    | 169/313 [01:22<01:27,  1.65it/s]Evaluating:  97%|█████████▋| 31/32 [00:19<00:00,  1.57it/s]Evaluating:  65%|██████▌   | 204/313 [01:37<01:10,  1.54it/s]Evaluating:  54%|█████▍    | 170/313 [01:23<01:24,  1.70it/s]Evaluating:  65%|██████▌   | 205/313 [01:38<01:05,  1.65it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.68it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.62it/s]
10/10/2021 15:18:20 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/10/2021 15:18:20 - INFO - __main__ -     f1 = 0.39519359145527366
10/10/2021 15:18:20 - INFO - __main__ -     loss = 4.545034572482109
10/10/2021 15:18:20 - INFO - __main__ -     precision = 0.38341968911917096
10/10/2021 15:18:20 - INFO - __main__ -     recall = 0.40771349862258954
10/10/2021 15:18:20 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/10/2021 15:18:20 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:18:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/10/2021 15:18:20 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/10/2021 15:18:20 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:18:20 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  55%|█████▍    | 171/313 [01:23<01:18,  1.81it/s]Evaluating:  66%|██████▌   | 206/313 [01:38<01:02,  1.73it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:19,  1.62it/s]Evaluating:  55%|█████▍    | 172/313 [01:24<01:21,  1.73it/s]Evaluating:  66%|██████▌   | 207/313 [01:39<01:03,  1.68it/s]Evaluating:   6%|▋         | 2/32 [00:01<00:18,  1.59it/s]Evaluating:  55%|█████▌    | 173/313 [01:25<01:23,  1.68it/s]Evaluating:  66%|██████▋   | 208/313 [01:40<01:03,  1.64it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:18,  1.60it/s]Evaluating:  56%|█████▌    | 174/313 [01:25<01:23,  1.66it/s]Evaluating:  67%|██████▋   | 209/313 [01:40<01:03,  1.64it/s]Evaluating:  12%|█▎        | 4/32 [00:02<00:17,  1.62it/s]Evaluating:  56%|█████▌    | 175/313 [01:26<01:23,  1.65it/s]Evaluating:  67%|██████▋   | 210/313 [01:41<01:03,  1.63it/s]Evaluating:  16%|█▌        | 5/32 [00:03<00:16,  1.61it/s]Evaluating:  56%|█████▌    | 176/313 [01:27<01:24,  1.62it/s]Evaluating:  67%|██████▋   | 211/313 [01:42<01:03,  1.61it/s]Evaluating:  19%|█▉        | 6/32 [00:03<00:16,  1.61it/s]Evaluating:  57%|█████▋    | 177/313 [01:27<01:24,  1.60it/s]Evaluating:  68%|██████▊   | 212/313 [01:42<01:03,  1.60it/s]Evaluating:  22%|██▏       | 7/32 [00:04<00:15,  1.62it/s]Evaluating:  57%|█████▋    | 178/313 [01:28<01:23,  1.61it/s]Evaluating:  68%|██████▊   | 213/313 [01:43<01:01,  1.62it/s]Evaluating:  25%|██▌       | 8/32 [00:04<00:14,  1.62it/s]Evaluating:  57%|█████▋    | 179/313 [01:28<01:23,  1.61it/s]Evaluating:  68%|██████▊   | 214/313 [01:43<01:01,  1.60it/s]Evaluating:  28%|██▊       | 9/32 [00:05<00:14,  1.62it/s]Evaluating:  58%|█████▊    | 180/313 [01:29<01:23,  1.60it/s]Evaluating:  69%|██████▊   | 215/313 [01:44<01:01,  1.59it/s]Evaluating:  31%|███▏      | 10/32 [00:06<00:13,  1.61it/s]Evaluating:  58%|█████▊    | 181/313 [01:30<01:22,  1.59it/s]Evaluating:  69%|██████▉   | 216/313 [01:45<01:00,  1.60it/s]Evaluating:  34%|███▍      | 11/32 [00:06<00:12,  1.62it/s]Evaluating:  58%|█████▊    | 182/313 [01:30<01:21,  1.61it/s]Evaluating:  69%|██████▉   | 217/313 [01:45<00:59,  1.61it/s]Evaluating:  38%|███▊      | 12/32 [00:07<00:12,  1.62it/s]Evaluating:  58%|█████▊    | 183/313 [01:31<01:21,  1.60it/s]Evaluating:  70%|██████▉   | 218/313 [01:46<00:59,  1.60it/s]Evaluating:  41%|████      | 13/32 [00:08<00:11,  1.62it/s]Evaluating:  59%|█████▉    | 184/313 [01:32<01:20,  1.59it/s]Evaluating:  70%|██████▉   | 219/313 [01:47<00:59,  1.59it/s]Evaluating:  44%|████▍     | 14/32 [00:08<00:11,  1.61it/s]Evaluating:  59%|█████▉    | 185/313 [01:32<01:19,  1.60it/s]Evaluating:  70%|███████   | 220/313 [01:47<00:58,  1.60it/s]Evaluating:  47%|████▋     | 15/32 [00:09<00:10,  1.63it/s]Evaluating:  59%|█████▉    | 186/313 [01:33<01:19,  1.60it/s]Evaluating:  71%|███████   | 221/313 [01:48<00:57,  1.60it/s]Evaluating:  50%|█████     | 16/32 [00:09<00:09,  1.62it/s]Evaluating:  60%|█████▉    | 187/313 [01:33<01:19,  1.59it/s]Evaluating:  71%|███████   | 222/313 [01:48<00:57,  1.60it/s]Evaluating:  53%|█████▎    | 17/32 [00:10<00:09,  1.61it/s]Evaluating:  60%|██████    | 188/313 [01:34<01:18,  1.58it/s]Evaluating:  71%|███████   | 223/313 [01:49<00:56,  1.59it/s]Evaluating:  56%|█████▋    | 18/32 [00:11<00:08,  1.62it/s]Evaluating:  60%|██████    | 189/313 [01:35<01:17,  1.60it/s]Evaluating:  72%|███████▏  | 224/313 [01:50<00:55,  1.61it/s]Evaluating:  59%|█████▉    | 19/32 [00:11<00:07,  1.63it/s]Evaluating:  61%|██████    | 190/313 [01:35<01:17,  1.60it/s]Evaluating:  72%|███████▏  | 225/313 [01:50<00:55,  1.60it/s]Evaluating:  62%|██████▎   | 20/32 [00:12<00:07,  1.62it/s]Evaluating:  61%|██████    | 191/313 [01:36<01:16,  1.59it/s]Evaluating:  72%|███████▏  | 226/313 [01:51<00:54,  1.59it/s]Evaluating:  66%|██████▌   | 21/32 [00:12<00:06,  1.61it/s]Evaluating:  61%|██████▏   | 192/313 [01:37<01:16,  1.58it/s]Evaluating:  73%|███████▎  | 227/313 [01:52<00:53,  1.60it/s]Evaluating:  69%|██████▉   | 22/32 [00:13<00:06,  1.62it/s]Evaluating:  62%|██████▏   | 193/313 [01:37<01:11,  1.67it/s]Evaluating:  73%|███████▎  | 228/313 [01:52<00:50,  1.68it/s]Evaluating:  72%|███████▏  | 23/32 [00:14<00:06,  1.50it/s]Evaluating:  62%|██████▏   | 194/313 [01:38<01:12,  1.64it/s]Evaluating:  73%|███████▎  | 229/313 [01:53<00:50,  1.65it/s]Evaluating:  75%|███████▌  | 24/32 [00:15<00:05,  1.53it/s]Evaluating:  62%|██████▏   | 195/313 [01:38<01:12,  1.62it/s]Evaluating:  73%|███████▎  | 230/313 [01:53<00:50,  1.63it/s]Evaluating:  78%|███████▊  | 25/32 [00:15<00:04,  1.56it/s]Evaluating:  63%|██████▎   | 196/313 [01:39<01:12,  1.62it/s]Evaluating:  74%|███████▍  | 231/313 [01:54<00:51,  1.60it/s]Evaluating:  81%|████████▏ | 26/32 [00:16<00:03,  1.59it/s]Evaluating:  63%|██████▎   | 197/313 [01:40<01:11,  1.63it/s]Evaluating:  74%|███████▍  | 232/313 [01:55<00:50,  1.60it/s]Evaluating:  84%|████████▍ | 27/32 [00:16<00:03,  1.59it/s]Evaluating:  63%|██████▎   | 198/313 [01:40<01:11,  1.61it/s]Evaluating:  74%|███████▍  | 233/313 [01:55<00:50,  1.59it/s]Evaluating:  88%|████████▊ | 28/32 [00:17<00:02,  1.59it/s]Evaluating:  64%|██████▎   | 199/313 [01:41<01:11,  1.60it/s]Evaluating:  75%|███████▍  | 234/313 [01:56<00:49,  1.59it/s]Evaluating:  91%|█████████ | 29/32 [00:18<00:01,  1.60it/s]Evaluating:  64%|██████▍   | 200/313 [01:41<01:10,  1.61it/s]Evaluating:  75%|███████▌  | 235/313 [01:56<00:48,  1.60it/s]Evaluating:  94%|█████████▍| 30/32 [00:18<00:01,  1.61it/s]Evaluating:  64%|██████▍   | 201/313 [01:42<01:09,  1.61it/s]Evaluating:  75%|███████▌  | 236/313 [01:57<00:48,  1.60it/s]Evaluating:  97%|█████████▋| 31/32 [00:19<00:00,  1.60it/s]Evaluating:  65%|██████▍   | 202/313 [01:43<01:08,  1.61it/s]Evaluating:  76%|███████▌  | 237/313 [01:58<00:46,  1.63it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.69it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.61it/s]Evaluating:  65%|██████▍   | 203/313 [01:43<01:04,  1.70it/s]
10/10/2021 15:18:40 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/10/2021 15:18:40 - INFO - __main__ -     f1 = 0.4606851549755302
10/10/2021 15:18:40 - INFO - __main__ -     loss = 2.2967870384454727
10/10/2021 15:18:40 - INFO - __main__ -     precision = 0.3920044419766796
10/10/2021 15:18:40 - INFO - __main__ -     recall = 0.5585443037974683
10/10/2021 15:18:40 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/10/2021 15:18:40 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:18:40 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
Evaluating:  76%|███████▌  | 238/313 [01:58<00:42,  1.75it/s]10/10/2021 15:18:40 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/10/2021 15:18:40 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:18:40 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  65%|██████▌   | 204/313 [01:44<01:02,  1.75it/s]Evaluating:  76%|███████▋  | 239/313 [01:59<00:42,  1.73it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:19,  1.63it/s]Evaluating:  65%|██████▌   | 205/313 [01:44<01:03,  1.70it/s]Evaluating:  77%|███████▋  | 240/313 [01:59<00:43,  1.68it/s]Evaluating:   6%|▋         | 2/32 [00:01<00:18,  1.61it/s]Evaluating:  66%|██████▌   | 206/313 [01:45<01:04,  1.66it/s]Evaluating:  77%|███████▋  | 241/313 [02:00<00:43,  1.65it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:18,  1.60it/s]Evaluating:  66%|██████▌   | 207/313 [01:46<01:04,  1.65it/s]Evaluating:  77%|███████▋  | 242/313 [02:01<00:43,  1.64it/s]Evaluating:  12%|█▎        | 4/32 [00:02<00:17,  1.62it/s]Evaluating:  66%|██████▋   | 208/313 [01:46<01:04,  1.64it/s]Evaluating:  78%|███████▊  | 243/313 [02:01<00:42,  1.63it/s]Evaluating:  16%|█▌        | 5/32 [00:03<00:16,  1.62it/s]Evaluating:  67%|██████▋   | 209/313 [01:47<01:04,  1.62it/s]Evaluating:  78%|███████▊  | 244/313 [02:02<00:42,  1.62it/s]Evaluating:  19%|█▉        | 6/32 [00:03<00:15,  1.63it/s]Evaluating:  67%|██████▋   | 210/313 [01:48<01:05,  1.57it/s]Evaluating:  78%|███████▊  | 245/313 [02:03<00:42,  1.61it/s]Evaluating:  22%|██▏       | 7/32 [00:04<00:15,  1.62it/s]Evaluating:  67%|██████▋   | 211/313 [01:48<01:04,  1.59it/s]Evaluating:  79%|███████▊  | 246/313 [02:03<00:41,  1.62it/s]Evaluating:  25%|██▌       | 8/32 [00:04<00:14,  1.63it/s]Evaluating:  68%|██████▊   | 212/313 [01:49<01:03,  1.59it/s]Evaluating:  79%|███████▉  | 247/313 [02:04<00:41,  1.61it/s]Evaluating:  28%|██▊       | 9/32 [00:05<00:14,  1.62it/s]Evaluating:  68%|██████▊   | 213/313 [01:49<01:03,  1.59it/s]Evaluating:  79%|███████▉  | 248/313 [02:04<00:40,  1.60it/s]Evaluating:  31%|███▏      | 10/32 [00:06<00:13,  1.61it/s]Evaluating:  68%|██████▊   | 214/313 [01:50<01:02,  1.59it/s]Evaluating:  80%|███████▉  | 249/313 [02:05<00:39,  1.61it/s]Evaluating:  34%|███▍      | 11/32 [00:06<00:12,  1.62it/s]Evaluating:  69%|██████▊   | 215/313 [01:51<01:01,  1.59it/s]Evaluating:  80%|███████▉  | 250/313 [02:06<00:39,  1.61it/s]Evaluating:  38%|███▊      | 12/32 [00:07<00:12,  1.63it/s]Evaluating:  69%|██████▉   | 216/313 [01:51<01:01,  1.58it/s]Evaluating:  80%|████████  | 251/313 [02:06<00:38,  1.60it/s]Evaluating:  41%|████      | 13/32 [00:08<00:11,  1.61it/s]Evaluating:  69%|██████▉   | 217/313 [01:52<01:00,  1.58it/s]Evaluating:  44%|████▍     | 14/32 [00:08<00:11,  1.60it/s]Evaluating:  81%|████████  | 252/313 [02:07<00:38,  1.58it/s]Evaluating:  70%|██████▉   | 218/313 [01:53<00:59,  1.60it/s]Evaluating:  47%|████▋     | 15/32 [00:09<00:10,  1.61it/s]Evaluating:  81%|████████  | 253/313 [02:08<00:37,  1.59it/s]Evaluating:  70%|██████▉   | 219/313 [01:53<00:58,  1.60it/s]Evaluating:  50%|█████     | 16/32 [00:09<00:09,  1.61it/s]Evaluating:  81%|████████  | 254/313 [02:08<00:36,  1.60it/s]Evaluating:  70%|███████   | 220/313 [01:54<00:58,  1.60it/s]Evaluating:  53%|█████▎    | 17/32 [00:10<00:09,  1.61it/s]Evaluating:  81%|████████▏ | 255/313 [02:09<00:36,  1.58it/s]Evaluating:  71%|███████   | 221/313 [01:54<00:57,  1.59it/s]Evaluating:  56%|█████▋    | 18/32 [00:11<00:08,  1.60it/s]Evaluating:  82%|████████▏ | 256/313 [02:09<00:36,  1.58it/s]Evaluating:  71%|███████   | 222/313 [01:55<00:56,  1.60it/s]Evaluating:  59%|█████▉    | 19/32 [00:11<00:07,  1.63it/s]Evaluating:  82%|████████▏ | 257/313 [02:10<00:34,  1.60it/s]Evaluating:  71%|███████   | 223/313 [01:56<00:56,  1.60it/s]Evaluating:  62%|██████▎   | 20/32 [00:12<00:07,  1.62it/s]Evaluating:  82%|████████▏ | 258/313 [02:11<00:34,  1.59it/s]Evaluating:  72%|███████▏  | 224/313 [01:56<00:55,  1.59it/s]Evaluating:  66%|██████▌   | 21/32 [00:13<00:06,  1.61it/s]Evaluating:  83%|████████▎ | 259/313 [02:11<00:34,  1.59it/s]Evaluating:  72%|███████▏  | 225/313 [01:57<00:55,  1.59it/s]Evaluating:  69%|██████▉   | 22/32 [00:13<00:06,  1.61it/s]Evaluating:  83%|████████▎ | 260/313 [02:12<00:33,  1.60it/s]Evaluating:  72%|███████▏  | 226/313 [01:58<00:54,  1.61it/s]Evaluating:  72%|███████▏  | 23/32 [00:14<00:05,  1.62it/s]Evaluating:  83%|████████▎ | 261/313 [02:13<00:32,  1.60it/s]Evaluating:  73%|███████▎  | 227/313 [01:58<00:53,  1.60it/s]Evaluating:  75%|███████▌  | 24/32 [00:14<00:04,  1.61it/s]Evaluating:  84%|████████▎ | 262/313 [02:13<00:31,  1.60it/s]Evaluating:  73%|███████▎  | 228/313 [01:59<00:53,  1.59it/s]Evaluating:  78%|███████▊  | 25/32 [00:15<00:04,  1.60it/s]Evaluating:  84%|████████▍ | 263/313 [02:14<00:31,  1.59it/s]Evaluating:  73%|███████▎  | 229/313 [01:59<00:52,  1.61it/s]Evaluating:  81%|████████▏ | 26/32 [00:16<00:03,  1.63it/s]Evaluating:  84%|████████▍ | 264/313 [02:14<00:30,  1.61it/s]Evaluating:  73%|███████▎  | 230/313 [02:00<00:51,  1.61it/s]Evaluating:  84%|████████▍ | 27/32 [00:16<00:03,  1.63it/s]Evaluating:  85%|████████▍ | 265/313 [02:15<00:29,  1.62it/s]Evaluating:  74%|███████▍  | 231/313 [02:01<00:51,  1.60it/s]Evaluating:  88%|████████▊ | 28/32 [00:17<00:02,  1.62it/s]Evaluating:  85%|████████▍ | 266/313 [02:16<00:29,  1.60it/s]Evaluating:  74%|███████▍  | 232/313 [02:01<00:51,  1.58it/s]Evaluating:  91%|█████████ | 29/32 [00:17<00:01,  1.61it/s]Evaluating:  85%|████████▌ | 267/313 [02:16<00:28,  1.60it/s]Evaluating:  74%|███████▍  | 233/313 [02:02<00:49,  1.60it/s]Evaluating:  94%|█████████▍| 30/32 [00:18<00:01,  1.62it/s]Evaluating:  86%|████████▌ | 268/313 [02:17<00:27,  1.62it/s]Evaluating:  75%|███████▍  | 234/313 [02:03<00:49,  1.60it/s]Evaluating:  97%|█████████▋| 31/32 [00:19<00:00,  1.62it/s]Evaluating:  86%|████████▌ | 269/313 [02:17<00:27,  1.61it/s]Evaluating:  75%|███████▌  | 235/313 [02:03<00:46,  1.69it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.70it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.62it/s]Evaluating:  86%|████████▋ | 270/313 [02:18<00:25,  1.70it/s]
10/10/2021 15:19:00 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/10/2021 15:19:00 - INFO - __main__ -     f1 = 0.21976074435090828
10/10/2021 15:19:00 - INFO - __main__ -     loss = 3.7088574543595314
10/10/2021 15:19:00 - INFO - __main__ -     precision = 0.2440944881889764
10/10/2021 15:19:00 - INFO - __main__ -     recall = 0.1998388396454472
10/10/2021 15:19:00 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  75%|███████▌  | 236/313 [02:04<00:42,  1.80it/s]Evaluating:  87%|████████▋ | 271/313 [02:18<00:23,  1.82it/s]Evaluating:  76%|███████▌  | 237/313 [02:04<00:39,  1.93it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  87%|████████▋ | 272/313 [02:19<00:21,  1.92it/s]Evaluating:  76%|███████▌  | 238/313 [02:04<00:37,  1.98it/s]Evaluating:  87%|████████▋ | 273/313 [02:19<00:20,  1.98it/s]Evaluating:  76%|███████▋  | 239/313 [02:05<00:36,  2.02it/s]Evaluating:  88%|████████▊ | 274/313 [02:20<00:19,  2.02it/s]Evaluating:  77%|███████▋  | 240/313 [02:05<00:35,  2.07it/s]Evaluating:  88%|████████▊ | 275/313 [02:20<00:18,  2.08it/s]Evaluating:  77%|███████▋  | 241/313 [02:06<00:34,  2.10it/s]Evaluating:  88%|████████▊ | 276/313 [02:21<00:17,  2.11it/s]Evaluating:  77%|███████▋  | 242/313 [02:06<00:33,  2.10it/s]Evaluating:  88%|████████▊ | 277/313 [02:21<00:16,  2.12it/s]Evaluating:  78%|███████▊  | 243/313 [02:07<00:33,  2.11it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  89%|████████▉ | 278/313 [02:22<00:16,  2.13it/s]Evaluating:  78%|███████▊  | 244/313 [02:07<00:32,  2.15it/s]Evaluating:  89%|████████▉ | 279/313 [02:22<00:15,  2.19it/s]Evaluating:  78%|███████▊  | 245/313 [02:08<00:31,  2.14it/s]Evaluating:  89%|████████▉ | 280/313 [02:23<00:15,  2.17it/s]Evaluating:  79%|███████▊  | 246/313 [02:08<00:31,  2.13it/s]Evaluating:  90%|████████▉ | 281/313 [02:23<00:14,  2.16it/s]Evaluating:  79%|███████▉  | 247/313 [02:09<00:31,  2.12it/s]Evaluating:  90%|█████████ | 282/313 [02:24<00:14,  2.17it/s]Evaluating:  79%|███████▉  | 248/313 [02:09<00:30,  2.15it/s]Evaluating:  90%|█████████ | 283/313 [02:24<00:13,  2.18it/s]Evaluating:  80%|███████▉  | 249/313 [02:10<00:29,  2.14it/s]Evaluating:  91%|█████████ | 284/313 [02:24<00:13,  2.16it/s]Evaluating:  80%|███████▉  | 250/313 [02:10<00:29,  2.12it/s]Evaluating:  91%|█████████ | 285/313 [02:25<00:12,  2.16it/s]Evaluating:  80%|████████  | 251/313 [02:11<00:29,  2.14it/s]Evaluating:  91%|█████████▏| 286/313 [02:25<00:12,  2.18it/s]Evaluating:  81%|████████  | 252/313 [02:11<00:28,  2.15it/s]Evaluating:  92%|█████████▏| 287/313 [02:26<00:11,  2.17it/s]Evaluating:  81%|████████  | 253/313 [02:11<00:28,  2.14it/s]Evaluating:  92%|█████████▏| 288/313 [02:26<00:11,  2.15it/s]Evaluating:  81%|████████  | 254/313 [02:12<00:27,  2.13it/s]Evaluating:  92%|█████████▏| 289/313 [02:27<00:11,  2.14it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  81%|████████▏ | 255/313 [02:12<00:26,  2.17it/s]Evaluating:  93%|█████████▎| 290/313 [02:27<00:10,  2.18it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  82%|████████▏ | 256/313 [02:13<00:26,  2.15it/s]Evaluating:  93%|█████████▎| 291/313 [02:28<00:10,  2.16it/s]Evaluating:  82%|████████▏ | 257/313 [02:13<00:26,  2.15it/s]Evaluating:  93%|█████████▎| 292/313 [02:28<00:09,  2.15it/s]Evaluating:  82%|████████▏ | 258/313 [02:14<00:25,  2.16it/s]Evaluating:  94%|█████████▎| 293/313 [02:29<00:09,  2.17it/s]Evaluating:  83%|████████▎ | 259/313 [02:14<00:24,  2.17it/s]Evaluating:  94%|█████████▍| 294/313 [02:29<00:08,  2.17it/s]Evaluating:  83%|████████▎ | 260/313 [02:15<00:24,  2.21it/s]Evaluating:  94%|█████████▍| 295/313 [02:30<00:08,  2.07it/s]Evaluating:  83%|████████▎ | 261/313 [02:15<00:23,  2.17it/s]Evaluating:  84%|████████▎ | 262/313 [02:15<00:21,  2.39it/s]Evaluating:  95%|█████████▍| 296/313 [02:30<00:09,  1.84it/s]Evaluating:  84%|████████▍ | 263/313 [02:16<00:21,  2.30it/s]Evaluating:  95%|█████████▍| 297/313 [02:31<00:08,  1.88it/s]Evaluating:  84%|████████▍ | 264/313 [02:16<00:21,  2.29it/s]Evaluating:  95%|█████████▌| 298/313 [02:31<00:08,  1.81it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  85%|████████▍ | 265/313 [02:17<00:20,  2.37it/s]10/10/2021 15:19:14 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:19:14 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  96%|█████████▌| 299/313 [02:32<00:07,  1.91it/s]Evaluating:  85%|████████▍ | 266/313 [02:17<00:19,  2.36it/s]Evaluating:  85%|████████▌ | 267/313 [02:18<00:19,  2.36it/s]Evaluating:  96%|█████████▌| 300/313 [02:32<00:07,  1.83it/s]Evaluating:  86%|████████▌ | 268/313 [02:18<00:20,  2.24it/s]Evaluating:  96%|█████████▌| 301/313 [02:33<00:06,  1.88it/s]Evaluating:  86%|████████▌ | 269/313 [02:19<00:19,  2.24it/s]Evaluating:  96%|█████████▋| 302/313 [02:34<00:06,  1.80it/s]Evaluating:  86%|████████▋ | 270/313 [02:19<00:18,  2.34it/s]Evaluating:  97%|█████████▋| 303/313 [02:34<00:05,  1.86it/s]Evaluating:  87%|████████▋ | 271/313 [02:19<00:18,  2.29it/s]Evaluating:  87%|████████▋ | 272/313 [02:20<00:17,  2.30it/s]Evaluating:  97%|█████████▋| 304/313 [02:35<00:05,  1.78it/s]Evaluating:  87%|████████▋ | 273/313 [02:20<00:17,  2.28it/s]Evaluating:  97%|█████████▋| 305/313 [02:35<00:04,  1.87it/s]Evaluating:  88%|████████▊ | 274/313 [02:21<00:17,  2.28it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  98%|█████████▊| 306/313 [02:36<00:03,  1.81it/s]Evaluating:  88%|████████▊ | 275/313 [02:21<00:16,  2.37it/s]Evaluating:  98%|█████████▊| 307/313 [02:36<00:03,  1.89it/s]Evaluating:  88%|████████▊ | 276/313 [02:22<00:15,  2.39it/s]Evaluating:  88%|████████▊ | 277/313 [02:22<00:15,  2.35it/s]Evaluating:  98%|█████████▊| 308/313 [02:37<00:02,  1.74it/s]Evaluating:  89%|████████▉ | 278/313 [02:22<00:14,  2.38it/s]Evaluating:  99%|█████████▊| 309/313 [02:37<00:02,  1.81it/s]Evaluating:  89%|████████▉ | 279/313 [02:23<00:14,  2.32it/s]Evaluating:  99%|█████████▉| 310/313 [02:38<00:01,  1.78it/s]Evaluating:  89%|████████▉ | 280/313 [02:23<00:13,  2.44it/s]Evaluating:  99%|█████████▉| 311/313 [02:39<00:01,  1.77it/s]Evaluating:  90%|████████▉ | 281/313 [02:24<00:14,  2.18it/s]Evaluating: 100%|█████████▉| 312/313 [02:39<00:00,  1.85it/s]Evaluating:  90%|█████████ | 282/313 [02:24<00:14,  2.15it/s]Evaluating:  90%|█████████ | 283/313 [02:25<00:13,  2.30it/s]Evaluating: 100%|██████████| 313/313 [02:39<00:00,  1.98it/s]Evaluating: 100%|██████████| 313/313 [02:39<00:00,  1.96it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  91%|█████████ | 284/313 [02:25<00:11,  2.52it/s]Evaluating:  91%|█████████ | 285/313 [02:25<00:10,  2.68it/s]Evaluating:  91%|█████████▏| 286/313 [02:25<00:09,  2.92it/s]Evaluating:  92%|█████████▏| 287/313 [02:26<00:08,  3.01it/s]
10/10/2021 15:19:23 - INFO - __main__ -   ***** Evaluation result  in da *****
10/10/2021 15:19:23 - INFO - __main__ -     f1 = 0.7474085043367886
10/10/2021 15:19:23 - INFO - __main__ -     loss = 0.25417487056681903
10/10/2021 15:19:23 - INFO - __main__ -     precision = 0.7227905073649754
10/10/2021 15:19:23 - INFO - __main__ -     recall = 0.7737625930792816
Evaluating:  92%|█████████▏| 288/313 [02:26<00:08,  3.08it/s]10/10/2021 15:19:23 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  92%|█████████▏| 289/313 [02:26<00:07,  3.13it/s]Evaluating:  93%|█████████▎| 290/313 [02:27<00:07,  2.99it/s]Evaluating:  93%|█████████▎| 291/313 [02:27<00:07,  3.06it/s]Evaluating:  93%|█████████▎| 292/313 [02:27<00:06,  3.11it/s]Evaluating:  94%|█████████▎| 293/313 [02:28<00:06,  3.26it/s]Evaluating:  94%|█████████▍| 294/313 [02:28<00:05,  3.26it/s]Evaluating:  94%|█████████▍| 295/313 [02:28<00:05,  3.28it/s]Evaluating:  95%|█████████▍| 296/313 [02:29<00:05,  3.28it/s]Evaluating:  95%|█████████▍| 297/313 [02:29<00:04,  3.33it/s]Evaluating:  95%|█████████▌| 298/313 [02:29<00:04,  3.34it/s]Evaluating:  96%|█████████▌| 299/313 [02:29<00:04,  3.33it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  96%|█████████▌| 300/313 [02:30<00:03,  3.35it/s]Evaluating:  96%|█████████▌| 301/313 [02:30<00:03,  3.33it/s]Evaluating:  96%|█████████▋| 302/313 [02:30<00:03,  3.32it/s]Evaluating:  97%|█████████▋| 303/313 [02:31<00:03,  3.30it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  97%|█████████▋| 304/313 [02:31<00:02,  3.32it/s]Evaluating:  97%|█████████▋| 305/313 [02:31<00:02,  3.32it/s]Evaluating:  98%|█████████▊| 306/313 [02:32<00:02,  3.32it/s]Evaluating:  98%|█████████▊| 307/313 [02:32<00:01,  3.47it/s]Evaluating:  98%|█████████▊| 308/313 [02:32<00:01,  3.41it/s]Evaluating:  99%|█████████▊| 309/313 [02:32<00:01,  3.35it/s]Evaluating:  99%|█████████▉| 310/313 [02:33<00:00,  3.31it/s]Evaluating:  99%|█████████▉| 311/313 [02:33<00:00,  3.27it/s]Evaluating: 100%|█████████▉| 312/313 [02:33<00:00,  3.23it/s]Evaluating: 100%|██████████| 313/313 [02:34<00:00,  3.62it/s]Evaluating: 100%|██████████| 313/313 [02:34<00:00,  2.03it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052

10/10/2021 15:19:32 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/10/2021 15:19:32 - INFO - __main__ -     f1 = 0.5622837370242214
10/10/2021 15:19:32 - INFO - __main__ -     loss = 0.4726919701781136
10/10/2021 15:19:32 - INFO - __main__ -     precision = 0.5248368968412893
10/10/2021 15:19:32 - INFO - __main__ -     recall = 0.6054847604143379
10/10/2021 15:19:32 - INFO - __main__ -   Language adapter for bg not found, using ru instead
10/10/2021 15:19:32 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:19:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/10/2021 15:19:33 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/10/2021 15:19:33 - INFO - __main__ -     Num examples = 10004
10/10/2021 15:19:33 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:30,  3.46it/s]Evaluating:   1%|          | 2/313 [00:00<01:30,  3.44it/s]Evaluating:   1%|          | 3/313 [00:00<01:28,  3.49it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:30,  3.42it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:29,  3.45it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:30,  3.40it/s]Evaluating:   2%|▏         | 7/313 [00:02<01:29,  3.43it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:29,  3.40it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:28,  3.44it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:   3%|▎         | 10/313 [00:02<01:29,  3.39it/s]Evaluating:   4%|▎         | 11/313 [00:03<01:28,  3.43it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:28,  3.40it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:27,  3.43it/s]Evaluating:   4%|▍         | 14/313 [00:04<01:28,  3.39it/s]Evaluating:   5%|▍         | 15/313 [00:04<01:26,  3.44it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:27,  3.40it/s]Evaluating:   5%|▌         | 17/313 [00:05<01:30,  3.29it/s]Evaluating:   6%|▌         | 18/313 [00:05<01:28,  3.35it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:28,  3.33it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:26,  3.37it/s]Evaluating:   7%|▋         | 21/313 [00:06<01:27,  3.35it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:25,  3.39it/s]Evaluating:   7%|▋         | 23/313 [00:06<01:26,  3.36it/s]Evaluating:   8%|▊         | 24/313 [00:07<01:25,  3.39it/s]Evaluating:   8%|▊         | 25/313 [00:07<01:25,  3.36it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:24,  3.40it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:24,  3.37it/s]Evaluating:   9%|▉         | 28/313 [00:08<01:23,  3.40it/s]Evaluating:   9%|▉         | 29/313 [00:08<01:24,  3.37it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:23,  3.41it/s]Evaluating:  10%|▉         | 31/313 [00:09<01:23,  3.37it/s]Evaluating:  10%|█         | 32/313 [00:09<01:22,  3.40it/s]Evaluating:  11%|█         | 33/313 [00:09<01:22,  3.39it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  11%|█         | 34/313 [00:10<01:22,  3.38it/s]Evaluating:  11%|█         | 35/313 [00:10<01:21,  3.41it/s]Evaluating:  12%|█▏        | 36/313 [00:10<01:22,  3.37it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:20,  3.42it/s]Evaluating:  12%|█▏        | 38/313 [00:11<01:21,  3.38it/s]Evaluating:  12%|█▏        | 39/313 [00:11<01:20,  3.41it/s]Evaluating:  13%|█▎        | 40/313 [00:11<01:20,  3.38it/s]Evaluating:  13%|█▎        | 41/313 [00:12<01:19,  3.41it/s]Evaluating:  13%|█▎        | 42/313 [00:12<01:20,  3.38it/s]Evaluating:  14%|█▎        | 43/313 [00:12<01:19,  3.41it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  14%|█▍        | 44/313 [00:12<01:20,  3.36it/s]10/10/2021 15:19:47 - INFO - __main__ -   Using lang2id = None
Evaluating:  14%|█▍        | 45/313 [00:13<01:18,  3.41it/s]Evaluating:  15%|█▍        | 46/313 [00:13<01:18,  3.38it/s]Evaluating:  15%|█▌        | 47/313 [00:13<01:18,  3.39it/s]Evaluating:  15%|█▌        | 48/313 [00:14<01:18,  3.37it/s]Evaluating:  16%|█▌        | 49/313 [00:14<01:17,  3.41it/s]Evaluating:  16%|█▌        | 50/313 [00:14<01:16,  3.42it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  16%|█▋        | 51/313 [00:15<01:17,  3.39it/s]10/10/2021 15:19:49 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:19:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:19:49 - INFO - __main__ -   Seed = 42
10/10/2021 15:19:49 - INFO - root -   save model
10/10/2021 15:19:49 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:19:49 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  17%|█▋        | 52/313 [00:15<01:17,  3.36it/s]Evaluating:  17%|█▋        | 53/313 [00:15<01:16,  3.40it/s]Evaluating:  17%|█▋        | 54/313 [00:15<01:15,  3.42it/s]Evaluating:  18%|█▊        | 55/313 [00:16<01:16,  3.39it/s]Evaluating:  18%|█▊        | 56/313 [00:16<01:15,  3.40it/s]Evaluating:  18%|█▊        | 57/313 [00:16<01:15,  3.40it/s]Evaluating:  19%|█▊        | 58/313 [00:17<01:14,  3.42it/s]Evaluating:  19%|█▉        | 59/313 [00:17<01:15,  3.38it/s]Evaluating:  19%|█▉        | 60/313 [00:17<01:14,  3.41it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  19%|█▉        | 61/313 [00:17<01:14,  3.38it/s]10/10/2021 15:19:52 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:19:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  20%|█▉        | 62/313 [00:18<01:13,  3.41it/s]Evaluating:  20%|██        | 63/313 [00:18<01:14,  3.37it/s]Evaluating:  20%|██        | 64/313 [00:18<01:13,  3.41it/s]Evaluating:  21%|██        | 65/313 [00:19<01:13,  3.38it/s]Evaluating:  21%|██        | 66/313 [00:19<01:13,  3.38it/s]Evaluating:  21%|██▏       | 67/313 [00:19<01:13,  3.35it/s]Evaluating:  22%|██▏       | 68/313 [00:20<01:12,  3.39it/s]Evaluating:  22%|██▏       | 69/313 [00:20<01:11,  3.43it/s]Evaluating:  22%|██▏       | 70/313 [00:20<01:11,  3.38it/s]Evaluating:  23%|██▎       | 71/313 [00:20<01:12,  3.35it/s]Evaluating:  23%|██▎       | 72/313 [00:21<01:11,  3.39it/s]Evaluating:  23%|██▎       | 73/313 [00:21<01:10,  3.41it/s]Evaluating:  24%|██▎       | 74/313 [00:21<01:10,  3.37it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  24%|██▍       | 75/313 [00:22<01:10,  3.39it/s]Evaluating:  24%|██▍       | 76/313 [00:22<01:10,  3.37it/s]Evaluating:  25%|██▍       | 77/313 [00:22<01:09,  3.40it/s]Evaluating:  25%|██▍       | 78/313 [00:23<01:10,  3.35it/s]Evaluating:  25%|██▌       | 79/313 [00:23<01:09,  3.39it/s]Evaluating:  26%|██▌       | 80/313 [00:23<01:08,  3.38it/s]Evaluating:  26%|██▌       | 81/313 [00:23<01:09,  3.36it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  26%|██▌       | 82/313 [00:24<01:09,  3.33it/s]Evaluating:  27%|██▋       | 83/313 [00:24<01:08,  3.37it/s]Evaluating:  27%|██▋       | 84/313 [00:24<01:07,  3.39it/s]Evaluating:  27%|██▋       | 85/313 [00:25<01:08,  3.33it/s]Evaluating:  27%|██▋       | 86/313 [00:25<01:08,  3.32it/s]Evaluating:  28%|██▊       | 87/313 [00:25<01:07,  3.37it/s]Evaluating:  28%|██▊       | 88/313 [00:25<01:06,  3.39it/s]Evaluating:  28%|██▊       | 89/313 [00:26<01:06,  3.35it/s]Evaluating:  29%|██▉       | 90/313 [00:26<01:06,  3.37it/s]Evaluating:  29%|██▉       | 91/313 [00:26<01:05,  3.38it/s]Evaluating:  29%|██▉       | 92/313 [00:27<01:05,  3.37it/s]Evaluating:  30%|██▉       | 93/313 [00:27<01:05,  3.33it/s]Evaluating:  30%|███       | 94/313 [00:27<01:05,  3.36it/s]Evaluating:  30%|███       | 95/313 [00:28<01:04,  3.40it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  31%|███       | 96/313 [00:28<01:04,  3.38it/s]Evaluating:  31%|███       | 97/313 [00:28<01:04,  3.35it/s]Evaluating:  31%|███▏      | 98/313 [00:28<01:03,  3.39it/s]Evaluating:  32%|███▏      | 99/313 [00:29<01:02,  3.41it/s]Evaluating:  32%|███▏      | 100/313 [00:29<01:03,  3.36it/s]Evaluating:  32%|███▏      | 101/313 [00:29<01:03,  3.33it/s]Evaluating:  33%|███▎      | 102/313 [00:30<01:02,  3.39it/s]Evaluating:  33%|███▎      | 103/313 [00:30<01:01,  3.39it/s]Evaluating:  33%|███▎      | 104/313 [00:30<01:02,  3.35it/s]Evaluating:  34%|███▎      | 105/313 [00:31<01:01,  3.36it/s]Evaluating:  34%|███▍      | 106/313 [00:31<01:00,  3.42it/s]Evaluating:  34%|███▍      | 107/313 [00:31<01:01,  3.37it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  35%|███▍      | 108/313 [00:31<01:01,  3.34it/s]Evaluating:  35%|███▍      | 109/313 [00:32<01:00,  3.37it/s]Evaluating:  35%|███▌      | 110/313 [00:32<00:59,  3.40it/s]Evaluating:  35%|███▌      | 111/313 [00:32<01:00,  3.36it/s]Evaluating:  36%|███▌      | 112/313 [00:33<01:00,  3.34it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  36%|███▌      | 113/313 [00:33<00:58,  3.39it/s]Evaluating:  36%|███▋      | 114/313 [00:33<00:58,  3.41it/s]Evaluating:  37%|███▋      | 115/313 [00:33<00:58,  3.37it/s]Evaluating:  37%|███▋      | 116/313 [00:34<00:58,  3.34it/s]Evaluating:  37%|███▋      | 117/313 [00:34<00:57,  3.41it/s]Evaluating:  38%|███▊      | 118/313 [00:34<00:57,  3.38it/s]Evaluating:  38%|███▊      | 119/313 [00:35<00:58,  3.33it/s]Evaluating:  38%|███▊      | 120/313 [00:35<00:58,  3.31it/s]Evaluating:  39%|███▊      | 121/313 [00:35<00:56,  3.40it/s]Evaluating:  39%|███▉      | 122/313 [00:36<00:56,  3.37it/s]Evaluating:  39%|███▉      | 123/313 [00:36<00:56,  3.33it/s]Evaluating:  40%|███▉      | 124/313 [00:36<00:55,  3.38it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  40%|███▉      | 125/313 [00:36<00:55,  3.40it/s]Evaluating:  40%|████      | 126/313 [00:37<00:55,  3.36it/s]Evaluating:  41%|████      | 127/313 [00:37<00:55,  3.33it/s]Evaluating:  41%|████      | 128/313 [00:37<00:54,  3.41it/s]Evaluating:  41%|████      | 129/313 [00:38<00:54,  3.39it/s]Evaluating:  42%|████▏     | 130/313 [00:38<00:54,  3.35it/s]Evaluating:  42%|████▏     | 131/313 [00:38<00:54,  3.33it/s]Evaluating:  42%|████▏     | 132/313 [00:39<00:52,  3.44it/s]Evaluating:  42%|████▏     | 133/313 [00:39<00:53,  3.39it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  43%|████▎     | 134/313 [00:39<00:53,  3.35it/s]Evaluating:  43%|████▎     | 135/313 [00:39<00:53,  3.32it/s]Evaluating:  43%|████▎     | 136/313 [00:40<00:52,  3.39it/s]Evaluating:  44%|████▍     | 137/313 [00:40<00:52,  3.35it/s]Evaluating:  44%|████▍     | 138/313 [00:40<00:52,  3.30it/s]Evaluating:  44%|████▍     | 139/313 [00:41<00:51,  3.37it/s]Evaluating:  45%|████▍     | 140/313 [00:41<00:51,  3.35it/s]Evaluating:  45%|████▌     | 141/313 [00:41<00:51,  3.32it/s]Evaluating:  45%|████▌     | 142/313 [00:42<00:51,  3.31it/s]Evaluating:  46%|████▌     | 143/313 [00:42<00:49,  3.40it/s]Evaluating:  46%|████▌     | 144/313 [00:42<00:50,  3.36it/s]Evaluating:  46%|████▋     | 145/313 [00:42<00:50,  3.34it/s]Evaluating:  47%|████▋     | 146/313 [00:43<00:50,  3.32it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  47%|████▋     | 147/313 [00:43<00:48,  3.41it/s]10/10/2021 15:20:17 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:20:17 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:20:17 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/10/2021 15:20:17 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:20:17 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:20:17 - INFO - root -   loading lang adpater hi/wiki@ukp
10/10/2021 15:20:17 - INFO - __main__ -   Language = hi
10/10/2021 15:20:17 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
Evaluating:  47%|████▋     | 148/313 [00:43<00:49,  3.35it/s]No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:  48%|████▊     | 149/313 [00:44<00:49,  3.31it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Evaluating:  48%|████▊     | 150/313 [00:44<00:48,  3.36it/s]Evaluating:  48%|████▊     | 151/313 [00:44<00:47,  3.41it/s]Evaluating:  49%|████▊     | 152/313 [00:45<00:48,  3.31it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
Evaluating:  49%|████▉     | 153/313 [00:45<00:48,  3.30it/s]Evaluating:  49%|████▉     | 154/313 [00:45<00:47,  3.38it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  50%|████▉     | 155/313 [00:45<00:47,  3.33it/s]Evaluating:  50%|████▉     | 156/313 [00:46<00:47,  3.30it/s]Evaluating:  50%|█████     | 157/313 [00:46<00:47,  3.28it/s]Evaluating:  50%|█████     | 158/313 [00:46<00:45,  3.38it/s]Evaluating:  51%|█████     | 159/313 [00:47<00:46,  3.33it/s]Evaluating:  51%|█████     | 160/313 [00:47<00:46,  3.30it/s]Evaluating:  51%|█████▏    | 161/313 [00:47<00:45,  3.33it/s]Evaluating:  52%|█████▏    | 162/313 [00:48<00:45,  3.35it/s]Evaluating:  52%|█████▏    | 163/313 [00:48<00:45,  3.29it/s]Evaluating:  52%|█████▏    | 164/313 [00:48<00:45,  3.27it/s]Evaluating:  53%|█████▎    | 165/313 [00:48<00:44,  3.34it/s]Evaluating:  53%|█████▎    | 166/313 [00:49<00:44,  3.33it/s]Evaluating:  53%|█████▎    | 167/313 [00:49<00:44,  3.30it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:20:23 - INFO - __main__ -   Using lang2id = None
Evaluating:  54%|█████▎    | 168/313 [00:49<00:44,  3.28it/s]Evaluating:  54%|█████▍    | 169/313 [00:50<00:42,  3.38it/s]Evaluating:  54%|█████▍    | 170/313 [00:50<00:42,  3.33it/s]Evaluating:  55%|█████▍    | 171/313 [00:50<00:43,  3.27it/s]Evaluating:  55%|█████▍    | 172/313 [00:51<00:42,  3.31it/s]Evaluating:  55%|█████▌    | 173/313 [00:51<00:42,  3.33it/s]Evaluating:  56%|█████▌    | 174/313 [00:51<00:42,  3.29it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  56%|█████▌    | 175/313 [00:51<00:42,  3.27it/s]10/10/2021 15:20:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:20:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:20:26 - INFO - __main__ -   Seed = 22
10/10/2021 15:20:26 - INFO - root -   save model
10/10/2021 15:20:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:20:26 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  56%|█████▌    | 176/313 [00:52<00:41,  3.34it/s]Evaluating:  57%|█████▋    | 177/313 [00:52<00:41,  3.31it/s]Evaluating:  57%|█████▋    | 178/313 [00:52<00:41,  3.27it/s]Evaluating:  57%|█████▋    | 179/313 [00:53<00:41,  3.26it/s]10/10/2021 15:20:27 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/10/2021 15:20:27 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:20:27 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
Evaluating:  58%|█████▊    | 180/313 [00:53<00:47,  2.79it/s]10/10/2021 15:20:27 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/10/2021 15:20:27 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:20:27 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  58%|█████▊    | 181/313 [00:54<00:51,  2.58it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.11it/s]Evaluating:  58%|█████▊    | 182/313 [00:54<00:54,  2.42it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:14,  2.13it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  58%|█████▊    | 183/313 [00:55<00:54,  2.38it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:13,  2.21it/s]Evaluating:  59%|█████▉    | 184/313 [00:55<00:56,  2.29it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:12,  2.18it/s]Evaluating:  59%|█████▉    | 185/313 [00:55<00:57,  2.23it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:12,  2.17it/s]Evaluating:  59%|█████▉    | 186/313 [00:56<00:57,  2.20it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:12,  2.16it/s]Evaluating:  60%|█████▉    | 187/313 [00:56<00:56,  2.24it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:11,  2.22it/s]Evaluating:  60%|██████    | 188/313 [00:57<00:56,  2.21it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:10,  2.20it/s]Evaluating:  60%|██████    | 189/313 [00:57<00:56,  2.18it/s]Evaluating:  28%|██▊       | 9/32 [00:04<00:10,  2.18it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:09,  2.21it/s]Evaluating:  61%|██████    | 190/313 [00:58<00:56,  2.20it/s]Evaluating:  34%|███▍      | 11/32 [00:05<00:09,  2.13it/s]Evaluating:  61%|██████    | 191/313 [00:58<00:57,  2.13it/s]Evaluating:  38%|███▊      | 12/32 [00:05<00:09,  2.14it/s]Evaluating:  61%|██████▏   | 192/313 [00:59<00:57,  2.12it/s]Evaluating:  41%|████      | 13/32 [00:05<00:08,  2.15it/s]Evaluating:  62%|██████▏   | 193/313 [00:59<00:56,  2.12it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  44%|████▍     | 14/32 [00:06<00:08,  2.20it/s]Evaluating:  62%|██████▏   | 194/313 [01:00<00:55,  2.16it/s]Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.18it/s]Evaluating:  62%|██████▏   | 195/313 [01:00<00:54,  2.15it/s]Evaluating:  50%|█████     | 16/32 [00:07<00:07,  2.17it/s]Evaluating:  63%|██████▎   | 196/313 [01:01<00:54,  2.14it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.18it/s]Evaluating:  63%|██████▎   | 197/313 [01:01<00:53,  2.16it/s]Evaluating:  56%|█████▋    | 18/32 [00:08<00:06,  2.21it/s]Evaluating:  63%|██████▎   | 198/313 [01:02<00:53,  2.17it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:05,  2.19it/s]Evaluating:  64%|██████▎   | 199/313 [01:02<00:52,  2.15it/s]Evaluating:  62%|██████▎   | 20/32 [00:09<00:05,  2.18it/s]Evaluating:  64%|██████▍   | 200/313 [01:02<00:52,  2.14it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:05,  2.19it/s]Evaluating:  64%|██████▍   | 201/313 [01:03<00:51,  2.19it/s]Evaluating:  65%|██████▍   | 202/313 [01:03<00:50,  2.18it/s]Evaluating:  69%|██████▉   | 22/32 [00:10<00:04,  2.09it/s]Evaluating:  72%|███████▏  | 23/32 [00:10<00:04,  2.10it/s]Evaluating:  65%|██████▍   | 203/313 [01:04<00:51,  2.15it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  75%|███████▌  | 24/32 [00:11<00:03,  2.12it/s]Evaluating:  65%|██████▌   | 204/313 [01:04<00:50,  2.14it/s]Evaluating:  78%|███████▊  | 25/32 [00:11<00:03,  2.18it/s]Evaluating:  65%|██████▌   | 205/313 [01:05<00:49,  2.17it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.17it/s]Evaluating:  66%|██████▌   | 206/313 [01:05<00:49,  2.15it/s]Evaluating:  84%|████████▍ | 27/32 [00:12<00:02,  2.16it/s]Evaluating:  66%|██████▌   | 207/313 [01:06<00:49,  2.14it/s]Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.18it/s]Evaluating:  66%|██████▋   | 208/313 [01:06<00:48,  2.15it/s]Evaluating:  91%|█████████ | 29/32 [00:13<00:01,  2.19it/s]Evaluating:  67%|██████▋   | 209/313 [01:07<00:47,  2.17it/s]Evaluating:  94%|█████████▍| 30/32 [00:13<00:00,  2.18it/s]Evaluating:  67%|██████▋   | 210/313 [01:07<00:47,  2.15it/s]Evaluating:  97%|█████████▋| 31/32 [00:14<00:00,  2.17it/s]Evaluating:  67%|██████▋   | 211/313 [01:08<00:47,  2.16it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.37it/s]Evaluating:  68%|██████▊   | 212/313 [01:08<00:42,  2.38it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.18it/s]
10/10/2021 15:20:42 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/10/2021 15:20:42 - INFO - __main__ -     f1 = 0.4220779220779221
10/10/2021 15:20:42 - INFO - __main__ -     loss = 4.491160623729229
10/10/2021 15:20:42 - INFO - __main__ -     precision = 0.42642924086223055
10/10/2021 15:20:42 - INFO - __main__ -     recall = 0.41781450872359965
10/10/2021 15:20:42 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/10/2021 15:20:42 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:20:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/10/2021 15:20:42 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/10/2021 15:20:42 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:20:42 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  68%|██████▊   | 213/313 [01:08<00:39,  2.55it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.15it/s]Evaluating:  68%|██████▊   | 214/313 [01:09<00:41,  2.41it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:   6%|▋         | 2/32 [00:00<00:13,  2.15it/s]Evaluating:  69%|██████▊   | 215/313 [01:09<00:42,  2.32it/s]10/10/2021 15:20:43 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:20:43 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:20:43 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/10/2021 15:20:43 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:20:43 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:20:43 - INFO - root -   loading lang adpater is/wiki@ukp
10/10/2021 15:20:43 - INFO - __main__ -   Language = is
10/10/2021 15:20:43 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:   9%|▉         | 3/32 [00:01<00:13,  2.23it/s]Evaluating:  69%|██████▉   | 216/313 [01:10<00:42,  2.29it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Evaluating:  12%|█▎        | 4/32 [00:01<00:12,  2.19it/s]Evaluating:  69%|██████▉   | 217/313 [01:10<00:42,  2.23it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Evaluating:  16%|█▌        | 5/32 [00:02<00:12,  2.17it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
Evaluating:  70%|██████▉   | 218/313 [01:11<00:43,  2.20it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:11,  2.19it/s]Evaluating:  70%|██████▉   | 219/313 [01:11<00:43,  2.18it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:11,  2.19it/s]Evaluating:  70%|███████   | 220/313 [01:11<00:42,  2.18it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:11,  2.17it/s]Evaluating:  71%|███████   | 221/313 [01:12<00:42,  2.15it/s]Evaluating:  28%|██▊       | 9/32 [00:04<00:10,  2.16it/s]Evaluating:  71%|███████   | 222/313 [01:12<00:42,  2.13it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:10,  2.17it/s]Evaluating:  71%|███████   | 223/313 [01:13<00:41,  2.16it/s]Evaluating:  34%|███▍      | 11/32 [00:05<00:09,  2.19it/s]Evaluating:  72%|███████▏  | 224/313 [01:13<00:41,  2.13it/s]Evaluating:  38%|███▊      | 12/32 [00:05<00:09,  2.18it/s]Evaluating:  72%|███████▏  | 225/313 [01:14<00:41,  2.12it/s]Evaluating:  41%|████      | 13/32 [00:05<00:08,  2.17it/s]Evaluating:  72%|███████▏  | 226/313 [01:14<00:40,  2.13it/s]Evaluating:  44%|████▍     | 14/32 [00:06<00:08,  2.20it/s]Evaluating:  73%|███████▎  | 227/313 [01:15<00:40,  2.14it/s]Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.19it/s]Evaluating:  73%|███████▎  | 228/313 [01:15<00:40,  2.12it/s]Evaluating:  50%|█████     | 16/32 [00:07<00:07,  2.17it/s]Evaluating:  73%|███████▎  | 229/313 [01:16<00:39,  2.10it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.18it/s]Evaluating:  73%|███████▎  | 230/313 [01:16<00:38,  2.14it/s]Evaluating:  56%|█████▋    | 18/32 [00:08<00:06,  2.20it/s]Evaluating:  74%|███████▍  | 231/313 [01:17<00:38,  2.14it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:05,  2.18it/s]Evaluating:  74%|███████▍  | 232/313 [01:17<00:38,  2.13it/s]Evaluating:  62%|██████▎   | 20/32 [00:09<00:05,  2.17it/s]Evaluating:  74%|███████▍  | 233/313 [01:18<00:37,  2.11it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:05,  2.18it/s]Evaluating:  75%|███████▍  | 234/313 [01:18<00:36,  2.15it/s]Evaluating:  69%|██████▉   | 22/32 [00:10<00:04,  2.27it/s]Evaluating:  72%|███████▏  | 23/32 [00:10<00:04,  2.20it/s]Evaluating:  75%|███████▌  | 235/313 [01:19<00:39,  1.96it/s]Evaluating:  75%|███████▌  | 24/32 [00:10<00:03,  2.19it/s]Evaluating:  75%|███████▌  | 236/313 [01:19<00:38,  2.01it/s]Evaluating:  78%|███████▊  | 25/32 [00:11<00:03,  2.23it/s]Evaluating:  76%|███████▌  | 237/313 [01:20<00:36,  2.07it/s]Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.20it/s]Evaluating:  76%|███████▌  | 238/313 [01:20<00:36,  2.08it/s]Evaluating:  84%|████████▍ | 27/32 [00:12<00:02,  2.18it/s]Evaluating:  76%|███████▋  | 239/313 [01:21<00:35,  2.08it/s]10/10/2021 15:20:55 - INFO - __main__ -   Language adapter for fo not found, using is instead
10/10/2021 15:20:55 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:20:55 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/10/2021 15:20:55 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/10/2021 15:20:55 - INFO - __main__ -     Num examples = 100
10/10/2021 15:20:55 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.18it/s]Evaluating:  77%|███████▋  | 240/313 [01:21<00:35,  2.06it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:01,  1.63it/s]Evaluating:  91%|█████████ | 29/32 [00:13<00:01,  1.98it/s]Evaluating:  77%|███████▋  | 241/313 [01:22<00:37,  1.90it/s]Evaluating:  50%|█████     | 2/4 [00:01<00:01,  1.61it/s]Evaluating:  94%|█████████▍| 30/32 [00:14<00:01,  1.85it/s]Evaluating:  77%|███████▋  | 242/313 [01:22<00:39,  1.79it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  1.60it/s]Evaluating:  97%|█████████▋| 31/32 [00:14<00:00,  1.77it/s]Evaluating:  78%|███████▊  | 243/313 [01:23<00:39,  1.76it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s]Evaluating: 100%|██████████| 4/4 [00:02<00:00,  1.83it/s]
10/10/2021 15:20:57 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/10/2021 15:20:57 - INFO - __main__ -     f1 = 0.6028368794326241
10/10/2021 15:20:57 - INFO - __main__ -     loss = 1.1976349204778671
10/10/2021 15:20:57 - INFO - __main__ -     precision = 0.5246913580246914
10/10/2021 15:20:57 - INFO - __main__ -     recall = 0.7083333333333334
10/10/2021 15:20:57 - INFO - __main__ -   Language adapter for no not found, using is instead
10/10/2021 15:20:57 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:20:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
Evaluating: 100%|██████████| 32/32 [00:15<00:00,  1.98it/s]Evaluating: 100%|██████████| 32/32 [00:15<00:00,  2.13it/s]Evaluating:  78%|███████▊  | 244/313 [01:23<00:34,  2.00it/s]
10/10/2021 15:20:57 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/10/2021 15:20:57 - INFO - __main__ -     f1 = 0.4320862824401753
10/10/2021 15:20:57 - INFO - __main__ -     loss = 2.3186681754887104
10/10/2021 15:20:57 - INFO - __main__ -     precision = 0.3763945977686436
10/10/2021 15:20:57 - INFO - __main__ -     recall = 0.507120253164557
10/10/2021 15:20:57 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/10/2021 15:20:57 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:20:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/10/2021 15:20:57 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/10/2021 15:20:57 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:20:57 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  78%|███████▊  | 245/313 [01:24<00:31,  2.16it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.16it/s]Evaluating:  79%|███████▊  | 246/313 [01:24<00:31,  2.14it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:13,  2.16it/s]10/10/2021 15:20:58 - INFO - __main__ -   ***** Running evaluation  in no *****
10/10/2021 15:20:58 - INFO - __main__ -     Num examples = 10000
10/10/2021 15:20:58 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  79%|███████▉  | 247/313 [01:25<00:30,  2.14it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:14,  2.02it/s]Evaluating:   0%|          | 1/313 [00:00<03:07,  1.67it/s]Evaluating:  79%|███████▉  | 248/313 [01:25<00:33,  1.96it/s]Evaluating:  12%|█▎        | 4/32 [00:02<00:15,  1.83it/s]Evaluating:   1%|          | 2/313 [00:01<03:11,  1.62it/s]Evaluating:  80%|███████▉  | 249/313 [01:26<00:35,  1.82it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:15,  1.73it/s]Evaluating:   1%|          | 3/313 [00:01<03:13,  1.60it/s]Evaluating:  80%|███████▉  | 250/313 [01:26<00:36,  1.74it/s]Evaluating:  19%|█▉        | 6/32 [00:03<00:15,  1.68it/s]Evaluating:   1%|▏         | 4/313 [00:02<03:11,  1.62it/s]Evaluating:  80%|████████  | 251/313 [01:27<00:36,  1.70it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:14,  1.68it/s]Evaluating:   2%|▏         | 5/313 [00:03<03:09,  1.62it/s]Evaluating:  81%|████████  | 252/313 [01:28<00:36,  1.67it/s]Evaluating:  25%|██▌       | 8/32 [00:04<00:14,  1.65it/s]Evaluating:   2%|▏         | 6/313 [00:03<03:10,  1.61it/s]Evaluating:  81%|████████  | 253/313 [01:28<00:36,  1.64it/s]Evaluating:  28%|██▊       | 9/32 [00:05<00:14,  1.63it/s]Evaluating:   2%|▏         | 7/313 [00:04<03:10,  1.61it/s]Evaluating:  81%|████████  | 254/313 [01:29<00:36,  1.61it/s]Evaluating:  31%|███▏      | 10/32 [00:05<00:13,  1.64it/s]Evaluating:   3%|▎         | 8/313 [00:04<03:06,  1.64it/s]Evaluating:  81%|████████▏ | 255/313 [01:30<00:35,  1.63it/s]Evaluating:  34%|███▍      | 11/32 [00:06<00:12,  1.65it/s]Evaluating:   3%|▎         | 9/313 [00:05<03:03,  1.66it/s]Evaluating:  82%|████████▏ | 256/313 [01:30<00:34,  1.63it/s]Evaluating:  38%|███▊      | 12/32 [00:07<00:12,  1.55it/s]Evaluating:   3%|▎         | 10/313 [00:06<03:05,  1.64it/s]Evaluating:  82%|████████▏ | 257/313 [01:31<00:34,  1.61it/s]Evaluating:  41%|████      | 13/32 [00:07<00:12,  1.56it/s]Evaluating:   4%|▎         | 11/313 [00:06<03:06,  1.62it/s]Evaluating:  82%|████████▏ | 258/313 [01:31<00:34,  1.61it/s]Evaluating:  44%|████▍     | 14/32 [00:08<00:11,  1.60it/s]Evaluating:   4%|▍         | 12/313 [00:07<03:03,  1.64it/s]Evaluating:  83%|████████▎ | 259/313 [01:32<00:33,  1.61it/s]Evaluating:  47%|████▋     | 15/32 [00:08<00:10,  1.60it/s]Evaluating:   4%|▍         | 13/313 [00:07<03:04,  1.62it/s]Evaluating:  83%|████████▎ | 260/313 [01:33<00:33,  1.60it/s]Evaluating:  50%|█████     | 16/32 [00:09<00:10,  1.59it/s]Evaluating:   4%|▍         | 14/313 [00:08<03:05,  1.61it/s]Evaluating:  83%|████████▎ | 261/313 [01:33<00:32,  1.59it/s]Evaluating:  53%|█████▎    | 17/32 [00:10<00:09,  1.60it/s]Evaluating:   5%|▍         | 15/313 [00:09<03:04,  1.62it/s]Evaluating:  84%|████████▎ | 262/313 [01:34<00:31,  1.60it/s]Evaluating:  56%|█████▋    | 18/32 [00:10<00:08,  1.62it/s]Evaluating:   5%|▌         | 16/313 [00:09<03:02,  1.62it/s]Evaluating:  84%|████████▍ | 263/313 [01:35<00:31,  1.60it/s]Evaluating:  59%|█████▉    | 19/32 [00:11<00:08,  1.61it/s]Evaluating:   5%|▌         | 17/313 [00:10<03:01,  1.63it/s]Evaluating:  84%|████████▍ | 264/313 [01:35<00:29,  1.65it/s]Evaluating:  62%|██████▎   | 20/32 [00:12<00:07,  1.51it/s]Evaluating:   6%|▌         | 18/313 [00:11<03:05,  1.59it/s]Evaluating:  85%|████████▍ | 265/313 [01:36<00:29,  1.63it/s]Evaluating:   6%|▌         | 19/313 [00:11<03:01,  1.62it/s]Evaluating:  66%|██████▌   | 21/32 [00:12<00:07,  1.56it/s]Evaluating:  85%|████████▍ | 266/313 [01:36<00:28,  1.64it/s]Evaluating:   6%|▋         | 20/313 [00:12<03:01,  1.61it/s]Evaluating:  69%|██████▉   | 22/32 [00:13<00:06,  1.57it/s]Evaluating:  85%|████████▌ | 267/313 [01:37<00:28,  1.62it/s]Evaluating:   7%|▋         | 21/313 [00:12<03:01,  1.61it/s]Evaluating:  72%|███████▏  | 23/32 [00:14<00:05,  1.58it/s]Evaluating:  86%|████████▌ | 268/313 [01:38<00:28,  1.60it/s]Evaluating:  75%|███████▌  | 24/32 [00:14<00:05,  1.58it/s]Evaluating:   7%|▋         | 22/313 [00:13<03:01,  1.60it/s]Evaluating:  86%|████████▌ | 269/313 [01:38<00:27,  1.60it/s]Evaluating:  78%|███████▊  | 25/32 [00:15<00:04,  1.61it/s]Evaluating:   7%|▋         | 23/313 [00:14<02:58,  1.62it/s]Evaluating:  86%|████████▋ | 270/313 [01:39<00:26,  1.61it/s]Evaluating:   8%|▊         | 24/313 [00:14<02:59,  1.61it/s]Evaluating:  81%|████████▏ | 26/32 [00:15<00:03,  1.60it/s]Evaluating:  87%|████████▋ | 271/313 [01:39<00:26,  1.60it/s]Evaluating:  84%|████████▍ | 27/32 [00:16<00:03,  1.61it/s]Evaluating:   8%|▊         | 25/313 [00:15<02:58,  1.61it/s]Evaluating:  87%|████████▋ | 272/313 [01:40<00:25,  1.60it/s]Evaluating:   8%|▊         | 26/313 [00:16<02:56,  1.63it/s]Evaluating:  88%|████████▊ | 28/32 [00:17<00:02,  1.62it/s]Evaluating:  87%|████████▋ | 273/313 [01:41<00:24,  1.61it/s]Evaluating:   9%|▊         | 27/313 [00:16<02:55,  1.63it/s]Evaluating:  91%|█████████ | 29/32 [00:17<00:01,  1.62it/s]Evaluating:  88%|████████▊ | 274/313 [01:41<00:24,  1.60it/s]Evaluating:   9%|▉         | 28/313 [00:17<02:56,  1.62it/s]Evaluating:  94%|█████████▍| 30/32 [00:18<00:01,  1.61it/s]Evaluating:  88%|████████▊ | 275/313 [01:42<00:23,  1.59it/s]Evaluating:   9%|▉         | 29/313 [00:17<02:56,  1.61it/s]Evaluating:  97%|█████████▋| 31/32 [00:19<00:00,  1.61it/s]Evaluating:  88%|████████▊ | 276/313 [01:43<00:22,  1.61it/s]Evaluating:  10%|▉         | 30/313 [00:18<02:44,  1.72it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.71it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.64it/s]
10/10/2021 15:21:17 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/10/2021 15:21:17 - INFO - __main__ -     f1 = 0.21067548417572035
10/10/2021 15:21:17 - INFO - __main__ -     loss = 4.025712043046951
10/10/2021 15:21:17 - INFO - __main__ -     precision = 0.2545662100456621
10/10/2021 15:21:17 - INFO - __main__ -     recall = 0.17969379532634971
10/10/2021 15:21:17 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  88%|████████▊ | 277/313 [01:43<00:20,  1.74it/s]Evaluating:  10%|▉         | 31/313 [00:18<02:32,  1.85it/s]Evaluating:  89%|████████▉ | 278/313 [01:44<00:19,  1.83it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  10%|█         | 32/313 [00:19<02:25,  1.93it/s]Evaluating:  89%|████████▉ | 279/313 [01:44<00:17,  1.90it/s]Evaluating:  11%|█         | 33/313 [00:19<02:20,  1.99it/s]Evaluating:  89%|████████▉ | 280/313 [01:44<00:16,  1.98it/s]Evaluating:  11%|█         | 34/313 [00:20<02:13,  2.08it/s]Evaluating:  90%|████████▉ | 281/313 [01:45<00:15,  2.04it/s]Evaluating:  11%|█         | 35/313 [00:20<02:12,  2.10it/s]Evaluating:  90%|█████████ | 282/313 [01:45<00:15,  2.06it/s]Evaluating:  12%|█▏        | 36/313 [00:21<02:10,  2.12it/s]Evaluating:  90%|█████████ | 283/313 [01:46<00:14,  2.07it/s]Evaluating:  12%|█▏        | 37/313 [00:21<02:09,  2.14it/s]Evaluating:  91%|█████████ | 284/313 [01:46<00:13,  2.12it/s]Evaluating:  12%|█▏        | 38/313 [00:22<02:06,  2.18it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  91%|█████████ | 285/313 [01:47<00:13,  2.12it/s]Evaluating:  12%|█▏        | 39/313 [00:22<02:05,  2.18it/s]Evaluating:  91%|█████████▏| 286/313 [01:47<00:12,  2.11it/s]Evaluating:  13%|█▎        | 40/313 [00:22<02:05,  2.17it/s]Evaluating:  92%|█████████▏| 287/313 [01:48<00:12,  2.11it/s]Evaluating:  13%|█▎        | 41/313 [00:23<02:04,  2.19it/s]Evaluating:  92%|█████████▏| 288/313 [01:48<00:11,  2.15it/s]Evaluating:  13%|█▎        | 42/313 [00:23<02:02,  2.21it/s]Evaluating:  92%|█████████▏| 289/313 [01:49<00:11,  2.13it/s]Evaluating:  14%|█▎        | 43/313 [00:24<02:02,  2.20it/s]Evaluating:  93%|█████████▎| 290/313 [01:49<00:10,  2.12it/s]Evaluating:  14%|█▍        | 44/313 [00:24<02:02,  2.19it/s]Evaluating:  93%|█████████▎| 291/313 [01:50<00:10,  2.14it/s]Evaluating:  14%|█▍        | 45/313 [00:25<02:00,  2.23it/s]Evaluating:  93%|█████████▎| 292/313 [01:50<00:09,  2.14it/s]Evaluating:  15%|█▍        | 46/313 [00:25<01:59,  2.24it/s]Evaluating:  15%|█▌        | 47/313 [00:26<02:01,  2.20it/s]Evaluating:  94%|█████████▎| 293/313 [01:51<00:09,  2.06it/s]Evaluating:  15%|█▌        | 48/313 [00:26<02:01,  2.18it/s]Evaluating:  94%|█████████▍| 294/313 [01:51<00:09,  2.08it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  16%|█▌        | 49/313 [00:26<01:58,  2.23it/s]Evaluating:  94%|█████████▍| 295/313 [01:52<00:08,  2.12it/s]Evaluating:  16%|█▌        | 50/313 [00:27<01:58,  2.22it/s]Evaluating:  95%|█████████▍| 296/313 [01:52<00:08,  2.11it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  16%|█▋        | 51/313 [00:27<01:58,  2.20it/s]Evaluating:  95%|█████████▍| 297/313 [01:52<00:07,  2.09it/s]Evaluating:  17%|█▋        | 52/313 [00:28<01:57,  2.21it/s]Evaluating:  95%|█████████▌| 298/313 [01:53<00:07,  2.12it/s]Evaluating:  17%|█▋        | 53/313 [00:28<01:56,  2.23it/s]Evaluating:  96%|█████████▌| 299/313 [01:53<00:06,  2.12it/s]Evaluating:  17%|█▋        | 54/313 [00:29<01:57,  2.21it/s]Evaluating:  96%|█████████▌| 300/313 [01:54<00:06,  2.10it/s]Evaluating:  18%|█▊        | 55/313 [00:29<01:57,  2.20it/s]Evaluating:  96%|█████████▌| 301/313 [01:54<00:05,  2.10it/s]Evaluating:  18%|█▊        | 56/313 [00:30<01:55,  2.23it/s]Evaluating:  96%|█████████▋| 302/313 [01:55<00:05,  2.14it/s]Evaluating:  18%|█▊        | 57/313 [00:30<01:55,  2.22it/s]Evaluating:  97%|█████████▋| 303/313 [01:55<00:04,  2.12it/s]Evaluating:  19%|█▊        | 58/313 [00:31<01:55,  2.20it/s]Evaluating:  97%|█████████▋| 304/313 [01:56<00:04,  2.10it/s]Evaluating:  19%|█▉        | 59/313 [00:31<01:55,  2.19it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  97%|█████████▋| 305/313 [01:56<00:03,  2.12it/s]10/10/2021 15:21:30 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:21:30 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  19%|█▉        | 60/313 [00:31<01:53,  2.24it/s]Evaluating:  98%|█████████▊| 306/313 [01:57<00:03,  2.13it/s]Evaluating:  19%|█▉        | 61/313 [00:32<01:53,  2.22it/s]Evaluating:  98%|█████████▊| 307/313 [01:57<00:02,  2.11it/s]Evaluating:  20%|█▉        | 62/313 [00:32<01:53,  2.21it/s]Evaluating:  98%|█████████▊| 308/313 [01:58<00:02,  2.09it/s]Evaluating:  20%|██        | 63/313 [00:33<01:53,  2.20it/s]Evaluating:  99%|█████████▊| 309/313 [01:58<00:01,  2.13it/s]Evaluating:  20%|██        | 64/313 [00:33<01:51,  2.23it/s]Evaluating:  99%|█████████▉| 310/313 [01:59<00:01,  2.11it/s]Evaluating:  21%|██        | 65/313 [00:34<01:52,  2.21it/s]Evaluating:  99%|█████████▉| 311/313 [01:59<00:00,  2.10it/s]Evaluating:  21%|██        | 66/313 [00:34<01:52,  2.20it/s]Evaluating: 100%|█████████▉| 312/313 [02:00<00:00,  2.11it/s]Evaluating:  21%|██▏       | 67/313 [00:35<01:50,  2.23it/s]Evaluating: 100%|██████████| 313/313 [02:00<00:00,  2.36it/s]Evaluating: 100%|██████████| 313/313 [02:00<00:00,  2.60it/s]Evaluating:  22%|██▏       | 68/313 [00:35<01:45,  2.31it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  22%|██▏       | 69/313 [00:35<01:36,  2.53it/s]Evaluating:  22%|██▏       | 70/313 [00:36<01:29,  2.71it/s]Evaluating:  23%|██▎       | 71/313 [00:36<01:22,  2.94it/s]
10/10/2021 15:21:35 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/10/2021 15:21:35 - INFO - __main__ -     f1 = 0.7254029467022644
10/10/2021 15:21:35 - INFO - __main__ -     loss = 0.32790018599063825
10/10/2021 15:21:35 - INFO - __main__ -     precision = 0.6760044231478068
10/10/2021 15:21:35 - INFO - __main__ -     recall = 0.7825901429485812
Evaluating:  23%|██▎       | 72/313 [00:36<01:19,  3.02it/s]10/10/2021 15:21:35 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  23%|██▎       | 73/313 [00:37<01:17,  3.09it/s]Evaluating:  24%|██▎       | 74/313 [00:37<01:16,  3.14it/s]Evaluating:  24%|██▍       | 75/313 [00:37<01:12,  3.27it/s]Evaluating:  24%|██▍       | 76/313 [00:37<01:12,  3.28it/s]Evaluating:  25%|██▍       | 77/313 [00:38<01:12,  3.28it/s]Evaluating:  25%|██▍       | 78/313 [00:38<01:10,  3.33it/s]Evaluating:  25%|██▌       | 79/313 [00:38<01:10,  3.32it/s]Evaluating:  26%|██▌       | 80/313 [00:39<01:10,  3.31it/s]Evaluating:  26%|██▌       | 81/313 [00:39<01:10,  3.29it/s]Evaluating:  26%|██▌       | 82/313 [00:39<01:07,  3.40it/s]Evaluating:  27%|██▋       | 83/313 [00:40<01:08,  3.35it/s]Evaluating:  27%|██▋       | 84/313 [00:40<01:22,  2.78it/s]Evaluating:  27%|██▋       | 85/313 [00:40<01:18,  2.92it/s]Evaluating:  27%|██▋       | 86/313 [00:41<01:15,  3.01it/s]Evaluating:  28%|██▊       | 87/313 [00:41<01:13,  3.08it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  28%|██▊       | 88/313 [00:41<01:09,  3.22it/s]Evaluating:  28%|██▊       | 89/313 [00:42<01:09,  3.23it/s]Evaluating:  29%|██▉       | 90/313 [00:42<01:08,  3.23it/s]Evaluating:  29%|██▉       | 91/313 [00:42<01:08,  3.26it/s]Evaluating:  29%|██▉       | 92/313 [00:42<01:06,  3.32it/s]Evaluating:  30%|██▉       | 93/313 [00:43<01:06,  3.31it/s]Evaluating:  30%|███       | 94/313 [00:43<01:06,  3.29it/s]Evaluating:  30%|███       | 95/313 [00:43<01:04,  3.37it/s]Evaluating:  31%|███       | 96/313 [00:44<01:05,  3.33it/s]Evaluating:  31%|███       | 97/313 [00:44<01:05,  3.30it/s]Evaluating:  31%|███▏      | 98/313 [00:44<01:05,  3.29it/s]Evaluating:  32%|███▏      | 99/313 [00:44<01:02,  3.41it/s]Evaluating:  32%|███▏      | 100/313 [00:45<01:03,  3.35it/s]Evaluating:  32%|███▏      | 101/313 [00:45<01:03,  3.32it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  33%|███▎      | 102/313 [00:45<01:02,  3.36it/s]Evaluating:  33%|███▎      | 103/313 [00:46<01:02,  3.37it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  33%|███▎      | 104/313 [00:46<01:02,  3.33it/s]Evaluating:  34%|███▎      | 105/313 [00:46<01:03,  3.30it/s]Evaluating:  34%|███▍      | 106/313 [00:47<01:01,  3.38it/s]Evaluating:  34%|███▍      | 107/313 [00:47<01:01,  3.33it/s]Evaluating:  35%|███▍      | 108/313 [00:47<01:01,  3.31it/s]Evaluating:  35%|███▍      | 109/313 [00:48<01:02,  3.28it/s]Evaluating:  35%|███▌      | 110/313 [00:48<00:59,  3.39it/s]Evaluating:  35%|███▌      | 111/313 [00:48<01:00,  3.33it/s]Evaluating:  36%|███▌      | 112/313 [00:48<01:00,  3.30it/s]Evaluating:  36%|███▌      | 113/313 [00:49<00:59,  3.35it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  36%|███▋      | 114/313 [00:49<01:00,  3.31it/s]Evaluating:  37%|███▋      | 115/313 [00:49<01:00,  3.29it/s]Evaluating:  37%|███▋      | 116/313 [00:50<01:00,  3.27it/s]Evaluating:  37%|███▋      | 117/313 [00:50<01:06,  2.95it/s]Evaluating:  38%|███▊      | 118/313 [00:50<01:02,  3.12it/s]Evaluating:  38%|███▊      | 119/313 [00:51<00:58,  3.29it/s]Evaluating:  38%|███▊      | 120/313 [00:51<00:56,  3.43it/s]Evaluating:  39%|███▊      | 121/313 [00:51<00:55,  3.45it/s]Evaluating:  39%|███▉      | 122/313 [00:51<00:55,  3.45it/s]Evaluating:  39%|███▉      | 123/313 [00:52<00:55,  3.41it/s]Evaluating:  40%|███▉      | 124/313 [00:52<00:54,  3.48it/s]Evaluating:  40%|███▉      | 125/313 [00:52<00:55,  3.40it/s]Evaluating:  40%|████      | 126/313 [00:53<00:55,  3.35it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  41%|████      | 127/313 [00:53<00:56,  3.31it/s]Evaluating:  41%|████      | 128/313 [00:53<00:54,  3.40it/s]Evaluating:  41%|████      | 129/313 [00:54<00:54,  3.35it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  42%|████▏     | 130/313 [00:54<00:55,  3.31it/s]Evaluating:  42%|████▏     | 131/313 [00:54<00:54,  3.34it/s]Evaluating:  42%|████▏     | 132/313 [00:54<00:53,  3.36it/s]Evaluating:  42%|████▏     | 133/313 [00:55<00:54,  3.31it/s]Evaluating:  43%|████▎     | 134/313 [00:55<00:54,  3.28it/s]Evaluating:  43%|████▎     | 135/313 [00:55<00:52,  3.37it/s]Evaluating:  43%|████▎     | 136/313 [00:56<00:53,  3.33it/s]Evaluating:  44%|████▍     | 137/313 [00:56<00:53,  3.29it/s]Evaluating:  44%|████▍     | 138/313 [00:56<00:53,  3.27it/s]Evaluating:  44%|████▍     | 139/313 [00:57<00:51,  3.36it/s]Evaluating:  45%|████▍     | 140/313 [00:57<00:52,  3.32it/s]Evaluating:  45%|████▌     | 141/313 [00:57<00:52,  3.29it/s]Evaluating:  45%|████▌     | 142/313 [00:57<00:50,  3.38it/s]Evaluating:  46%|████▌     | 143/313 [00:58<00:50,  3.34it/s]Evaluating:  46%|████▌     | 144/313 [00:58<00:51,  3.31it/s]Evaluating:  46%|████▋     | 145/313 [00:58<00:51,  3.29it/s]Evaluating:  47%|████▋     | 146/313 [00:59<00:49,  3.40it/s]Evaluating:  47%|████▋     | 147/313 [00:59<00:49,  3.34it/s]Evaluating:  47%|████▋     | 148/313 [00:59<00:49,  3.31it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  48%|████▊     | 149/313 [01:00<00:49,  3.31it/s]Evaluating:  48%|████▊     | 150/313 [01:00<00:49,  3.28it/s]Evaluating:  48%|████▊     | 151/313 [01:00<00:47,  3.39it/s]Evaluating:  49%|████▊     | 152/313 [01:00<00:46,  3.48it/s]Evaluating:  49%|████▉     | 153/313 [01:01<00:44,  3.61it/s]Evaluating:  49%|████▉     | 154/313 [01:01<00:43,  3.62it/s]Evaluating:  50%|████▉     | 155/313 [01:01<00:43,  3.60it/s]Evaluating:  50%|████▉     | 156/313 [01:01<00:43,  3.59it/s]Evaluating:  50%|█████     | 157/313 [01:02<00:42,  3.70it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  50%|█████     | 158/313 [01:02<00:42,  3.65it/s]10/10/2021 15:22:01 - INFO - __main__ -   Using lang2id = None
Evaluating:  51%|█████     | 159/313 [01:02<00:42,  3.59it/s]Evaluating:  51%|█████     | 160/313 [01:03<00:43,  3.52it/s]Evaluating:  51%|█████▏    | 161/313 [01:03<00:42,  3.56it/s]Evaluating:  52%|█████▏    | 162/313 [01:03<00:42,  3.53it/s]Evaluating:  52%|█████▏    | 163/313 [01:03<00:42,  3.50it/s]Evaluating:  52%|█████▏    | 164/313 [01:04<00:42,  3.47it/s]Evaluating:  53%|█████▎    | 165/313 [01:04<00:41,  3.53it/s]PyTorch version 1.9.0+cu102 available.
10/10/2021 15:22:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:22:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:22:03 - INFO - __main__ -   Seed = 52
10/10/2021 15:22:03 - INFO - root -   save model
10/10/2021 15:22:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:22:03 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  53%|█████▎    | 166/313 [01:04<00:42,  3.43it/s]Evaluating:  53%|█████▎    | 167/313 [01:05<00:43,  3.38it/s]Evaluating:  54%|█████▎    | 168/313 [01:05<00:43,  3.33it/s]Evaluating:  54%|█████▍    | 169/313 [01:05<00:43,  3.30it/s]Evaluating:  54%|█████▍    | 170/313 [01:06<00:43,  3.28it/s]Evaluating:  55%|█████▍    | 171/313 [01:06<00:43,  3.26it/s]Evaluating:  55%|█████▍    | 172/313 [01:06<00:41,  3.36it/s]Evaluating:  55%|█████▌    | 173/313 [01:06<00:42,  3.33it/s]Evaluating:  56%|█████▌    | 174/313 [01:07<00:42,  3.30it/s]Evaluating:  56%|█████▌    | 175/313 [01:07<00:41,  3.36it/s]Evaluating:  56%|█████▌    | 176/313 [01:07<00:40,  3.34it/s]Evaluating:  57%|█████▋    | 177/313 [01:08<00:41,  3.31it/s]Evaluating:  57%|█████▋    | 178/313 [01:08<00:41,  3.29it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  57%|█████▋    | 179/313 [01:08<00:39,  3.38it/s]Evaluating:  58%|█████▊    | 180/313 [01:09<00:39,  3.34it/s]Evaluating:  58%|█████▊    | 181/313 [01:09<00:39,  3.31it/s]Evaluating:  58%|█████▊    | 182/313 [01:09<00:39,  3.29it/s]Evaluating:  58%|█████▊    | 183/313 [01:09<00:38,  3.39it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:22:09 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:22:09 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  59%|█████▉    | 184/313 [01:10<00:38,  3.35it/s]Evaluating:  59%|█████▉    | 185/313 [01:10<00:38,  3.31it/s]Evaluating:  59%|█████▉    | 186/313 [01:10<00:37,  3.38it/s]Evaluating:  60%|█████▉    | 187/313 [01:11<00:37,  3.36it/s]Evaluating:  60%|██████    | 188/313 [01:11<00:37,  3.32it/s]Evaluating:  60%|██████    | 189/313 [01:11<00:37,  3.29it/s]Evaluating:  61%|██████    | 190/313 [01:12<00:36,  3.38it/s]Evaluating:  61%|██████    | 191/313 [01:12<00:36,  3.33it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  61%|██████▏   | 192/313 [01:12<00:36,  3.31it/s]Evaluating:  62%|██████▏   | 193/313 [01:12<00:36,  3.31it/s]Evaluating:  62%|██████▏   | 194/313 [01:13<00:35,  3.36it/s]Evaluating:  62%|██████▏   | 195/313 [01:13<00:35,  3.32it/s]Evaluating:  63%|██████▎   | 196/313 [01:13<00:35,  3.29it/s]Evaluating:  63%|██████▎   | 197/313 [01:14<00:34,  3.38it/s]Evaluating:  63%|██████▎   | 198/313 [01:14<00:34,  3.34it/s]Evaluating:  64%|██████▎   | 199/313 [01:14<00:34,  3.30it/s]Evaluating:  64%|██████▍   | 200/313 [01:15<00:34,  3.28it/s]Evaluating:  64%|██████▍   | 201/313 [01:15<00:33,  3.38it/s]Evaluating:  65%|██████▍   | 202/313 [01:15<00:33,  3.34it/s]Evaluating:  65%|██████▍   | 203/313 [01:15<00:33,  3.31it/s]Evaluating:  65%|██████▌   | 204/313 [01:16<00:32,  3.33it/s]Evaluating:  65%|██████▌   | 205/313 [01:16<00:32,  3.36it/s]Evaluating:  66%|██████▌   | 206/313 [01:16<00:32,  3.31it/s]Evaluating:  66%|██████▌   | 207/313 [01:17<00:31,  3.34it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  66%|██████▋   | 208/313 [01:17<00:30,  3.40it/s]Evaluating:  67%|██████▋   | 209/313 [01:17<00:30,  3.38it/s]Evaluating:  67%|██████▋   | 210/313 [01:18<00:30,  3.33it/s]Evaluating:  67%|██████▋   | 211/313 [01:18<00:30,  3.30it/s]Evaluating:  68%|██████▊   | 212/313 [01:18<00:29,  3.38it/s]Evaluating:  68%|██████▊   | 213/313 [01:18<00:29,  3.34it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  68%|██████▊   | 214/313 [01:19<00:29,  3.31it/s]Evaluating:  69%|██████▊   | 215/313 [01:19<00:29,  3.33it/s]Evaluating:  69%|██████▉   | 216/313 [01:19<00:28,  3.36it/s]Evaluating:  69%|██████▉   | 217/313 [01:20<00:28,  3.32it/s]Evaluating:  70%|██████▉   | 218/313 [01:20<00:29,  3.26it/s]Evaluating:  70%|██████▉   | 219/313 [01:20<00:28,  3.35it/s]Evaluating:  70%|███████   | 220/313 [01:21<00:28,  3.31it/s]Evaluating:  71%|███████   | 221/313 [01:21<00:28,  3.28it/s]Evaluating:  71%|███████   | 222/313 [01:21<00:27,  3.30it/s]Evaluating:  71%|███████   | 223/313 [01:21<00:27,  3.32it/s]Evaluating:  72%|███████▏  | 224/313 [01:22<00:27,  3.28it/s]Evaluating:  72%|███████▏  | 225/313 [01:22<00:26,  3.26it/s]Evaluating:  72%|███████▏  | 226/313 [01:22<00:26,  3.32it/s]Evaluating:  73%|███████▎  | 227/313 [01:23<00:25,  3.32it/s]Evaluating:  73%|███████▎  | 228/313 [01:23<00:25,  3.30it/s]Evaluating:  73%|███████▎  | 229/313 [01:23<00:25,  3.30it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  73%|███████▎  | 230/313 [01:24<00:24,  3.35it/s]Evaluating:  74%|███████▍  | 231/313 [01:24<00:24,  3.30it/s]Evaluating:  74%|███████▍  | 232/313 [01:24<00:24,  3.28it/s]Evaluating:  74%|███████▍  | 233/313 [01:24<00:24,  3.31it/s]Evaluating:  75%|███████▍  | 234/313 [01:25<00:23,  3.35it/s]Evaluating:  75%|███████▌  | 235/313 [01:25<00:27,  2.80it/s]Evaluating:  75%|███████▌  | 236/313 [01:26<00:26,  2.96it/s]Evaluating:  76%|███████▌  | 237/313 [01:26<00:24,  3.05it/s]Evaluating:  76%|███████▌  | 238/313 [01:26<00:24,  3.12it/s]Evaluating:  76%|███████▋  | 239/313 [01:26<00:23,  3.21it/s]Evaluating:  77%|███████▋  | 240/313 [01:27<00:22,  3.27it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  77%|███████▋  | 241/313 [01:27<00:22,  3.13it/s]Evaluating:  77%|███████▋  | 242/313 [01:27<00:22,  3.19it/s]Evaluating:  78%|███████▊  | 243/313 [01:28<00:21,  3.21it/s]Evaluating:  78%|███████▊  | 244/313 [01:28<00:24,  2.82it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  78%|███████▊  | 245/313 [01:28<00:23,  2.94it/s]Evaluating:  79%|███████▊  | 246/313 [01:29<00:25,  2.64it/s]Evaluating:  79%|███████▉  | 247/313 [01:29<00:23,  2.82it/s]Evaluating:  79%|███████▉  | 248/313 [01:30<00:22,  2.91it/s]Evaluating:  80%|███████▉  | 249/313 [01:30<00:23,  2.76it/s]Evaluating:  80%|███████▉  | 250/313 [01:30<00:23,  2.71it/s]Evaluating:  80%|████████  | 251/313 [01:31<00:21,  2.91it/s]Evaluating:  81%|████████  | 252/313 [01:31<00:22,  2.75it/s]Evaluating:  81%|████████  | 253/313 [01:31<00:20,  2.87it/s]Evaluating:  81%|████████  | 254/313 [01:32<00:21,  2.71it/s]Evaluating:  81%|████████▏ | 255/313 [01:32<00:20,  2.85it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  82%|████████▏ | 256/313 [01:32<00:20,  2.74it/s]Evaluating:  82%|████████▏ | 257/313 [01:33<00:19,  2.83it/s]Evaluating:  82%|████████▏ | 258/313 [01:33<00:19,  2.89it/s]Evaluating:  83%|████████▎ | 259/313 [01:34<00:19,  2.73it/s]Evaluating:  83%|████████▎ | 260/313 [01:34<00:18,  2.88it/s]Evaluating:  83%|████████▎ | 261/313 [01:34<00:19,  2.73it/s]Evaluating:  84%|████████▎ | 262/313 [01:35<00:18,  2.81it/s]Evaluating:  84%|████████▍ | 263/313 [01:35<00:17,  2.92it/s]Evaluating:  84%|████████▍ | 264/313 [01:36<00:21,  2.27it/s]Evaluating:  85%|████████▍ | 265/313 [01:36<00:19,  2.52it/s]Evaluating:  85%|████████▍ | 266/313 [01:36<00:18,  2.56it/s]Evaluating:  85%|████████▌ | 267/313 [01:37<00:16,  2.78it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  86%|████████▌ | 268/313 [01:37<00:16,  2.65it/s]10/10/2021 15:22:36 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:22:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:22:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/10/2021 15:22:36 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:22:36 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:22:36 - INFO - root -   loading lang adpater hi/wiki@ukp
10/10/2021 15:22:36 - INFO - __main__ -   Language = hi
10/10/2021 15:22:36 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Evaluating:  86%|████████▌ | 269/313 [01:37<00:15,  2.89it/s]Evaluating:  86%|████████▋ | 270/313 [01:38<00:14,  3.05it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
Evaluating:  87%|████████▋ | 271/313 [01:38<00:19,  2.18it/s]Evaluating:  87%|████████▋ | 272/313 [01:39<00:18,  2.24it/s]Evaluating:  87%|████████▋ | 273/313 [01:39<00:15,  2.52it/s]Evaluating:  88%|████████▊ | 274/313 [01:39<00:14,  2.61it/s]Evaluating:  88%|████████▊ | 275/313 [01:40<00:13,  2.88it/s]Evaluating:  88%|████████▊ | 276/313 [01:40<00:12,  3.03it/s]Evaluating:  88%|████████▊ | 277/313 [01:40<00:12,  2.79it/s]Evaluating:  89%|████████▉ | 278/313 [01:41<00:12,  2.89it/s]Evaluating:  89%|████████▉ | 279/313 [01:41<00:11,  3.04it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  89%|████████▉ | 280/313 [01:41<00:12,  2.73it/s]10/10/2021 15:22:40 - INFO - __main__ -   Using lang2id = None
Evaluating:  90%|████████▉ | 281/313 [01:42<00:11,  2.90it/s]Evaluating:  90%|█████████ | 282/313 [01:42<00:10,  3.04it/s]Evaluating:  90%|█████████ | 283/313 [01:43<00:11,  2.53it/s]Evaluating:  91%|█████████ | 284/313 [01:43<00:10,  2.79it/s]Evaluating:  91%|█████████ | 285/313 [01:43<00:09,  2.97it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  91%|█████████▏| 286/313 [01:44<00:10,  2.65it/s]10/10/2021 15:22:43 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:22:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:22:43 - INFO - __main__ -   Seed = 22
10/10/2021 15:22:43 - INFO - root -   save model
10/10/2021 15:22:43 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:22:43 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  92%|█████████▏| 287/313 [01:44<00:09,  2.67it/s]10/10/2021 15:22:44 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/10/2021 15:22:44 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:22:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
Evaluating:  92%|█████████▏| 288/313 [01:45<00:12,  2.07it/s]10/10/2021 15:22:44 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/10/2021 15:22:44 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:22:44 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.20it/s]Evaluating:  92%|█████████▏| 289/313 [01:45<00:11,  2.05it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   6%|▋         | 2/32 [00:00<00:14,  2.08it/s]Evaluating:  93%|█████████▎| 290/313 [01:46<00:11,  2.00it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:12,  2.27it/s]Evaluating:  93%|█████████▎| 291/313 [01:46<00:10,  2.09it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:12,  2.25it/s]Evaluating:  93%|█████████▎| 292/313 [01:47<00:10,  2.10it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:12,  2.25it/s]Evaluating:  94%|█████████▎| 293/313 [01:47<00:09,  2.11it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:11,  2.22it/s]Evaluating:  94%|█████████▍| 294/313 [01:48<00:08,  2.12it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:11,  2.23it/s]Evaluating:  94%|█████████▍| 295/313 [01:48<00:08,  2.11it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:10,  2.21it/s]Evaluating:  95%|█████████▍| 296/313 [01:48<00:08,  2.10it/s]Evaluating:  28%|██▊       | 9/32 [00:04<00:10,  2.22it/s]Evaluating:  95%|█████████▍| 297/313 [01:49<00:07,  2.11it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:09,  2.23it/s]Evaluating:  95%|█████████▌| 298/313 [01:49<00:07,  2.12it/s]Evaluating:  34%|███▍      | 11/32 [00:04<00:09,  2.22it/s]Evaluating:  96%|█████████▌| 299/313 [01:50<00:06,  2.10it/s]Evaluating:  38%|███▊      | 12/32 [00:05<00:09,  2.21it/s]Evaluating:  96%|█████████▌| 300/313 [01:50<00:06,  2.12it/s]Evaluating:  41%|████      | 13/32 [00:05<00:08,  2.20it/s]Evaluating:  96%|█████████▌| 301/313 [01:51<00:05,  2.12it/s]Evaluating:  44%|████▍     | 14/32 [00:06<00:08,  2.22it/s]Evaluating:  96%|█████████▋| 302/313 [01:51<00:05,  2.11it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.20it/s]Evaluating:  97%|█████████▋| 303/313 [01:52<00:04,  2.10it/s]Evaluating:  50%|█████     | 16/32 [00:07<00:07,  2.21it/s]Evaluating:  97%|█████████▋| 304/313 [01:52<00:04,  2.11it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.20it/s]Evaluating:  97%|█████████▋| 305/313 [01:53<00:03,  2.13it/s]Evaluating:  56%|█████▋    | 18/32 [00:08<00:06,  2.21it/s]Evaluating:  98%|█████████▊| 306/313 [01:53<00:03,  2.11it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:05,  2.20it/s]Evaluating:  98%|█████████▊| 307/313 [01:54<00:02,  2.12it/s]Evaluating:  62%|██████▎   | 20/32 [00:09<00:05,  2.22it/s]Evaluating:  98%|█████████▊| 308/313 [01:54<00:02,  2.10it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:04,  2.22it/s]Evaluating:  99%|█████████▊| 309/313 [01:55<00:01,  2.11it/s]Evaluating:  69%|██████▉   | 22/32 [00:09<00:04,  2.21it/s]Evaluating:  72%|███████▏  | 23/32 [00:10<00:04,  2.19it/s]Evaluating:  99%|█████████▉| 310/313 [01:55<00:01,  2.10it/s]Evaluating:  75%|███████▌  | 24/32 [00:10<00:03,  2.53it/s]Evaluating:  78%|███████▊  | 25/32 [00:11<00:03,  2.32it/s]Evaluating:  99%|█████████▉| 311/313 [01:56<00:01,  1.62it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.53it/s]Evaluating: 100%|█████████▉| 312/313 [01:56<00:00,  1.77it/s]Evaluating:  84%|████████▍ | 27/32 [00:11<00:02,  2.47it/s]Evaluating: 100%|██████████| 313/313 [01:57<00:00,  2.10it/s]Evaluating: 100%|██████████| 313/313 [01:57<00:00,  2.67it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.65it/s]Evaluating:  91%|█████████ | 29/32 [00:12<00:01,  2.83it/s]Evaluating:  94%|█████████▍| 30/32 [00:12<00:00,  2.95it/s]Evaluating:  97%|█████████▋| 31/32 [00:13<00:00,  3.05it/s]Evaluating: 100%|██████████| 32/32 [00:13<00:00,  3.66it/s]Evaluating: 100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
10/10/2021 15:22:57 - INFO - __main__ -   ***** Evaluation result  in no *****
10/10/2021 15:22:57 - INFO - __main__ -     f1 = 0.6887217057064949
10/10/2021 15:22:57 - INFO - __main__ -     loss = 0.7920597317500617
10/10/2021 15:22:57 - INFO - __main__ -     precision = 0.6536135187379185
10/10/2021 15:22:57 - INFO - __main__ -     recall = 0.727815581169282

10/10/2021 15:22:57 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/10/2021 15:22:57 - INFO - __main__ -     f1 = 0.4050514499532273
10/10/2021 15:22:57 - INFO - __main__ -     loss = 4.447327096015215
10/10/2021 15:22:57 - INFO - __main__ -     precision = 0.4127740705433746
10/10/2021 15:22:57 - INFO - __main__ -     recall = 0.39761248852157943
10/10/2021 15:22:57 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/10/2021 15:22:57 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:22:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/10/2021 15:22:57 - INFO - __main__ -   Language adapter for da not found, using is instead
10/10/2021 15:22:57 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:22:57 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/10/2021 15:22:57 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/10/2021 15:22:57 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:22:57 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:09,  3.36it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:08,  3.34it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:08,  3.50it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:08,  3.42it/s]10/10/2021 15:22:58 - INFO - __main__ -   ***** Running evaluation  in da *****
10/10/2021 15:22:58 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:22:58 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  16%|█▌        | 5/32 [00:01<00:09,  3.00it/s]Evaluating:   0%|          | 1/313 [00:00<02:26,  2.13it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:09,  2.68it/s]Evaluating:   1%|          | 2/313 [00:00<02:02,  2.54it/s]Evaluating:  22%|██▏       | 7/32 [00:02<00:11,  2.11it/s]Evaluating:   1%|          | 3/313 [00:01<02:35,  1.99it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:11,  2.13it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:30,  2.05it/s]Evaluating:  28%|██▊       | 9/32 [00:03<00:10,  2.14it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:27,  2.08it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:10,  2.19it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:22,  2.15it/s]Evaluating:  34%|███▍      | 11/32 [00:04<00:09,  2.17it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:22,  2.14it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:23:02 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:23:02 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:23:02 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/10/2021 15:23:02 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:23:02 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:23:02 - INFO - root -   loading lang adpater ru/wiki@ukp
10/10/2021 15:23:02 - INFO - __main__ -   Language = ru
10/10/2021 15:23:02 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
Evaluating:  38%|███▊      | 12/32 [00:05<00:09,  2.16it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:22,  2.14it/s]No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Evaluating:  41%|████      | 13/32 [00:05<00:08,  2.15it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:22,  2.14it/s]Evaluating:  44%|████▍     | 14/32 [00:05<00:08,  2.19it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:18,  2.19it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.17it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:19,  2.16it/s]Evaluating:  50%|█████     | 16/32 [00:06<00:07,  2.15it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:20,  2.15it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:07,  2.14it/s]Evaluating:   4%|▍         | 13/313 [00:06<02:20,  2.14it/s]Evaluating:  56%|█████▋    | 18/32 [00:07<00:06,  2.18it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:17,  2.18it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:06,  2.16it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:17,  2.16it/s]Evaluating:  62%|██████▎   | 20/32 [00:08<00:05,  2.15it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:18,  2.15it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:05,  2.17it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:15,  2.18it/s]Evaluating:  69%|██████▉   | 22/32 [00:09<00:04,  2.17it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:16,  2.16it/s]Evaluating:  72%|███████▏  | 23/32 [00:10<00:04,  2.15it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:16,  2.15it/s]Evaluating:  75%|███████▌  | 24/32 [00:10<00:03,  2.14it/s]Evaluating:   6%|▋         | 20/313 [00:09<02:17,  2.14it/s]Evaluating:  78%|███████▊  | 25/32 [00:11<00:03,  2.18it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:13,  2.18it/s]Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.17it/s]Evaluating:   7%|▋         | 22/313 [00:10<02:14,  2.17it/s]Evaluating:  84%|████████▍ | 27/32 [00:11<00:02,  2.15it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:14,  2.15it/s]Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.14it/s]Evaluating:   8%|▊         | 24/313 [00:11<02:15,  2.14it/s]Evaluating:  91%|█████████ | 29/32 [00:12<00:01,  2.18it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:12,  2.18it/s]Evaluating:  94%|█████████▍| 30/32 [00:13<00:00,  2.16it/s]Evaluating:   8%|▊         | 26/313 [00:12<02:12,  2.16it/s]Evaluating:  97%|█████████▋| 31/32 [00:13<00:00,  2.15it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:13,  2.15it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.32it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.26it/s]Evaluating:   9%|▉         | 28/313 [00:12<02:01,  2.34it/s]
10/10/2021 15:23:11 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/10/2021 15:23:11 - INFO - __main__ -     f1 = 0.40704368438875715
10/10/2021 15:23:11 - INFO - __main__ -     loss = 2.050574030727148
10/10/2021 15:23:11 - INFO - __main__ -     precision = 0.3558318531675548
10/10/2021 15:23:11 - INFO - __main__ -     recall = 0.4754746835443038
10/10/2021 15:23:11 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/10/2021 15:23:11 - INFO - __main__ -   Set active language adapter to hi
10/10/2021 15:23:11 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/10/2021 15:23:12 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/10/2021 15:23:12 - INFO - __main__ -     Num examples = 1000
10/10/2021 15:23:12 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   9%|▉         | 29/313 [00:13<01:52,  2.52it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.13it/s]Evaluating:  10%|▉         | 30/313 [00:13<01:58,  2.39it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:14,  2.12it/s]Evaluating:  10%|▉         | 31/313 [00:14<02:02,  2.30it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:13,  2.19it/s]Evaluating:  10%|█         | 32/313 [00:14<02:02,  2.29it/s]10/10/2021 15:23:13 - INFO - __main__ -   Language adapter for be not found, using ru instead
10/10/2021 15:23:13 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:23:13 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
Evaluating:  12%|█▎        | 4/32 [00:01<00:12,  2.16it/s]10/10/2021 15:23:13 - INFO - __main__ -   ***** Running evaluation  in be *****
10/10/2021 15:23:13 - INFO - __main__ -     Num examples = 1001
10/10/2021 15:23:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  11%|█         | 33/313 [00:15<02:05,  2.22it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:13,  2.04it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:17,  1.80it/s]Evaluating:  11%|█         | 34/313 [00:15<02:39,  1.75it/s]Evaluating:  19%|█▉        | 6/32 [00:03<00:13,  1.86it/s]Evaluating:   6%|▋         | 2/32 [00:01<00:18,  1.67it/s]Evaluating:  11%|█         | 35/313 [00:16<02:40,  1.73it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:13,  1.79it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:17,  1.67it/s]Evaluating:  12%|█▏        | 36/313 [00:17<02:44,  1.68it/s]Evaluating:  25%|██▌       | 8/32 [00:04<00:13,  1.72it/s]Evaluating:  12%|█▎        | 4/32 [00:02<00:17,  1.64it/s]Evaluating:  12%|█▏        | 37/313 [00:17<02:47,  1.65it/s]Evaluating:  28%|██▊       | 9/32 [00:04<00:13,  1.68it/s]Evaluating:  16%|█▌        | 5/32 [00:03<00:16,  1.62it/s]Evaluating:  12%|█▏        | 38/313 [00:18<02:48,  1.63it/s]Evaluating:  31%|███▏      | 10/32 [00:05<00:13,  1.65it/s]Evaluating:  19%|█▉        | 6/32 [00:03<00:16,  1.61it/s]Evaluating:  12%|█▏        | 39/313 [00:19<02:46,  1.64it/s]Evaluating:  34%|███▍      | 11/32 [00:06<00:12,  1.66it/s]Evaluating:  22%|██▏       | 7/32 [00:04<00:15,  1.63it/s]Evaluating:  13%|█▎        | 40/313 [00:19<02:47,  1.63it/s]Evaluating:  38%|███▊      | 12/32 [00:06<00:12,  1.64it/s]Evaluating:  25%|██▌       | 8/32 [00:04<00:14,  1.62it/s]Evaluating:  13%|█▎        | 41/313 [00:20<02:48,  1.62it/s]Evaluating:  41%|████      | 13/32 [00:07<00:11,  1.62it/s]Evaluating:  28%|██▊       | 9/32 [00:05<00:14,  1.61it/s]Evaluating:  13%|█▎        | 42/313 [00:20<02:48,  1.61it/s]Evaluating:  44%|████▍     | 14/32 [00:07<00:11,  1.63it/s]Evaluating:  31%|███▏      | 10/32 [00:06<00:13,  1.63it/s]Evaluating:  14%|█▎        | 43/313 [00:21<02:46,  1.63it/s]Evaluating:  47%|████▋     | 15/32 [00:08<00:10,  1.62it/s]Evaluating:  34%|███▍      | 11/32 [00:06<00:13,  1.61it/s]Evaluating:  14%|█▍        | 44/313 [00:22<02:46,  1.61it/s]Evaluating:  50%|█████     | 16/32 [00:09<00:09,  1.61it/s]Evaluating:  38%|███▊      | 12/32 [00:07<00:12,  1.60it/s]Evaluating:  14%|█▍        | 45/313 [00:22<02:47,  1.60it/s]Evaluating:  53%|█████▎    | 17/32 [00:09<00:09,  1.60it/s]Evaluating:  41%|████      | 13/32 [00:08<00:11,  1.60it/s]Evaluating:  15%|█▍        | 46/313 [00:23<02:45,  1.61it/s]Evaluating:  56%|█████▋    | 18/32 [00:10<00:08,  1.62it/s]Evaluating:  44%|████▍     | 14/32 [00:08<00:11,  1.62it/s]Evaluating:  15%|█▌        | 47/313 [00:23<02:44,  1.61it/s]Evaluating:  59%|█████▉    | 19/32 [00:11<00:08,  1.61it/s]Evaluating:  47%|████▋     | 15/32 [00:09<00:10,  1.61it/s]Evaluating:  15%|█▌        | 48/313 [00:24<02:45,  1.61it/s]Evaluating:  62%|██████▎   | 20/32 [00:11<00:07,  1.60it/s]Evaluating:  50%|█████     | 16/32 [00:09<00:09,  1.60it/s]Evaluating:  16%|█▌        | 49/313 [00:25<02:45,  1.60it/s]Evaluating:  66%|██████▌   | 21/32 [00:12<00:06,  1.60it/s]Evaluating:  53%|█████▎    | 17/32 [00:10<00:09,  1.60it/s]Evaluating:  16%|█▌        | 50/313 [00:25<02:41,  1.63it/s]Evaluating:  69%|██████▉   | 22/32 [00:12<00:06,  1.62it/s]Evaluating:  56%|█████▋    | 18/32 [00:11<00:08,  1.62it/s]Evaluating:  16%|█▋        | 51/313 [00:26<02:42,  1.62it/s]Evaluating:  72%|███████▏  | 23/32 [00:13<00:05,  1.61it/s]Evaluating:  59%|█████▉    | 19/32 [00:11<00:08,  1.61it/s]Evaluating:  17%|█▋        | 52/313 [00:27<02:41,  1.62it/s]Evaluating:  75%|███████▌  | 24/32 [00:14<00:04,  1.61it/s]Evaluating:  62%|██████▎   | 20/32 [00:12<00:07,  1.61it/s]Evaluating:  17%|█▋        | 53/313 [00:27<02:42,  1.60it/s]Evaluating:  78%|███████▊  | 25/32 [00:14<00:04,  1.61it/s]Evaluating:  66%|██████▌   | 21/32 [00:12<00:06,  1.62it/s]Evaluating:  17%|█▋        | 54/313 [00:28<02:39,  1.62it/s]Evaluating:  81%|████████▏ | 26/32 [00:15<00:03,  1.62it/s]Evaluating:  69%|██████▉   | 22/32 [00:13<00:06,  1.62it/s]Evaluating:  18%|█▊        | 55/313 [00:28<02:40,  1.61it/s]Evaluating:  84%|████████▍ | 27/32 [00:16<00:03,  1.61it/s]Evaluating:  72%|███████▏  | 23/32 [00:14<00:05,  1.61it/s]Evaluating:  18%|█▊        | 56/313 [00:29<02:40,  1.60it/s]Evaluating:  88%|████████▊ | 28/32 [00:16<00:02,  1.60it/s]Evaluating:  75%|███████▌  | 24/32 [00:14<00:04,  1.60it/s]Evaluating:  18%|█▊        | 57/313 [00:30<02:40,  1.60it/s]Evaluating:  91%|█████████ | 29/32 [00:17<00:01,  1.62it/s]Evaluating:  78%|███████▊  | 25/32 [00:15<00:04,  1.62it/s]Evaluating:  19%|█▊        | 58/313 [00:30<02:37,  1.62it/s]Evaluating:  94%|█████████▍| 30/32 [00:17<00:01,  1.61it/s]Evaluating:  81%|████████▏ | 26/32 [00:16<00:03,  1.61it/s]Evaluating:  19%|█▉        | 59/313 [00:31<02:37,  1.61it/s]Evaluating:  97%|█████████▋| 31/32 [00:18<00:00,  1.60it/s]Evaluating:  84%|████████▍ | 27/32 [00:16<00:03,  1.61it/s]Evaluating:  19%|█▉        | 60/313 [00:32<02:32,  1.66it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.68it/s]Evaluating: 100%|██████████| 32/32 [00:19<00:00,  1.68it/s]Evaluating:  88%|████████▊ | 28/32 [00:17<00:02,  1.70it/s]
10/10/2021 15:23:31 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/10/2021 15:23:31 - INFO - __main__ -     f1 = 0.13333333333333333
10/10/2021 15:23:31 - INFO - __main__ -     loss = 4.466664224863052
10/10/2021 15:23:31 - INFO - __main__ -     precision = 0.19126506024096385
10/10/2021 15:23:31 - INFO - __main__ -     recall = 0.10233682514101532
10/10/2021 15:23:31 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  19%|█▉        | 61/313 [00:32<02:20,  1.79it/s]Evaluating:  91%|█████████ | 29/32 [00:17<00:01,  1.84it/s]Evaluating:  20%|█▉        | 62/313 [00:32<02:13,  1.88it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  94%|█████████▍| 30/32 [00:18<00:01,  1.92it/s]Evaluating:  20%|██        | 63/313 [00:33<02:08,  1.95it/s]Evaluating:  97%|█████████▋| 31/32 [00:18<00:00,  1.98it/s]Evaluating:  20%|██        | 64/313 [00:33<01:59,  2.08it/s]Evaluating: 100%|██████████| 32/32 [00:18<00:00,  2.17it/s]Evaluating: 100%|██████████| 32/32 [00:18<00:00,  1.69it/s]
10/10/2021 15:23:32 - INFO - __main__ -   ***** Evaluation result  in be *****
10/10/2021 15:23:32 - INFO - __main__ -     f1 = 0.6218923933209648
10/10/2021 15:23:32 - INFO - __main__ -     loss = 0.9730028649792075
10/10/2021 15:23:32 - INFO - __main__ -     precision = 0.5669824086603519
10/10/2021 15:23:32 - INFO - __main__ -     recall = 0.6885784716516024
10/10/2021 15:23:32 - INFO - __main__ -   Language adapter for uk not found, using ru instead
10/10/2021 15:23:32 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:23:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
Evaluating:  21%|██        | 65/313 [00:34<01:45,  2.35it/s]Evaluating:  21%|██        | 66/313 [00:34<01:36,  2.57it/s]Evaluating:  21%|██▏       | 67/313 [00:34<01:29,  2.74it/s]Evaluating:  22%|██▏       | 68/313 [00:35<01:25,  2.88it/s]Evaluating:  22%|██▏       | 69/313 [00:35<01:19,  3.08it/s]10/10/2021 15:23:34 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/10/2021 15:23:34 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:23:34 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  22%|██▏       | 70/313 [00:35<01:22,  2.94it/s]Evaluating:   0%|          | 1/313 [00:00<02:26,  2.13it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  23%|██▎       | 71/313 [00:36<01:31,  2.64it/s]Evaluating:   1%|          | 2/313 [00:00<02:25,  2.13it/s]Evaluating:  23%|██▎       | 72/313 [00:36<01:35,  2.51it/s]Evaluating:   1%|          | 3/313 [00:01<02:20,  2.20it/s]Evaluating:  23%|██▎       | 73/313 [00:37<01:40,  2.40it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:22,  2.17it/s]Evaluating:  24%|██▎       | 74/313 [00:37<01:43,  2.31it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:22,  2.16it/s]Evaluating:  24%|██▍       | 75/313 [00:37<01:45,  2.26it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:21,  2.16it/s]Evaluating:  24%|██▍       | 76/313 [00:38<01:44,  2.27it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:19,  2.19it/s]Evaluating:  25%|██▍       | 77/313 [00:38<01:45,  2.23it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:20,  2.18it/s]Evaluating:  25%|██▍       | 78/313 [00:39<01:46,  2.20it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:20,  2.16it/s]Evaluating:  25%|██▌       | 79/313 [00:39<01:47,  2.18it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:17,  2.20it/s]Evaluating:  26%|██▌       | 80/313 [00:40<01:45,  2.21it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:18,  2.18it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  26%|██▌       | 81/313 [00:40<01:46,  2.19it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:18,  2.17it/s]Evaluating:  26%|██▌       | 82/313 [00:41<01:46,  2.17it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   4%|▍         | 13/313 [00:05<02:19,  2.16it/s]Evaluating:  27%|██▋       | 83/313 [00:41<01:45,  2.18it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:15,  2.20it/s]Evaluating:  27%|██▋       | 84/313 [00:42<01:44,  2.19it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:12,  2.25it/s]Evaluating:  27%|██▋       | 85/313 [00:42<01:50,  2.07it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:14,  2.21it/s]Evaluating:  27%|██▋       | 86/313 [00:43<01:48,  2.09it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:15,  2.19it/s]Evaluating:  28%|██▊       | 87/313 [00:43<01:45,  2.15it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:12,  2.22it/s]Evaluating:  28%|██▊       | 88/313 [00:44<01:44,  2.14it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:13,  2.19it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  28%|██▊       | 89/313 [00:44<01:44,  2.14it/s]10/10/2021 15:23:43 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:23:43 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:   6%|▋         | 20/313 [00:09<02:14,  2.18it/s]Evaluating:  29%|██▉       | 90/313 [00:44<01:44,  2.13it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:12,  2.20it/s]Evaluating:  29%|██▉       | 91/313 [00:45<01:41,  2.18it/s]Evaluating:   7%|▋         | 22/313 [00:10<02:11,  2.21it/s]Evaluating:  29%|██▉       | 92/313 [00:45<01:41,  2.17it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:12,  2.19it/s]Evaluating:  30%|██▉       | 93/313 [00:46<01:41,  2.16it/s]Evaluating:   8%|▊         | 24/313 [00:10<02:12,  2.18it/s]Evaluating:  30%|███       | 94/313 [00:46<01:40,  2.19it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:10,  2.21it/s]Evaluating:  30%|███       | 95/313 [00:47<01:40,  2.17it/s]Evaluating:   8%|▊         | 26/313 [00:11<02:10,  2.19it/s]Evaluating:  31%|███       | 96/313 [00:47<01:40,  2.16it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:11,  2.18it/s]Evaluating:  31%|███       | 97/313 [00:48<01:40,  2.15it/s]Evaluating:   9%|▉         | 28/313 [00:12<02:11,  2.17it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  31%|███▏      | 98/313 [00:48<01:38,  2.19it/s]Evaluating:   9%|▉         | 29/313 [00:13<02:08,  2.21it/s]Evaluating:  32%|███▏      | 99/313 [00:49<01:38,  2.17it/s]Evaluating:  10%|▉         | 30/313 [00:13<02:09,  2.19it/s]Evaluating:  32%|███▏      | 100/313 [00:49<01:38,  2.16it/s]Evaluating:  10%|▉         | 31/313 [00:14<02:09,  2.17it/s]Evaluating:  32%|███▏      | 101/313 [00:50<01:38,  2.15it/s]Evaluating:  10%|█         | 32/313 [00:14<02:09,  2.18it/s]Evaluating:  33%|███▎      | 102/313 [00:50<01:36,  2.19it/s]Evaluating:  11%|█         | 33/313 [00:15<02:07,  2.20it/s]Evaluating:  33%|███▎      | 103/313 [00:50<01:36,  2.17it/s]Evaluating:  11%|█         | 34/313 [00:15<02:07,  2.18it/s]Evaluating:  33%|███▎      | 104/313 [00:51<01:37,  2.15it/s]Evaluating:  11%|█         | 35/313 [00:16<02:08,  2.17it/s]Evaluating:  34%|███▎      | 105/313 [00:51<01:35,  2.17it/s]Evaluating:  12%|█▏        | 36/313 [00:16<02:05,  2.21it/s]Evaluating:  34%|███▍      | 106/313 [00:52<01:35,  2.18it/s]Evaluating:  12%|█▏        | 37/313 [00:16<02:06,  2.19it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  34%|███▍      | 107/313 [00:52<01:35,  2.16it/s]Evaluating:  12%|█▏        | 38/313 [00:17<02:06,  2.17it/s]Evaluating:  35%|███▍      | 108/313 [00:53<01:35,  2.14it/s]Evaluating:  12%|█▏        | 39/313 [00:17<02:06,  2.17it/s]Evaluating:  35%|███▍      | 109/313 [00:53<01:33,  2.18it/s]Evaluating:  13%|█▎        | 40/313 [00:18<02:03,  2.21it/s]Evaluating:  35%|███▌      | 110/313 [00:54<01:33,  2.17it/s]Evaluating:  13%|█▎        | 41/313 [00:18<02:04,  2.19it/s]Evaluating:  35%|███▌      | 111/313 [00:54<01:33,  2.15it/s]Evaluating:  13%|█▎        | 42/313 [00:19<02:04,  2.18it/s]Evaluating:  14%|█▎        | 43/313 [00:19<02:06,  2.13it/s]Evaluating:  36%|███▌      | 112/313 [00:55<01:37,  2.07it/s]Evaluating:  14%|█▍        | 44/313 [00:20<02:05,  2.15it/s]Evaluating:  36%|███▌      | 113/313 [00:55<01:35,  2.09it/s]Evaluating:  14%|█▍        | 45/313 [00:20<02:04,  2.15it/s]Evaluating:  36%|███▋      | 114/313 [00:56<01:34,  2.10it/s]Evaluating:  15%|█▍        | 46/313 [00:21<02:04,  2.15it/s]Evaluating:  37%|███▋      | 115/313 [00:56<01:34,  2.10it/s]Evaluating:  15%|█▌        | 47/313 [00:21<02:01,  2.20it/s]Evaluating:  37%|███▋      | 116/313 [00:57<01:31,  2.15it/s]Evaluating:  15%|█▌        | 48/313 [00:21<02:01,  2.18it/s]Evaluating:  37%|███▋      | 117/313 [00:57<01:31,  2.14it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  16%|█▌        | 49/313 [00:22<02:01,  2.17it/s]Evaluating:  38%|███▊      | 118/313 [00:57<01:31,  2.13it/s]Evaluating:  16%|█▌        | 50/313 [00:22<02:01,  2.16it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  38%|███▊      | 119/313 [00:58<01:31,  2.13it/s]Evaluating:  16%|█▋        | 51/313 [00:23<01:58,  2.21it/s]Evaluating:  38%|███▊      | 120/313 [00:58<01:28,  2.17it/s]Evaluating:  17%|█▋        | 52/313 [00:23<01:59,  2.19it/s]Evaluating:  39%|███▊      | 121/313 [00:59<01:28,  2.16it/s]Evaluating:  17%|█▋        | 53/313 [00:24<01:59,  2.17it/s]Evaluating:  39%|███▉      | 122/313 [00:59<01:28,  2.15it/s]Evaluating:  17%|█▋        | 54/313 [00:24<01:58,  2.18it/s]Evaluating:  39%|███▉      | 123/313 [01:00<01:26,  2.19it/s]Evaluating:  18%|█▊        | 55/313 [00:25<01:57,  2.20it/s]Evaluating:  40%|███▉      | 124/313 [01:00<01:27,  2.17it/s]Evaluating:  18%|█▊        | 56/313 [00:25<01:57,  2.18it/s]Evaluating:  40%|███▉      | 125/313 [01:01<01:27,  2.15it/s]Evaluating:  18%|█▊        | 57/313 [00:26<01:57,  2.17it/s]Evaluating:  40%|████      | 126/313 [01:01<01:27,  2.14it/s]Evaluating:  19%|█▊        | 58/313 [00:26<01:55,  2.20it/s]Evaluating:  41%|████      | 127/313 [01:02<01:25,  2.18it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  19%|█▉        | 59/313 [00:27<01:56,  2.19it/s]10/10/2021 15:24:01 - INFO - __main__ -   Using lang2id = None
Evaluating:  41%|████      | 128/313 [01:02<01:25,  2.17it/s]Evaluating:  19%|█▉        | 60/313 [00:27<01:56,  2.18it/s]Evaluating:  41%|████      | 129/313 [01:03<01:25,  2.15it/s]Evaluating:  19%|█▉        | 61/313 [00:27<01:56,  2.17it/s]Evaluating:  42%|████▏     | 130/313 [01:03<01:25,  2.14it/s]Evaluating:  20%|█▉        | 62/313 [00:28<01:53,  2.21it/s]Evaluating:  42%|████▏     | 131/313 [01:03<01:23,  2.18it/s]Evaluating:  20%|██        | 63/313 [00:28<01:54,  2.19it/s]Evaluating:  42%|████▏     | 132/313 [01:04<01:23,  2.16it/s]Evaluating:  20%|██        | 64/313 [00:29<01:54,  2.17it/s]Evaluating:  42%|████▏     | 133/313 [01:04<01:23,  2.15it/s]Evaluating:  21%|██        | 65/313 [00:29<01:54,  2.17it/s]Evaluating:  43%|████▎     | 134/313 [01:05<01:21,  2.19it/s]Evaluating:  21%|██        | 66/313 [00:30<01:51,  2.21it/s]Evaluating:  43%|████▎     | 135/313 [01:05<01:22,  2.16it/s]Evaluating:  21%|██▏       | 67/313 [00:30<01:52,  2.19it/s]Evaluating:  43%|████▎     | 136/313 [01:06<01:22,  2.15it/s]Evaluating:  22%|██▏       | 68/313 [00:31<01:52,  2.17it/s]Evaluating:  44%|████▍     | 137/313 [01:06<01:22,  2.14it/s]Evaluating:  22%|██▏       | 69/313 [00:31<01:50,  2.21it/s]Evaluating:  44%|████▍     | 138/313 [01:07<01:20,  2.18it/s]Evaluating:  22%|██▏       | 70/313 [00:32<01:50,  2.19it/s]Evaluating:  44%|████▍     | 139/313 [01:07<01:20,  2.16it/s]Evaluating:  23%|██▎       | 71/313 [00:32<01:51,  2.18it/s]Evaluating:  45%|████▍     | 140/313 [01:08<01:20,  2.14it/s]Evaluating:  23%|██▎       | 72/313 [00:32<01:51,  2.17it/s]Evaluating:  45%|████▌     | 141/313 [01:08<01:20,  2.14it/s]Evaluating:  23%|██▎       | 73/313 [00:33<01:48,  2.21it/s]Evaluating:  45%|████▌     | 142/313 [01:09<01:18,  2.18it/s]Evaluating:  24%|██▎       | 74/313 [00:33<01:49,  2.19it/s]Evaluating:  46%|████▌     | 143/313 [01:09<01:18,  2.16it/s]Evaluating:  24%|██▍       | 75/313 [00:34<01:49,  2.18it/s]Evaluating:  46%|████▌     | 144/313 [01:09<01:18,  2.15it/s]Evaluating:  24%|██▍       | 76/313 [00:34<01:49,  2.17it/s]Evaluating:  46%|████▋     | 145/313 [01:10<01:16,  2.19it/s]Evaluating:  25%|██▍       | 77/313 [00:35<01:46,  2.21it/s]Evaluating:  47%|████▋     | 146/313 [01:10<01:17,  2.16it/s]Evaluating:  25%|██▍       | 78/313 [00:35<01:47,  2.19it/s]Evaluating:  47%|████▋     | 147/313 [01:11<01:17,  2.15it/s]Evaluating:  25%|██▌       | 79/313 [00:36<01:47,  2.18it/s]Evaluating:  47%|████▋     | 148/313 [01:11<01:17,  2.14it/s]Evaluating:  26%|██▌       | 80/313 [00:36<01:46,  2.19it/s]Evaluating:  48%|████▊     | 149/313 [01:12<01:15,  2.18it/s]Evaluating:  26%|██▌       | 81/313 [00:37<01:45,  2.21it/s]Evaluating:  48%|████▊     | 150/313 [01:12<01:15,  2.16it/s]Evaluating:  26%|██▌       | 82/313 [00:37<01:45,  2.19it/s]Evaluating:  48%|████▊     | 151/313 [01:13<01:15,  2.15it/s]Evaluating:  27%|██▋       | 83/313 [00:38<01:46,  2.17it/s]Evaluating:  49%|████▊     | 152/313 [01:13<01:15,  2.14it/s]Evaluating:  27%|██▋       | 84/313 [00:38<01:43,  2.21it/s]Evaluating:  49%|████▉     | 153/313 [01:14<01:13,  2.18it/s]Evaluating:  27%|██▋       | 85/313 [00:38<01:44,  2.19it/s]Evaluating:  49%|████▉     | 154/313 [01:14<01:13,  2.17it/s]Evaluating:  27%|██▋       | 86/313 [00:39<01:44,  2.17it/s]Evaluating:  50%|████▉     | 155/313 [01:15<01:13,  2.15it/s]Evaluating:  28%|██▊       | 87/313 [00:39<01:44,  2.16it/s]Evaluating:  50%|████▉     | 156/313 [01:15<01:11,  2.20it/s]Evaluating:  28%|██▊       | 88/313 [00:40<01:42,  2.20it/s]Evaluating:  50%|█████     | 157/313 [01:15<01:11,  2.18it/s]Evaluating:  28%|██▊       | 89/313 [00:40<01:42,  2.18it/s]Evaluating:  50%|█████     | 158/313 [01:16<01:11,  2.16it/s]Evaluating:  29%|██▉       | 90/313 [00:41<01:42,  2.17it/s]Evaluating:  51%|█████     | 159/313 [01:16<01:11,  2.15it/s]Evaluating:  29%|██▉       | 91/313 [00:41<01:42,  2.17it/s]Evaluating:  51%|█████     | 160/313 [01:17<01:09,  2.19it/s]Evaluating:  29%|██▉       | 92/313 [00:42<01:38,  2.25it/s]Evaluating:  51%|█████▏    | 161/313 [01:17<01:12,  2.09it/s]Evaluating:  30%|██▉       | 93/313 [00:42<01:39,  2.22it/s]Evaluating:  52%|█████▏    | 162/313 [01:18<01:11,  2.10it/s]Evaluating:  30%|███       | 94/313 [00:43<01:39,  2.19it/s]Evaluating:  52%|█████▏    | 163/313 [01:18<01:10,  2.12it/s]Evaluating:  30%|███       | 95/313 [00:43<01:37,  2.23it/s]Evaluating:  52%|█████▏    | 164/313 [01:19<01:09,  2.15it/s]Evaluating:  31%|███       | 96/313 [00:43<01:37,  2.22it/s]Evaluating:  53%|█████▎    | 165/313 [01:19<01:09,  2.14it/s]Evaluating:  31%|███       | 97/313 [00:44<01:38,  2.20it/s]Evaluating:  53%|█████▎    | 166/313 [01:20<01:08,  2.13it/s]Evaluating:  31%|███▏      | 98/313 [00:44<01:38,  2.18it/s]Evaluating:  53%|█████▎    | 167/313 [01:20<01:07,  2.17it/s]Evaluating:  32%|███▏      | 99/313 [00:45<01:36,  2.22it/s]Evaluating:  54%|█████▎    | 168/313 [01:21<01:07,  2.15it/s]Evaluating:  32%|███▏      | 100/313 [00:45<01:37,  2.19it/s]Evaluating:  54%|█████▍    | 169/313 [01:21<01:08,  2.12it/s]Evaluating:  32%|███▏      | 101/313 [00:46<01:36,  2.19it/s]Evaluating:  54%|█████▍    | 170/313 [01:22<01:07,  2.12it/s]Evaluating:  33%|███▎      | 102/313 [00:46<01:37,  2.17it/s]Evaluating:  55%|█████▍    | 171/313 [01:22<01:05,  2.17it/s]Evaluating:  33%|███▎      | 103/313 [00:47<01:35,  2.20it/s]Evaluating:  55%|█████▍    | 172/313 [01:23<01:05,  2.15it/s]Evaluating:  33%|███▎      | 104/313 [00:47<01:35,  2.18it/s]Evaluating:  55%|█████▌    | 173/313 [01:23<01:05,  2.14it/s]Evaluating:  34%|███▎      | 105/313 [00:48<01:36,  2.16it/s]Evaluating:  56%|█████▌    | 174/313 [01:23<01:03,  2.18it/s]Evaluating:  34%|███▍      | 106/313 [00:48<01:34,  2.20it/s]Evaluating:  56%|█████▌    | 175/313 [01:24<01:03,  2.17it/s]Evaluating:  34%|███▍      | 107/313 [00:48<01:34,  2.17it/s]Evaluating:  56%|█████▌    | 176/313 [01:24<01:03,  2.15it/s]Evaluating:  35%|███▍      | 108/313 [00:49<01:35,  2.16it/s]Evaluating:  57%|█████▋    | 177/313 [01:25<01:03,  2.15it/s]Evaluating:  35%|███▍      | 109/313 [00:49<01:34,  2.15it/s]Evaluating:  57%|█████▋    | 178/313 [01:25<01:01,  2.19it/s]Evaluating:  35%|███▌      | 110/313 [00:50<01:32,  2.19it/s]Evaluating:  57%|█████▋    | 179/313 [01:26<01:01,  2.17it/s]Evaluating:  35%|███▌      | 111/313 [00:50<01:33,  2.17it/s]Evaluating:  58%|█████▊    | 180/313 [01:26<01:01,  2.16it/s]Evaluating:  36%|███▌      | 112/313 [00:51<01:33,  2.16it/s]Evaluating:  58%|█████▊    | 181/313 [01:27<01:01,  2.15it/s]Evaluating:  36%|███▌      | 113/313 [00:51<01:33,  2.15it/s]Evaluating:  58%|█████▊    | 182/313 [01:27<00:59,  2.19it/s]Evaluating:  36%|███▋      | 114/313 [00:52<01:30,  2.19it/s]Evaluating:  58%|█████▊    | 183/313 [01:28<00:59,  2.17it/s]Evaluating:  37%|███▋      | 115/313 [00:52<01:31,  2.17it/s]Evaluating:  59%|█████▉    | 184/313 [01:28<00:59,  2.15it/s]Evaluating:  37%|███▋      | 116/313 [00:53<01:31,  2.15it/s]Evaluating:  59%|█████▉    | 185/313 [01:28<00:58,  2.19it/s]Evaluating:  37%|███▋      | 117/313 [00:53<01:29,  2.20it/s]Evaluating:  59%|█████▉    | 186/313 [01:29<00:58,  2.17it/s]Evaluating:  38%|███▊      | 118/313 [00:54<01:29,  2.17it/s]Evaluating:  60%|█████▉    | 187/313 [01:29<00:58,  2.16it/s]Evaluating:  38%|███▊      | 119/313 [00:54<01:30,  2.15it/s]Evaluating:  60%|██████    | 188/313 [01:30<00:58,  2.15it/s]Evaluating:  38%|███▊      | 120/313 [00:54<01:29,  2.15it/s]Evaluating:  60%|██████    | 189/313 [01:30<00:56,  2.19it/s]Evaluating:  39%|███▊      | 121/313 [00:55<01:27,  2.19it/s]Evaluating:  61%|██████    | 190/313 [01:31<00:56,  2.17it/s]Evaluating:  39%|███▉      | 122/313 [00:55<01:28,  2.17it/s]Evaluating:  61%|██████    | 191/313 [01:31<00:56,  2.15it/s]Evaluating:  39%|███▉      | 123/313 [00:56<01:28,  2.16it/s]Evaluating:  61%|██████▏   | 192/313 [01:32<00:56,  2.15it/s]Evaluating:  40%|███▉      | 124/313 [00:56<01:27,  2.15it/s]Evaluating:  62%|██████▏   | 193/313 [01:32<00:54,  2.19it/s]Evaluating:  40%|███▉      | 125/313 [00:57<01:25,  2.19it/s]Evaluating:  40%|████      | 126/313 [00:57<01:28,  2.12it/s]Evaluating:  62%|██████▏   | 194/313 [01:33<00:56,  2.10it/s]Evaluating:  41%|████      | 127/313 [00:58<01:27,  2.12it/s]Evaluating:  62%|██████▏   | 195/313 [01:33<00:56,  2.10it/s]Evaluating:  41%|████      | 128/313 [00:58<01:25,  2.17it/s]Evaluating:  63%|██████▎   | 196/313 [01:34<00:54,  2.15it/s]Evaluating:  41%|████      | 129/313 [00:59<01:25,  2.15it/s]Evaluating:  63%|██████▎   | 197/313 [01:34<00:54,  2.13it/s]Evaluating:  42%|████▏     | 130/313 [00:59<01:25,  2.15it/s]Evaluating:  63%|██████▎   | 198/313 [01:35<00:54,  2.12it/s]Evaluating:  42%|████▏     | 131/313 [01:00<01:24,  2.15it/s]Evaluating:  64%|██████▎   | 199/313 [01:35<00:53,  2.12it/s]Evaluating:  42%|████▏     | 132/313 [01:00<01:22,  2.20it/s]Evaluating:  64%|██████▍   | 200/313 [01:35<00:51,  2.19it/s]Evaluating:  42%|████▏     | 133/313 [01:00<01:21,  2.20it/s]Evaluating:  64%|██████▍   | 201/313 [01:36<00:51,  2.19it/s]Evaluating:  43%|████▎     | 134/313 [01:01<01:22,  2.18it/s]Evaluating:  65%|██████▍   | 202/313 [01:36<00:50,  2.18it/s]Evaluating:  43%|████▎     | 135/313 [01:01<01:20,  2.20it/s]Evaluating:  65%|██████▍   | 203/313 [01:37<00:49,  2.21it/s]Evaluating:  43%|████▎     | 136/313 [01:02<01:20,  2.19it/s]Evaluating:  65%|██████▌   | 204/313 [01:37<00:49,  2.18it/s]Evaluating:  44%|████▍     | 137/313 [01:02<01:21,  2.17it/s]Evaluating:  65%|██████▌   | 205/313 [01:38<00:49,  2.16it/s]Evaluating:  44%|████▍     | 138/313 [01:03<01:21,  2.16it/s]Evaluating:  66%|██████▌   | 206/313 [01:38<00:49,  2.15it/s]Evaluating:  44%|████▍     | 139/313 [01:03<01:19,  2.20it/s]Evaluating:  66%|██████▌   | 207/313 [01:39<00:48,  2.19it/s]Evaluating:  45%|████▍     | 140/313 [01:04<01:19,  2.18it/s]Evaluating:  66%|██████▋   | 208/313 [01:39<00:48,  2.18it/s]Evaluating:  45%|████▌     | 141/313 [01:04<01:19,  2.16it/s]Evaluating:  67%|██████▋   | 209/313 [01:40<00:48,  2.16it/s]Evaluating:  45%|████▌     | 142/313 [01:05<01:19,  2.15it/s]Evaluating:  67%|██████▋   | 210/313 [01:40<00:47,  2.15it/s]Evaluating:  46%|████▌     | 143/313 [01:05<01:17,  2.19it/s]Evaluating:  67%|██████▋   | 211/313 [01:41<00:46,  2.19it/s]Evaluating:  46%|████▌     | 144/313 [01:06<01:17,  2.18it/s]Evaluating:  68%|██████▊   | 212/313 [01:41<00:46,  2.18it/s]Evaluating:  46%|████▋     | 145/313 [01:06<01:17,  2.16it/s]Evaluating:  68%|██████▊   | 213/313 [01:41<00:46,  2.16it/s]Evaluating:  47%|████▋     | 146/313 [01:06<01:16,  2.19it/s]Evaluating:  68%|██████▊   | 214/313 [01:42<00:44,  2.20it/s]Evaluating:  47%|████▋     | 147/313 [01:07<01:16,  2.18it/s]Evaluating:  69%|██████▊   | 215/313 [01:42<00:44,  2.18it/s]Evaluating:  47%|████▋     | 148/313 [01:07<01:15,  2.17it/s]Evaluating:  69%|██████▉   | 216/313 [01:43<00:44,  2.16it/s]Evaluating:  48%|████▊     | 149/313 [01:08<01:15,  2.16it/s]Evaluating:  69%|██████▉   | 217/313 [01:43<00:44,  2.15it/s]Evaluating:  48%|████▊     | 150/313 [01:08<01:14,  2.19it/s]Evaluating:  70%|██████▉   | 218/313 [01:44<00:43,  2.20it/s]Evaluating:  48%|████▊     | 151/313 [01:09<01:14,  2.17it/s]Evaluating:  70%|██████▉   | 219/313 [01:44<00:43,  2.17it/s]Evaluating:  49%|████▊     | 152/313 [01:09<01:14,  2.16it/s]Evaluating:  70%|███████   | 220/313 [01:45<00:43,  2.16it/s]Evaluating:  49%|████▉     | 153/313 [01:10<01:14,  2.15it/s]Evaluating:  71%|███████   | 221/313 [01:45<00:42,  2.15it/s]Evaluating:  49%|████▉     | 154/313 [01:10<01:12,  2.19it/s]Evaluating:  71%|███████   | 222/313 [01:46<00:41,  2.19it/s]Evaluating:  50%|████▉     | 155/313 [01:11<01:12,  2.17it/s]Evaluating:  71%|███████   | 223/313 [01:46<00:41,  2.17it/s]Evaluating:  50%|████▉     | 156/313 [01:11<01:12,  2.16it/s]Evaluating:  72%|███████▏  | 224/313 [01:47<00:41,  2.16it/s]Evaluating:  50%|█████     | 157/313 [01:11<01:11,  2.19it/s]Evaluating:  72%|███████▏  | 225/313 [01:47<00:40,  2.20it/s]Evaluating:  50%|█████     | 158/313 [01:12<01:11,  2.17it/s]Evaluating:  72%|███████▏  | 226/313 [01:47<00:40,  2.17it/s]Evaluating:  51%|█████     | 159/313 [01:12<01:11,  2.16it/s]Evaluating:  73%|███████▎  | 227/313 [01:48<00:39,  2.16it/s]Evaluating:  51%|█████     | 160/313 [01:13<01:11,  2.15it/s]Evaluating:  73%|███████▎  | 228/313 [01:48<00:39,  2.15it/s]Evaluating:  51%|█████▏    | 161/313 [01:13<01:09,  2.19it/s]Evaluating:  73%|███████▎  | 229/313 [01:49<00:38,  2.19it/s]Evaluating:  52%|█████▏    | 162/313 [01:14<01:09,  2.17it/s]Evaluating:  73%|███████▎  | 230/313 [01:49<00:38,  2.17it/s]Evaluating:  52%|█████▏    | 163/313 [01:14<01:09,  2.15it/s]Evaluating:  74%|███████▍  | 231/313 [01:50<00:38,  2.16it/s]Evaluating:  52%|█████▏    | 164/313 [01:15<01:09,  2.15it/s]Evaluating:  74%|███████▍  | 232/313 [01:50<00:37,  2.15it/s]Evaluating:  53%|█████▎    | 165/313 [01:15<01:07,  2.19it/s]Evaluating:  74%|███████▍  | 233/313 [01:51<00:36,  2.19it/s]Evaluating:  53%|█████▎    | 166/313 [01:16<01:07,  2.18it/s]Evaluating:  75%|███████▍  | 234/313 [01:51<00:36,  2.16it/s]Evaluating:  53%|█████▎    | 167/313 [01:16<01:07,  2.16it/s]Evaluating:  75%|███████▌  | 235/313 [01:52<00:36,  2.15it/s]Evaluating:  54%|█████▎    | 168/313 [01:17<01:06,  2.20it/s]Evaluating:  75%|███████▌  | 236/313 [01:52<00:35,  2.19it/s]Evaluating:  54%|█████▍    | 169/313 [01:17<01:06,  2.18it/s]Evaluating:  76%|███████▌  | 237/313 [01:53<00:35,  2.17it/s]Evaluating:  54%|█████▍    | 170/313 [01:17<01:06,  2.16it/s]Evaluating:  76%|███████▌  | 238/313 [01:53<00:34,  2.15it/s]Evaluating:  55%|█████▍    | 171/313 [01:18<01:05,  2.15it/s]Evaluating:  76%|███████▋  | 239/313 [01:53<00:34,  2.14it/s]Evaluating:  55%|█████▍    | 172/313 [01:18<01:04,  2.20it/s]Evaluating:  77%|███████▋  | 240/313 [01:54<00:33,  2.18it/s]Evaluating:  55%|█████▌    | 173/313 [01:19<01:04,  2.17it/s]Evaluating:  77%|███████▋  | 241/313 [01:54<00:33,  2.16it/s]Evaluating:  56%|█████▌    | 174/313 [01:19<01:04,  2.16it/s]Evaluating:  77%|███████▋  | 242/313 [01:55<00:33,  2.15it/s]Evaluating:  56%|█████▌    | 175/313 [01:20<01:04,  2.15it/s]Evaluating:  78%|███████▊  | 243/313 [01:55<00:32,  2.14it/s]Evaluating:  56%|█████▌    | 176/313 [01:20<01:02,  2.20it/s]Evaluating:  78%|███████▊  | 244/313 [01:56<00:31,  2.18it/s]Evaluating:  57%|█████▋    | 177/313 [01:21<01:02,  2.19it/s]Evaluating:  78%|███████▊  | 245/313 [01:56<00:31,  2.17it/s]Evaluating:  57%|█████▋    | 178/313 [01:21<01:02,  2.17it/s]Evaluating:  79%|███████▊  | 246/313 [01:57<00:31,  2.15it/s]Evaluating:  57%|█████▋    | 179/313 [01:22<01:01,  2.19it/s]Evaluating:  79%|███████▉  | 247/313 [01:57<00:30,  2.19it/s]Evaluating:  58%|█████▊    | 180/313 [01:22<01:00,  2.20it/s]Evaluating:  79%|███████▉  | 248/313 [01:58<00:29,  2.19it/s]Evaluating:  58%|█████▊    | 181/313 [01:23<01:02,  2.12it/s]Evaluating:  80%|███████▉  | 249/313 [01:58<00:29,  2.17it/s]Evaluating:  58%|█████▊    | 182/313 [01:23<01:01,  2.13it/s]Evaluating:  80%|███████▉  | 250/313 [01:59<00:29,  2.15it/s]Evaluating:  58%|█████▊    | 183/313 [01:23<00:59,  2.18it/s]Evaluating:  80%|████████  | 251/313 [01:59<00:28,  2.19it/s]Evaluating:  59%|█████▉    | 184/313 [01:24<00:59,  2.17it/s]Evaluating:  81%|████████  | 252/313 [01:59<00:28,  2.16it/s]Evaluating:  59%|█████▉    | 185/313 [01:24<00:59,  2.16it/s]Evaluating:  81%|████████  | 253/313 [02:00<00:27,  2.15it/s]Evaluating:  59%|█████▉    | 186/313 [01:25<00:58,  2.15it/s]Evaluating:  81%|████████  | 254/313 [02:00<00:27,  2.15it/s]Evaluating:  60%|█████▉    | 187/313 [01:25<00:57,  2.20it/s]Evaluating:  81%|████████▏ | 255/313 [02:01<00:26,  2.16it/s]Evaluating:  60%|██████    | 188/313 [01:26<00:57,  2.18it/s]Evaluating:  82%|████████▏ | 256/313 [02:01<00:26,  2.16it/s]Evaluating:  60%|██████    | 189/313 [01:26<00:57,  2.17it/s]Evaluating:  82%|████████▏ | 257/313 [02:02<00:26,  2.15it/s]Evaluating:  61%|██████    | 190/313 [01:27<00:55,  2.21it/s]Evaluating:  82%|████████▏ | 258/313 [02:02<00:25,  2.19it/s]Evaluating:  61%|██████    | 191/313 [01:27<00:55,  2.18it/s]Evaluating:  83%|████████▎ | 259/313 [02:03<00:24,  2.18it/s]Evaluating:  61%|██████▏   | 192/313 [01:28<00:56,  2.14it/s]Evaluating:  83%|████████▎ | 260/313 [02:03<00:24,  2.16it/s]Evaluating:  62%|██████▏   | 193/313 [01:28<00:56,  2.13it/s]Evaluating:  83%|████████▎ | 261/313 [02:04<00:24,  2.15it/s]Evaluating:  62%|██████▏   | 194/313 [01:29<00:54,  2.18it/s]Evaluating:  84%|████████▎ | 262/313 [02:04<00:23,  2.19it/s]Evaluating:  62%|██████▏   | 195/313 [01:29<00:54,  2.17it/s]Evaluating:  84%|████████▍ | 263/313 [02:05<00:23,  2.17it/s]Evaluating:  63%|██████▎   | 196/313 [01:29<00:54,  2.16it/s]Evaluating:  84%|████████▍ | 264/313 [02:05<00:22,  2.15it/s]Evaluating:  63%|██████▎   | 197/313 [01:30<00:53,  2.16it/s]Evaluating:  85%|████████▍ | 265/313 [02:05<00:22,  2.17it/s]Evaluating:  63%|██████▎   | 198/313 [01:30<00:52,  2.19it/s]Evaluating:  85%|████████▍ | 266/313 [02:06<00:21,  2.16it/s]Evaluating:  64%|██████▎   | 199/313 [01:31<00:52,  2.17it/s]Evaluating:  85%|████████▌ | 267/313 [02:06<00:21,  2.15it/s]Evaluating:  64%|██████▍   | 200/313 [01:31<00:52,  2.16it/s]Evaluating:  86%|████████▌ | 268/313 [02:07<00:21,  2.14it/s]Evaluating:  64%|██████▍   | 201/313 [01:32<00:50,  2.20it/s]Evaluating:  86%|████████▌ | 269/313 [02:07<00:20,  2.18it/s]Evaluating:  65%|██████▍   | 202/313 [01:32<00:50,  2.18it/s]Evaluating:  86%|████████▋ | 270/313 [02:08<00:19,  2.16it/s]Evaluating:  65%|██████▍   | 203/313 [01:33<00:50,  2.16it/s]Evaluating:  87%|████████▋ | 271/313 [02:08<00:19,  2.14it/s]Evaluating:  65%|██████▌   | 204/313 [01:33<00:50,  2.16it/s]Evaluating:  87%|████████▋ | 272/313 [02:09<00:19,  2.13it/s]Evaluating:  65%|██████▌   | 205/313 [01:34<00:49,  2.20it/s]Evaluating:  87%|████████▋ | 273/313 [02:09<00:18,  2.17it/s]Evaluating:  66%|██████▌   | 206/313 [01:34<00:49,  2.18it/s]Evaluating:  88%|████████▊ | 274/313 [02:10<00:18,  2.15it/s]Evaluating:  66%|██████▌   | 207/313 [01:35<00:48,  2.16it/s]Evaluating:  88%|████████▊ | 275/313 [02:10<00:17,  2.14it/s]Evaluating:  66%|██████▋   | 208/313 [01:35<00:48,  2.17it/s]Evaluating:  88%|████████▊ | 276/313 [02:11<00:16,  2.18it/s]Evaluating:  67%|██████▋   | 209/313 [01:35<00:47,  2.19it/s]Evaluating:  88%|████████▊ | 277/313 [02:11<00:16,  2.16it/s]Evaluating:  67%|██████▋   | 210/313 [01:36<00:47,  2.17it/s]Evaluating:  89%|████████▉ | 278/313 [02:11<00:16,  2.14it/s]Evaluating:  67%|██████▋   | 211/313 [01:36<00:47,  2.16it/s]Evaluating:  89%|████████▉ | 279/313 [02:12<00:15,  2.13it/s]Evaluating:  68%|██████▊   | 212/313 [01:37<00:45,  2.20it/s]Evaluating:  89%|████████▉ | 280/313 [02:12<00:15,  2.17it/s]Evaluating:  68%|██████▊   | 213/313 [01:37<00:45,  2.19it/s]Evaluating:  90%|████████▉ | 281/313 [02:13<00:14,  2.16it/s]Evaluating:  68%|██████▊   | 214/313 [01:38<00:45,  2.17it/s]Evaluating:  90%|█████████ | 282/313 [02:13<00:14,  2.14it/s]Evaluating:  69%|██████▊   | 215/313 [01:38<00:45,  2.16it/s]Evaluating:  90%|█████████ | 283/313 [02:14<00:13,  2.15it/s]Evaluating:  69%|██████▉   | 216/313 [01:39<00:44,  2.20it/s]Evaluating:  91%|█████████ | 284/313 [02:14<00:13,  2.17it/s]Evaluating:  69%|██████▉   | 217/313 [01:39<00:44,  2.18it/s]Evaluating:  91%|█████████ | 285/313 [02:15<00:13,  2.15it/s]Evaluating:  70%|██████▉   | 218/313 [01:40<00:43,  2.17it/s]Evaluating:  91%|█████████▏| 286/313 [02:15<00:12,  2.10it/s]Evaluating:  70%|██████▉   | 219/313 [01:40<00:42,  2.20it/s]Evaluating:  92%|█████████▏| 287/313 [02:16<00:12,  2.15it/s]Evaluating:  70%|███████   | 220/313 [01:40<00:42,  2.20it/s]Evaluating:  92%|█████████▏| 288/313 [02:16<00:11,  2.14it/s]Evaluating:  71%|███████   | 221/313 [01:41<00:42,  2.18it/s]Evaluating:  92%|█████████▏| 289/313 [02:17<00:11,  2.13it/s]Evaluating:  71%|███████   | 222/313 [01:41<00:41,  2.17it/s]Evaluating:  93%|█████████▎| 290/313 [02:17<00:10,  2.13it/s]Evaluating:  71%|███████   | 223/313 [01:42<00:40,  2.21it/s]Evaluating:  93%|█████████▎| 291/313 [02:18<00:10,  2.16it/s]Evaluating:  72%|███████▏  | 224/313 [01:42<00:40,  2.18it/s]Evaluating:  93%|█████████▎| 292/313 [02:18<00:09,  2.14it/s]Evaluating:  72%|███████▏  | 225/313 [01:43<00:40,  2.17it/s]Evaluating:  94%|█████████▎| 293/313 [02:19<00:09,  2.10it/s]Evaluating:  72%|███████▏  | 226/313 [01:43<00:40,  2.17it/s]Evaluating:  94%|█████████▍| 294/313 [02:19<00:09,  2.09it/s]Evaluating:  73%|███████▎  | 227/313 [01:44<00:38,  2.25it/s]Evaluating:  94%|█████████▍| 295/313 [02:20<00:08,  2.05it/s]Evaluating:  73%|███████▎  | 228/313 [01:44<00:37,  2.25it/s]Evaluating:  73%|███████▎  | 229/313 [01:45<00:37,  2.21it/s]Evaluating:  95%|█████████▍| 296/313 [02:20<00:08,  2.04it/s]Evaluating:  73%|███████▎  | 230/313 [01:45<00:37,  2.21it/s]Evaluating:  95%|█████████▍| 297/313 [02:20<00:07,  2.07it/s]Evaluating:  74%|███████▍  | 231/313 [01:45<00:36,  2.22it/s]Evaluating:  95%|█████████▌| 298/313 [02:21<00:07,  2.09it/s]Evaluating:  74%|███████▍  | 232/313 [01:46<00:36,  2.20it/s]Evaluating:  96%|█████████▌| 299/313 [02:21<00:06,  2.09it/s]Evaluating:  74%|███████▍  | 233/313 [01:46<00:36,  2.18it/s]Evaluating:  96%|█████████▌| 300/313 [02:22<00:06,  2.09it/s]Evaluating:  75%|███████▍  | 234/313 [01:47<00:35,  2.22it/s]Evaluating:  96%|█████████▌| 301/313 [02:22<00:05,  2.14it/s]Evaluating:  75%|███████▌  | 235/313 [01:47<00:35,  2.20it/s]Evaluating:  96%|█████████▋| 302/313 [02:23<00:05,  2.12it/s]Evaluating:  75%|███████▌  | 236/313 [01:48<00:35,  2.18it/s]Evaluating:  97%|█████████▋| 303/313 [02:23<00:04,  2.12it/s]Evaluating:  76%|███████▌  | 237/313 [01:48<00:35,  2.17it/s]Evaluating:  97%|█████████▋| 304/313 [02:24<00:04,  2.13it/s]Evaluating:  76%|███████▌  | 238/313 [01:49<00:33,  2.21it/s]Evaluating:  97%|█████████▋| 305/313 [02:24<00:03,  2.14it/s]Evaluating:  76%|███████▋  | 239/313 [01:49<00:33,  2.19it/s]Evaluating:  98%|█████████▊| 306/313 [02:25<00:03,  2.13it/s]Evaluating:  77%|███████▋  | 240/313 [01:50<00:33,  2.17it/s]Evaluating:  98%|█████████▊| 307/313 [02:25<00:02,  2.11it/s]Evaluating:  77%|███████▋  | 241/313 [01:50<00:32,  2.20it/s]Evaluating:  98%|█████████▊| 308/313 [02:26<00:02,  2.15it/s]Evaluating:  77%|███████▋  | 242/313 [01:50<00:32,  2.20it/s]Evaluating:  99%|█████████▊| 309/313 [02:26<00:01,  2.14it/s]Evaluating:  78%|███████▊  | 243/313 [01:51<00:32,  2.18it/s]Evaluating:  99%|█████████▉| 310/313 [02:27<00:01,  2.13it/s]Evaluating:  78%|███████▊  | 244/313 [01:51<00:31,  2.16it/s]Evaluating:  99%|█████████▉| 311/313 [02:27<00:00,  2.12it/s]Evaluating:  78%|███████▊  | 245/313 [01:52<00:30,  2.20it/s]Evaluating: 100%|█████████▉| 312/313 [02:28<00:00,  2.16it/s]Evaluating:  79%|███████▊  | 246/313 [01:52<00:30,  2.21it/s]Evaluating: 100%|██████████| 313/313 [02:28<00:00,  2.40it/s]Evaluating: 100%|██████████| 313/313 [02:28<00:00,  2.11it/s]Evaluating:  79%|███████▉  | 247/313 [01:53<00:27,  2.42it/s]Evaluating:  79%|███████▉  | 248/313 [01:53<00:24,  2.61it/s]Evaluating:  80%|███████▉  | 249/313 [01:53<00:22,  2.80it/s]Evaluating:  80%|███████▉  | 250/313 [01:54<00:21,  2.92it/s]
10/10/2021 15:25:28 - INFO - __main__ -   ***** Evaluation result  in da *****
10/10/2021 15:25:28 - INFO - __main__ -     f1 = 0.7345976566623272
10/10/2021 15:25:28 - INFO - __main__ -     loss = 0.6712212385460973
10/10/2021 15:25:28 - INFO - __main__ -     precision = 0.7090266929294301
10/10/2021 15:25:28 - INFO - __main__ -     recall = 0.7620820557745657
10/10/2021 15:25:28 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  80%|████████  | 251/313 [01:54<00:20,  3.00it/s]Evaluating:  81%|████████  | 252/313 [01:54<00:19,  3.09it/s]Evaluating:  81%|████████  | 253/313 [01:54<00:19,  3.10it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  81%|████████  | 254/313 [01:55<00:18,  3.13it/s]Evaluating:  81%|████████▏ | 255/313 [01:55<00:18,  3.18it/s]Evaluating:  82%|████████▏ | 256/313 [01:55<00:17,  3.25it/s]Evaluating:  82%|████████▏ | 257/313 [01:56<00:17,  3.24it/s]Evaluating:  82%|████████▏ | 258/313 [01:56<00:17,  3.23it/s]Evaluating:  83%|████████▎ | 259/313 [01:56<00:16,  3.28it/s]Evaluating:  83%|████████▎ | 260/313 [01:57<00:16,  3.27it/s]Evaluating:  83%|████████▎ | 261/313 [01:57<00:15,  3.26it/s]Evaluating:  84%|████████▎ | 262/313 [01:57<00:15,  3.25it/s]Evaluating:  84%|████████▍ | 263/313 [01:58<00:15,  3.28it/s]Evaluating:  84%|████████▍ | 264/313 [01:58<00:15,  3.26it/s]Evaluating:  85%|████████▍ | 265/313 [01:58<00:14,  3.24it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  85%|████████▍ | 266/313 [01:58<00:14,  3.27it/s]Evaluating:  85%|████████▌ | 267/313 [01:59<00:14,  3.26it/s]Evaluating:  86%|████████▌ | 268/313 [01:59<00:13,  3.24it/s]Evaluating:  86%|████████▌ | 269/313 [01:59<00:13,  3.25it/s]Evaluating:  86%|████████▋ | 270/313 [02:00<00:13,  3.28it/s]Evaluating:  87%|████████▋ | 271/313 [02:00<00:12,  3.26it/s]Evaluating:  87%|████████▋ | 272/313 [02:00<00:12,  3.24it/s]Evaluating:  87%|████████▋ | 273/313 [02:01<00:12,  3.29it/s]Evaluating:  88%|████████▊ | 274/313 [02:01<00:12,  3.25it/s]Evaluating:  88%|████████▊ | 275/313 [02:01<00:11,  3.21it/s]Evaluating:  88%|████████▊ | 276/313 [02:02<00:11,  3.25it/s]Evaluating:  88%|████████▊ | 277/313 [02:02<00:10,  3.29it/s]Evaluating:  89%|████████▉ | 278/313 [02:02<00:10,  3.26it/s]Evaluating:  89%|████████▉ | 279/313 [02:02<00:10,  3.23it/s]Evaluating:  89%|████████▉ | 280/313 [02:03<00:10,  3.29it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  90%|████████▉ | 281/313 [02:03<00:09,  3.28it/s]Evaluating:  90%|█████████ | 282/313 [02:03<00:09,  3.28it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  90%|█████████ | 283/313 [02:04<00:09,  3.26it/s]Evaluating:  91%|█████████ | 284/313 [02:04<00:08,  3.29it/s]Evaluating:  91%|█████████ | 285/313 [02:04<00:08,  3.24it/s]Evaluating:  91%|█████████▏| 286/313 [02:05<00:08,  3.23it/s]Evaluating:  92%|█████████▏| 287/313 [02:05<00:07,  3.29it/s]Evaluating:  92%|█████████▏| 288/313 [02:05<00:07,  3.32it/s]Evaluating:  92%|█████████▏| 289/313 [02:06<00:07,  3.27it/s]Evaluating:  93%|█████████▎| 290/313 [02:06<00:07,  3.27it/s]Evaluating:  93%|█████████▎| 291/313 [02:06<00:06,  3.28it/s]Evaluating:  93%|█████████▎| 292/313 [02:06<00:06,  3.29it/s]Evaluating:  94%|█████████▎| 293/313 [02:07<00:06,  3.30it/s]Evaluating:  94%|█████████▍| 294/313 [02:07<00:05,  3.30it/s]Evaluating:  94%|█████████▍| 295/313 [02:07<00:05,  3.32it/s]Evaluating:  95%|█████████▍| 296/313 [02:08<00:05,  3.27it/s]Evaluating:  95%|█████████▍| 297/313 [02:08<00:04,  3.25it/s]Evaluating:  95%|█████████▌| 298/313 [02:08<00:04,  3.28it/s]Evaluating:  96%|█████████▌| 299/313 [02:09<00:04,  3.26it/s]Evaluating:  96%|█████████▌| 300/313 [02:09<00:04,  3.24it/s]Evaluating:  96%|█████████▌| 301/313 [02:09<00:03,  3.29it/s]Evaluating:  96%|█████████▋| 302/313 [02:09<00:03,  3.31it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  97%|█████████▋| 303/313 [02:10<00:03,  3.29it/s]10/10/2021 15:25:44 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:25:44 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  97%|█████████▋| 304/313 [02:10<00:02,  3.28it/s]Evaluating:  97%|█████████▋| 305/313 [02:10<00:02,  3.25it/s]Evaluating:  98%|█████████▊| 306/313 [02:11<00:02,  3.21it/s]Evaluating:  98%|█████████▊| 307/313 [02:11<00:01,  3.19it/s]Evaluating:  98%|█████████▊| 308/313 [02:11<00:01,  3.22it/s]Evaluating:  99%|█████████▊| 309/313 [02:12<00:01,  3.24it/s]Evaluating:  99%|█████████▉| 310/313 [02:12<00:00,  3.20it/s]Evaluating:  99%|█████████▉| 311/313 [02:12<00:00,  3.23it/s]Evaluating: 100%|█████████▉| 312/313 [02:13<00:00,  3.19it/s]Evaluating: 100%|██████████| 313/313 [02:13<00:00,  3.61it/s]Evaluating: 100%|██████████| 313/313 [02:13<00:00,  2.35it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}


10/10/2021 15:25:48 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/10/2021 15:25:48 - INFO - __main__ -     f1 = 0.5037085874402506
10/10/2021 15:25:48 - INFO - __main__ -     loss = 1.2417030721998061
10/10/2021 15:25:48 - INFO - __main__ -     precision = 0.45164341451879875
10/10/2021 15:25:48 - INFO - __main__ -     recall = 0.569341977792682
10/10/2021 15:25:48 - INFO - __main__ -   Language adapter for bg not found, using ru instead
10/10/2021 15:25:48 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:25:48 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/10/2021 15:25:50 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/10/2021 15:25:50 - INFO - __main__ -     Num examples = 10004
10/10/2021 15:25:50 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:28,  3.52it/s]Evaluating:   1%|          | 2/313 [00:00<01:32,  3.36it/s]Evaluating:   1%|          | 3/313 [00:00<01:33,  3.32it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:31,  3.38it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:30,  3.41it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:31,  3.37it/s]Evaluating:   2%|▏         | 7/313 [00:02<01:31,  3.33it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:   3%|▎         | 8/313 [00:02<01:30,  3.38it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:29,  3.41it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:30,  3.37it/s]Evaluating:   4%|▎         | 11/313 [00:03<01:29,  3.39it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:28,  3.41it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:28,  3.38it/s]Evaluating:   4%|▍         | 14/313 [00:04<01:29,  3.35it/s]Evaluating:   5%|▍         | 15/313 [00:04<01:27,  3.39it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:26,  3.42it/s]Evaluating:   5%|▌         | 17/313 [00:05<01:27,  3.38it/s]Evaluating:   6%|▌         | 18/313 [00:05<01:27,  3.36it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:26,  3.40it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:25,  3.41it/s]Evaluating:   7%|▋         | 21/313 [00:06<01:26,  3.38it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:25,  3.40it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:   7%|▋         | 23/313 [00:06<01:24,  3.43it/s]Evaluating:   8%|▊         | 24/313 [00:07<01:35,  3.03it/s]Evaluating:   8%|▊         | 25/313 [00:07<01:29,  3.21it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:24,  3.40it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:22,  3.48it/s]Evaluating:   9%|▉         | 28/313 [00:08<01:20,  3.53it/s]Evaluating:   9%|▉         | 29/313 [00:08<01:20,  3.52it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  10%|▉         | 30/313 [00:08<01:19,  3.57it/s]Evaluating:  10%|▉         | 31/313 [00:09<01:18,  3.60it/s]Evaluating:  10%|█         | 32/313 [00:09<01:19,  3.53it/s]Evaluating:  11%|█         | 33/313 [00:09<01:20,  3.47it/s]Evaluating:  11%|█         | 34/313 [00:09<01:20,  3.48it/s]Evaluating:  11%|█         | 35/313 [00:10<01:19,  3.49it/s]Evaluating:  12%|█▏        | 36/313 [00:10<01:20,  3.43it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:21,  3.37it/s]Evaluating:  12%|█▏        | 38/313 [00:11<01:20,  3.40it/s]Evaluating:  12%|█▏        | 39/313 [00:11<01:20,  3.41it/s]Evaluating:  13%|█▎        | 40/313 [00:11<01:20,  3.37it/s]Evaluating:  13%|█▎        | 41/313 [00:12<01:21,  3.34it/s]Evaluating:  13%|█▎        | 42/313 [00:12<01:19,  3.42it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:26:02 - INFO - __main__ -   Using lang2id = None
Evaluating:  14%|█▎        | 43/313 [00:12<01:19,  3.38it/s]Evaluating:  14%|█▍        | 44/313 [00:12<01:20,  3.35it/s]Evaluating:  14%|█▍        | 45/313 [00:13<01:19,  3.37it/s]Evaluating:  15%|█▍        | 46/313 [00:13<01:18,  3.40it/s]Evaluating:  15%|█▌        | 47/313 [00:13<01:19,  3.36it/s]Evaluating:  15%|█▌        | 48/313 [00:14<01:19,  3.34it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  16%|█▌        | 49/313 [00:14<01:18,  3.37it/s]10/10/2021 15:26:04 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:26:04 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:26:04 - INFO - __main__ -   Seed = 32
10/10/2021 15:26:04 - INFO - root -   save model
10/10/2021 15:26:04 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:26:04 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  16%|█▌        | 50/313 [00:14<01:17,  3.40it/s]Evaluating:  16%|█▋        | 51/313 [00:15<01:18,  3.36it/s]Evaluating:  17%|█▋        | 52/313 [00:15<01:18,  3.33it/s]Evaluating:  17%|█▋        | 53/313 [00:15<01:16,  3.40it/s]Evaluating:  17%|█▋        | 54/313 [00:15<01:16,  3.38it/s]Evaluating:  18%|█▊        | 55/313 [00:16<01:16,  3.35it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  18%|█▊        | 56/313 [00:16<01:17,  3.33it/s]Evaluating:  18%|█▊        | 57/313 [00:16<01:14,  3.43it/s]Evaluating:  19%|█▊        | 58/313 [00:17<01:19,  3.21it/s]Evaluating:  19%|█▉        | 59/313 [00:17<01:18,  3.23it/s]Evaluating:  19%|█▉        | 60/313 [00:17<01:16,  3.31it/s]Evaluating:  19%|█▉        | 61/313 [00:18<01:15,  3.33it/s]Evaluating:  20%|█▉        | 62/313 [00:18<01:15,  3.31it/s]Evaluating:  20%|██        | 63/313 [00:18<01:15,  3.30it/s]Evaluating:  20%|██        | 64/313 [00:18<01:13,  3.41it/s]Evaluating:  21%|██        | 65/313 [00:19<01:13,  3.37it/s]Evaluating:  21%|██        | 66/313 [00:19<01:13,  3.34it/s]Evaluating:  21%|██▏       | 67/313 [00:19<01:13,  3.33it/s]Evaluating:  22%|██▏       | 68/313 [00:20<01:11,  3.42it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  22%|██▏       | 69/313 [00:20<01:12,  3.38it/s]Evaluating:  22%|██▏       | 70/313 [00:20<01:12,  3.37it/s]Evaluating:  23%|██▎       | 71/313 [00:21<01:12,  3.35it/s]Evaluating:  23%|██▎       | 72/313 [00:21<01:10,  3.44it/s]Evaluating:  23%|██▎       | 73/313 [00:21<01:10,  3.39it/s]Evaluating:  24%|██▎       | 74/313 [00:21<01:11,  3.36it/s]Evaluating:  24%|██▍       | 75/313 [00:22<01:09,  3.41it/s]Evaluating:  24%|██▍       | 76/313 [00:22<01:10,  3.37it/s]Evaluating:  25%|██▍       | 77/313 [00:22<01:10,  3.34it/s]Evaluating:  25%|██▍       | 78/313 [00:23<01:10,  3.32it/s]Evaluating:  25%|██▌       | 79/313 [00:23<01:08,  3.42it/s]Evaluating:  26%|██▌       | 80/313 [00:23<01:09,  3.37it/s]Evaluating:  26%|██▌       | 81/313 [00:23<01:09,  3.35it/s]Evaluating:  26%|██▌       | 82/313 [00:24<01:09,  3.33it/s]Evaluating:  27%|██▋       | 83/313 [00:24<01:07,  3.42it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  27%|██▋       | 84/313 [00:24<01:07,  3.39it/s]Evaluating:  27%|██▋       | 85/313 [00:25<01:07,  3.36it/s]Evaluating:  27%|██▋       | 86/313 [00:25<01:07,  3.36it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  28%|██▊       | 87/313 [00:25<01:06,  3.41it/s]Evaluating:  28%|██▊       | 88/313 [00:26<01:06,  3.37it/s]Evaluating:  28%|██▊       | 89/313 [00:26<01:07,  3.34it/s]Evaluating:  29%|██▉       | 90/313 [00:26<01:06,  3.37it/s]Evaluating:  29%|██▉       | 91/313 [00:26<01:05,  3.39it/s]Evaluating:  29%|██▉       | 92/313 [00:27<01:05,  3.36it/s]Evaluating:  30%|██▉       | 93/313 [00:27<01:06,  3.33it/s]Evaluating:  30%|███       | 94/313 [00:27<01:04,  3.40it/s]Evaluating:  30%|███       | 95/313 [00:28<01:04,  3.37it/s]Evaluating:  31%|███       | 96/313 [00:28<01:05,  3.33it/s]Evaluating:  31%|███       | 97/313 [00:28<01:04,  3.36it/s]Evaluating:  31%|███▏      | 98/313 [00:29<01:03,  3.39it/s]Evaluating:  32%|███▏      | 99/313 [00:29<01:03,  3.35it/s]Evaluating:  32%|███▏      | 100/313 [00:29<01:03,  3.34it/s]Evaluating:  32%|███▏      | 101/313 [00:29<01:03,  3.36it/s]Evaluating:  33%|███▎      | 102/313 [00:30<01:02,  3.39it/s]Evaluating:  33%|███▎      | 103/313 [00:30<01:02,  3.35it/s]Evaluating:  33%|███▎      | 104/313 [00:30<01:03,  3.32it/s]Evaluating:  34%|███▎      | 105/313 [00:31<01:02,  3.35it/s]Evaluating:  34%|███▍      | 106/313 [00:31<01:01,  3.38it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  34%|███▍      | 107/313 [00:31<01:01,  3.35it/s]10/10/2021 15:26:22 - INFO - __main__ -   Using lang2id = None
Evaluating:  35%|███▍      | 108/313 [00:31<01:00,  3.38it/s]10/10/2021 15:26:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:26:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/10/2021 15:26:22 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:26:22 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:26:22 - INFO - root -   loading lang adpater is/wiki@ukp
10/10/2021 15:26:22 - INFO - __main__ -   Language = is
10/10/2021 15:26:22 - INFO - __main__ -   Adapter Name = is/wiki@ukp
Evaluating:  35%|███▍      | 109/313 [00:32<00:59,  3.42it/s]No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:  35%|███▌      | 110/313 [00:32<00:59,  3.44it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Evaluating:  35%|███▌      | 111/313 [00:32<00:59,  3.41it/s]Evaluating:  36%|███▌      | 112/313 [00:33<00:58,  3.43it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
Evaluating:  36%|███▌      | 113/313 [00:33<00:58,  3.44it/s]Evaluating:  36%|███▋      | 114/313 [00:33<00:59,  3.37it/s]Evaluating:  37%|███▋      | 115/313 [00:34<00:59,  3.33it/s]Evaluating:  37%|███▋      | 116/313 [00:34<00:58,  3.36it/s]Evaluating:  37%|███▋      | 117/313 [00:34<00:58,  3.37it/s]Evaluating:  38%|███▊      | 118/313 [00:34<00:58,  3.32it/s]Evaluating:  38%|███▊      | 119/313 [00:35<00:59,  3.28it/s]Evaluating:  38%|███▊      | 120/313 [00:35<00:58,  3.32it/s]Evaluating:  39%|███▊      | 121/313 [00:35<00:57,  3.34it/s]Evaluating:  39%|███▉      | 122/313 [00:36<00:57,  3.31it/s]Evaluating:  39%|███▉      | 123/313 [00:36<00:56,  3.33it/s]Evaluating:  40%|███▉      | 124/313 [00:36<00:56,  3.33it/s]Evaluating:  40%|███▉      | 125/313 [00:37<00:58,  3.19it/s]Evaluating:  40%|████      | 126/313 [00:37<00:58,  3.20it/s]Evaluating:  41%|████      | 127/313 [00:37<00:57,  3.26it/s]Evaluating:  41%|████      | 128/313 [00:37<00:56,  3.30it/s]Evaluating:  41%|████      | 129/313 [00:38<00:56,  3.28it/s]Evaluating:  42%|████▏     | 130/313 [00:38<00:55,  3.31it/s]Evaluating:  42%|████▏     | 131/313 [00:38<00:55,  3.29it/s]Evaluating:  42%|████▏     | 132/313 [00:39<00:54,  3.32it/s]Evaluating:  42%|████▏     | 133/313 [00:39<00:54,  3.29it/s]Evaluating:  43%|████▎     | 134/313 [00:39<00:54,  3.31it/s]Evaluating:  43%|████▎     | 135/313 [00:40<00:53,  3.34it/s]Evaluating:  43%|████▎     | 136/313 [00:40<00:53,  3.30it/s]Evaluating:  44%|████▍     | 137/313 [00:40<00:53,  3.28it/s]Evaluating:  44%|████▍     | 138/313 [00:41<00:52,  3.31it/s]10/10/2021 15:26:31 - INFO - __main__ -   Language adapter for fo not found, using is instead
10/10/2021 15:26:31 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:26:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/10/2021 15:26:31 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/10/2021 15:26:31 - INFO - __main__ -     Num examples = 100
10/10/2021 15:26:31 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  44%|████▍     | 139/313 [00:41<00:59,  2.91it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:01,  2.20it/s]Evaluating:  45%|████▍     | 140/313 [00:41<01:05,  2.62it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  2.17it/s]Evaluating:  45%|████▌     | 141/313 [00:42<01:09,  2.49it/s]Evaluating:  75%|███████▌  | 3/4 [00:01<00:00,  2.22it/s]Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.55it/s]Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.41it/s]
10/10/2021 15:26:33 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/10/2021 15:26:33 - INFO - __main__ -     f1 = 0.6738351254480287
10/10/2021 15:26:33 - INFO - __main__ -     loss = 1.495644971728325
10/10/2021 15:26:33 - INFO - __main__ -     precision = 0.5911949685534591
10/10/2021 15:26:33 - INFO - __main__ -     recall = 0.7833333333333333
10/10/2021 15:26:33 - INFO - __main__ -   Language adapter for no not found, using is instead
10/10/2021 15:26:33 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:26:33 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
Evaluating:  45%|████▌     | 142/313 [00:42<01:05,  2.61it/s]Evaluating:  46%|████▌     | 143/313 [00:43<01:00,  2.82it/s]Evaluating:  46%|████▌     | 144/313 [00:43<00:57,  2.94it/s]Evaluating:  46%|████▋     | 145/313 [00:43<00:54,  3.07it/s]Evaluating:  47%|████▋     | 146/313 [00:43<00:52,  3.15it/s]10/10/2021 15:26:34 - INFO - __main__ -   ***** Running evaluation  in no *****
10/10/2021 15:26:34 - INFO - __main__ -     Num examples = 10000
10/10/2021 15:26:34 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  47%|████▋     | 147/313 [00:44<00:54,  3.03it/s]Evaluating:   0%|          | 1/313 [00:00<02:25,  2.14it/s]Evaluating:  47%|████▋     | 148/313 [00:44<01:01,  2.68it/s]Evaluating:   1%|          | 2/313 [00:00<02:22,  2.19it/s]Evaluating:  48%|████▊     | 149/313 [00:45<01:05,  2.51it/s]Evaluating:   1%|          | 3/313 [00:01<02:22,  2.17it/s]Evaluating:  48%|████▊     | 150/313 [00:45<01:07,  2.41it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:21,  2.19it/s]Evaluating:  48%|████▊     | 151/313 [00:46<01:09,  2.32it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:21,  2.18it/s]Evaluating:  49%|████▊     | 152/313 [00:46<01:10,  2.27it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:19,  2.20it/s]Evaluating:  49%|████▉     | 153/313 [00:47<01:11,  2.24it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:19,  2.20it/s]Evaluating:  49%|████▉     | 154/313 [00:47<01:11,  2.23it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:19,  2.18it/s]Evaluating:  50%|████▉     | 155/313 [00:47<01:11,  2.20it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:20,  2.17it/s]Evaluating:  50%|████▉     | 156/313 [00:48<01:11,  2.19it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:18,  2.19it/s]Evaluating:  50%|█████     | 157/313 [00:48<01:11,  2.18it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:17,  2.20it/s]Evaluating:  50%|█████     | 158/313 [00:49<01:11,  2.17it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:17,  2.18it/s]Evaluating:  51%|█████     | 159/313 [00:49<01:11,  2.15it/s]Evaluating:   4%|▍         | 13/313 [00:05<02:16,  2.20it/s]Evaluating:  51%|█████     | 160/313 [00:50<01:10,  2.16it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:17,  2.18it/s]Evaluating:  51%|█████▏    | 161/313 [00:50<01:09,  2.17it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:15,  2.19it/s]Evaluating:  52%|█████▏    | 162/313 [00:51<01:10,  2.16it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:16,  2.18it/s]Evaluating:  52%|█████▏    | 163/313 [00:51<01:09,  2.15it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:14,  2.20it/s]Evaluating:  52%|█████▏    | 164/313 [00:52<01:08,  2.16it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:15,  2.18it/s]Evaluating:  53%|█████▎    | 165/313 [00:52<01:08,  2.17it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:15,  2.17it/s]Evaluating:  53%|█████▎    | 166/313 [00:53<01:08,  2.15it/s]Evaluating:   6%|▋         | 20/313 [00:09<02:15,  2.16it/s]Evaluating:  53%|█████▎    | 167/313 [00:53<01:07,  2.17it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:14,  2.18it/s]Evaluating:  54%|█████▎    | 168/313 [00:53<01:07,  2.16it/s]Evaluating:   7%|▋         | 22/313 [00:10<02:12,  2.19it/s]Evaluating:  54%|█████▍    | 169/313 [00:54<01:06,  2.16it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:13,  2.18it/s]Evaluating:  54%|█████▍    | 170/313 [00:54<01:06,  2.15it/s]Evaluating:   8%|▊         | 24/313 [00:10<02:12,  2.19it/s]Evaluating:  55%|█████▍    | 171/313 [00:55<01:05,  2.16it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:12,  2.18it/s]Evaluating:  55%|█████▍    | 172/313 [00:55<01:04,  2.17it/s]Evaluating:   8%|▊         | 26/313 [00:11<02:10,  2.19it/s]Evaluating:  55%|█████▌    | 173/313 [00:56<01:05,  2.15it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:11,  2.18it/s]Evaluating:  56%|█████▌    | 174/313 [00:56<01:04,  2.16it/s]Evaluating:   9%|▉         | 28/313 [00:12<02:06,  2.25it/s]Evaluating:   9%|▉         | 29/313 [00:13<02:07,  2.22it/s]Evaluating:  56%|█████▌    | 175/313 [00:57<01:07,  2.04it/s]Evaluating:  10%|▉         | 30/313 [00:13<02:06,  2.24it/s]Evaluating:  56%|█████▌    | 176/313 [00:57<01:05,  2.09it/s]Evaluating:  10%|▉         | 31/313 [00:14<02:07,  2.21it/s]Evaluating:  57%|█████▋    | 177/313 [00:58<01:04,  2.10it/s]Evaluating:  10%|█         | 32/313 [00:14<02:06,  2.22it/s]Evaluating:  57%|█████▋    | 178/313 [00:58<01:03,  2.12it/s]Evaluating:  11%|█         | 33/313 [00:15<02:06,  2.22it/s]Evaluating:  57%|█████▋    | 179/313 [00:59<01:02,  2.15it/s]Evaluating:  11%|█         | 34/313 [00:15<02:06,  2.20it/s]Evaluating:  58%|█████▊    | 180/313 [00:59<01:02,  2.14it/s]Evaluating:  11%|█         | 35/313 [00:15<02:06,  2.20it/s]Evaluating:  58%|█████▊    | 181/313 [01:00<01:01,  2.15it/s]Evaluating:  12%|█▏        | 36/313 [00:16<02:06,  2.19it/s]Evaluating:  58%|█████▊    | 182/313 [01:00<01:01,  2.14it/s]Evaluating:  12%|█▏        | 37/313 [00:16<02:05,  2.20it/s]Evaluating:  58%|█████▊    | 183/313 [01:01<01:00,  2.16it/s]Evaluating:  12%|█▏        | 38/313 [00:17<02:05,  2.19it/s]Evaluating:  59%|█████▉    | 184/313 [01:01<01:00,  2.15it/s]Evaluating:  12%|█▏        | 39/313 [00:17<02:03,  2.21it/s]Evaluating:  59%|█████▉    | 185/313 [01:01<00:58,  2.18it/s]Evaluating:  13%|█▎        | 40/313 [00:18<02:04,  2.20it/s]Evaluating:  59%|█████▉    | 186/313 [01:02<00:58,  2.17it/s]Evaluating:  13%|█▎        | 41/313 [00:18<02:03,  2.21it/s]Evaluating:  60%|█████▉    | 187/313 [01:02<00:58,  2.16it/s]Evaluating:  13%|█▎        | 42/313 [00:19<02:03,  2.19it/s]Evaluating:  60%|██████    | 188/313 [01:03<00:58,  2.15it/s]Evaluating:  14%|█▎        | 43/313 [00:19<02:02,  2.20it/s]Evaluating:  60%|██████    | 189/313 [01:03<00:57,  2.16it/s]Evaluating:  14%|█▍        | 44/313 [00:20<02:01,  2.21it/s]Evaluating:  61%|██████    | 190/313 [01:04<00:56,  2.17it/s]Evaluating:  14%|█▍        | 45/313 [00:20<02:02,  2.19it/s]Evaluating:  61%|██████    | 191/313 [01:04<00:56,  2.16it/s]Evaluating:  15%|█▍        | 46/313 [00:20<02:01,  2.19it/s]Evaluating:  61%|██████▏   | 192/313 [01:05<00:55,  2.18it/s]Evaluating:  15%|█▌        | 47/313 [00:21<02:01,  2.20it/s]Evaluating:  62%|██████▏   | 193/313 [01:05<00:55,  2.16it/s]Evaluating:  15%|█▌        | 48/313 [00:21<01:59,  2.21it/s]Evaluating:  62%|██████▏   | 194/313 [01:06<00:54,  2.18it/s]Evaluating:  16%|█▌        | 49/313 [00:22<02:00,  2.20it/s]Evaluating:  62%|██████▏   | 195/313 [01:06<00:54,  2.16it/s]Evaluating:  16%|█▌        | 50/313 [00:22<01:59,  2.21it/s]Evaluating:  63%|██████▎   | 196/313 [01:07<00:53,  2.17it/s]Evaluating:  16%|█▋        | 51/313 [00:23<01:59,  2.19it/s]Evaluating:  63%|██████▎   | 197/313 [01:07<00:53,  2.17it/s]Evaluating:  17%|█▋        | 52/313 [00:23<01:58,  2.20it/s]Evaluating:  63%|██████▎   | 198/313 [01:07<00:53,  2.16it/s]Evaluating:  17%|█▋        | 53/313 [00:24<01:58,  2.19it/s]Evaluating:  64%|██████▎   | 199/313 [01:08<00:53,  2.14it/s]Evaluating:  17%|█▋        | 54/313 [00:24<01:57,  2.20it/s]Evaluating:  64%|██████▍   | 200/313 [01:08<00:52,  2.16it/s]Evaluating:  18%|█▊        | 55/313 [00:25<01:57,  2.19it/s]Evaluating:  64%|██████▍   | 201/313 [01:09<00:51,  2.17it/s]Evaluating:  18%|█▊        | 56/313 [00:25<01:57,  2.19it/s]Evaluating:  65%|██████▍   | 202/313 [01:09<00:51,  2.15it/s]Evaluating:  18%|█▊        | 57/313 [00:25<01:57,  2.18it/s]Evaluating:  65%|██████▍   | 203/313 [01:10<00:50,  2.16it/s]Evaluating:  19%|█▊        | 58/313 [00:26<01:56,  2.19it/s]Evaluating:  65%|██████▌   | 204/313 [01:10<00:50,  2.15it/s]Evaluating:  19%|█▉        | 59/313 [00:26<01:55,  2.20it/s]Evaluating:  65%|██████▌   | 205/313 [01:11<00:49,  2.16it/s]Evaluating:  19%|█▉        | 60/313 [00:27<01:56,  2.18it/s]Evaluating:  66%|██████▌   | 206/313 [01:11<00:49,  2.15it/s]Evaluating:  19%|█▉        | 61/313 [00:27<01:55,  2.18it/s]Evaluating:  66%|██████▌   | 207/313 [01:12<00:48,  2.17it/s]Evaluating:  20%|█▉        | 62/313 [00:28<01:54,  2.19it/s]Evaluating:  66%|██████▋   | 208/313 [01:12<00:48,  2.18it/s]Evaluating:  20%|██        | 63/313 [00:28<01:53,  2.20it/s]Evaluating:  67%|██████▋   | 209/313 [01:13<00:48,  2.16it/s]Evaluating:  20%|██        | 64/313 [00:29<01:53,  2.19it/s]Evaluating:  67%|██████▋   | 210/313 [01:13<00:47,  2.15it/s]Evaluating:  21%|██        | 65/313 [00:29<01:52,  2.20it/s]Evaluating:  67%|██████▋   | 211/313 [01:13<00:47,  2.16it/s]Evaluating:  21%|██        | 66/313 [00:30<01:53,  2.18it/s]Evaluating:  68%|██████▊   | 212/313 [01:14<00:46,  2.17it/s]Evaluating:  21%|██▏       | 67/313 [00:30<01:52,  2.20it/s]Evaluating:  68%|██████▊   | 213/313 [01:14<00:46,  2.15it/s]Evaluating:  22%|██▏       | 68/313 [00:31<01:52,  2.18it/s]Evaluating:  68%|██████▊   | 214/313 [01:15<00:45,  2.15it/s]Evaluating:  22%|██▏       | 69/313 [00:31<01:51,  2.20it/s]Evaluating:  69%|██████▊   | 215/313 [01:15<00:45,  2.14it/s]Evaluating:  22%|██▏       | 70/313 [00:31<01:50,  2.21it/s]Evaluating:  69%|██████▉   | 216/313 [01:16<00:44,  2.16it/s]Evaluating:  23%|██▎       | 71/313 [00:32<01:50,  2.19it/s]Evaluating:  69%|██████▉   | 217/313 [01:16<00:44,  2.14it/s]Evaluating:  23%|██▎       | 72/313 [00:32<01:47,  2.25it/s]Evaluating:  23%|██▎       | 73/313 [00:33<01:48,  2.22it/s]Evaluating:  70%|██████▉   | 218/313 [01:17<00:47,  2.01it/s]Evaluating:  24%|██▎       | 74/313 [00:33<01:44,  2.29it/s]Evaluating:  70%|██████▉   | 219/313 [01:17<00:44,  2.09it/s]Evaluating:  24%|██▍       | 75/313 [00:34<01:43,  2.30it/s]Evaluating:  70%|███████   | 220/313 [01:18<00:43,  2.14it/s]Evaluating:  24%|██▍       | 76/313 [00:34<01:41,  2.33it/s]Evaluating:  71%|███████   | 221/313 [01:18<00:41,  2.20it/s]Evaluating:  25%|██▍       | 77/313 [00:34<01:41,  2.32it/s]Evaluating:  71%|███████   | 222/313 [01:19<00:41,  2.21it/s]Evaluating:  25%|██▍       | 78/313 [00:35<01:41,  2.32it/s]Evaluating:  71%|███████   | 223/313 [01:19<00:40,  2.22it/s]Evaluating:  25%|██▌       | 79/313 [00:35<01:43,  2.26it/s]Evaluating:  72%|███████▏  | 224/313 [01:19<00:40,  2.19it/s]Evaluating:  26%|██▌       | 80/313 [00:36<01:43,  2.25it/s]Evaluating:  72%|███████▏  | 225/313 [01:20<00:40,  2.19it/s]Evaluating:  26%|██▌       | 81/313 [00:36<01:44,  2.22it/s]Evaluating:  72%|███████▏  | 226/313 [01:20<00:39,  2.18it/s]Evaluating:  26%|██▌       | 82/313 [00:37<01:43,  2.23it/s]Evaluating:  73%|███████▎  | 227/313 [01:21<00:39,  2.17it/s]Evaluating:  27%|██▋       | 83/313 [00:37<01:44,  2.20it/s]Evaluating:  73%|███████▎  | 228/313 [01:21<00:39,  2.15it/s]Evaluating:  27%|██▋       | 84/313 [00:38<01:42,  2.23it/s]Evaluating:  73%|███████▎  | 229/313 [01:22<00:39,  2.11it/s]Evaluating:  27%|██▋       | 85/313 [00:38<01:42,  2.24it/s]Evaluating:  73%|███████▎  | 230/313 [01:22<00:38,  2.14it/s]Evaluating:  27%|██▋       | 86/313 [00:39<01:42,  2.21it/s]Evaluating:  74%|███████▍  | 231/313 [01:23<00:38,  2.14it/s]Evaluating:  28%|██▊       | 87/313 [00:39<01:43,  2.18it/s]Evaluating:  74%|███████▍  | 232/313 [01:23<00:37,  2.15it/s]Evaluating:  28%|██▊       | 88/313 [00:39<01:42,  2.20it/s]Evaluating:  74%|███████▍  | 233/313 [01:24<00:37,  2.13it/s]Evaluating:  28%|██▊       | 89/313 [00:40<01:41,  2.21it/s]Evaluating:  75%|███████▍  | 234/313 [01:24<00:36,  2.15it/s]Evaluating:  29%|██▉       | 90/313 [00:40<01:41,  2.19it/s]Evaluating:  75%|███████▌  | 235/313 [01:25<00:36,  2.14it/s]Evaluating:  29%|██▉       | 91/313 [00:41<01:41,  2.20it/s]Evaluating:  75%|███████▌  | 236/313 [01:25<00:35,  2.16it/s]Evaluating:  29%|██▉       | 92/313 [00:41<01:41,  2.19it/s]Evaluating:  76%|███████▌  | 237/313 [01:26<00:35,  2.17it/s]Evaluating:  30%|██▉       | 93/313 [00:42<01:40,  2.20it/s]Evaluating:  76%|███████▌  | 238/313 [01:26<00:34,  2.15it/s]Evaluating:  30%|███       | 94/313 [00:42<01:40,  2.19it/s]Evaluating:  76%|███████▋  | 239/313 [01:26<00:34,  2.14it/s]Evaluating:  30%|███       | 95/313 [00:43<01:39,  2.20it/s]Evaluating:  77%|███████▋  | 240/313 [01:27<00:34,  2.15it/s]Evaluating:  31%|███       | 96/313 [00:43<01:38,  2.20it/s]Evaluating:  77%|███████▋  | 241/313 [01:27<00:33,  2.16it/s]Evaluating:  31%|███       | 97/313 [00:44<01:38,  2.20it/s]Evaluating:  77%|███████▋  | 242/313 [01:28<00:33,  2.14it/s]Evaluating:  31%|███▏      | 98/313 [00:44<01:38,  2.18it/s]Evaluating:  78%|███████▊  | 243/313 [01:28<00:32,  2.16it/s]Evaluating:  32%|███▏      | 99/313 [00:44<01:37,  2.20it/s]Evaluating:  78%|███████▊  | 244/313 [01:29<00:32,  2.14it/s]Evaluating:  32%|███▏      | 100/313 [00:45<01:36,  2.21it/s]Evaluating:  78%|███████▊  | 245/313 [01:29<00:31,  2.15it/s]Evaluating:  32%|███▏      | 101/313 [00:45<01:36,  2.19it/s]Evaluating:  79%|███████▊  | 246/313 [01:30<00:31,  2.14it/s]Evaluating:  33%|███▎      | 102/313 [00:46<01:36,  2.19it/s]Evaluating:  79%|███████▉  | 247/313 [01:30<00:30,  2.15it/s]Evaluating:  33%|███▎      | 103/313 [00:46<01:35,  2.19it/s]Evaluating:  79%|███████▉  | 248/313 [01:31<00:29,  2.17it/s]Evaluating:  33%|███▎      | 104/313 [00:47<01:35,  2.20it/s]Evaluating:  80%|███████▉  | 249/313 [01:31<00:29,  2.15it/s]Evaluating:  34%|███▎      | 105/313 [00:47<01:35,  2.18it/s]Evaluating:  80%|███████▉  | 250/313 [01:32<00:29,  2.15it/s]Evaluating:  34%|███▍      | 106/313 [00:48<01:34,  2.19it/s]Evaluating:  80%|████████  | 251/313 [01:32<00:28,  2.15it/s]Evaluating:  34%|███▍      | 107/313 [00:48<01:34,  2.18it/s]Evaluating:  81%|████████  | 252/313 [01:33<00:28,  2.16it/s]Evaluating:  35%|███▍      | 108/313 [00:49<01:33,  2.20it/s]Evaluating:  81%|████████  | 253/313 [01:33<00:27,  2.14it/s]Evaluating:  35%|███▍      | 109/313 [00:49<01:33,  2.18it/s]Evaluating:  81%|████████  | 254/313 [01:33<00:27,  2.15it/s]Evaluating:  35%|███▌      | 110/313 [00:49<01:32,  2.20it/s]Evaluating:  81%|████████▏ | 255/313 [01:34<00:27,  2.14it/s]Evaluating:  35%|███▌      | 111/313 [00:50<01:31,  2.20it/s]Evaluating:  82%|████████▏ | 256/313 [01:34<00:26,  2.15it/s]Evaluating:  36%|███▌      | 112/313 [00:50<01:31,  2.19it/s]Evaluating:  82%|████████▏ | 257/313 [01:35<00:26,  2.14it/s]Evaluating:  36%|███▌      | 113/313 [00:51<01:31,  2.18it/s]Evaluating:  82%|████████▏ | 258/313 [01:35<00:25,  2.15it/s]Evaluating:  36%|███▋      | 114/313 [00:51<01:31,  2.18it/s]Evaluating:  83%|████████▎ | 259/313 [01:36<00:24,  2.16it/s]Evaluating:  37%|███▋      | 115/313 [00:52<01:30,  2.19it/s]Evaluating:  83%|████████▎ | 260/313 [01:36<00:24,  2.15it/s]Evaluating:  37%|███▋      | 116/313 [00:52<01:30,  2.18it/s]Evaluating:  83%|████████▎ | 261/313 [01:37<00:24,  2.16it/s]Evaluating:  37%|███▋      | 117/313 [00:53<01:29,  2.19it/s]Evaluating:  84%|████████▎ | 262/313 [01:37<00:23,  2.14it/s]Evaluating:  38%|███▊      | 118/313 [00:53<01:29,  2.17it/s]Evaluating:  84%|████████▍ | 263/313 [01:38<00:23,  2.16it/s]Evaluating:  38%|███▊      | 119/313 [00:54<01:28,  2.18it/s]Evaluating:  84%|████████▍ | 264/313 [01:38<00:22,  2.14it/s]Evaluating:  38%|███▊      | 120/313 [00:54<01:29,  2.16it/s]Evaluating:  85%|████████▍ | 265/313 [01:39<00:22,  2.15it/s]Evaluating:  39%|███▊      | 121/313 [00:55<01:28,  2.17it/s]Evaluating:  85%|████████▍ | 266/313 [01:39<00:21,  2.16it/s]Evaluating:  39%|███▉      | 122/313 [00:55<01:28,  2.17it/s]Evaluating:  85%|████████▌ | 267/313 [01:40<00:21,  2.16it/s]Evaluating:  39%|███▉      | 123/313 [00:55<01:28,  2.16it/s]Evaluating:  86%|████████▌ | 268/313 [01:40<00:20,  2.15it/s]Evaluating:  40%|███▉      | 124/313 [00:56<01:27,  2.16it/s]Evaluating:  86%|████████▌ | 269/313 [01:40<00:20,  2.15it/s]Evaluating:  40%|███▉      | 125/313 [00:56<01:27,  2.15it/s]Evaluating:  86%|████████▋ | 270/313 [01:41<00:19,  2.17it/s]Evaluating:  40%|████      | 126/313 [00:57<01:26,  2.17it/s]Evaluating:  87%|████████▋ | 271/313 [01:41<00:19,  2.15it/s]Evaluating:  41%|████      | 127/313 [00:57<01:26,  2.15it/s]Evaluating:  87%|████████▋ | 272/313 [01:42<00:18,  2.16it/s]Evaluating:  41%|████      | 128/313 [00:58<01:25,  2.16it/s]Evaluating:  87%|████████▋ | 273/313 [01:42<00:18,  2.15it/s]Evaluating:  41%|████      | 129/313 [00:58<01:25,  2.15it/s]Evaluating:  88%|████████▊ | 274/313 [01:43<00:18,  2.16it/s]Evaluating:  42%|████▏     | 130/313 [00:59<01:24,  2.16it/s]Evaluating:  88%|████████▊ | 275/313 [01:43<00:17,  2.15it/s]Evaluating:  42%|████▏     | 131/313 [00:59<01:24,  2.15it/s]Evaluating:  88%|████████▊ | 276/313 [01:44<00:17,  2.16it/s]Evaluating:  42%|████▏     | 132/313 [01:00<01:23,  2.16it/s]Evaluating:  88%|████████▊ | 277/313 [01:44<00:16,  2.17it/s]Evaluating:  42%|████▏     | 133/313 [01:00<01:22,  2.18it/s]Evaluating:  89%|████████▉ | 278/313 [01:45<00:16,  2.16it/s]Evaluating:  43%|████▎     | 134/313 [01:01<01:22,  2.16it/s]Evaluating:  89%|████████▉ | 279/313 [01:45<00:15,  2.17it/s]Evaluating:  43%|████▎     | 135/313 [01:01<01:22,  2.16it/s]Evaluating:  89%|████████▉ | 280/313 [01:46<00:15,  2.16it/s]Evaluating:  43%|████▎     | 136/313 [01:01<01:22,  2.16it/s]Evaluating:  90%|████████▉ | 281/313 [01:46<00:14,  2.17it/s]Evaluating:  44%|████▍     | 137/313 [01:02<01:20,  2.17it/s]Evaluating:  90%|█████████ | 282/313 [01:46<00:14,  2.15it/s]Evaluating:  44%|████▍     | 138/313 [01:02<01:21,  2.16it/s]Evaluating:  90%|█████████ | 283/313 [01:47<00:13,  2.17it/s]Evaluating:  44%|████▍     | 139/313 [01:03<01:19,  2.18it/s]Evaluating:  91%|█████████ | 284/313 [01:47<00:13,  2.16it/s]Evaluating:  45%|████▍     | 140/313 [01:03<01:20,  2.16it/s]Evaluating:  91%|█████████ | 285/313 [01:48<00:12,  2.18it/s]Evaluating:  45%|████▌     | 141/313 [01:04<01:19,  2.17it/s]Evaluating:  91%|█████████▏| 286/313 [01:48<00:12,  2.16it/s]Evaluating:  45%|████▌     | 142/313 [01:04<01:19,  2.16it/s]Evaluating:  92%|█████████▏| 287/313 [01:49<00:11,  2.17it/s]Evaluating:  46%|████▌     | 143/313 [01:05<01:18,  2.17it/s]Evaluating:  92%|█████████▏| 288/313 [01:49<00:11,  2.19it/s]Evaluating:  46%|████▌     | 144/313 [01:05<01:17,  2.19it/s]Evaluating:  92%|█████████▏| 289/313 [01:50<00:11,  2.16it/s]Evaluating:  46%|████▋     | 145/313 [01:06<01:17,  2.17it/s]Evaluating:  93%|█████████▎| 290/313 [01:50<00:10,  2.17it/s]Evaluating:  47%|████▋     | 146/313 [01:06<01:16,  2.17it/s]Evaluating:  93%|█████████▎| 291/313 [01:51<00:10,  2.16it/s]Evaluating:  47%|████▋     | 147/313 [01:07<01:16,  2.16it/s]Evaluating:  93%|█████████▎| 292/313 [01:51<00:09,  2.19it/s]Evaluating:  47%|████▋     | 148/313 [01:07<01:15,  2.18it/s]Evaluating:  94%|█████████▎| 293/313 [01:52<00:09,  2.16it/s]Evaluating:  48%|████▊     | 149/313 [01:07<01:15,  2.16it/s]Evaluating:  94%|█████████▍| 294/313 [01:52<00:08,  2.19it/s]Evaluating:  48%|████▊     | 150/313 [01:08<01:14,  2.19it/s]Evaluating:  94%|█████████▍| 295/313 [01:52<00:08,  2.15it/s]Evaluating:  48%|████▊     | 151/313 [01:08<01:14,  2.17it/s]Evaluating:  95%|█████████▍| 296/313 [01:53<00:07,  2.16it/s]Evaluating:  49%|████▊     | 152/313 [01:09<01:14,  2.17it/s]Evaluating:  95%|█████████▍| 297/313 [01:53<00:07,  2.16it/s]Evaluating:  49%|████▉     | 153/313 [01:09<01:14,  2.16it/s]Evaluating:  95%|█████████▌| 298/313 [01:54<00:06,  2.16it/s]Evaluating:  49%|████▉     | 154/313 [01:10<01:13,  2.16it/s]Evaluating:  96%|█████████▌| 299/313 [01:54<00:06,  2.16it/s]Evaluating:  50%|████▉     | 155/313 [01:10<01:12,  2.17it/s]Evaluating:  50%|████▉     | 156/313 [01:11<01:12,  2.17it/s]Evaluating:  96%|█████████▌| 300/313 [01:55<00:06,  2.15it/s]Evaluating:  50%|█████     | 157/313 [01:11<01:11,  2.19it/s]Evaluating:  96%|█████████▌| 301/313 [01:55<00:05,  2.16it/s]Evaluating:  50%|█████     | 158/313 [01:12<01:10,  2.19it/s]Evaluating:  96%|█████████▋| 302/313 [01:56<00:05,  2.14it/s]Evaluating:  51%|█████     | 159/313 [01:12<01:09,  2.21it/s]Evaluating:  97%|█████████▋| 303/313 [01:56<00:04,  2.15it/s]Evaluating:  51%|█████     | 160/313 [01:12<01:09,  2.20it/s]Evaluating:  97%|█████████▋| 304/313 [01:57<00:04,  2.13it/s]Evaluating:  51%|█████▏    | 161/313 [01:13<01:08,  2.21it/s]Evaluating:  97%|█████████▋| 305/313 [01:57<00:03,  2.13it/s]Evaluating:  52%|█████▏    | 162/313 [01:13<01:08,  2.19it/s]Evaluating:  98%|█████████▊| 306/313 [01:58<00:03,  2.13it/s]Evaluating:  52%|█████▏    | 163/313 [01:14<01:07,  2.21it/s]Evaluating:  98%|█████████▊| 307/313 [01:58<00:02,  2.12it/s]Evaluating:  52%|█████▏    | 164/313 [01:14<01:07,  2.21it/s]Evaluating:  98%|█████████▊| 308/313 [01:59<00:02,  2.13it/s]Evaluating:  53%|█████▎    | 165/313 [01:15<01:07,  2.19it/s]Evaluating:  99%|█████████▊| 309/313 [01:59<00:01,  2.11it/s]Evaluating:  53%|█████▎    | 166/313 [01:15<01:06,  2.21it/s]Evaluating:  99%|█████████▉| 310/313 [01:59<00:01,  2.14it/s]Evaluating:  53%|█████▎    | 167/313 [01:16<01:06,  2.19it/s]Evaluating:  99%|█████████▉| 311/313 [02:00<00:00,  2.12it/s]Evaluating:  54%|█████▎    | 168/313 [01:16<01:05,  2.20it/s]Evaluating: 100%|█████████▉| 312/313 [02:00<00:00,  2.13it/s]Evaluating:  54%|█████▍    | 169/313 [01:17<01:05,  2.19it/s]Evaluating: 100%|██████████| 313/313 [02:01<00:00,  2.33it/s]Evaluating: 100%|██████████| 313/313 [02:01<00:00,  2.58it/s]Evaluating:  54%|█████▍    | 170/313 [01:17<00:59,  2.41it/s]Evaluating:  55%|█████▍    | 171/313 [01:17<00:54,  2.61it/s]Evaluating:  55%|█████▍    | 172/313 [01:17<00:50,  2.81it/s]Evaluating:  55%|█████▌    | 173/313 [01:18<00:47,  2.93it/s]
10/10/2021 15:27:52 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/10/2021 15:27:52 - INFO - __main__ -     f1 = 0.6618397028269218
10/10/2021 15:27:52 - INFO - __main__ -     loss = 0.9200205036436027
10/10/2021 15:27:52 - INFO - __main__ -     precision = 0.593730584580627
10/10/2021 15:27:52 - INFO - __main__ -     recall = 0.7475997439726905
10/10/2021 15:27:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  56%|█████▌    | 174/313 [01:18<00:45,  3.06it/s]Evaluating:  56%|█████▌    | 175/313 [01:18<00:43,  3.16it/s]Evaluating:  56%|█████▌    | 176/313 [01:19<00:43,  3.18it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  57%|█████▋    | 177/313 [01:19<00:41,  3.25it/s]Evaluating:  57%|█████▋    | 178/313 [01:19<00:41,  3.26it/s]Evaluating:  57%|█████▋    | 179/313 [01:20<00:40,  3.30it/s]Evaluating:  58%|█████▊    | 180/313 [01:20<00:40,  3.29it/s]Evaluating:  58%|█████▊    | 181/313 [01:20<00:39,  3.33it/s]Evaluating:  58%|█████▊    | 182/313 [01:20<00:39,  3.30it/s]Evaluating:  58%|█████▊    | 183/313 [01:21<00:39,  3.31it/s]Evaluating:  59%|█████▉    | 184/313 [01:21<00:39,  3.29it/s]Evaluating:  59%|█████▉    | 185/313 [01:21<00:38,  3.33it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  59%|█████▉    | 186/313 [01:22<00:37,  3.35it/s]Evaluating:  60%|█████▉    | 187/313 [01:22<00:38,  3.31it/s]Evaluating:  60%|██████    | 188/313 [01:22<00:37,  3.34it/s]Evaluating:  60%|██████    | 189/313 [01:23<00:37,  3.30it/s]Evaluating:  61%|██████    | 190/313 [01:23<00:36,  3.34it/s]Evaluating:  61%|██████    | 191/313 [01:23<00:38,  3.20it/s]Evaluating:  61%|██████▏   | 192/313 [01:24<00:37,  3.25it/s]Evaluating:  62%|██████▏   | 193/313 [01:24<00:36,  3.30it/s]Evaluating:  62%|██████▏   | 194/313 [01:24<00:36,  3.28it/s]Evaluating:  62%|██████▏   | 195/313 [01:24<00:35,  3.29it/s]Evaluating:  63%|██████▎   | 196/313 [01:25<00:35,  3.28it/s]Evaluating:  63%|██████▎   | 197/313 [01:25<00:35,  3.31it/s]Evaluating:  63%|██████▎   | 198/313 [01:25<00:34,  3.29it/s]Evaluating:  64%|██████▎   | 199/313 [01:26<00:34,  3.33it/s]Evaluating:  64%|██████▍   | 200/313 [01:26<00:33,  3.40it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  64%|██████▍   | 201/313 [01:26<00:33,  3.36it/s]Evaluating:  65%|██████▍   | 202/313 [01:27<00:33,  3.33it/s]Evaluating:  65%|██████▍   | 203/313 [01:27<00:32,  3.36it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  65%|██████▌   | 204/313 [01:27<00:32,  3.38it/s]Evaluating:  65%|██████▌   | 205/313 [01:27<00:32,  3.33it/s]Evaluating:  66%|██████▌   | 206/313 [01:28<00:32,  3.32it/s]Evaluating:  66%|██████▌   | 207/313 [01:28<00:31,  3.36it/s]Evaluating:  66%|██████▋   | 208/313 [01:28<00:31,  3.37it/s]Evaluating:  67%|██████▋   | 209/313 [01:29<00:31,  3.33it/s]Evaluating:  67%|██████▋   | 210/313 [01:29<00:30,  3.36it/s]Evaluating:  67%|██████▋   | 211/313 [01:29<00:30,  3.37it/s]Evaluating:  68%|██████▊   | 212/313 [01:30<00:30,  3.33it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  68%|██████▊   | 213/313 [01:30<00:30,  3.30it/s]10/10/2021 15:28:04 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:28:04 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  68%|██████▊   | 214/313 [01:30<00:29,  3.34it/s]Evaluating:  69%|██████▊   | 215/313 [01:30<00:29,  3.38it/s]Evaluating:  69%|██████▉   | 216/313 [01:31<00:29,  3.33it/s]Evaluating:  69%|██████▉   | 217/313 [01:31<00:28,  3.31it/s]Evaluating:  70%|██████▉   | 218/313 [01:31<00:28,  3.36it/s]Evaluating:  70%|██████▉   | 219/313 [01:32<00:27,  3.36it/s]Evaluating:  70%|███████   | 220/313 [01:32<00:27,  3.33it/s]Evaluating:  71%|███████   | 221/313 [01:32<00:27,  3.36it/s]Evaluating:  71%|███████   | 222/313 [01:33<00:26,  3.37it/s]Evaluating:  71%|███████   | 223/313 [01:33<00:27,  3.32it/s]Evaluating:  72%|███████▏  | 224/313 [01:33<00:26,  3.31it/s]Evaluating:  72%|███████▏  | 225/313 [01:33<00:26,  3.35it/s]Evaluating:  72%|███████▏  | 226/313 [01:34<00:25,  3.36it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  73%|███████▎  | 227/313 [01:34<00:25,  3.33it/s]Evaluating:  73%|███████▎  | 228/313 [01:34<00:25,  3.30it/s]Evaluating:  73%|███████▎  | 229/313 [01:35<00:25,  3.35it/s]Evaluating:  73%|███████▎  | 230/313 [01:35<00:24,  3.34it/s]Evaluating:  74%|███████▍  | 231/313 [01:35<00:24,  3.32it/s]Evaluating:  74%|███████▍  | 232/313 [01:36<00:24,  3.36it/s]Evaluating:  74%|███████▍  | 233/313 [01:36<00:23,  3.37it/s]Evaluating:  75%|███████▍  | 234/313 [01:36<00:23,  3.33it/s]Evaluating:  75%|███████▌  | 235/313 [01:36<00:23,  3.29it/s]Evaluating:  75%|███████▌  | 236/313 [01:37<00:23,  3.33it/s]Evaluating:  76%|███████▌  | 237/313 [01:37<00:22,  3.34it/s]Evaluating:  76%|███████▌  | 238/313 [01:37<00:22,  3.31it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  76%|███████▋  | 239/313 [01:38<00:22,  3.28it/s]Evaluating:  77%|███████▋  | 240/313 [01:38<00:21,  3.39it/s]Evaluating:  77%|███████▋  | 241/313 [01:38<00:26,  2.69it/s]Evaluating:  77%|███████▋  | 242/313 [01:39<00:24,  2.92it/s]Evaluating:  78%|███████▊  | 243/313 [01:39<00:23,  2.98it/s]Evaluating:  78%|███████▊  | 244/313 [01:39<00:22,  3.03it/s]Evaluating:  78%|███████▊  | 245/313 [01:40<00:21,  3.11it/s]Evaluating:  79%|███████▊  | 246/313 [01:40<00:21,  3.17it/s]Evaluating:  79%|███████▉  | 247/313 [01:40<00:20,  3.16it/s]Evaluating:  79%|███████▉  | 248/313 [01:41<00:20,  3.18it/s]Evaluating:  80%|███████▉  | 249/313 [01:41<00:19,  3.31it/s]Evaluating:  80%|███████▉  | 250/313 [01:41<00:19,  3.29it/s]Evaluating:  80%|████████  | 251/313 [01:41<00:18,  3.27it/s]Evaluating:  81%|████████  | 252/313 [01:42<00:18,  3.27it/s]Evaluating:  81%|████████  | 253/313 [01:42<00:18,  3.32it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  81%|████████  | 254/313 [01:42<00:18,  3.27it/s]Evaluating:  81%|████████▏ | 255/313 [01:43<00:17,  3.25it/s]Evaluating:  82%|████████▏ | 256/313 [01:43<00:17,  3.29it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  82%|████████▏ | 257/313 [01:43<00:17,  3.24it/s]Evaluating:  82%|████████▏ | 258/313 [01:44<00:17,  3.22it/s]Evaluating:  83%|████████▎ | 259/313 [01:44<00:16,  3.25it/s]Evaluating:  83%|████████▎ | 260/313 [01:44<00:15,  3.33it/s]Evaluating:  83%|████████▎ | 261/313 [01:45<00:15,  3.32it/s]Evaluating:  84%|████████▎ | 262/313 [01:45<00:15,  3.29it/s]Evaluating:  84%|████████▍ | 263/313 [01:45<00:14,  3.36it/s]Evaluating:  84%|████████▍ | 264/313 [01:45<00:14,  3.30it/s]Evaluating:  85%|████████▍ | 265/313 [01:46<00:14,  3.29it/s]Evaluating:  85%|████████▍ | 266/313 [01:46<00:14,  3.27it/s]Evaluating:  85%|████████▌ | 267/313 [01:46<00:14,  3.28it/s]Evaluating:  86%|████████▌ | 268/313 [01:47<00:13,  3.24it/s]Evaluating:  86%|████████▌ | 269/313 [01:47<00:13,  3.22it/s]Evaluating:  86%|████████▋ | 270/313 [01:47<00:13,  3.30it/s]Evaluating:  87%|████████▋ | 271/313 [01:48<00:12,  3.27it/s]Evaluating:  87%|████████▋ | 272/313 [01:48<00:12,  3.24it/s]Evaluating:  87%|████████▋ | 273/313 [01:48<00:12,  3.23it/s]Evaluating:  88%|████████▊ | 274/313 [01:49<00:11,  3.26it/s]Evaluating:  88%|████████▊ | 275/313 [01:49<00:11,  3.24it/s]Evaluating:  88%|████████▊ | 276/313 [01:49<00:11,  3.21it/s]Evaluating:  88%|████████▊ | 277/313 [01:49<00:10,  3.29it/s]Evaluating:  89%|████████▉ | 278/313 [01:50<00:10,  3.27it/s]Evaluating:  89%|████████▉ | 279/313 [01:50<00:10,  3.23it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:28:25 - INFO - __main__ -   Using lang2id = None
Evaluating:  89%|████████▉ | 280/313 [01:50<00:10,  3.21it/s]Evaluating:  90%|████████▉ | 281/313 [01:51<00:10,  3.19it/s]Evaluating:  90%|█████████ | 282/313 [01:51<00:09,  3.18it/s]Evaluating:  90%|█████████ | 283/313 [01:51<00:09,  3.19it/s]Evaluating:  91%|█████████ | 284/313 [01:52<00:08,  3.29it/s]Evaluating:  91%|█████████ | 285/313 [01:52<00:08,  3.28it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  91%|█████████▏| 286/313 [01:52<00:08,  3.26it/s]10/10/2021 15:28:27 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:28:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:28:27 - INFO - __main__ -   Seed = 32
10/10/2021 15:28:27 - INFO - root -   save model
10/10/2021 15:28:27 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:28:27 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  92%|█████████▏| 287/313 [01:53<00:07,  3.26it/s]Evaluating:  92%|█████████▏| 288/313 [01:53<00:07,  3.22it/s]Evaluating:  92%|█████████▏| 289/313 [01:53<00:07,  3.20it/s]Evaluating:  93%|█████████▎| 290/313 [01:53<00:07,  3.21it/s]Evaluating:  93%|█████████▎| 291/313 [01:54<00:06,  3.31it/s]Evaluating:  93%|█████████▎| 292/313 [01:54<00:06,  3.29it/s]Evaluating:  94%|█████████▎| 293/313 [01:54<00:06,  3.28it/s]Evaluating:  94%|█████████▍| 294/313 [01:55<00:05,  3.39it/s]Evaluating:  94%|█████████▍| 295/313 [01:55<00:05,  3.31it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  95%|█████████▍| 296/313 [01:55<00:05,  3.28it/s]Evaluating:  95%|█████████▍| 297/313 [01:56<00:04,  3.25it/s]Evaluating:  95%|█████████▌| 298/313 [01:56<00:04,  3.34it/s]Evaluating:  96%|█████████▌| 299/313 [01:56<00:04,  3.29it/s]Evaluating:  96%|█████████▌| 300/313 [01:57<00:04,  3.24it/s]Evaluating:  96%|█████████▌| 301/313 [01:57<00:03,  3.29it/s]Evaluating:  96%|█████████▋| 302/313 [01:57<00:03,  3.23it/s]Evaluating:  97%|█████████▋| 303/313 [01:57<00:03,  3.22it/s]Evaluating:  97%|█████████▋| 304/313 [01:58<00:02,  3.20it/s]Evaluating:  97%|█████████▋| 305/313 [01:58<00:02,  3.31it/s]Evaluating:  98%|█████████▊| 306/313 [01:58<00:02,  3.30it/s]Evaluating:  98%|█████████▊| 307/313 [01:59<00:01,  3.28it/s]Evaluating:  98%|█████████▊| 308/313 [01:59<00:01,  3.34it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  99%|█████████▊| 309/313 [01:59<00:01,  3.30it/s]Evaluating:  99%|█████████▉| 310/313 [02:00<00:00,  3.26it/s]Evaluating:  99%|█████████▉| 311/313 [02:00<00:00,  3.25it/s]Evaluating: 100%|█████████▉| 312/313 [02:00<00:00,  3.34it/s]Evaluating: 100%|██████████| 313/313 [02:00<00:00,  3.76it/s]Evaluating: 100%|██████████| 313/313 [02:00<00:00,  2.59it/s]
10/10/2021 15:28:36 - INFO - __main__ -   ***** Evaluation result  in no *****
10/10/2021 15:28:36 - INFO - __main__ -     f1 = 0.714540765721087
10/10/2021 15:28:36 - INFO - __main__ -     loss = 0.8063129221383756
10/10/2021 15:28:36 - INFO - __main__ -     precision = 0.6820247412269629
10/10/2021 15:28:36 - INFO - __main__ -     recall = 0.7503124566032495
10/10/2021 15:28:36 - INFO - __main__ -   Language adapter for da not found, using is instead
10/10/2021 15:28:36 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:28:36 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/10/2021 15:28:37 - INFO - __main__ -   ***** Running evaluation  in da *****
10/10/2021 15:28:37 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:28:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:34,  3.31it/s]Evaluating:   1%|          | 2/313 [00:00<01:51,  2.79it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:   1%|          | 3/313 [00:00<01:40,  3.08it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:37,  3.16it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   2%|▏         | 5/313 [00:01<01:36,  3.20it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:33,  3.29it/s]Evaluating:   2%|▏         | 7/313 [00:02<01:31,  3.36it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:31,  3.33it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:30,  3.34it/s]Evaluating:   3%|▎         | 10/313 [00:03<01:30,  3.36it/s]Evaluating:   4%|▎         | 11/313 [00:03<01:28,  3.40it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:29,  3.35it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:28,  3.39it/s]Evaluating:   4%|▍         | 14/313 [00:04<01:29,  3.36it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:   5%|▍         | 15/313 [00:04<01:27,  3.40it/s]10/10/2021 15:28:42 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:28:42 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:28:42 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/10/2021 15:28:42 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:28:42 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:28:42 - INFO - root -   loading lang adpater ru/wiki@ukp
10/10/2021 15:28:42 - INFO - __main__ -   Language = ru
10/10/2021 15:28:42 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:   5%|▌         | 16/313 [00:04<01:28,  3.37it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Evaluating:   5%|▌         | 17/313 [00:05<01:27,  3.40it/s]Evaluating:   6%|▌         | 18/313 [00:05<01:27,  3.38it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Evaluating:   6%|▌         | 19/313 [00:05<01:26,  3.39it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
Evaluating:   6%|▋         | 20/313 [00:06<01:27,  3.35it/s]Evaluating:   7%|▋         | 21/313 [00:06<01:26,  3.38it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:25,  3.41it/s]Evaluating:   7%|▋         | 23/313 [00:06<01:25,  3.37it/s]Evaluating:   8%|▊         | 24/313 [00:07<01:26,  3.35it/s]Evaluating:   8%|▊         | 25/313 [00:07<01:25,  3.36it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:24,  3.40it/s]Evaluating:   9%|▊         | 27/313 [00:08<01:25,  3.36it/s]Evaluating:   9%|▉         | 28/313 [00:08<01:23,  3.40it/s]Evaluating:   9%|▉         | 29/313 [00:08<01:24,  3.36it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:23,  3.39it/s]Evaluating:  10%|▉         | 31/313 [00:09<01:24,  3.35it/s]Evaluating:  10%|█         | 32/313 [00:09<01:23,  3.38it/s]Evaluating:  11%|█         | 33/313 [00:09<01:23,  3.35it/s]Evaluating:  11%|█         | 34/313 [00:10<01:22,  3.38it/s]Evaluating:  11%|█         | 35/313 [00:10<01:22,  3.35it/s]Evaluating:  12%|█▏        | 36/313 [00:10<01:21,  3.38it/s]Evaluating:  12%|█▏        | 37/313 [00:11<01:22,  3.35it/s]Evaluating:  12%|█▏        | 38/313 [00:11<01:21,  3.39it/s]Evaluating:  12%|█▏        | 39/313 [00:11<01:21,  3.34it/s]Evaluating:  13%|█▎        | 40/313 [00:11<01:21,  3.37it/s]Evaluating:  13%|█▎        | 41/313 [00:12<01:20,  3.37it/s]Evaluating:  13%|█▎        | 42/313 [00:12<01:20,  3.37it/s]Evaluating:  14%|█▎        | 43/313 [00:12<01:20,  3.34it/s]Evaluating:  14%|█▍        | 44/313 [00:13<01:19,  3.37it/s]10/10/2021 15:28:51 - INFO - __main__ -   Language adapter for be not found, using ru instead
10/10/2021 15:28:51 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:28:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
Evaluating:  14%|█▍        | 45/313 [00:13<01:19,  3.39it/s]10/10/2021 15:28:51 - INFO - __main__ -   ***** Running evaluation  in be *****
10/10/2021 15:28:51 - INFO - __main__ -     Num examples = 1001
10/10/2021 15:28:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  15%|█▍        | 46/313 [00:13<01:24,  3.15it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.07it/s]Evaluating:  15%|█▌        | 47/313 [00:14<01:35,  2.79it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:14,  2.13it/s]Evaluating:  15%|█▌        | 48/313 [00:14<01:43,  2.56it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:13,  2.17it/s]Evaluating:  16%|█▌        | 49/313 [00:15<01:47,  2.45it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:12,  2.16it/s]Evaluating:  16%|█▌        | 50/313 [00:15<01:52,  2.34it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:12,  2.16it/s]Evaluating:  16%|█▋        | 51/313 [00:16<01:53,  2.30it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:12,  2.16it/s]Evaluating:  17%|█▋        | 52/313 [00:16<01:56,  2.25it/s]Evaluating:  22%|██▏       | 7/32 [00:03<00:11,  2.18it/s]Evaluating:  17%|█▋        | 53/313 [00:17<01:56,  2.24it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:11,  2.16it/s]Evaluating:  17%|█▋        | 54/313 [00:17<01:57,  2.20it/s]Evaluating:  28%|██▊       | 9/32 [00:04<00:10,  2.17it/s]Evaluating:  18%|█▊        | 55/313 [00:17<01:57,  2.20it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:10,  2.16it/s]Evaluating:  18%|█▊        | 56/313 [00:18<01:56,  2.21it/s]Evaluating:  34%|███▍      | 11/32 [00:05<00:09,  2.18it/s]Evaluating:  18%|█▊        | 57/313 [00:18<01:57,  2.18it/s]Evaluating:  38%|███▊      | 12/32 [00:05<00:09,  2.16it/s]Evaluating:  19%|█▊        | 58/313 [00:19<01:57,  2.18it/s]Evaluating:  41%|████      | 13/32 [00:06<00:08,  2.16it/s]Evaluating:  19%|█▉        | 59/313 [00:19<01:56,  2.18it/s]Evaluating:  44%|████▍     | 14/32 [00:06<00:08,  2.17it/s]Evaluating:  19%|█▉        | 60/313 [00:20<01:55,  2.19it/s]Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.25it/s]Evaluating:  19%|█▉        | 61/313 [00:20<02:02,  2.06it/s]Evaluating:  50%|█████     | 16/32 [00:07<00:07,  2.20it/s]Evaluating:  20%|█▉        | 62/313 [00:21<01:58,  2.11it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.20it/s]Evaluating:  20%|██        | 63/313 [00:21<01:57,  2.12it/s]Evaluating:  56%|█████▋    | 18/32 [00:08<00:06,  2.20it/s]Evaluating:  20%|██        | 64/313 [00:22<01:56,  2.14it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:05,  2.18it/s]Evaluating:  21%|██        | 65/313 [00:22<01:55,  2.15it/s]Evaluating:  62%|██████▎   | 20/32 [00:09<00:05,  2.19it/s]Evaluating:  21%|██        | 66/313 [00:23<01:53,  2.17it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:05,  2.17it/s]Evaluating:  21%|██▏       | 67/313 [00:23<01:52,  2.19it/s]Evaluating:  69%|██████▉   | 22/32 [00:10<00:04,  2.19it/s]Evaluating:  22%|██▏       | 68/313 [00:23<01:52,  2.17it/s]Evaluating:  72%|███████▏  | 23/32 [00:10<00:04,  2.17it/s]Evaluating:  22%|██▏       | 69/313 [00:24<01:52,  2.18it/s]Evaluating:  75%|███████▌  | 24/32 [00:11<00:03,  2.18it/s]Evaluating:  22%|██▏       | 70/313 [00:24<01:52,  2.17it/s]Evaluating:  78%|███████▊  | 25/32 [00:11<00:03,  2.17it/s]Evaluating:  23%|██▎       | 71/313 [00:25<01:50,  2.18it/s]Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.18it/s]Evaluating:  23%|██▎       | 72/313 [00:25<01:51,  2.16it/s]Evaluating:  84%|████████▍ | 27/32 [00:12<00:02,  2.16it/s]Evaluating:  23%|██▎       | 73/313 [00:26<01:50,  2.18it/s]Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.16it/s]Evaluating:  24%|██▎       | 74/313 [00:26<01:50,  2.17it/s]Evaluating:  91%|█████████ | 29/32 [00:13<00:01,  2.17it/s]Evaluating:  24%|██▍       | 75/313 [00:27<01:48,  2.18it/s]Evaluating:  94%|█████████▍| 30/32 [00:13<00:00,  2.16it/s]Evaluating:  24%|██▍       | 76/313 [00:27<01:49,  2.17it/s]Evaluating:  97%|█████████▋| 31/32 [00:14<00:00,  2.17it/s]Evaluating:  25%|██▍       | 77/313 [00:28<01:43,  2.28it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.32it/s]Evaluating: 100%|██████████| 32/32 [00:14<00:00,  2.19it/s]
10/10/2021 15:29:06 - INFO - __main__ -   ***** Evaluation result  in be *****
10/10/2021 15:29:06 - INFO - __main__ -     f1 = 0.589804994868286
10/10/2021 15:29:06 - INFO - __main__ -     loss = 1.225804515182972
10/10/2021 15:29:06 - INFO - __main__ -     precision = 0.5052754982415005
10/10/2021 15:29:06 - INFO - __main__ -     recall = 0.7082990961380444
10/10/2021 15:29:06 - INFO - __main__ -   Language adapter for uk not found, using ru instead
10/10/2021 15:29:06 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:29:06 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
Evaluating:  25%|██▍       | 78/313 [00:28<01:34,  2.48it/s]Evaluating:  25%|██▌       | 79/313 [00:28<01:27,  2.69it/s]Evaluating:  26%|██▌       | 80/313 [00:28<01:21,  2.85it/s]Evaluating:  26%|██▌       | 81/313 [00:29<01:17,  2.99it/s]Evaluating:  26%|██▌       | 82/313 [00:29<01:13,  3.12it/s]10/10/2021 15:29:07 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/10/2021 15:29:07 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:29:07 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  27%|██▋       | 83/313 [00:29<01:15,  3.05it/s]Evaluating:   0%|          | 1/313 [00:00<02:26,  2.13it/s]Evaluating:  27%|██▋       | 84/313 [00:30<01:23,  2.75it/s]Evaluating:   1%|          | 2/313 [00:00<02:24,  2.16it/s]Evaluating:  27%|██▋       | 85/313 [00:30<01:30,  2.53it/s]Evaluating:   1%|          | 3/313 [00:01<02:21,  2.19it/s]Evaluating:  27%|██▋       | 86/313 [00:31<01:33,  2.43it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:22,  2.16it/s]Evaluating:  28%|██▊       | 87/313 [00:31<01:37,  2.33it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:21,  2.18it/s]Evaluating:  28%|██▊       | 88/313 [00:32<01:38,  2.29it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:22,  2.16it/s]Evaluating:  28%|██▊       | 89/313 [00:32<01:40,  2.24it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:20,  2.18it/s]Evaluating:  29%|██▉       | 90/313 [00:33<01:39,  2.23it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:21,  2.15it/s]Evaluating:  29%|██▉       | 91/313 [00:33<01:40,  2.20it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:20,  2.17it/s]Evaluating:  29%|██▉       | 92/313 [00:34<01:40,  2.20it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:20,  2.15it/s]Evaluating:  30%|██▉       | 93/313 [00:34<01:40,  2.20it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:18,  2.18it/s]Evaluating:  30%|███       | 94/313 [00:34<01:40,  2.17it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:19,  2.16it/s]Evaluating:  30%|███       | 95/313 [00:35<01:39,  2.18it/s]Evaluating:   4%|▍         | 13/313 [00:05<02:17,  2.18it/s]Evaluating:  31%|███       | 96/313 [00:35<01:40,  2.16it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:16,  2.20it/s]Evaluating:  31%|███       | 97/313 [00:36<01:39,  2.18it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:16,  2.18it/s]Evaluating:  31%|███▏      | 98/313 [00:36<01:39,  2.16it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:16,  2.18it/s]Evaluating:  32%|███▏      | 99/313 [00:37<01:38,  2.17it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:16,  2.16it/s]Evaluating:  32%|███▏      | 100/313 [00:37<01:38,  2.16it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:15,  2.18it/s]Evaluating:  32%|███▏      | 101/313 [00:38<01:37,  2.18it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:16,  2.16it/s]Evaluating:  33%|███▎      | 102/313 [00:38<01:37,  2.16it/s]Evaluating:   6%|▋         | 20/313 [00:09<02:14,  2.18it/s]Evaluating:  33%|███▎      | 103/313 [00:39<01:36,  2.18it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:15,  2.16it/s]Evaluating:  33%|███▎      | 104/313 [00:39<01:35,  2.18it/s]Evaluating:   7%|▋         | 22/313 [00:10<02:13,  2.18it/s]Evaluating:  34%|███▎      | 105/313 [00:40<01:35,  2.17it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:13,  2.17it/s]Evaluating:  34%|███▍      | 106/313 [00:40<01:34,  2.19it/s]Evaluating:   8%|▊         | 24/313 [00:11<02:14,  2.15it/s]Evaluating:  34%|███▍      | 107/313 [00:40<01:34,  2.17it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:13,  2.16it/s]Evaluating:  35%|███▍      | 108/313 [00:41<01:33,  2.19it/s]Evaluating:   8%|▊         | 26/313 [00:11<02:13,  2.16it/s]Evaluating:  35%|███▍      | 109/313 [00:41<01:34,  2.16it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:11,  2.17it/s]Evaluating:  35%|███▌      | 110/313 [00:42<01:33,  2.18it/s]Evaluating:   9%|▉         | 28/313 [00:12<02:11,  2.16it/s]Evaluating:  35%|███▌      | 111/313 [00:42<01:33,  2.15it/s]Evaluating:   9%|▉         | 29/313 [00:13<02:10,  2.18it/s]Evaluating:  36%|███▌      | 112/313 [00:43<01:32,  2.17it/s]Evaluating:  10%|▉         | 30/313 [00:13<02:10,  2.18it/s]Evaluating:  36%|███▌      | 113/313 [00:43<01:32,  2.16it/s]Evaluating:  10%|▉         | 31/313 [00:14<02:09,  2.19it/s]Evaluating:  36%|███▋      | 114/313 [00:44<01:31,  2.18it/s]Evaluating:  10%|█         | 32/313 [00:14<02:08,  2.18it/s]Evaluating:  37%|███▋      | 115/313 [00:44<01:30,  2.20it/s]Evaluating:  11%|█         | 33/313 [00:15<02:06,  2.22it/s]Evaluating:  37%|███▋      | 116/313 [00:45<01:29,  2.21it/s]Evaluating:  11%|█         | 34/313 [00:15<02:05,  2.22it/s]Evaluating:  37%|███▋      | 117/313 [00:45<01:28,  2.21it/s]Evaluating:  11%|█         | 35/313 [00:16<02:04,  2.23it/s]Evaluating:  38%|███▊      | 118/313 [00:45<01:28,  2.20it/s]Evaluating:  12%|█▏        | 36/313 [00:16<02:05,  2.20it/s]Evaluating:  38%|███▊      | 119/313 [00:46<01:28,  2.20it/s]Evaluating:  12%|█▏        | 37/313 [00:16<02:05,  2.20it/s]Evaluating:  38%|███▊      | 120/313 [00:46<01:28,  2.18it/s]Evaluating:  12%|█▏        | 38/313 [00:17<02:05,  2.18it/s]Evaluating:  39%|███▊      | 121/313 [00:47<01:27,  2.19it/s]Evaluating:  12%|█▏        | 39/313 [00:17<02:05,  2.19it/s]Evaluating:  39%|███▉      | 122/313 [00:47<01:28,  2.17it/s]Evaluating:  13%|█▎        | 40/313 [00:18<02:03,  2.20it/s]Evaluating:  39%|███▉      | 123/313 [00:48<01:27,  2.18it/s]Evaluating:  13%|█▎        | 41/313 [00:18<02:04,  2.18it/s]Evaluating:  40%|███▉      | 124/313 [00:48<01:27,  2.16it/s]Evaluating:  13%|█▎        | 42/313 [00:19<02:03,  2.19it/s]Evaluating:  40%|███▉      | 125/313 [00:49<01:26,  2.17it/s]Evaluating:  14%|█▎        | 43/313 [00:19<02:03,  2.18it/s]Evaluating:  40%|████      | 126/313 [00:49<01:27,  2.15it/s]Evaluating:  14%|█▍        | 44/313 [00:20<02:03,  2.19it/s]Evaluating:  41%|████      | 127/313 [00:50<01:25,  2.16it/s]Evaluating:  14%|█▍        | 45/313 [00:20<02:03,  2.17it/s]Evaluating:  41%|████      | 128/313 [00:50<01:22,  2.25it/s]Evaluating:  15%|█▍        | 46/313 [00:21<02:15,  1.98it/s]Evaluating:  41%|████      | 129/313 [00:51<01:24,  2.17it/s]Evaluating:  15%|█▌        | 47/313 [00:21<02:10,  2.04it/s]Evaluating:  42%|████▏     | 130/313 [00:51<01:23,  2.19it/s]Evaluating:  15%|█▌        | 48/313 [00:22<02:08,  2.06it/s]Evaluating:  42%|████▏     | 131/313 [00:51<01:24,  2.16it/s]Evaluating:  16%|█▌        | 49/313 [00:22<02:05,  2.10it/s]Evaluating:  42%|████▏     | 132/313 [00:52<01:23,  2.17it/s]Evaluating:  16%|█▌        | 50/313 [00:23<02:04,  2.12it/s]Evaluating:  42%|████▏     | 133/313 [00:52<01:23,  2.15it/s]Evaluating:  16%|█▋        | 51/313 [00:23<02:01,  2.15it/s]Evaluating:  43%|████▎     | 134/313 [00:53<01:22,  2.17it/s]Evaluating:  17%|█▋        | 52/313 [00:24<02:01,  2.15it/s]Evaluating:  43%|████▎     | 135/313 [00:53<01:22,  2.15it/s]Evaluating:  17%|█▋        | 53/313 [00:24<01:59,  2.18it/s]Evaluating:  43%|████▎     | 136/313 [00:54<01:21,  2.17it/s]Evaluating:  17%|█▋        | 54/313 [00:24<01:58,  2.18it/s]Evaluating:  44%|████▍     | 137/313 [00:54<01:20,  2.18it/s]Evaluating:  18%|█▊        | 55/313 [00:25<01:58,  2.18it/s]Evaluating:  44%|████▍     | 138/313 [00:55<01:20,  2.16it/s]Evaluating:  18%|█▊        | 56/313 [00:25<01:58,  2.17it/s]Evaluating:  44%|████▍     | 139/313 [00:55<01:20,  2.16it/s]Evaluating:  18%|█▊        | 57/313 [00:26<01:57,  2.18it/s]Evaluating:  45%|████▍     | 140/313 [00:56<01:20,  2.16it/s]Evaluating:  19%|█▊        | 58/313 [00:26<01:56,  2.20it/s]Evaluating:  45%|████▌     | 141/313 [00:56<01:19,  2.17it/s]Evaluating:  19%|█▉        | 59/313 [00:27<01:56,  2.17it/s]Evaluating:  45%|████▌     | 142/313 [00:57<01:19,  2.15it/s]Evaluating:  19%|█▉        | 60/313 [00:27<01:56,  2.18it/s]Evaluating:  46%|████▌     | 143/313 [00:57<01:18,  2.17it/s]Evaluating:  19%|█▉        | 61/313 [00:28<01:56,  2.17it/s]Evaluating:  46%|████▌     | 144/313 [00:57<01:18,  2.15it/s]Evaluating:  20%|█▉        | 62/313 [00:28<01:54,  2.19it/s]Evaluating:  46%|████▋     | 145/313 [00:58<01:17,  2.16it/s]Evaluating:  20%|██        | 63/313 [00:29<01:55,  2.17it/s]Evaluating:  47%|████▋     | 146/313 [00:58<01:17,  2.15it/s]Evaluating:  20%|██        | 64/313 [00:29<01:54,  2.18it/s]Evaluating:  47%|████▋     | 147/313 [00:59<01:16,  2.16it/s]Evaluating:  21%|██        | 65/313 [00:29<01:53,  2.18it/s]Evaluating:  47%|████▋     | 148/313 [00:59<01:15,  2.17it/s]Evaluating:  21%|██        | 66/313 [00:30<01:53,  2.18it/s]Evaluating:  48%|████▊     | 149/313 [01:00<01:16,  2.16it/s]Evaluating:  21%|██▏       | 67/313 [00:30<01:53,  2.16it/s]Evaluating:  48%|████▊     | 150/313 [01:00<01:15,  2.15it/s]Evaluating:  22%|██▏       | 68/313 [00:31<01:52,  2.18it/s]Evaluating:  48%|████▊     | 151/313 [01:01<01:15,  2.15it/s]Evaluating:  22%|██▏       | 69/313 [00:31<01:51,  2.19it/s]Evaluating:  49%|████▊     | 152/313 [01:01<01:14,  2.16it/s]Evaluating:  22%|██▏       | 70/313 [00:32<01:51,  2.18it/s]Evaluating:  49%|████▉     | 153/313 [01:02<01:14,  2.15it/s]Evaluating:  23%|██▎       | 71/313 [00:32<01:50,  2.19it/s]Evaluating:  49%|████▉     | 154/313 [01:02<01:13,  2.16it/s]Evaluating:  23%|██▎       | 72/313 [00:33<01:50,  2.17it/s]Evaluating:  50%|████▉     | 155/313 [01:03<01:13,  2.15it/s]Evaluating:  23%|██▎       | 73/313 [00:33<01:50,  2.18it/s]Evaluating:  50%|████▉     | 156/313 [01:03<01:12,  2.17it/s]Evaluating:  24%|██▎       | 74/313 [00:34<01:50,  2.16it/s]Evaluating:  50%|█████     | 157/313 [01:04<01:12,  2.15it/s]Evaluating:  24%|██▍       | 75/313 [00:34<01:49,  2.17it/s]Evaluating:  50%|█████     | 158/313 [01:04<01:11,  2.17it/s]Evaluating:  24%|██▍       | 76/313 [00:35<01:49,  2.16it/s]Evaluating:  51%|█████     | 159/313 [01:04<01:10,  2.18it/s]Evaluating:  25%|██▍       | 77/313 [00:35<01:49,  2.16it/s]Evaluating:  51%|█████     | 160/313 [01:05<01:09,  2.21it/s]Evaluating:  25%|██▍       | 78/313 [00:36<01:53,  2.07it/s]Evaluating:  51%|█████▏    | 161/313 [01:05<01:09,  2.20it/s]Evaluating:  52%|█████▏    | 162/313 [01:06<01:09,  2.17it/s]Evaluating:  25%|██▌       | 79/313 [00:36<01:52,  2.08it/s]Evaluating:  26%|██▌       | 80/313 [00:36<01:50,  2.12it/s]Evaluating:  52%|█████▏    | 163/313 [01:06<01:08,  2.18it/s]Evaluating:  26%|██▌       | 81/313 [00:37<01:49,  2.11it/s]Evaluating:  52%|█████▏    | 164/313 [01:07<01:09,  2.16it/s]Evaluating:  26%|██▌       | 82/313 [00:37<01:47,  2.14it/s]Evaluating:  53%|█████▎    | 165/313 [01:07<01:08,  2.17it/s]Evaluating:  27%|██▋       | 83/313 [00:38<01:47,  2.14it/s]Evaluating:  53%|█████▎    | 166/313 [01:08<01:08,  2.15it/s]Evaluating:  27%|██▋       | 84/313 [00:38<01:45,  2.16it/s]Evaluating:  53%|█████▎    | 167/313 [01:08<01:07,  2.16it/s]Evaluating:  27%|██▋       | 85/313 [00:39<01:45,  2.16it/s]Evaluating:  54%|█████▎    | 168/313 [01:09<01:07,  2.15it/s]Evaluating:  27%|██▋       | 86/313 [00:39<01:44,  2.17it/s]Evaluating:  54%|█████▍    | 169/313 [01:09<01:06,  2.16it/s]Evaluating:  28%|██▊       | 87/313 [00:40<01:43,  2.18it/s]Evaluating:  54%|█████▍    | 170/313 [01:09<01:05,  2.18it/s]Evaluating:  28%|██▊       | 88/313 [00:40<01:44,  2.16it/s]Evaluating:  55%|█████▍    | 171/313 [01:10<01:05,  2.16it/s]Evaluating:  28%|██▊       | 89/313 [00:41<01:43,  2.17it/s]Evaluating:  55%|█████▍    | 172/313 [01:10<01:04,  2.17it/s]Evaluating:  29%|██▉       | 90/313 [00:41<01:43,  2.15it/s]Evaluating:  55%|█████▌    | 173/313 [01:11<01:04,  2.15it/s]Evaluating:  29%|██▉       | 91/313 [00:42<01:41,  2.18it/s]Evaluating:  56%|█████▌    | 174/313 [01:11<01:04,  2.16it/s]Evaluating:  29%|██▉       | 92/313 [00:42<01:42,  2.16it/s]Evaluating:  56%|█████▌    | 175/313 [01:12<01:04,  2.16it/s]Evaluating:  30%|██▉       | 93/313 [00:42<01:41,  2.16it/s]Evaluating:  56%|█████▌    | 176/313 [01:12<01:03,  2.17it/s]Evaluating:  30%|███       | 94/313 [00:43<01:41,  2.16it/s]Evaluating:  57%|█████▋    | 177/313 [01:13<01:03,  2.16it/s]Evaluating:  30%|███       | 95/313 [00:43<01:40,  2.18it/s]Evaluating:  57%|█████▋    | 178/313 [01:13<01:02,  2.17it/s]Evaluating:  31%|███       | 96/313 [00:44<01:40,  2.16it/s]Evaluating:  57%|█████▋    | 179/313 [01:14<01:02,  2.15it/s]Evaluating:  31%|███       | 97/313 [00:44<01:39,  2.17it/s]Evaluating:  58%|█████▊    | 180/313 [01:14<01:01,  2.17it/s]Evaluating:  31%|███▏      | 98/313 [00:45<01:38,  2.18it/s]Evaluating:  58%|█████▊    | 181/313 [01:15<01:00,  2.18it/s]Evaluating:  32%|███▏      | 99/313 [00:45<01:38,  2.17it/s]Evaluating:  58%|█████▊    | 182/313 [01:15<01:00,  2.16it/s]Evaluating:  32%|███▏      | 100/313 [00:46<01:37,  2.18it/s]Evaluating:  58%|█████▊    | 183/313 [01:15<00:59,  2.19it/s]Evaluating:  32%|███▏      | 101/313 [00:46<01:38,  2.15it/s]Evaluating:  59%|█████▉    | 184/313 [01:16<00:59,  2.15it/s]Evaluating:  33%|███▎      | 102/313 [00:47<01:37,  2.17it/s]Evaluating:  59%|█████▉    | 185/313 [01:16<00:58,  2.17it/s]Evaluating:  33%|███▎      | 103/313 [00:47<01:37,  2.15it/s]Evaluating:  59%|█████▉    | 186/313 [01:17<00:58,  2.15it/s]Evaluating:  33%|███▎      | 104/313 [00:48<01:36,  2.16it/s]Evaluating:  60%|█████▉    | 187/313 [01:17<00:58,  2.16it/s]Evaluating:  34%|███▎      | 105/313 [00:48<01:36,  2.15it/s]Evaluating:  60%|██████    | 188/313 [01:18<00:58,  2.15it/s]Evaluating:  34%|███▍      | 106/313 [00:48<01:35,  2.16it/s]Evaluating:  60%|██████    | 189/313 [01:18<00:57,  2.17it/s]Evaluating:  34%|███▍      | 107/313 [00:49<01:35,  2.17it/s]Evaluating:  61%|██████    | 190/313 [01:19<00:56,  2.16it/s]Evaluating:  35%|███▍      | 108/313 [00:49<01:36,  2.13it/s]Evaluating:  61%|██████    | 191/313 [01:19<00:56,  2.16it/s]Evaluating:  35%|███▍      | 109/313 [00:50<01:33,  2.18it/s]Evaluating:  61%|██████▏   | 192/313 [01:20<00:55,  2.17it/s]Evaluating:  35%|███▌      | 110/313 [00:50<01:33,  2.17it/s]Evaluating:  62%|██████▏   | 193/313 [01:20<00:55,  2.16it/s]Evaluating:  35%|███▌      | 111/313 [00:51<01:32,  2.18it/s]Evaluating:  62%|██████▏   | 194/313 [01:21<00:54,  2.17it/s]Evaluating:  36%|███▌      | 112/313 [00:51<01:33,  2.15it/s]Evaluating:  62%|██████▏   | 195/313 [01:21<00:54,  2.15it/s]Evaluating:  36%|███▌      | 113/313 [00:52<01:32,  2.17it/s]Evaluating:  63%|██████▎   | 196/313 [01:21<00:53,  2.17it/s]Evaluating:  36%|███▋      | 114/313 [00:52<01:32,  2.16it/s]Evaluating:  63%|██████▎   | 197/313 [01:22<00:54,  2.14it/s]Evaluating:  37%|███▋      | 115/313 [00:53<01:30,  2.18it/s]Evaluating:  63%|██████▎   | 198/313 [01:22<00:53,  2.15it/s]Evaluating:  37%|███▋      | 116/313 [00:53<01:30,  2.17it/s]Evaluating:  64%|██████▎   | 199/313 [01:23<00:53,  2.14it/s]Evaluating:  37%|███▋      | 117/313 [00:54<01:30,  2.17it/s]Evaluating:  64%|██████▍   | 200/313 [01:23<00:52,  2.15it/s]Evaluating:  38%|███▊      | 118/313 [00:54<01:30,  2.16it/s]Evaluating:  64%|██████▍   | 201/313 [01:24<00:51,  2.17it/s]Evaluating:  38%|███▊      | 119/313 [00:55<01:35,  2.04it/s]Evaluating:  65%|██████▍   | 202/313 [01:24<00:53,  2.09it/s]Evaluating:  38%|███▊      | 120/313 [00:55<01:32,  2.10it/s]Evaluating:  65%|██████▍   | 203/313 [01:25<00:51,  2.13it/s]Evaluating:  39%|███▊      | 121/313 [00:55<01:30,  2.12it/s]Evaluating:  65%|██████▌   | 204/313 [01:25<00:51,  2.13it/s]Evaluating:  39%|███▉      | 122/313 [00:56<01:29,  2.14it/s]Evaluating:  65%|██████▌   | 205/313 [01:26<00:50,  2.14it/s]Evaluating:  39%|███▉      | 123/313 [00:56<01:28,  2.15it/s]Evaluating:  66%|██████▌   | 206/313 [01:26<00:49,  2.15it/s]Evaluating:  40%|███▉      | 124/313 [00:57<01:28,  2.15it/s]Evaluating:  66%|██████▌   | 207/313 [01:27<00:49,  2.14it/s]Evaluating:  40%|███▉      | 125/313 [00:57<01:27,  2.14it/s]Evaluating:  66%|██████▋   | 208/313 [01:27<00:49,  2.13it/s]Evaluating:  40%|████      | 126/313 [00:58<01:26,  2.15it/s]Evaluating:  67%|██████▋   | 209/313 [01:28<00:48,  2.15it/s]Evaluating:  41%|████      | 127/313 [00:58<01:25,  2.17it/s]Evaluating:  67%|██████▋   | 210/313 [01:28<00:47,  2.16it/s]Evaluating:  41%|████      | 128/313 [00:59<01:25,  2.16it/s]Evaluating:  67%|██████▋   | 211/313 [01:29<00:47,  2.14it/s]Evaluating:  41%|████      | 129/313 [00:59<01:25,  2.16it/s]Evaluating:  68%|██████▊   | 212/313 [01:29<00:46,  2.16it/s]Evaluating:  42%|████▏     | 130/313 [01:00<01:24,  2.16it/s]Evaluating:  68%|██████▊   | 213/313 [01:29<00:46,  2.15it/s]Evaluating:  42%|████▏     | 131/313 [01:00<01:23,  2.17it/s]Evaluating:  68%|██████▊   | 214/313 [01:30<00:45,  2.16it/s]Evaluating:  42%|████▏     | 132/313 [01:01<01:24,  2.15it/s]Evaluating:  69%|██████▊   | 215/313 [01:30<00:45,  2.15it/s]Evaluating:  42%|████▏     | 133/313 [01:01<01:23,  2.16it/s]Evaluating:  69%|██████▉   | 216/313 [01:31<00:44,  2.17it/s]Evaluating:  43%|████▎     | 134/313 [01:01<01:22,  2.18it/s]Evaluating:  69%|██████▉   | 217/313 [01:31<00:43,  2.19it/s]Evaluating:  43%|████▎     | 135/313 [01:02<01:22,  2.17it/s]Evaluating:  70%|██████▉   | 218/313 [01:32<00:43,  2.17it/s]Evaluating:  43%|████▎     | 136/313 [01:02<01:22,  2.15it/s]Evaluating:  70%|██████▉   | 219/313 [01:32<00:43,  2.15it/s]Evaluating:  44%|████▍     | 137/313 [01:03<01:21,  2.17it/s]Evaluating:  70%|███████   | 220/313 [01:33<00:42,  2.17it/s]Evaluating:  44%|████▍     | 138/313 [01:03<01:20,  2.17it/s]Evaluating:  71%|███████   | 221/313 [01:33<00:42,  2.18it/s]Evaluating:  44%|████▍     | 139/313 [01:04<01:20,  2.16it/s]Evaluating:  71%|███████   | 222/313 [01:34<00:42,  2.16it/s]Evaluating:  45%|████▍     | 140/313 [01:04<01:19,  2.16it/s]Evaluating:  71%|███████   | 223/313 [01:34<00:41,  2.17it/s]Evaluating:  45%|████▌     | 141/313 [01:05<01:19,  2.15it/s]Evaluating:  72%|███████▏  | 224/313 [01:35<00:41,  2.15it/s]Evaluating:  45%|████▌     | 142/313 [01:05<01:18,  2.17it/s]Evaluating:  72%|███████▏  | 225/313 [01:35<00:40,  2.17it/s]Evaluating:  46%|████▌     | 143/313 [01:06<01:18,  2.16it/s]Evaluating:  72%|███████▏  | 226/313 [01:35<00:40,  2.15it/s]Evaluating:  46%|████▌     | 144/313 [01:06<01:18,  2.17it/s]Evaluating:  73%|███████▎  | 227/313 [01:36<00:39,  2.17it/s]Evaluating:  46%|████▋     | 145/313 [01:07<01:17,  2.18it/s]Evaluating:  73%|███████▎  | 228/313 [01:36<00:39,  2.18it/s]Evaluating:  47%|████▋     | 146/313 [01:07<01:17,  2.16it/s]Evaluating:  73%|███████▎  | 229/313 [01:37<00:38,  2.16it/s]Evaluating:  47%|████▋     | 147/313 [01:08<01:17,  2.15it/s]Evaluating:  73%|███████▎  | 230/313 [01:37<00:38,  2.15it/s]Evaluating:  47%|████▋     | 148/313 [01:08<01:16,  2.16it/s]Evaluating:  74%|███████▍  | 231/313 [01:38<00:37,  2.17it/s]Evaluating:  48%|████▊     | 149/313 [01:08<01:14,  2.19it/s]Evaluating:  74%|███████▍  | 232/313 [01:38<00:36,  2.19it/s]Evaluating:  48%|████▊     | 150/313 [01:09<01:15,  2.17it/s]Evaluating:  74%|███████▍  | 233/313 [01:39<00:36,  2.17it/s]Evaluating:  48%|████▊     | 151/313 [01:09<01:14,  2.18it/s]Evaluating:  75%|███████▍  | 234/313 [01:39<00:36,  2.18it/s]Evaluating:  49%|████▊     | 152/313 [01:10<01:14,  2.16it/s]Evaluating:  75%|███████▌  | 235/313 [01:40<00:36,  2.16it/s]Evaluating:  49%|████▉     | 153/313 [01:10<01:13,  2.18it/s]Evaluating:  75%|███████▌  | 236/313 [01:40<00:35,  2.18it/s]Evaluating:  49%|████▉     | 154/313 [01:11<01:13,  2.15it/s]Evaluating:  76%|███████▌  | 237/313 [01:41<00:35,  2.16it/s]Evaluating:  50%|████▉     | 155/313 [01:11<01:12,  2.17it/s]Evaluating:  76%|███████▌  | 238/313 [01:41<00:34,  2.17it/s]Evaluating:  50%|████▉     | 156/313 [01:12<01:12,  2.17it/s]Evaluating:  76%|███████▋  | 239/313 [01:41<00:33,  2.18it/s]Evaluating:  50%|█████     | 157/313 [01:12<01:12,  2.16it/s]Evaluating:  77%|███████▋  | 240/313 [01:42<00:33,  2.16it/s]Evaluating:  50%|█████     | 158/313 [01:13<01:12,  2.14it/s]Evaluating:  77%|███████▋  | 241/313 [01:42<00:33,  2.15it/s]Evaluating:  51%|█████     | 159/313 [01:13<01:11,  2.17it/s]Evaluating:  77%|███████▋  | 242/313 [01:43<00:32,  2.16it/s]Evaluating:  51%|█████     | 160/313 [01:13<01:10,  2.18it/s]Evaluating:  78%|███████▊  | 243/313 [01:43<00:32,  2.18it/s]Evaluating:  51%|█████▏    | 161/313 [01:14<01:10,  2.16it/s]Evaluating:  78%|███████▊  | 244/313 [01:44<00:31,  2.16it/s]Evaluating:  52%|█████▏    | 162/313 [01:14<01:11,  2.10it/s]Evaluating:  78%|███████▊  | 245/313 [01:44<00:31,  2.13it/s]Evaluating:  52%|█████▏    | 163/313 [01:15<01:10,  2.12it/s]Evaluating:  79%|███████▊  | 246/313 [01:45<00:31,  2.14it/s]Evaluating:  52%|█████▏    | 164/313 [01:15<01:10,  2.12it/s]Evaluating:  79%|███████▉  | 247/313 [01:45<00:31,  2.12it/s]Evaluating:  53%|█████▎    | 165/313 [01:16<01:09,  2.13it/s]Evaluating:  79%|███████▉  | 248/313 [01:46<00:30,  2.12it/s]Evaluating:  53%|█████▎    | 166/313 [01:16<01:08,  2.15it/s]Evaluating:  80%|███████▉  | 249/313 [01:46<00:29,  2.14it/s]Evaluating:  53%|█████▎    | 167/313 [01:17<01:06,  2.18it/s]Evaluating:  80%|███████▉  | 250/313 [01:47<00:29,  2.17it/s]Evaluating:  54%|█████▎    | 168/313 [01:17<01:06,  2.18it/s]Evaluating:  80%|████████  | 251/313 [01:47<00:28,  2.16it/s]Evaluating:  54%|█████▍    | 169/313 [01:18<01:05,  2.19it/s]Evaluating:  81%|████████  | 252/313 [01:47<00:28,  2.17it/s]Evaluating:  54%|█████▍    | 170/313 [01:18<01:06,  2.17it/s]Evaluating:  81%|████████  | 253/313 [01:48<00:27,  2.16it/s]Evaluating:  55%|█████▍    | 171/313 [01:19<01:05,  2.18it/s]Evaluating:  81%|████████  | 254/313 [01:48<00:27,  2.17it/s]Evaluating:  55%|█████▍    | 172/313 [01:19<01:05,  2.17it/s]Evaluating:  81%|████████▏ | 255/313 [01:49<00:26,  2.16it/s]Evaluating:  55%|█████▌    | 173/313 [01:20<01:07,  2.06it/s]Evaluating:  82%|████████▏ | 256/313 [01:49<00:27,  2.10it/s]Evaluating:  56%|█████▌    | 174/313 [01:20<01:04,  2.16it/s]Evaluating:  82%|████████▏ | 257/313 [01:50<00:25,  2.19it/s]Evaluating:  56%|█████▌    | 175/313 [01:20<01:02,  2.20it/s]Evaluating:  82%|████████▏ | 258/313 [01:50<00:24,  2.22it/s]Evaluating:  56%|█████▌    | 176/313 [01:21<01:01,  2.22it/s]Evaluating:  83%|████████▎ | 259/313 [01:51<00:24,  2.24it/s]Evaluating:  57%|█████▋    | 177/313 [01:21<01:00,  2.24it/s]Evaluating:  83%|████████▎ | 260/313 [01:51<00:23,  2.24it/s]Evaluating:  57%|█████▋    | 178/313 [01:22<01:00,  2.23it/s]Evaluating:  83%|████████▎ | 261/313 [01:52<00:23,  2.22it/s]Evaluating:  57%|█████▋    | 179/313 [01:22<01:01,  2.19it/s]Evaluating:  84%|████████▎ | 262/313 [01:52<00:23,  2.18it/s]Evaluating:  58%|█████▊    | 180/313 [01:23<01:00,  2.19it/s]Evaluating:  84%|████████▍ | 263/313 [01:53<00:22,  2.18it/s]Evaluating:  58%|█████▊    | 181/313 [01:23<01:00,  2.20it/s]Evaluating:  84%|████████▍ | 264/313 [01:53<00:22,  2.19it/s]Evaluating:  58%|█████▊    | 182/313 [01:24<01:00,  2.17it/s]Evaluating:  85%|████████▍ | 265/313 [01:53<00:22,  2.16it/s]Evaluating:  58%|█████▊    | 183/313 [01:24<01:00,  2.16it/s]Evaluating:  85%|████████▍ | 266/313 [01:54<00:21,  2.15it/s]Evaluating:  59%|█████▉    | 184/313 [01:25<01:01,  2.10it/s]Evaluating:  85%|████████▌ | 267/313 [01:54<00:21,  2.14it/s]Evaluating:  59%|█████▉    | 185/313 [01:25<01:00,  2.11it/s]Evaluating:  86%|████████▌ | 268/313 [01:55<00:21,  2.12it/s]Evaluating:  59%|█████▉    | 186/313 [01:26<01:00,  2.10it/s]Evaluating:  86%|████████▌ | 269/313 [01:55<00:20,  2.11it/s]Evaluating:  60%|█████▉    | 187/313 [01:26<00:59,  2.13it/s]Evaluating:  86%|████████▋ | 270/313 [01:56<00:20,  2.13it/s]Evaluating:  60%|██████    | 188/313 [01:26<00:57,  2.16it/s]Evaluating:  87%|████████▋ | 271/313 [01:56<00:19,  2.14it/s]Evaluating:  60%|██████    | 189/313 [01:27<00:57,  2.15it/s]Evaluating:  87%|████████▋ | 272/313 [01:57<00:19,  2.13it/s]Evaluating:  61%|██████    | 190/313 [01:27<00:57,  2.15it/s]Evaluating:  87%|████████▋ | 273/313 [01:57<00:18,  2.14it/s]Evaluating:  61%|██████    | 191/313 [01:28<00:56,  2.16it/s]Evaluating:  88%|████████▊ | 274/313 [01:58<00:18,  2.15it/s]Evaluating:  61%|██████▏   | 192/313 [01:28<00:55,  2.18it/s]Evaluating:  88%|████████▊ | 275/313 [01:58<00:17,  2.16it/s]Evaluating:  62%|██████▏   | 193/313 [01:29<00:55,  2.17it/s]Evaluating:  88%|████████▊ | 276/313 [01:59<00:17,  2.15it/s]Evaluating:  62%|██████▏   | 194/313 [01:29<00:54,  2.18it/s]Evaluating:  88%|████████▊ | 277/313 [01:59<00:16,  2.17it/s]Evaluating:  62%|██████▏   | 195/313 [01:30<00:53,  2.21it/s]Evaluating:  89%|████████▉ | 278/313 [01:59<00:16,  2.18it/s]Evaluating:  63%|██████▎   | 196/313 [01:30<00:53,  2.17it/s]Evaluating:  89%|████████▉ | 279/313 [02:00<00:15,  2.16it/s]Evaluating:  63%|██████▎   | 197/313 [01:31<00:54,  2.14it/s]Evaluating:  89%|████████▉ | 280/313 [02:00<00:15,  2.15it/s]Evaluating:  63%|██████▎   | 198/313 [01:31<00:53,  2.17it/s]Evaluating:  90%|████████▉ | 281/313 [02:01<00:14,  2.17it/s]Evaluating:  64%|██████▎   | 199/313 [01:32<00:52,  2.18it/s]Evaluating:  90%|█████████ | 282/313 [02:01<00:14,  2.18it/s]Evaluating:  64%|██████▍   | 200/313 [01:32<00:52,  2.16it/s]Evaluating:  90%|█████████ | 283/313 [02:02<00:13,  2.16it/s]Evaluating:  64%|██████▍   | 201/313 [01:32<00:52,  2.15it/s]Evaluating:  91%|█████████ | 284/313 [02:02<00:13,  2.14it/s]Evaluating:  65%|██████▍   | 202/313 [01:33<00:51,  2.16it/s]Evaluating:  91%|█████████ | 285/313 [02:03<00:12,  2.16it/s]Evaluating:  65%|██████▍   | 203/313 [01:33<00:50,  2.18it/s]Evaluating:  91%|█████████▏| 286/313 [02:03<00:12,  2.16it/s]Evaluating:  65%|██████▌   | 204/313 [01:34<00:50,  2.16it/s]Evaluating:  92%|█████████▏| 287/313 [02:04<00:12,  2.14it/s]Evaluating:  65%|██████▌   | 205/313 [01:34<00:49,  2.17it/s]Evaluating:  92%|█████████▏| 288/313 [02:04<00:11,  2.16it/s]Evaluating:  66%|██████▌   | 206/313 [01:35<00:49,  2.18it/s]Evaluating:  92%|█████████▏| 289/313 [02:05<00:11,  2.15it/s]Evaluating:  66%|██████▌   | 207/313 [01:35<00:48,  2.17it/s]Evaluating:  93%|█████████▎| 290/313 [02:05<00:10,  2.13it/s]Evaluating:  66%|██████▋   | 208/313 [01:36<00:48,  2.16it/s]Evaluating:  93%|█████████▎| 291/313 [02:06<00:10,  2.13it/s]Evaluating:  67%|██████▋   | 209/313 [01:36<00:47,  2.17it/s]Evaluating:  93%|█████████▎| 292/313 [02:06<00:09,  2.14it/s]Evaluating:  67%|██████▋   | 210/313 [01:37<00:47,  2.18it/s]Evaluating:  94%|█████████▎| 293/313 [02:06<00:09,  2.16it/s]Evaluating:  67%|██████▋   | 211/313 [01:37<00:47,  2.16it/s]Evaluating:  94%|█████████▍| 294/313 [02:07<00:08,  2.13it/s]Evaluating:  68%|██████▊   | 212/313 [01:38<00:46,  2.15it/s]Evaluating:  94%|█████████▍| 295/313 [02:07<00:08,  2.14it/s]Evaluating:  68%|██████▊   | 213/313 [01:38<00:46,  2.17it/s]Evaluating:  95%|█████████▍| 296/313 [02:08<00:07,  2.15it/s]Evaluating:  68%|██████▊   | 214/313 [01:38<00:45,  2.18it/s]Evaluating:  95%|█████████▍| 297/313 [02:08<00:07,  2.13it/s]Evaluating:  69%|██████▊   | 215/313 [01:39<00:45,  2.17it/s]Evaluating:  95%|█████████▌| 298/313 [02:09<00:07,  2.12it/s]Evaluating:  69%|██████▉   | 216/313 [01:39<00:43,  2.22it/s]Evaluating:  96%|█████████▌| 299/313 [02:09<00:06,  2.08it/s]Evaluating:  69%|██████▉   | 217/313 [01:40<00:43,  2.22it/s]Evaluating:  96%|█████████▌| 300/313 [02:10<00:06,  2.11it/s]Evaluating:  70%|██████▉   | 218/313 [01:40<00:43,  2.20it/s]Evaluating:  96%|█████████▌| 301/313 [02:10<00:05,  2.10it/s]Evaluating:  70%|██████▉   | 219/313 [01:41<00:43,  2.18it/s]Evaluating:  96%|█████████▋| 302/313 [02:11<00:05,  2.10it/s]Evaluating:  70%|███████   | 220/313 [01:41<00:42,  2.20it/s]Evaluating:  97%|█████████▋| 303/313 [02:11<00:04,  2.12it/s]Evaluating:  71%|███████   | 221/313 [01:42<00:41,  2.22it/s]Evaluating:  97%|█████████▋| 304/313 [02:12<00:04,  2.10it/s]Evaluating:  71%|███████   | 222/313 [01:42<00:41,  2.19it/s]Evaluating:  97%|█████████▋| 305/313 [02:12<00:03,  2.10it/s]Evaluating:  71%|███████   | 223/313 [01:43<00:41,  2.18it/s]Evaluating:  98%|█████████▊| 306/313 [02:13<00:03,  2.12it/s]Evaluating:  72%|███████▏  | 224/313 [01:43<00:40,  2.19it/s]Evaluating:  98%|█████████▊| 307/313 [02:13<00:02,  2.14it/s]Evaluating:  72%|███████▏  | 225/313 [01:43<00:40,  2.20it/s]Evaluating:  98%|█████████▊| 308/313 [02:14<00:02,  2.12it/s]Evaluating:  72%|███████▏  | 226/313 [01:44<00:39,  2.19it/s]Evaluating:  99%|█████████▊| 309/313 [02:14<00:01,  2.11it/s]Evaluating:  73%|███████▎  | 227/313 [01:44<00:39,  2.20it/s]Evaluating:  99%|█████████▉| 310/313 [02:15<00:01,  2.12it/s]Evaluating:  73%|███████▎  | 228/313 [01:45<00:38,  2.21it/s]Evaluating:  99%|█████████▉| 311/313 [02:15<00:00,  2.12it/s]Evaluating:  73%|███████▎  | 229/313 [01:45<00:38,  2.19it/s]Evaluating: 100%|█████████▉| 312/313 [02:15<00:00,  2.11it/s]Evaluating:  73%|███████▎  | 230/313 [01:46<00:38,  2.17it/s]Evaluating: 100%|██████████| 313/313 [02:16<00:00,  2.37it/s]Evaluating: 100%|██████████| 313/313 [02:16<00:00,  2.30it/s]Evaluating:  74%|███████▍  | 231/313 [01:46<00:35,  2.30it/s]Evaluating:  74%|███████▍  | 232/313 [01:46<00:31,  2.56it/s]Evaluating:  74%|███████▍  | 233/313 [01:47<00:29,  2.73it/s]Evaluating:  75%|███████▍  | 234/313 [01:47<00:27,  2.86it/s]
10/10/2021 15:30:55 - INFO - __main__ -   ***** Evaluation result  in da *****
10/10/2021 15:30:55 - INFO - __main__ -     f1 = 0.7498950314905529
10/10/2021 15:30:55 - INFO - __main__ -     loss = 0.7439033948004056
10/10/2021 15:30:55 - INFO - __main__ -     precision = 0.7200645074586749
10/10/2021 15:30:55 - INFO - __main__ -     recall = 0.7823039859833553
10/10/2021 15:30:55 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  75%|███████▌  | 235/313 [01:47<00:25,  3.00it/s]Evaluating:  75%|███████▌  | 236/313 [01:48<00:24,  3.12it/s]Evaluating:  76%|███████▌  | 237/313 [01:48<00:24,  3.15it/s]Evaluating:  76%|███████▌  | 238/313 [01:48<00:23,  3.22it/s]Evaluating:  76%|███████▋  | 239/313 [01:49<00:22,  3.28it/s]Evaluating:  77%|███████▋  | 240/313 [01:49<00:22,  3.26it/s]Evaluating:  77%|███████▋  | 241/313 [01:49<00:22,  3.26it/s]Evaluating:  77%|███████▋  | 242/313 [01:49<00:21,  3.28it/s]Evaluating:  78%|███████▊  | 243/313 [01:50<00:21,  3.31it/s]Evaluating:  78%|███████▊  | 244/313 [01:50<00:20,  3.30it/s]Evaluating:  78%|███████▊  | 245/313 [01:50<00:20,  3.28it/s]Evaluating:  79%|███████▊  | 246/313 [01:51<00:20,  3.26it/s]Evaluating:  79%|███████▉  | 247/313 [01:51<00:20,  3.26it/s]Evaluating:  79%|███████▉  | 248/313 [01:51<00:20,  3.23it/s]Evaluating:  80%|███████▉  | 249/313 [01:52<00:19,  3.26it/s]Evaluating:  80%|███████▉  | 250/313 [01:52<00:19,  3.29it/s]Evaluating:  80%|████████  | 251/313 [01:52<00:19,  3.25it/s]Evaluating:  81%|████████  | 252/313 [01:53<00:18,  3.23it/s]Evaluating:  81%|████████  | 253/313 [01:53<00:18,  3.21it/s]Evaluating:  81%|████████  | 254/313 [01:53<00:18,  3.21it/s]Evaluating:  81%|████████▏ | 255/313 [01:53<00:18,  3.20it/s]Evaluating:  82%|████████▏ | 256/313 [01:54<00:17,  3.24it/s]Evaluating:  82%|████████▏ | 257/313 [01:54<00:17,  3.28it/s]Evaluating:  82%|████████▏ | 258/313 [01:54<00:16,  3.25it/s]Evaluating:  83%|████████▎ | 259/313 [01:55<00:16,  3.27it/s]Evaluating:  83%|████████▎ | 260/313 [01:55<00:16,  3.25it/s]Evaluating:  83%|████████▎ | 261/313 [01:55<00:16,  3.24it/s]Evaluating:  84%|████████▎ | 262/313 [01:56<00:15,  3.21it/s]Evaluating:  84%|████████▍ | 263/313 [01:56<00:15,  3.22it/s]Evaluating:  84%|████████▍ | 264/313 [01:56<00:15,  3.26it/s]Evaluating:  85%|████████▍ | 265/313 [01:57<00:14,  3.23it/s]Evaluating:  85%|████████▍ | 266/313 [01:57<00:14,  3.27it/s]Evaluating:  85%|████████▌ | 267/313 [01:57<00:14,  3.28it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  86%|████████▌ | 268/313 [01:57<00:13,  3.24it/s]Evaluating:  86%|████████▌ | 269/313 [01:58<00:13,  3.22it/s]Evaluating:  86%|████████▋ | 270/313 [01:58<00:13,  3.29it/s]Evaluating:  87%|████████▋ | 271/313 [01:58<00:12,  3.34it/s]Evaluating:  87%|████████▋ | 272/313 [01:59<00:12,  3.33it/s]Evaluating:  87%|████████▋ | 273/313 [01:59<00:12,  3.28it/s]Evaluating:  88%|████████▊ | 274/313 [01:59<00:12,  3.24it/s]Evaluating:  88%|████████▊ | 275/313 [02:00<00:11,  3.25it/s]Evaluating:  88%|████████▊ | 276/313 [02:00<00:11,  3.24it/s]Evaluating:  88%|████████▊ | 277/313 [02:00<00:10,  3.29it/s]Evaluating:  89%|████████▉ | 278/313 [02:00<00:10,  3.32it/s]Evaluating:  89%|████████▉ | 279/313 [02:01<00:10,  3.29it/s]Evaluating:  89%|████████▉ | 280/313 [02:01<00:10,  3.24it/s]Evaluating:  90%|████████▉ | 281/313 [02:01<00:09,  3.22it/s]Evaluating:  90%|█████████ | 282/313 [02:02<00:09,  3.20it/s]Evaluating:  90%|█████████ | 283/313 [02:02<00:09,  3.19it/s]Evaluating:  91%|█████████ | 284/313 [02:02<00:08,  3.22it/s]Evaluating:  91%|█████████ | 285/313 [02:03<00:08,  3.26it/s]Evaluating:  91%|█████████▏| 286/313 [02:03<00:08,  3.25it/s]Evaluating:  92%|█████████▏| 287/313 [02:03<00:07,  3.29it/s]Evaluating:  92%|█████████▏| 288/313 [02:04<00:07,  3.27it/s]Evaluating:  92%|█████████▏| 289/313 [02:04<00:07,  3.27it/s]Evaluating:  93%|█████████▎| 290/313 [02:04<00:07,  3.27it/s]Evaluating:  93%|█████████▎| 291/313 [02:04<00:06,  3.32it/s]Evaluating:  93%|█████████▎| 292/313 [02:05<00:06,  3.33it/s]Evaluating:  94%|█████████▎| 293/313 [02:05<00:06,  3.28it/s]Evaluating:  94%|█████████▍| 294/313 [02:05<00:05,  3.30it/s]Evaluating:  94%|█████████▍| 295/313 [02:06<00:05,  3.26it/s]Evaluating:  95%|█████████▍| 296/313 [02:06<00:05,  3.22it/s]Evaluating:  95%|█████████▍| 297/313 [02:06<00:05,  3.19it/s]Evaluating:  95%|█████████▌| 298/313 [02:07<00:04,  3.18it/s]Evaluating:  96%|█████████▌| 299/313 [02:07<00:04,  3.21it/s]Evaluating:  96%|█████████▌| 300/313 [02:07<00:04,  3.18it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  96%|█████████▌| 301/313 [02:08<00:03,  3.22it/s]Evaluating:  96%|█████████▋| 302/313 [02:08<00:03,  3.26it/s]Evaluating:  97%|█████████▋| 303/313 [02:08<00:03,  3.23it/s]Evaluating:  97%|█████████▋| 304/313 [02:08<00:02,  3.25it/s]Evaluating:  97%|█████████▋| 305/313 [02:09<00:02,  3.23it/s]Evaluating:  98%|█████████▊| 306/313 [02:09<00:02,  3.28it/s]Evaluating:  98%|█████████▊| 307/313 [02:09<00:01,  3.24it/s]Evaluating:  98%|█████████▊| 308/313 [02:10<00:01,  3.06it/s]Evaluating:  99%|█████████▊| 309/313 [02:10<00:01,  3.15it/s]Evaluating:  99%|█████████▉| 310/313 [02:10<00:00,  3.14it/s]Evaluating:  99%|█████████▉| 311/313 [02:11<00:00,  3.20it/s]Evaluating: 100%|█████████▉| 312/313 [02:11<00:00,  3.26it/s]Evaluating: 100%|██████████| 313/313 [02:11<00:00,  3.63it/s]Evaluating: 100%|██████████| 313/313 [02:11<00:00,  2.38it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

10/10/2021 15:31:20 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/10/2021 15:31:20 - INFO - __main__ -     f1 = 0.5056109334837942
10/10/2021 15:31:20 - INFO - __main__ -     loss = 1.755156885130337
10/10/2021 15:31:20 - INFO - __main__ -     precision = 0.4363469133798626
10/10/2021 15:31:20 - INFO - __main__ -     recall = 0.601013488337432
10/10/2021 15:31:20 - INFO - __main__ -   Language adapter for bg not found, using ru instead
10/10/2021 15:31:20 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:31:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/10/2021 15:31:22 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/10/2021 15:31:22 - INFO - __main__ -     Num examples = 10004
10/10/2021 15:31:22 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:28,  3.53it/s]Evaluating:   1%|          | 2/313 [00:00<01:57,  2.64it/s]Evaluating:   1%|          | 3/313 [00:01<01:46,  2.90it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:39,  3.10it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:37,  3.16it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:33,  3.27it/s]Evaluating:   2%|▏         | 7/313 [00:02<01:33,  3.26it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:31,  3.32it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:30,  3.36it/s]Evaluating:   3%|▎         | 10/313 [00:03<01:30,  3.35it/s]Evaluating:   4%|▎         | 11/313 [00:03<01:30,  3.35it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:29,  3.36it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:28,  3.40it/s]Evaluating:   4%|▍         | 14/313 [00:04<01:29,  3.36it/s]Evaluating:   5%|▍         | 15/313 [00:04<01:27,  3.39it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:28,  3.36it/s]Evaluating:   5%|▌         | 17/313 [00:05<01:27,  3.40it/s]Evaluating:   6%|▌         | 18/313 [00:05<01:27,  3.36it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:27,  3.38it/s]Evaluating:   6%|▋         | 20/313 [00:06<01:27,  3.35it/s]Evaluating:   7%|▋         | 21/313 [00:06<01:25,  3.41it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:26,  3.37it/s]Evaluating:   7%|▋         | 23/313 [00:06<01:25,  3.39it/s]Evaluating:   8%|▊         | 24/313 [00:07<01:26,  3.36it/s]Evaluating:   8%|▊         | 25/313 [00:07<01:24,  3.40it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:24,  3.40it/s]Evaluating:   9%|▊         | 27/313 [00:08<01:24,  3.39it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   9%|▉         | 28/313 [00:08<01:24,  3.37it/s]Evaluating:   9%|▉         | 29/313 [00:08<01:23,  3.40it/s]Evaluating:  10%|▉         | 30/313 [00:09<01:22,  3.41it/s]Evaluating:  10%|▉         | 31/313 [00:09<01:23,  3.37it/s]Evaluating:  10%|█         | 32/313 [00:09<01:22,  3.40it/s]Evaluating:  11%|█         | 33/313 [00:09<01:23,  3.37it/s]Evaluating:  11%|█         | 34/313 [00:10<01:22,  3.39it/s]Evaluating:  11%|█         | 35/313 [00:10<01:22,  3.36it/s]Evaluating:  12%|█▏        | 36/313 [00:10<01:21,  3.39it/s]Evaluating:  12%|█▏        | 37/313 [00:11<01:22,  3.36it/s]Evaluating:  12%|█▏        | 38/313 [00:11<01:21,  3.39it/s]Evaluating:  12%|█▏        | 39/313 [00:11<01:21,  3.36it/s]Evaluating:  13%|█▎        | 40/313 [00:11<01:20,  3.40it/s]Evaluating:  13%|█▎        | 41/313 [00:12<01:20,  3.38it/s]Evaluating:  13%|█▎        | 42/313 [00:12<01:20,  3.37it/s]Evaluating:  14%|█▎        | 43/313 [00:12<01:20,  3.34it/s]Evaluating:  14%|█▍        | 44/313 [00:13<01:19,  3.38it/s]Evaluating:  14%|█▍        | 45/313 [00:13<01:18,  3.40it/s]Evaluating:  15%|█▍        | 46/313 [00:13<01:19,  3.37it/s]Evaluating:  15%|█▌        | 47/313 [00:14<01:19,  3.35it/s]Evaluating:  15%|█▌        | 48/313 [00:14<01:18,  3.36it/s]Evaluating:  16%|█▌        | 49/313 [00:14<01:17,  3.39it/s]Evaluating:  16%|█▌        | 50/313 [00:14<01:18,  3.36it/s]Evaluating:  16%|█▋        | 51/313 [00:15<01:17,  3.39it/s]Evaluating:  17%|█▋        | 52/313 [00:15<01:17,  3.36it/s]Evaluating:  17%|█▋        | 53/313 [00:15<01:21,  3.19it/s]Evaluating:  17%|█▋        | 54/313 [00:16<01:20,  3.23it/s]Evaluating:  18%|█▊        | 55/313 [00:16<01:17,  3.32it/s]Evaluating:  18%|█▊        | 56/313 [00:16<01:16,  3.37it/s]Evaluating:  18%|█▊        | 57/313 [00:17<01:16,  3.35it/s]Evaluating:  19%|█▊        | 58/313 [00:17<01:17,  3.31it/s]Evaluating:  19%|█▉        | 59/313 [00:17<01:15,  3.35it/s]Evaluating:  19%|█▉        | 60/313 [00:17<01:15,  3.37it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  19%|█▉        | 61/313 [00:18<01:15,  3.33it/s]10/10/2021 15:31:40 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:31:40 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  20%|█▉        | 62/313 [00:18<01:14,  3.36it/s]Evaluating:  20%|██        | 63/313 [00:18<01:15,  3.32it/s]Evaluating:  20%|██        | 64/313 [00:19<01:14,  3.35it/s]Evaluating:  21%|██        | 65/313 [00:19<01:14,  3.32it/s]Evaluating:  21%|██        | 66/313 [00:19<01:13,  3.34it/s]Evaluating:  21%|██▏       | 67/313 [00:20<01:13,  3.37it/s]Evaluating:  22%|██▏       | 68/313 [00:20<01:13,  3.32it/s]Evaluating:  22%|██▏       | 69/313 [00:20<01:13,  3.31it/s]Evaluating:  22%|██▏       | 70/313 [00:20<01:15,  3.21it/s]Evaluating:  23%|██▎       | 71/313 [00:21<01:13,  3.28it/s]Evaluating:  23%|██▎       | 72/313 [00:21<01:13,  3.28it/s]Evaluating:  23%|██▎       | 73/313 [00:21<01:11,  3.34it/s]Evaluating:  24%|██▎       | 74/313 [00:22<01:11,  3.36it/s]Evaluating:  24%|██▍       | 75/313 [00:22<01:11,  3.34it/s]Evaluating:  24%|██▍       | 76/313 [00:22<01:11,  3.32it/s]Evaluating:  25%|██▍       | 77/313 [00:23<01:10,  3.37it/s]Evaluating:  25%|██▍       | 78/313 [00:23<01:09,  3.39it/s]Evaluating:  25%|██▌       | 79/313 [00:23<01:09,  3.35it/s]Evaluating:  26%|██▌       | 80/313 [00:23<01:09,  3.33it/s]Evaluating:  26%|██▌       | 81/313 [00:24<01:08,  3.36it/s]Evaluating:  26%|██▌       | 82/313 [00:24<01:07,  3.41it/s]Evaluating:  27%|██▋       | 83/313 [00:24<01:08,  3.36it/s]Evaluating:  27%|██▋       | 84/313 [00:25<01:08,  3.34it/s]Evaluating:  27%|██▋       | 85/313 [00:25<01:07,  3.38it/s]Evaluating:  27%|██▋       | 86/313 [00:25<01:06,  3.39it/s]Evaluating:  28%|██▊       | 87/313 [00:26<01:07,  3.35it/s]Evaluating:  28%|██▊       | 88/313 [00:26<01:07,  3.35it/s]Evaluating:  28%|██▊       | 89/313 [00:26<01:06,  3.39it/s]Evaluating:  29%|██▉       | 90/313 [00:26<01:06,  3.35it/s]Evaluating:  29%|██▉       | 91/313 [00:27<01:06,  3.32it/s]Evaluating:  29%|██▉       | 92/313 [00:27<01:05,  3.36it/s]Evaluating:  30%|██▉       | 93/313 [00:27<01:05,  3.38it/s]Evaluating:  30%|███       | 94/313 [00:28<01:05,  3.34it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  30%|███       | 95/313 [00:28<01:05,  3.33it/s]Evaluating:  31%|███       | 96/313 [00:28<01:04,  3.37it/s]Evaluating:  31%|███       | 97/313 [00:29<01:03,  3.38it/s]Evaluating:  31%|███▏      | 98/313 [00:29<01:04,  3.34it/s]Evaluating:  32%|███▏      | 99/313 [00:29<01:04,  3.31it/s]Evaluating:  32%|███▏      | 100/313 [00:29<01:02,  3.41it/s]Evaluating:  32%|███▏      | 101/313 [00:30<01:03,  3.35it/s]Evaluating:  33%|███▎      | 102/313 [00:30<01:03,  3.31it/s]Evaluating:  33%|███▎      | 103/313 [00:30<01:03,  3.33it/s]Evaluating:  33%|███▎      | 104/313 [00:31<01:02,  3.34it/s]Evaluating:  34%|███▎      | 105/313 [00:31<01:02,  3.35it/s]Evaluating:  34%|███▍      | 106/313 [00:31<01:01,  3.37it/s]Evaluating:  34%|███▍      | 107/313 [00:31<01:00,  3.42it/s]Evaluating:  35%|███▍      | 108/313 [00:32<01:00,  3.37it/s]Evaluating:  35%|███▍      | 109/313 [00:32<01:01,  3.31it/s]Evaluating:  35%|███▌      | 110/313 [00:32<01:01,  3.28it/s]Evaluating:  35%|███▌      | 111/313 [00:33<00:59,  3.39it/s]Evaluating:  36%|███▌      | 112/313 [00:33<01:00,  3.33it/s]Evaluating:  36%|███▌      | 113/313 [00:33<01:00,  3.30it/s]Evaluating:  36%|███▋      | 114/313 [00:34<01:00,  3.28it/s]Evaluating:  37%|███▋      | 115/313 [00:34<00:59,  3.35it/s]Evaluating:  37%|███▋      | 116/313 [00:34<00:59,  3.32it/s]Evaluating:  37%|███▋      | 117/313 [00:35<00:59,  3.30it/s]Evaluating:  38%|███▊      | 118/313 [00:35<00:57,  3.40it/s]Evaluating:  38%|███▊      | 119/313 [00:35<00:57,  3.34it/s]Evaluating:  38%|███▊      | 120/313 [00:35<00:58,  3.31it/s]Evaluating:  39%|███▊      | 121/313 [00:36<00:58,  3.31it/s]Evaluating:  39%|███▉      | 122/313 [00:36<00:56,  3.40it/s]Evaluating:  39%|███▉      | 123/313 [00:36<00:56,  3.37it/s]Evaluating:  40%|███▉      | 124/313 [00:37<00:56,  3.34it/s]Evaluating:  40%|███▉      | 125/313 [00:37<00:56,  3.33it/s]Evaluating:  40%|████      | 126/313 [00:37<00:54,  3.43it/s]Evaluating:  41%|████      | 127/313 [00:37<00:54,  3.40it/s]Evaluating:  41%|████      | 128/313 [00:38<00:55,  3.33it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  41%|████      | 129/313 [00:38<00:54,  3.35it/s]Evaluating:  42%|████▏     | 130/313 [00:38<00:54,  3.38it/s]Evaluating:  42%|████▏     | 131/313 [00:39<00:54,  3.35it/s]Evaluating:  42%|████▏     | 132/313 [00:39<00:54,  3.32it/s]Evaluating:  42%|████▏     | 133/313 [00:39<00:52,  3.40it/s]Evaluating:  43%|████▎     | 134/313 [00:40<00:53,  3.35it/s]Evaluating:  43%|████▎     | 135/313 [00:40<00:54,  3.29it/s]Evaluating:  43%|████▎     | 136/313 [00:40<00:53,  3.29it/s]Evaluating:  44%|████▍     | 137/313 [00:40<00:51,  3.39it/s]Evaluating:  44%|████▍     | 138/313 [00:41<00:52,  3.35it/s]Evaluating:  44%|████▍     | 139/313 [00:41<00:52,  3.32it/s]Evaluating:  45%|████▍     | 140/313 [00:41<00:51,  3.35it/s]Evaluating:  45%|████▌     | 141/313 [00:42<00:50,  3.38it/s]Evaluating:  45%|████▌     | 142/313 [00:42<00:51,  3.34it/s]Evaluating:  46%|████▌     | 143/313 [00:42<00:51,  3.32it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  46%|████▌     | 144/313 [00:43<00:50,  3.38it/s]Evaluating:  46%|████▋     | 145/313 [00:43<00:50,  3.36it/s]Evaluating:  47%|████▋     | 146/313 [00:43<00:50,  3.32it/s]Evaluating:  47%|████▋     | 147/313 [00:43<00:50,  3.26it/s]Evaluating:  47%|████▋     | 148/313 [00:44<00:49,  3.37it/s]Evaluating:  48%|████▊     | 149/313 [00:44<00:49,  3.33it/s]Evaluating:  48%|████▊     | 150/313 [00:44<00:49,  3.31it/s]Evaluating:  48%|████▊     | 151/313 [00:45<00:48,  3.33it/s]Evaluating:  49%|████▊     | 152/313 [00:45<00:47,  3.38it/s]Evaluating:  49%|████▉     | 153/313 [00:45<00:47,  3.34it/s]Evaluating:  49%|████▉     | 154/313 [00:46<00:47,  3.31it/s]Evaluating:  50%|████▉     | 155/313 [00:46<00:46,  3.38it/s]Evaluating:  50%|████▉     | 156/313 [00:46<00:46,  3.35it/s]Evaluating:  50%|█████     | 157/313 [00:46<00:46,  3.32it/s]Evaluating:  50%|█████     | 158/313 [00:47<00:46,  3.31it/s]Evaluating:  51%|█████     | 159/313 [00:47<00:45,  3.40it/s]Evaluating:  51%|█████     | 160/313 [00:47<00:45,  3.34it/s]Evaluating:  51%|█████▏    | 161/313 [00:48<00:45,  3.33it/s]Evaluating:  52%|█████▏    | 162/313 [00:48<00:45,  3.31it/s]Evaluating:  52%|█████▏    | 163/313 [00:48<00:43,  3.41it/s]Evaluating:  52%|█████▏    | 164/313 [00:49<00:44,  3.36it/s]Evaluating:  53%|█████▎    | 165/313 [00:49<00:44,  3.33it/s]Evaluating:  53%|█████▎    | 166/313 [00:49<00:43,  3.36it/s]Evaluating:  53%|█████▎    | 167/313 [00:49<00:43,  3.32it/s]Evaluating:  54%|█████▎    | 168/313 [00:50<00:43,  3.30it/s]Evaluating:  54%|█████▍    | 169/313 [00:50<00:43,  3.29it/s]Evaluating:  54%|█████▍    | 170/313 [00:50<00:42,  3.38it/s]Evaluating:  55%|█████▍    | 171/313 [00:51<00:42,  3.31it/s]Evaluating:  55%|█████▍    | 172/313 [00:51<00:43,  3.27it/s]Evaluating:  55%|█████▌    | 173/313 [00:51<00:42,  3.27it/s]Evaluating:  56%|█████▌    | 174/313 [00:52<00:41,  3.38it/s]Evaluating:  56%|█████▌    | 175/313 [00:52<00:41,  3.32it/s]Evaluating:  56%|█████▌    | 176/313 [00:52<00:41,  3.29it/s]Evaluating:  57%|█████▋    | 177/313 [00:52<00:40,  3.40it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  57%|█████▋    | 178/313 [00:53<00:40,  3.33it/s]Evaluating:  57%|█████▋    | 179/313 [00:53<00:40,  3.30it/s]Evaluating:  58%|█████▊    | 180/313 [00:53<00:40,  3.30it/s]Evaluating:  58%|█████▊    | 181/313 [00:54<00:38,  3.40it/s]Evaluating:  58%|█████▊    | 182/313 [00:54<00:39,  3.35it/s]Evaluating:  58%|█████▊    | 183/313 [00:54<00:39,  3.32it/s]Evaluating:  59%|█████▉    | 184/313 [00:55<00:39,  3.29it/s]Evaluating:  59%|█████▉    | 185/313 [00:55<00:37,  3.39it/s]Evaluating:  59%|█████▉    | 186/313 [00:55<00:38,  3.34it/s]Evaluating:  60%|█████▉    | 187/313 [00:55<00:38,  3.31it/s]Evaluating:  60%|██████    | 188/313 [00:56<00:39,  3.17it/s]Evaluating:  60%|██████    | 189/313 [00:56<00:38,  3.19it/s]Evaluating:  61%|██████    | 190/313 [00:56<00:38,  3.21it/s]Evaluating:  61%|██████    | 191/313 [00:57<00:37,  3.23it/s]Evaluating:  61%|██████▏   | 192/313 [00:57<00:36,  3.33it/s]Evaluating:  62%|██████▏   | 193/313 [00:57<00:36,  3.30it/s]Evaluating:  62%|██████▏   | 194/313 [00:58<00:36,  3.27it/s]Evaluating:  62%|██████▏   | 195/313 [00:58<00:34,  3.39it/s]Evaluating:  63%|██████▎   | 196/313 [00:58<00:35,  3.33it/s]Evaluating:  63%|██████▎   | 197/313 [00:59<00:35,  3.30it/s]Evaluating:  63%|██████▎   | 198/313 [00:59<00:34,  3.29it/s]Evaluating:  64%|██████▎   | 199/313 [00:59<00:33,  3.40it/s]Evaluating:  64%|██████▍   | 200/313 [00:59<00:33,  3.33it/s]Evaluating:  64%|██████▍   | 201/313 [01:00<00:33,  3.31it/s]Evaluating:  65%|██████▍   | 202/313 [01:00<00:33,  3.29it/s]Evaluating:  65%|██████▍   | 203/313 [01:00<00:32,  3.39it/s]Evaluating:  65%|██████▌   | 204/313 [01:01<00:32,  3.36it/s]Evaluating:  65%|██████▌   | 205/313 [01:01<00:32,  3.32it/s]Evaluating:  66%|██████▌   | 206/313 [01:01<00:31,  3.38it/s]Evaluating:  66%|██████▌   | 207/313 [01:02<00:31,  3.33it/s]Evaluating:  66%|██████▋   | 208/313 [01:02<00:31,  3.30it/s]Evaluating:  67%|██████▋   | 209/313 [01:02<00:31,  3.27it/s]Evaluating:  67%|██████▋   | 210/313 [01:02<00:30,  3.37it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:32:25 - INFO - __main__ -   Using lang2id = None
Evaluating:  67%|██████▋   | 211/313 [01:03<00:30,  3.31it/s]Evaluating:  68%|██████▊   | 212/313 [01:03<00:30,  3.29it/s]Evaluating:  68%|██████▊   | 213/313 [01:03<00:30,  3.27it/s]Evaluating:  68%|██████▊   | 214/313 [01:04<00:29,  3.38it/s]Evaluating:  69%|██████▊   | 215/313 [01:04<00:29,  3.34it/s]Evaluating:  69%|██████▉   | 216/313 [01:04<00:29,  3.30it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  69%|██████▉   | 217/313 [01:05<00:28,  3.39it/s]10/10/2021 15:32:27 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:32:27 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:32:27 - INFO - __main__ -   Seed = 42
10/10/2021 15:32:27 - INFO - root -   save model
10/10/2021 15:32:27 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:32:27 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  70%|██████▉   | 218/313 [01:05<00:28,  3.33it/s]Evaluating:  70%|██████▉   | 219/313 [01:05<00:28,  3.29it/s]Evaluating:  70%|███████   | 220/313 [01:05<00:28,  3.27it/s]Evaluating:  71%|███████   | 221/313 [01:06<00:27,  3.38it/s]Evaluating:  71%|███████   | 222/313 [01:06<00:27,  3.33it/s]Evaluating:  71%|███████   | 223/313 [01:06<00:27,  3.31it/s]Evaluating:  72%|███████▏  | 224/313 [01:07<00:27,  3.28it/s]Evaluating:  72%|███████▏  | 225/313 [01:07<00:26,  3.37it/s]Evaluating:  72%|███████▏  | 226/313 [01:07<00:26,  3.33it/s]Evaluating:  73%|███████▎  | 227/313 [01:08<00:26,  3.31it/s]Evaluating:  73%|███████▎  | 228/313 [01:08<00:24,  3.41it/s]Evaluating:  73%|███████▎  | 229/313 [01:08<00:25,  3.34it/s]Evaluating:  73%|███████▎  | 230/313 [01:08<00:25,  3.30it/s]Evaluating:  74%|███████▍  | 231/313 [01:09<00:25,  3.27it/s]Evaluating:  74%|███████▍  | 232/313 [01:09<00:24,  3.34it/s]Evaluating:  74%|███████▍  | 233/313 [01:09<00:24,  3.28it/s]Evaluating:  75%|███████▍  | 234/313 [01:10<00:24,  3.25it/s]Evaluating:  75%|███████▌  | 235/313 [01:10<00:23,  3.30it/s]Evaluating:  75%|███████▌  | 236/313 [01:10<00:22,  3.35it/s]Evaluating:  76%|███████▌  | 237/313 [01:11<00:22,  3.33it/s]Evaluating:  76%|███████▌  | 238/313 [01:11<00:22,  3.30it/s]Evaluating:  76%|███████▋  | 239/313 [01:11<00:21,  3.37it/s]Evaluating:  77%|███████▋  | 240/313 [01:11<00:21,  3.32it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  77%|███████▋  | 241/313 [01:12<00:21,  3.31it/s]Evaluating:  77%|███████▋  | 242/313 [01:12<00:21,  3.30it/s]Evaluating:  78%|███████▊  | 243/313 [01:12<00:20,  3.40it/s]Evaluating:  78%|███████▊  | 244/313 [01:13<00:20,  3.35it/s]Evaluating:  78%|███████▊  | 245/313 [01:13<00:20,  3.31it/s]Evaluating:  79%|███████▊  | 246/313 [01:13<00:20,  3.34it/s]Evaluating:  79%|███████▉  | 247/313 [01:14<00:19,  3.35it/s]Evaluating:  79%|███████▉  | 248/313 [01:14<00:19,  3.30it/s]Evaluating:  80%|███████▉  | 249/313 [01:14<00:19,  3.25it/s]Evaluating:  80%|███████▉  | 250/313 [01:14<00:18,  3.34it/s]Evaluating:  80%|████████  | 251/313 [01:15<00:18,  3.31it/s]Evaluating:  81%|████████  | 252/313 [01:15<00:18,  3.29it/s]Evaluating:  81%|████████  | 253/313 [01:15<00:18,  3.27it/s]Evaluating:  81%|████████  | 254/313 [01:16<00:17,  3.37it/s]Evaluating:  81%|████████▏ | 255/313 [01:16<00:17,  3.33it/s]Evaluating:  82%|████████▏ | 256/313 [01:16<00:17,  3.29it/s]Evaluating:  82%|████████▏ | 257/313 [01:17<00:16,  3.35it/s]Evaluating:  82%|████████▏ | 258/313 [01:17<00:16,  3.32it/s]Evaluating:  83%|████████▎ | 259/313 [01:17<00:16,  3.30it/s]Evaluating:  83%|████████▎ | 260/313 [01:18<00:16,  3.28it/s]Evaluating:  83%|████████▎ | 261/313 [01:18<00:15,  3.38it/s]Evaluating:  84%|████████▎ | 262/313 [01:18<00:15,  3.33it/s]Evaluating:  84%|████████▍ | 263/313 [01:18<00:15,  3.29it/s]Evaluating:  84%|████████▍ | 264/313 [01:19<00:14,  3.28it/s]Evaluating:  85%|████████▍ | 265/313 [01:19<00:14,  3.38it/s]Evaluating:  85%|████████▍ | 266/313 [01:19<00:14,  3.35it/s]Evaluating:  85%|████████▌ | 267/313 [01:20<00:13,  3.32it/s]Evaluating:  86%|████████▌ | 268/313 [01:20<00:13,  3.40it/s]Evaluating:  86%|████████▌ | 269/313 [01:20<00:13,  3.33it/s]Evaluating:  86%|████████▋ | 270/313 [01:21<00:13,  3.26it/s]Evaluating:  87%|████████▋ | 271/313 [01:21<00:13,  3.23it/s]Evaluating:  87%|████████▋ | 272/313 [01:21<00:12,  3.33it/s]Evaluating:  87%|████████▋ | 273/313 [01:21<00:12,  3.30it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  88%|████████▊ | 274/313 [01:22<00:11,  3.28it/s]Evaluating:  88%|████████▊ | 275/313 [01:22<00:11,  3.31it/s]Evaluating:  88%|████████▊ | 276/313 [01:22<00:11,  3.29it/s]Evaluating:  88%|████████▊ | 277/313 [01:23<00:10,  3.28it/s]Evaluating:  89%|████████▉ | 278/313 [01:23<00:10,  3.26it/s]Evaluating:  89%|████████▉ | 279/313 [01:23<00:10,  3.37it/s]Evaluating:  89%|████████▉ | 280/313 [01:24<00:09,  3.33it/s]Evaluating:  90%|████████▉ | 281/313 [01:24<00:09,  3.29it/s]Evaluating:  90%|█████████ | 282/313 [01:24<00:09,  3.27it/s]Evaluating:  90%|█████████ | 283/313 [01:24<00:08,  3.37it/s]Evaluating:  91%|█████████ | 284/313 [01:25<00:08,  3.32it/s]Evaluating:  91%|█████████ | 285/313 [01:25<00:08,  3.29it/s]Evaluating:  91%|█████████▏| 286/313 [01:25<00:08,  3.37it/s]Evaluating:  92%|█████████▏| 287/313 [01:26<00:07,  3.34it/s]Evaluating:  92%|█████████▏| 288/313 [01:26<00:07,  3.13it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  92%|█████████▏| 289/313 [01:26<00:07,  3.22it/s]Evaluating:  93%|█████████▎| 290/313 [01:27<00:06,  3.29it/s]Evaluating:  93%|█████████▎| 291/313 [01:27<00:06,  3.34it/s]Evaluating:  93%|█████████▎| 292/313 [01:27<00:06,  3.38it/s]Evaluating:  94%|█████████▎| 293/313 [01:27<00:05,  3.48it/s]Evaluating:  94%|█████████▍| 294/313 [01:28<00:05,  3.46it/s]Evaluating:  94%|█████████▍| 295/313 [01:28<00:05,  3.44it/s]Evaluating:  95%|█████████▍| 296/313 [01:28<00:04,  3.41it/s]Evaluating:  95%|█████████▍| 297/313 [01:29<00:04,  3.52it/s]Evaluating:  95%|█████████▌| 298/313 [01:29<00:04,  3.43it/s]Evaluating:  96%|█████████▌| 299/313 [01:29<00:04,  3.36it/s]Evaluating:  96%|█████████▌| 300/313 [01:29<00:03,  3.36it/s]Evaluating:  96%|█████████▌| 301/313 [01:30<00:03,  3.31it/s]Evaluating:  96%|█████████▋| 302/313 [01:30<00:03,  3.26it/s]Evaluating:  97%|█████████▋| 303/313 [01:30<00:03,  3.24it/s]Evaluating:  97%|█████████▋| 304/313 [01:31<00:02,  3.36it/s]Evaluating:  97%|█████████▋| 305/313 [01:31<00:02,  3.29it/s]Evaluating:  98%|█████████▊| 306/313 [01:31<00:02,  3.24it/s]Evaluating:  98%|█████████▊| 307/313 [01:32<00:01,  3.28it/s]Evaluating:  98%|█████████▊| 308/313 [01:32<00:01,  3.25it/s]Evaluating:  99%|█████████▊| 309/313 [01:32<00:01,  3.21it/s]Evaluating:  99%|█████████▉| 310/313 [01:33<00:00,  3.18it/s]Evaluating:  99%|█████████▉| 311/313 [01:33<00:00,  3.28it/s]Evaluating: 100%|█████████▉| 312/313 [01:33<00:00,  3.25it/s]Evaluating: 100%|██████████| 313/313 [01:33<00:00,  3.58it/s]Evaluating: 100%|██████████| 313/313 [01:33<00:00,  3.33it/s]
10/10/2021 15:32:57 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/10/2021 15:32:57 - INFO - __main__ -     f1 = 0.6253924474047123
10/10/2021 15:32:57 - INFO - __main__ -     loss = 1.1883288274367396
10/10/2021 15:32:57 - INFO - __main__ -     precision = 0.5323176823176823
10/10/2021 15:32:57 - INFO - __main__ -     recall = 0.7579119550529835
10/10/2021 15:32:57 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:33:08 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:33:08 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:33:08 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/10/2021 15:33:08 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:33:08 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:33:08 - INFO - root -   loading lang adpater is/wiki@ukp
10/10/2021 15:33:08 - INFO - __main__ -   Language = is
10/10/2021 15:33:08 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/10/2021 15:33:14 - INFO - __main__ -   Language adapter for fo not found, using is instead
10/10/2021 15:33:14 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:33:14 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/10/2021 15:33:14 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/10/2021 15:33:14 - INFO - __main__ -     Num examples = 100
10/10/2021 15:33:14 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  3.60it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  3.76it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  3.68it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.52it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.18it/s]
10/10/2021 15:33:15 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/10/2021 15:33:15 - INFO - __main__ -     f1 = 0.5907473309608541
10/10/2021 15:33:15 - INFO - __main__ -     loss = 1.6051550805568695
10/10/2021 15:33:15 - INFO - __main__ -     precision = 0.515527950310559
10/10/2021 15:33:15 - INFO - __main__ -     recall = 0.6916666666666667
10/10/2021 15:33:15 - INFO - __main__ -   Language adapter for no not found, using is instead
10/10/2021 15:33:15 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:33:15 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/10/2021 15:33:16 - INFO - __main__ -   ***** Running evaluation  in no *****
10/10/2021 15:33:16 - INFO - __main__ -     Num examples = 10000
10/10/2021 15:33:16 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:34,  3.31it/s]Evaluating:   1%|          | 2/313 [00:00<01:33,  3.31it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:   1%|          | 3/313 [00:00<01:29,  3.45it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:30,  3.43it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:31,  3.38it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:31,  3.35it/s]Evaluating:   2%|▏         | 7/313 [00:02<01:28,  3.45it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:29,  3.43it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:29,  3.39it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:30,  3.36it/s]Evaluating:   4%|▎         | 11/313 [00:03<01:27,  3.47it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:27,  3.42it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:28,  3.39it/s]Evaluating:   4%|▍         | 14/313 [00:04<01:28,  3.37it/s]Evaluating:   5%|▍         | 15/313 [00:04<01:26,  3.45it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:27,  3.41it/s]Evaluating:   5%|▌         | 17/313 [00:05<01:27,  3.38it/s]Evaluating:   6%|▌         | 18/313 [00:05<01:27,  3.36it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:24,  3.46it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:   6%|▋         | 20/313 [00:05<01:26,  3.41it/s]Evaluating:   7%|▋         | 21/313 [00:06<01:26,  3.37it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:26,  3.37it/s]Evaluating:   7%|▋         | 23/313 [00:06<01:26,  3.35it/s]Evaluating:   8%|▊         | 24/313 [00:07<01:26,  3.35it/s]Evaluating:   8%|▊         | 25/313 [00:07<01:26,  3.34it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:23,  3.43it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:24,  3.39it/s]Evaluating:   9%|▉         | 28/313 [00:08<01:25,  3.35it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   9%|▉         | 29/313 [00:08<01:25,  3.33it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:22,  3.43it/s]Evaluating:  10%|▉         | 31/313 [00:09<01:23,  3.38it/s]Evaluating:  10%|█         | 32/313 [00:09<01:23,  3.36it/s]Evaluating:  11%|█         | 33/313 [00:09<01:23,  3.33it/s]Evaluating:  11%|█         | 34/313 [00:10<01:21,  3.41it/s]Evaluating:  11%|█         | 35/313 [00:10<01:22,  3.37it/s]Evaluating:  12%|█▏        | 36/313 [00:10<01:22,  3.34it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:21,  3.39it/s]Evaluating:  12%|█▏        | 38/313 [00:11<01:21,  3.38it/s]Evaluating:  12%|█▏        | 39/313 [00:11<01:22,  3.34it/s]Evaluating:  13%|█▎        | 40/313 [00:11<01:22,  3.32it/s]Evaluating:  13%|█▎        | 41/313 [00:12<01:19,  3.41it/s]Evaluating:  13%|█▎        | 42/313 [00:12<01:20,  3.36it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:33:29 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:33:29 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  14%|█▎        | 43/313 [00:12<01:20,  3.34it/s]Evaluating:  14%|█▍        | 44/313 [00:13<01:21,  3.32it/s]Evaluating:  14%|█▍        | 45/313 [00:13<01:18,  3.42it/s]Evaluating:  15%|█▍        | 46/313 [00:13<01:19,  3.37it/s]Evaluating:  15%|█▌        | 47/313 [00:13<01:19,  3.33it/s]Evaluating:  15%|█▌        | 48/313 [00:14<01:18,  3.36it/s]Evaluating:  16%|█▌        | 49/313 [00:14<01:17,  3.39it/s]Evaluating:  16%|█▌        | 50/313 [00:14<01:18,  3.35it/s]Evaluating:  16%|█▋        | 51/313 [00:15<01:18,  3.32it/s]Evaluating:  17%|█▋        | 52/313 [00:15<01:16,  3.42it/s]Evaluating:  17%|█▋        | 53/313 [00:15<01:16,  3.39it/s]Evaluating:  17%|█▋        | 54/313 [00:15<01:16,  3.37it/s]Evaluating:  18%|█▊        | 55/313 [00:16<01:17,  3.35it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  18%|█▊        | 56/313 [00:16<01:14,  3.44it/s]Evaluating:  18%|█▊        | 57/313 [00:16<01:15,  3.41it/s]Evaluating:  19%|█▊        | 58/313 [00:17<01:15,  3.38it/s]Evaluating:  19%|█▉        | 59/313 [00:17<01:15,  3.36it/s]Evaluating:  19%|█▉        | 60/313 [00:17<01:13,  3.44it/s]Evaluating:  19%|█▉        | 61/313 [00:18<01:14,  3.38it/s]Evaluating:  20%|█▉        | 62/313 [00:18<01:15,  3.32it/s]Evaluating:  20%|██        | 63/313 [00:18<01:14,  3.35it/s]Evaluating:  20%|██        | 64/313 [00:18<01:13,  3.38it/s]Evaluating:  21%|██        | 65/313 [00:19<01:14,  3.34it/s]Evaluating:  21%|██        | 66/313 [00:19<01:14,  3.32it/s]Evaluating:  21%|██▏       | 67/313 [00:19<01:12,  3.42it/s]Evaluating:  22%|██▏       | 68/313 [00:20<01:12,  3.36it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  22%|██▏       | 69/313 [00:20<01:13,  3.32it/s]Evaluating:  22%|██▏       | 70/313 [00:20<01:13,  3.30it/s]Evaluating:  23%|██▎       | 71/313 [00:21<01:11,  3.40it/s]Evaluating:  23%|██▎       | 72/313 [00:21<01:11,  3.36it/s]Evaluating:  23%|██▎       | 73/313 [00:21<01:11,  3.34it/s]Evaluating:  24%|██▎       | 74/313 [00:22<01:18,  3.04it/s]Evaluating:  24%|██▍       | 75/313 [00:22<01:16,  3.11it/s]Evaluating:  24%|██▍       | 76/313 [00:22<01:15,  3.16it/s]Evaluating:  25%|██▍       | 77/313 [00:22<01:12,  3.25it/s]Evaluating:  25%|██▍       | 78/313 [00:23<01:10,  3.32it/s]Evaluating:  25%|██▌       | 79/313 [00:23<01:10,  3.31it/s]Evaluating:  26%|██▌       | 80/313 [00:23<01:10,  3.30it/s]Evaluating:  26%|██▌       | 81/313 [00:24<01:08,  3.36it/s]Evaluating:  26%|██▌       | 82/313 [00:24<01:08,  3.36it/s]Evaluating:  27%|██▋       | 83/313 [00:24<01:09,  3.33it/s]Evaluating:  27%|██▋       | 84/313 [00:25<01:08,  3.32it/s]Evaluating:  27%|██▋       | 85/313 [00:25<01:06,  3.42it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  27%|██▋       | 86/313 [00:25<01:07,  3.37it/s]Evaluating:  28%|██▊       | 87/313 [00:25<01:07,  3.34it/s]Evaluating:  28%|██▊       | 88/313 [00:26<01:07,  3.35it/s]Evaluating:  28%|██▊       | 89/313 [00:26<01:06,  3.39it/s]Evaluating:  29%|██▉       | 90/313 [00:26<01:06,  3.34it/s]Evaluating:  29%|██▉       | 91/313 [00:27<01:06,  3.32it/s]Evaluating:  29%|██▉       | 92/313 [00:27<01:05,  3.36it/s]Evaluating:  30%|██▉       | 93/313 [00:27<01:04,  3.40it/s]Evaluating:  30%|███       | 94/313 [00:27<01:05,  3.36it/s]Evaluating:  30%|███       | 95/313 [00:28<01:05,  3.34it/s]Evaluating:  31%|███       | 96/313 [00:28<01:03,  3.40it/s]Evaluating:  31%|███       | 97/313 [00:28<01:03,  3.38it/s]Evaluating:  31%|███▏      | 98/313 [00:29<01:04,  3.34it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  32%|███▏      | 99/313 [00:29<01:04,  3.32it/s]Evaluating:  32%|███▏      | 100/313 [00:29<01:02,  3.40it/s]Evaluating:  32%|███▏      | 101/313 [00:30<01:02,  3.37it/s]Evaluating:  33%|███▎      | 102/313 [00:30<01:02,  3.37it/s]Evaluating:  33%|███▎      | 103/313 [00:30<01:01,  3.40it/s]Evaluating:  33%|███▎      | 104/313 [00:30<01:00,  3.43it/s]Evaluating:  34%|███▎      | 105/313 [00:31<01:01,  3.40it/s]Evaluating:  34%|███▍      | 106/313 [00:31<01:01,  3.37it/s]Evaluating:  34%|███▍      | 107/313 [00:31<00:59,  3.44it/s]Evaluating:  35%|███▍      | 108/313 [00:32<00:59,  3.44it/s]Evaluating:  35%|███▍      | 109/313 [00:32<01:00,  3.38it/s]Evaluating:  35%|███▌      | 110/313 [00:32<01:00,  3.35it/s]Evaluating:  35%|███▌      | 111/313 [00:33<00:59,  3.39it/s]Evaluating:  36%|███▌      | 112/313 [00:33<00:59,  3.36it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:33:50 - INFO - __main__ -   Using lang2id = None
Evaluating:  36%|███▌      | 113/313 [00:33<00:59,  3.33it/s]Evaluating:  36%|███▋      | 114/313 [00:33<01:00,  3.31it/s]Evaluating:  37%|███▋      | 115/313 [00:34<00:58,  3.38it/s]Evaluating:  37%|███▋      | 116/313 [00:34<00:59,  3.33it/s]Evaluating:  37%|███▋      | 117/313 [00:34<00:59,  3.31it/s]Evaluating:  38%|███▊      | 118/313 [00:35<00:58,  3.34it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  38%|███▊      | 119/313 [00:35<00:57,  3.36it/s]10/10/2021 15:33:52 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:33:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:33:52 - INFO - __main__ -   Seed = 42
10/10/2021 15:33:52 - INFO - root -   save model
10/10/2021 15:33:52 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:33:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  38%|███▊      | 120/313 [00:35<00:57,  3.33it/s]Evaluating:  39%|███▊      | 121/313 [00:36<00:58,  3.31it/s]Evaluating:  39%|███▉      | 122/313 [00:36<00:56,  3.37it/s]Evaluating:  39%|███▉      | 123/313 [00:36<00:56,  3.35it/s]Evaluating:  40%|███▉      | 124/313 [00:36<00:56,  3.33it/s]Evaluating:  40%|███▉      | 125/313 [00:37<00:56,  3.31it/s]Evaluating:  40%|████      | 126/313 [00:37<00:54,  3.41it/s]Evaluating:  41%|████      | 127/313 [00:37<00:55,  3.36it/s]Evaluating:  41%|████      | 128/313 [00:38<00:55,  3.34it/s]Evaluating:  41%|████      | 129/313 [00:38<00:54,  3.38it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  42%|████▏     | 130/313 [00:38<00:53,  3.40it/s]Evaluating:  42%|████▏     | 131/313 [00:38<00:54,  3.36it/s]Evaluating:  42%|████▏     | 132/313 [00:39<00:54,  3.32it/s]Evaluating:  42%|████▏     | 133/313 [00:39<00:53,  3.35it/s]Evaluating:  43%|████▎     | 134/313 [00:39<00:52,  3.39it/s]Evaluating:  43%|████▎     | 135/313 [00:40<00:53,  3.34it/s]Evaluating:  43%|████▎     | 136/313 [00:40<00:53,  3.31it/s]Evaluating:  44%|████▍     | 137/313 [00:40<00:51,  3.39it/s]Evaluating:  44%|████▍     | 138/313 [00:41<00:52,  3.36it/s]Evaluating:  44%|████▍     | 139/313 [00:41<00:52,  3.33it/s]Evaluating:  45%|████▍     | 140/313 [00:41<00:51,  3.36it/s]Evaluating:  45%|████▌     | 141/313 [00:41<00:50,  3.38it/s]Evaluating:  45%|████▌     | 142/313 [00:42<00:51,  3.35it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  46%|████▌     | 143/313 [00:42<00:51,  3.32it/s]Evaluating:  46%|████▌     | 144/313 [00:42<00:50,  3.37it/s]Evaluating:  46%|████▋     | 145/313 [00:43<00:49,  3.39it/s]Evaluating:  47%|████▋     | 146/313 [00:43<00:49,  3.35it/s]Evaluating:  47%|████▋     | 147/313 [00:43<00:49,  3.33it/s]Evaluating:  47%|████▋     | 148/313 [00:44<00:49,  3.36it/s]Evaluating:  48%|████▊     | 149/313 [00:44<00:48,  3.38it/s]Evaluating:  48%|████▊     | 150/313 [00:44<00:48,  3.35it/s]Evaluating:  48%|████▊     | 151/313 [00:44<00:48,  3.37it/s]Evaluating:  49%|████▊     | 152/313 [00:45<00:47,  3.38it/s]Evaluating:  49%|████▉     | 153/313 [00:45<00:47,  3.34it/s]Evaluating:  49%|████▉     | 154/313 [00:45<00:47,  3.32it/s]Evaluating:  50%|████▉     | 155/313 [00:46<00:47,  3.35it/s]Evaluating:  50%|████▉     | 156/313 [00:46<00:46,  3.38it/s]Evaluating:  50%|█████     | 157/313 [00:46<00:46,  3.34it/s]Evaluating:  50%|█████     | 158/313 [00:47<00:46,  3.34it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  51%|█████     | 159/313 [00:47<00:45,  3.36it/s]Evaluating:  51%|█████     | 160/313 [00:47<00:45,  3.38it/s]Evaluating:  51%|█████▏    | 161/313 [00:47<00:45,  3.35it/s]Evaluating:  52%|█████▏    | 162/313 [00:48<00:44,  3.38it/s]Evaluating:  52%|█████▏    | 163/313 [00:48<00:44,  3.34it/s]Evaluating:  52%|█████▏    | 164/313 [00:48<00:44,  3.37it/s]Evaluating:  53%|█████▎    | 165/313 [00:49<00:44,  3.34it/s]Evaluating:  53%|█████▎    | 166/313 [00:49<00:43,  3.36it/s]Evaluating:  53%|█████▎    | 167/313 [00:49<00:43,  3.39it/s]Evaluating:  54%|█████▎    | 168/313 [00:50<00:43,  3.36it/s]Evaluating:  54%|█████▍    | 169/313 [00:50<00:43,  3.34it/s]Evaluating:  54%|█████▍    | 170/313 [00:50<00:42,  3.36it/s]Evaluating:  55%|█████▍    | 171/313 [00:50<00:41,  3.39it/s]Evaluating:  55%|█████▍    | 172/313 [00:51<00:42,  3.34it/s]Evaluating:  55%|█████▌    | 173/313 [00:51<00:41,  3.37it/s]Evaluating:  56%|█████▌    | 174/313 [00:51<00:41,  3.34it/s]Evaluating:  56%|█████▌    | 175/313 [00:52<00:41,  3.36it/s]Evaluating:  56%|█████▌    | 176/313 [00:52<00:41,  3.34it/s]Evaluating:  57%|█████▋    | 177/313 [00:52<00:40,  3.36it/s]Evaluating:  57%|█████▋    | 178/313 [00:52<00:40,  3.35it/s]Evaluating:  57%|█████▋    | 179/313 [00:53<00:40,  3.34it/s]Evaluating:  58%|█████▊    | 180/313 [00:53<00:40,  3.31it/s]Evaluating:  58%|█████▊    | 181/313 [00:53<00:39,  3.34it/s]Evaluating:  58%|█████▊    | 182/313 [00:54<00:38,  3.37it/s]Evaluating:  58%|█████▊    | 183/313 [00:54<00:38,  3.35it/s]Evaluating:  59%|█████▉    | 184/313 [00:54<00:38,  3.36it/s]Evaluating:  59%|█████▉    | 185/313 [00:55<00:38,  3.32it/s]Evaluating:  59%|█████▉    | 186/313 [00:55<00:37,  3.36it/s]Evaluating:  60%|█████▉    | 187/313 [00:55<00:37,  3.33it/s]Evaluating:  60%|██████    | 188/313 [00:55<00:37,  3.36it/s]Evaluating:  60%|██████    | 189/313 [00:56<00:37,  3.34it/s]Evaluating:  61%|██████    | 190/313 [00:56<00:36,  3.33it/s]Evaluating:  61%|██████    | 191/313 [00:56<00:36,  3.30it/s]Evaluating:  61%|██████▏   | 192/313 [00:57<00:36,  3.33it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  62%|██████▏   | 193/313 [00:57<00:35,  3.36it/s]Evaluating:  62%|██████▏   | 194/313 [00:57<00:35,  3.33it/s]Evaluating:  62%|██████▏   | 195/313 [00:58<00:35,  3.36it/s]Evaluating:  63%|██████▎   | 196/313 [00:58<00:35,  3.32it/s]Evaluating:  63%|██████▎   | 197/313 [00:58<00:34,  3.36it/s]Evaluating:  63%|██████▎   | 198/313 [00:58<00:34,  3.33it/s]Evaluating:  64%|██████▎   | 199/313 [00:59<00:34,  3.35it/s]Evaluating:  64%|██████▍   | 200/313 [00:59<00:33,  3.36it/s]Evaluating:  64%|██████▍   | 201/313 [00:59<00:33,  3.33it/s]Evaluating:  65%|██████▍   | 202/313 [01:00<00:33,  3.30it/s]Evaluating:  65%|██████▍   | 203/313 [01:00<00:33,  3.33it/s]Evaluating:  65%|██████▌   | 204/313 [01:00<00:32,  3.37it/s]Evaluating:  65%|██████▌   | 205/313 [01:01<00:32,  3.33it/s]Evaluating:  66%|██████▌   | 206/313 [01:01<00:31,  3.36it/s]Evaluating:  66%|██████▌   | 207/313 [01:01<00:31,  3.32it/s]Evaluating:  66%|██████▋   | 208/313 [01:01<00:31,  3.30it/s]Evaluating:  67%|██████▋   | 209/313 [01:02<00:31,  3.34it/s]Evaluating:  67%|██████▋   | 210/313 [01:02<00:30,  3.41it/s]Evaluating:  67%|██████▋   | 211/313 [01:02<00:29,  3.43it/s]Evaluating:  68%|██████▊   | 212/313 [01:03<00:30,  3.37it/s]Evaluating:  68%|██████▊   | 213/313 [01:03<00:30,  3.33it/s]Evaluating:  68%|██████▊   | 214/313 [01:03<00:29,  3.35it/s]Evaluating:  69%|██████▊   | 215/313 [01:04<00:29,  3.38it/s]Evaluating:  69%|██████▉   | 216/313 [01:04<00:29,  3.34it/s]Evaluating:  69%|██████▉   | 217/313 [01:04<00:28,  3.35it/s]Evaluating:  70%|██████▉   | 218/313 [01:04<00:28,  3.32it/s]Evaluating:  70%|██████▉   | 219/313 [01:05<00:27,  3.36it/s]Evaluating:  70%|███████   | 220/313 [01:05<00:28,  3.31it/s]Evaluating:  71%|███████   | 221/313 [01:05<00:27,  3.34it/s]Evaluating:  71%|███████   | 222/313 [01:06<00:26,  3.38it/s]Evaluating:  71%|███████   | 223/313 [01:06<00:27,  3.32it/s]Evaluating:  72%|███████▏  | 224/313 [01:06<00:26,  3.30it/s]Evaluating:  72%|███████▏  | 225/313 [01:07<00:26,  3.30it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  72%|███████▏  | 226/313 [01:07<00:26,  3.33it/s]10/10/2021 15:34:24 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:34:24 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:34:24 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/10/2021 15:34:24 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:34:24 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:34:24 - INFO - root -   loading lang adpater ru/wiki@ukp
10/10/2021 15:34:24 - INFO - __main__ -   Language = ru
10/10/2021 15:34:24 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:  73%|███████▎  | 227/313 [01:07<00:26,  3.29it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Evaluating:  73%|███████▎  | 228/313 [01:07<00:25,  3.33it/s]Evaluating:  73%|███████▎  | 229/313 [01:08<00:25,  3.29it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Evaluating:  73%|███████▎  | 230/313 [01:08<00:24,  3.34it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
Evaluating:  74%|███████▍  | 231/313 [01:08<00:24,  3.31it/s]Evaluating:  74%|███████▍  | 232/313 [01:09<00:24,  3.33it/s]Evaluating:  74%|███████▍  | 233/313 [01:09<00:23,  3.35it/s]Evaluating:  75%|███████▍  | 234/313 [01:09<00:23,  3.30it/s]Evaluating:  75%|███████▌  | 235/313 [01:10<00:23,  3.33it/s]Evaluating:  75%|███████▌  | 236/313 [01:10<00:23,  3.28it/s]Evaluating:  76%|███████▌  | 237/313 [01:10<00:22,  3.36it/s]Evaluating:  76%|███████▌  | 238/313 [01:10<00:22,  3.31it/s]Evaluating:  76%|███████▋  | 239/313 [01:11<00:22,  3.34it/s]Evaluating:  77%|███████▋  | 240/313 [01:11<00:22,  3.30it/s]Evaluating:  77%|███████▋  | 241/313 [01:11<00:22,  3.13it/s]Evaluating:  77%|███████▋  | 242/313 [01:12<00:22,  3.20it/s]Evaluating:  78%|███████▊  | 243/313 [01:12<00:23,  2.94it/s]Evaluating:  78%|███████▊  | 244/313 [01:12<00:22,  3.05it/s]Evaluating:  78%|███████▊  | 245/313 [01:13<00:23,  2.94it/s]Evaluating:  79%|███████▊  | 246/313 [01:13<00:22,  3.02it/s]Evaluating:  79%|███████▉  | 247/313 [01:14<00:24,  2.73it/s]Evaluating:  79%|███████▉  | 248/313 [01:14<00:22,  2.86it/s]Evaluating:  80%|███████▉  | 249/313 [01:14<00:22,  2.84it/s]Evaluating:  80%|███████▉  | 250/313 [01:15<00:20,  3.03it/s]Evaluating:  80%|████████  | 251/313 [01:15<00:20,  3.07it/s]Evaluating:  81%|████████  | 252/313 [01:15<00:19,  3.10it/s]10/10/2021 15:34:32 - INFO - __main__ -   Language adapter for be not found, using ru instead
10/10/2021 15:34:32 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:34:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/10/2021 15:34:32 - INFO - __main__ -   ***** Running evaluation  in be *****
10/10/2021 15:34:32 - INFO - __main__ -     Num examples = 1001
10/10/2021 15:34:32 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:09,  3.29it/s]Evaluating:  81%|████████  | 253/313 [01:16<00:27,  2.21it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:11,  2.56it/s]Evaluating:  81%|████████  | 254/313 [01:16<00:27,  2.17it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:11,  2.48it/s]Evaluating:  81%|████████▏ | 255/313 [01:17<00:27,  2.09it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:11,  2.45it/s]Evaluating:  82%|████████▏ | 256/313 [01:17<00:27,  2.08it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:11,  2.35it/s]Evaluating:  82%|████████▏ | 257/313 [01:18<00:27,  2.07it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:11,  2.29it/s]Evaluating:  82%|████████▏ | 258/313 [01:18<00:26,  2.10it/s]Evaluating:  22%|██▏       | 7/32 [00:02<00:10,  2.31it/s]Evaluating:  83%|████████▎ | 259/313 [01:19<00:26,  2.04it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:10,  2.31it/s]Evaluating:  83%|████████▎ | 260/313 [01:19<00:26,  2.04it/s]Evaluating:  28%|██▊       | 9/32 [00:03<00:10,  2.27it/s]Evaluating:  31%|███▏      | 10/32 [00:04<00:09,  2.25it/s]Evaluating:  83%|████████▎ | 261/313 [01:20<00:25,  2.02it/s]Evaluating:  34%|███▍      | 11/32 [00:04<00:09,  2.27it/s]Evaluating:  84%|████████▎ | 262/313 [01:20<00:24,  2.07it/s]Evaluating:  38%|███▊      | 12/32 [00:05<00:08,  2.24it/s]Evaluating:  84%|████████▍ | 263/313 [01:21<00:24,  2.02it/s]Evaluating:  41%|████      | 13/32 [00:05<00:08,  2.25it/s]Evaluating:  84%|████████▍ | 264/313 [01:21<00:24,  2.04it/s]Evaluating:  44%|████▍     | 14/32 [00:06<00:07,  2.26it/s]Evaluating:  85%|████████▍ | 265/313 [01:22<00:25,  1.89it/s]Evaluating:  47%|████▋     | 15/32 [00:06<00:07,  2.40it/s]Evaluating:  85%|████████▍ | 266/313 [01:22<00:24,  1.94it/s]Evaluating:  50%|█████     | 16/32 [00:06<00:06,  2.33it/s]Evaluating:  53%|█████▎    | 17/32 [00:07<00:06,  2.27it/s]Evaluating:  85%|████████▌ | 267/313 [01:23<00:23,  1.95it/s]Evaluating:  56%|█████▋    | 18/32 [00:07<00:06,  2.31it/s]Evaluating:  86%|████████▌ | 268/313 [01:23<00:22,  2.02it/s]Evaluating:  59%|█████▉    | 19/32 [00:08<00:05,  2.27it/s]Evaluating:  86%|████████▌ | 269/313 [01:24<00:23,  1.83it/s]Evaluating:  62%|██████▎   | 20/32 [00:08<00:04,  2.40it/s]Evaluating:  86%|████████▋ | 270/313 [01:25<00:22,  1.90it/s]Evaluating:  66%|██████▌   | 21/32 [00:09<00:04,  2.33it/s]Evaluating:  87%|████████▋ | 271/313 [01:25<00:21,  1.95it/s]Evaluating:  69%|██████▉   | 22/32 [00:09<00:04,  2.35it/s]Evaluating:  72%|███████▏  | 23/32 [00:09<00:03,  2.28it/s]Evaluating:  87%|████████▋ | 272/313 [01:26<00:20,  1.96it/s]Evaluating:  75%|███████▌  | 24/32 [00:10<00:03,  2.26it/s]Evaluating:  87%|████████▋ | 273/313 [01:26<00:20,  1.95it/s]Evaluating:  78%|███████▊  | 25/32 [00:10<00:03,  2.29it/s]Evaluating:  88%|████████▊ | 274/313 [01:26<00:19,  2.02it/s]Evaluating:  81%|████████▏ | 26/32 [00:11<00:02,  2.30it/s]Evaluating:  84%|████████▍ | 27/32 [00:11<00:02,  2.42it/s]Evaluating:  88%|████████▊ | 275/313 [01:27<00:20,  1.83it/s]Evaluating:  88%|████████▊ | 28/32 [00:12<00:01,  2.32it/s]Evaluating:  88%|████████▊ | 276/313 [01:28<00:19,  1.87it/s]Evaluating:  91%|█████████ | 29/32 [00:12<00:01,  2.34it/s]Evaluating:  88%|████████▊ | 277/313 [01:28<00:18,  1.92it/s]Evaluating:  94%|█████████▍| 30/32 [00:12<00:00,  2.34it/s]Evaluating:  89%|████████▉ | 278/313 [01:29<00:17,  1.95it/s]Evaluating:  97%|█████████▋| 31/32 [00:13<00:00,  2.29it/s]Evaluating: 100%|██████████| 32/32 [00:13<00:00,  2.63it/s]Evaluating: 100%|██████████| 32/32 [00:13<00:00,  2.35it/s]Evaluating:  89%|████████▉ | 279/313 [01:29<00:18,  1.84it/s]
10/10/2021 15:34:46 - INFO - __main__ -   ***** Evaluation result  in be *****
10/10/2021 15:34:46 - INFO - __main__ -     f1 = 0.6167938931297711
10/10/2021 15:34:46 - INFO - __main__ -     loss = 1.1054657083004713
10/10/2021 15:34:46 - INFO - __main__ -     precision = 0.5759087669280114
10/10/2021 15:34:46 - INFO - __main__ -     recall = 0.6639276910435498
10/10/2021 15:34:46 - INFO - __main__ -   Language adapter for uk not found, using ru instead
10/10/2021 15:34:46 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:34:46 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
Evaluating:  89%|████████▉ | 280/313 [01:30<00:15,  2.12it/s]Evaluating:  90%|████████▉ | 281/313 [01:30<00:13,  2.32it/s]Evaluating:  90%|█████████ | 282/313 [01:30<00:12,  2.57it/s]Evaluating:  90%|█████████ | 283/313 [01:31<00:11,  2.70it/s]10/10/2021 15:34:47 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/10/2021 15:34:47 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:34:47 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  91%|█████████ | 284/313 [01:31<00:10,  2.69it/s]Evaluating:   0%|          | 1/313 [00:00<02:07,  2.44it/s]Evaluating:   1%|          | 2/313 [00:00<01:58,  2.63it/s]Evaluating:  91%|█████████ | 285/313 [01:31<00:12,  2.26it/s]Evaluating:   1%|          | 3/313 [00:01<02:00,  2.57it/s]Evaluating:  91%|█████████▏| 286/313 [01:32<00:12,  2.21it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:06,  2.43it/s]Evaluating:  92%|█████████▏| 287/313 [01:32<00:12,  2.10it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:09,  2.38it/s]Evaluating:  92%|█████████▏| 288/313 [01:33<00:11,  2.11it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:11,  2.34it/s]Evaluating:   2%|▏         | 7/313 [00:02<02:06,  2.42it/s]Evaluating:  92%|█████████▏| 289/313 [01:34<00:12,  1.89it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:06,  2.41it/s]Evaluating:  93%|█████████▎| 290/313 [01:34<00:11,  1.93it/s]Evaluating:   3%|▎         | 9/313 [00:03<02:10,  2.33it/s]Evaluating:  93%|█████████▎| 291/313 [01:35<00:11,  1.96it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:09,  2.33it/s]Evaluating:  93%|█████████▎| 292/313 [01:35<00:10,  2.00it/s]Evaluating:   4%|▎         | 11/313 [00:04<02:10,  2.32it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:06,  2.37it/s]Evaluating:  94%|█████████▎| 293/313 [01:36<00:11,  1.80it/s]Evaluating:   4%|▍         | 13/313 [00:05<02:04,  2.41it/s]Evaluating:  94%|█████████▍| 294/313 [01:36<00:10,  1.88it/s]Evaluating:   4%|▍         | 14/313 [00:05<02:06,  2.37it/s]Evaluating:  94%|█████████▍| 295/313 [01:37<00:09,  1.89it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:06,  2.36it/s]Evaluating:  95%|█████████▍| 296/313 [01:37<00:08,  1.93it/s]Evaluating:   5%|▌         | 16/313 [00:06<02:08,  2.31it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:05,  2.35it/s]Evaluating:  95%|█████████▍| 297/313 [01:38<00:08,  1.79it/s]Evaluating:   6%|▌         | 18/313 [00:07<02:01,  2.42it/s]Evaluating:  95%|█████████▌| 298/313 [01:38<00:08,  1.87it/s]Evaluating:   6%|▌         | 19/313 [00:07<02:04,  2.35it/s]Evaluating:  96%|█████████▌| 299/313 [01:39<00:07,  1.88it/s]Evaluating:   6%|▋         | 20/313 [00:08<02:05,  2.34it/s]Evaluating:  96%|█████████▌| 300/313 [01:39<00:06,  1.93it/s]Evaluating:   7%|▋         | 21/313 [00:08<02:05,  2.33it/s]Evaluating:   7%|▋         | 22/313 [00:09<02:03,  2.35it/s]Evaluating:  96%|█████████▌| 301/313 [01:40<00:06,  1.77it/s]Evaluating:   7%|▋         | 23/313 [00:09<02:00,  2.41it/s]Evaluating:  96%|█████████▋| 302/313 [01:41<00:05,  1.84it/s]Evaluating:   8%|▊         | 24/313 [00:10<02:02,  2.36it/s]Evaluating:  97%|█████████▋| 303/313 [01:41<00:05,  1.86it/s]Evaluating:   8%|▊         | 25/313 [00:10<02:01,  2.36it/s]Evaluating:  97%|█████████▋| 304/313 [01:42<00:04,  1.98it/s]Evaluating:   8%|▊         | 26/313 [00:11<02:08,  2.23it/s]Evaluating:   9%|▊         | 27/313 [00:11<02:05,  2.28it/s]Evaluating:  97%|█████████▋| 305/313 [01:42<00:04,  1.74it/s]Evaluating:   9%|▉         | 28/313 [00:11<01:57,  2.43it/s]Evaluating:  98%|█████████▊| 306/313 [01:43<00:03,  1.81it/s]Evaluating:   9%|▉         | 29/313 [00:12<01:58,  2.39it/s]Evaluating:  98%|█████████▊| 307/313 [01:43<00:03,  1.88it/s]Evaluating:  10%|▉         | 30/313 [00:12<02:01,  2.34it/s]Evaluating:  98%|█████████▊| 308/313 [01:44<00:02,  1.93it/s]Evaluating:  10%|▉         | 31/313 [00:13<02:03,  2.29it/s]Evaluating:  99%|█████████▊| 309/313 [01:44<00:02,  1.99it/s]Evaluating:  10%|█         | 32/313 [00:13<02:03,  2.27it/s]Evaluating:  99%|█████████▉| 310/313 [01:45<00:01,  2.03it/s]Evaluating:  11%|█         | 33/313 [00:14<02:03,  2.27it/s]Evaluating:  99%|█████████▉| 311/313 [01:45<00:00,  2.04it/s]Evaluating:  11%|█         | 34/313 [00:14<02:05,  2.23it/s]Evaluating:  11%|█         | 35/313 [00:14<02:05,  2.22it/s]Evaluating: 100%|█████████▉| 312/313 [01:46<00:00,  2.04it/s]Evaluating: 100%|██████████| 313/313 [01:46<00:00,  2.28it/s]Evaluating: 100%|██████████| 313/313 [01:46<00:00,  2.94it/s]Evaluating:  12%|█▏        | 36/313 [00:15<01:59,  2.32it/s]Evaluating:  12%|█▏        | 37/313 [00:15<01:46,  2.58it/s]Evaluating:  12%|█▏        | 38/313 [00:15<01:40,  2.74it/s]Evaluating:  12%|█▏        | 39/313 [00:16<01:33,  2.93it/s]Evaluating:  13%|█▎        | 40/313 [00:16<01:30,  3.03it/s]
10/10/2021 15:35:04 - INFO - __main__ -   ***** Evaluation result  in no *****
10/10/2021 15:35:04 - INFO - __main__ -     f1 = 0.7276113307413338
10/10/2021 15:35:04 - INFO - __main__ -     loss = 0.7667463391352767
10/10/2021 15:35:04 - INFO - __main__ -     precision = 0.6871336007405122
10/10/2021 15:35:04 - INFO - __main__ -     recall = 0.7731565060408276
10/10/2021 15:35:04 - INFO - __main__ -   Language adapter for da not found, using is instead
10/10/2021 15:35:04 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:35:04 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
Evaluating:  13%|█▎        | 41/313 [00:16<01:26,  3.13it/s]Evaluating:  13%|█▎        | 42/313 [00:17<01:25,  3.18it/s]Evaluating:  14%|█▎        | 43/313 [00:17<01:22,  3.26it/s]Evaluating:  14%|█▍        | 44/313 [00:17<01:21,  3.32it/s]Evaluating:  14%|█▍        | 45/313 [00:18<01:20,  3.31it/s]10/10/2021 15:35:05 - INFO - __main__ -   ***** Running evaluation  in da *****
10/10/2021 15:35:05 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:35:05 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  15%|█▍        | 46/313 [00:18<01:32,  2.90it/s]Evaluating:   0%|          | 1/313 [00:00<02:20,  2.21it/s]Evaluating:  15%|█▌        | 47/313 [00:18<01:41,  2.63it/s]Evaluating:   1%|          | 2/313 [00:00<02:23,  2.16it/s]Evaluating:  15%|█▌        | 48/313 [00:19<01:46,  2.49it/s]Evaluating:   1%|          | 3/313 [00:01<02:21,  2.19it/s]Evaluating:  16%|█▌        | 49/313 [00:19<01:51,  2.37it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:22,  2.17it/s]Evaluating:  16%|█▌        | 50/313 [00:20<01:53,  2.32it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:21,  2.18it/s]Evaluating:  16%|█▋        | 51/313 [00:20<01:55,  2.26it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:21,  2.17it/s]Evaluating:  17%|█▋        | 52/313 [00:21<01:56,  2.25it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:20,  2.18it/s]Evaluating:  17%|█▋        | 53/313 [00:21<01:57,  2.21it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:20,  2.17it/s]Evaluating:  17%|█▋        | 54/313 [00:22<01:57,  2.21it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:19,  2.18it/s]Evaluating:  18%|█▊        | 55/313 [00:22<01:57,  2.20it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:18,  2.19it/s]Evaluating:  18%|█▊        | 56/313 [00:23<01:57,  2.19it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:18,  2.17it/s]Evaluating:  18%|█▊        | 57/313 [00:23<01:57,  2.18it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:18,  2.18it/s]Evaluating:  19%|█▊        | 58/313 [00:23<01:56,  2.19it/s]Evaluating:   4%|▍         | 13/313 [00:05<02:18,  2.17it/s]Evaluating:  19%|█▉        | 59/313 [00:24<01:55,  2.20it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:16,  2.19it/s]Evaluating:  19%|█▉        | 60/313 [00:24<01:55,  2.18it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:16,  2.18it/s]Evaluating:  19%|█▉        | 61/313 [00:25<01:55,  2.19it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:15,  2.19it/s]Evaluating:  20%|█▉        | 62/313 [00:25<01:55,  2.17it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:16,  2.17it/s]Evaluating:  20%|██        | 63/313 [00:26<01:54,  2.18it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:14,  2.19it/s]Evaluating:  20%|██        | 64/313 [00:26<01:54,  2.17it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:15,  2.17it/s]Evaluating:  21%|██        | 65/313 [00:27<01:53,  2.18it/s]Evaluating:   6%|▋         | 20/313 [00:09<02:14,  2.18it/s]Evaluating:  21%|██        | 66/313 [00:27<01:53,  2.17it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:14,  2.17it/s]Evaluating:  21%|██▏       | 67/313 [00:28<01:52,  2.19it/s]Evaluating:   7%|▋         | 22/313 [00:10<02:13,  2.18it/s]Evaluating:  22%|██▏       | 68/313 [00:28<01:52,  2.17it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:13,  2.17it/s]Evaluating:  22%|██▏       | 69/313 [00:29<01:51,  2.18it/s]Evaluating:   8%|▊         | 24/313 [00:11<02:12,  2.18it/s]Evaluating:  22%|██▏       | 70/313 [00:29<01:50,  2.19it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:11,  2.19it/s]Evaluating:  23%|██▎       | 71/313 [00:29<01:51,  2.18it/s]Evaluating:   8%|▊         | 26/313 [00:11<02:11,  2.17it/s]Evaluating:  23%|██▎       | 72/313 [00:30<01:50,  2.18it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:10,  2.18it/s]Evaluating:  23%|██▎       | 73/313 [00:30<01:50,  2.17it/s]Evaluating:   9%|▉         | 28/313 [00:12<02:11,  2.17it/s]Evaluating:  24%|██▎       | 74/313 [00:31<01:49,  2.19it/s]Evaluating:   9%|▉         | 29/313 [00:13<02:10,  2.18it/s]Evaluating:  24%|██▍       | 75/313 [00:31<01:49,  2.17it/s]Evaluating:  10%|▉         | 30/313 [00:13<02:10,  2.17it/s]Evaluating:  24%|██▍       | 76/313 [00:32<01:48,  2.18it/s]Evaluating:  10%|▉         | 31/313 [00:14<02:09,  2.18it/s]Evaluating:  25%|██▍       | 77/313 [00:32<01:48,  2.17it/s]Evaluating:  10%|█         | 32/313 [00:14<02:09,  2.16it/s]Evaluating:  25%|██▍       | 78/313 [00:33<01:47,  2.18it/s]Evaluating:  11%|█         | 33/313 [00:15<02:08,  2.18it/s]Evaluating:  25%|██▌       | 79/313 [00:33<01:48,  2.16it/s]Evaluating:  11%|█         | 34/313 [00:15<02:08,  2.17it/s]Evaluating:  26%|██▌       | 80/313 [00:34<01:47,  2.17it/s]Evaluating:  11%|█         | 35/313 [00:16<02:07,  2.18it/s]Evaluating:  26%|██▌       | 81/313 [00:34<01:46,  2.17it/s]Evaluating:  12%|█▏        | 36/313 [00:16<02:06,  2.19it/s]Evaluating:  26%|██▌       | 82/313 [00:34<01:46,  2.16it/s]Evaluating:  12%|█▏        | 37/313 [00:16<02:06,  2.18it/s]Evaluating:  27%|██▋       | 83/313 [00:35<01:46,  2.16it/s]Evaluating:  12%|█▏        | 38/313 [00:17<02:06,  2.18it/s]Evaluating:  12%|█▏        | 39/313 [00:17<02:11,  2.08it/s]Evaluating:  27%|██▋       | 84/313 [00:36<01:51,  2.05it/s]Evaluating:  13%|█▎        | 40/313 [00:18<02:09,  2.11it/s]Evaluating:  27%|██▋       | 85/313 [00:36<01:48,  2.10it/s]Evaluating:  13%|█▎        | 41/313 [00:18<02:08,  2.12it/s]Evaluating:  27%|██▋       | 86/313 [00:36<01:47,  2.11it/s]Evaluating:  13%|█▎        | 42/313 [00:19<02:06,  2.14it/s]Evaluating:  28%|██▊       | 87/313 [00:37<01:45,  2.14it/s]Evaluating:  14%|█▎        | 43/313 [00:19<02:04,  2.16it/s]Evaluating:  28%|██▊       | 88/313 [00:37<01:44,  2.16it/s]Evaluating:  14%|█▍        | 44/313 [00:20<02:04,  2.15it/s]Evaluating:  28%|██▊       | 89/313 [00:38<01:44,  2.15it/s]Evaluating:  14%|█▍        | 45/313 [00:20<02:03,  2.17it/s]Evaluating:  29%|██▉       | 90/313 [00:38<01:43,  2.16it/s]Evaluating:  15%|█▍        | 46/313 [00:21<02:03,  2.15it/s]Evaluating:  29%|██▉       | 91/313 [00:39<01:43,  2.15it/s]Evaluating:  15%|█▌        | 47/313 [00:21<02:02,  2.17it/s]Evaluating:  29%|██▉       | 92/313 [00:39<01:41,  2.17it/s]Evaluating:  15%|█▌        | 48/313 [00:22<02:02,  2.16it/s]Evaluating:  30%|██▉       | 93/313 [00:40<01:41,  2.16it/s]Evaluating:  16%|█▌        | 49/313 [00:22<02:01,  2.17it/s]Evaluating:  30%|███       | 94/313 [00:40<01:40,  2.17it/s]Evaluating:  16%|█▌        | 50/313 [00:23<02:01,  2.16it/s]Evaluating:  30%|███       | 95/313 [00:41<01:41,  2.16it/s]Evaluating:  16%|█▋        | 51/313 [00:23<02:00,  2.18it/s]Evaluating:  31%|███       | 96/313 [00:41<01:39,  2.17it/s]Evaluating:  17%|█▋        | 52/313 [00:23<02:00,  2.17it/s]Evaluating:  31%|███       | 97/313 [00:41<01:40,  2.15it/s]Evaluating:  17%|█▋        | 53/313 [00:24<01:59,  2.18it/s]Evaluating:  31%|███▏      | 98/313 [00:42<01:39,  2.17it/s]Evaluating:  17%|█▋        | 54/313 [00:24<01:58,  2.19it/s]Evaluating:  32%|███▏      | 99/313 [00:42<01:38,  2.18it/s]Evaluating:  18%|█▊        | 55/313 [00:25<01:58,  2.18it/s]Evaluating:  32%|███▏      | 100/313 [00:43<01:38,  2.16it/s]Evaluating:  18%|█▊        | 56/313 [00:25<01:57,  2.19it/s]Evaluating:  32%|███▏      | 101/313 [00:43<01:37,  2.18it/s]Evaluating:  18%|█▊        | 57/313 [00:26<01:57,  2.18it/s]Evaluating:  33%|███▎      | 102/313 [00:44<01:37,  2.16it/s]Evaluating:  19%|█▊        | 58/313 [00:26<01:56,  2.19it/s]Evaluating:  33%|███▎      | 103/313 [00:44<01:36,  2.18it/s]Evaluating:  19%|█▉        | 59/313 [00:27<01:56,  2.17it/s]Evaluating:  33%|███▎      | 104/313 [00:45<01:36,  2.16it/s]Evaluating:  19%|█▉        | 60/313 [00:27<01:55,  2.19it/s]Evaluating:  34%|███▎      | 105/313 [00:45<01:36,  2.17it/s]Evaluating:  19%|█▉        | 61/313 [00:28<01:55,  2.17it/s]Evaluating:  34%|███▍      | 106/313 [00:46<01:36,  2.15it/s]Evaluating:  20%|█▉        | 62/313 [00:28<01:54,  2.19it/s]Evaluating:  34%|███▍      | 107/313 [00:46<01:35,  2.16it/s]Evaluating:  20%|██        | 63/313 [00:28<01:54,  2.18it/s]Evaluating:  35%|███▍      | 108/313 [00:47<01:35,  2.15it/s]Evaluating:  20%|██        | 64/313 [00:29<01:53,  2.19it/s]Evaluating:  35%|███▍      | 109/313 [00:47<01:34,  2.16it/s]Evaluating:  21%|██        | 65/313 [00:29<01:53,  2.18it/s]Evaluating:  35%|███▌      | 110/313 [00:47<01:33,  2.18it/s]Evaluating:  21%|██        | 66/313 [00:30<01:52,  2.19it/s]Evaluating:  35%|███▌      | 111/313 [00:48<01:33,  2.16it/s]Evaluating:  21%|██▏       | 67/313 [00:30<01:52,  2.18it/s]Evaluating:  36%|███▌      | 112/313 [00:48<01:32,  2.17it/s]Evaluating:  22%|██▏       | 68/313 [00:31<01:52,  2.18it/s]Evaluating:  36%|███▌      | 113/313 [00:49<01:32,  2.16it/s]Evaluating:  22%|██▏       | 69/313 [00:31<01:51,  2.19it/s]Evaluating:  36%|███▋      | 114/313 [00:49<01:31,  2.18it/s]Evaluating:  22%|██▏       | 70/313 [00:32<01:51,  2.17it/s]Evaluating:  37%|███▋      | 115/313 [00:50<01:31,  2.16it/s]Evaluating:  23%|██▎       | 71/313 [00:32<01:50,  2.19it/s]Evaluating:  37%|███▋      | 116/313 [00:50<01:30,  2.18it/s]Evaluating:  23%|██▎       | 72/313 [00:33<01:50,  2.18it/s]Evaluating:  37%|███▋      | 117/313 [00:51<01:30,  2.16it/s]Evaluating:  23%|██▎       | 73/313 [00:33<01:49,  2.19it/s]Evaluating:  38%|███▊      | 118/313 [00:51<01:29,  2.17it/s]Evaluating:  24%|██▎       | 74/313 [00:34<01:49,  2.17it/s]Evaluating:  38%|███▊      | 119/313 [00:52<01:29,  2.16it/s]Evaluating:  24%|██▍       | 75/313 [00:34<01:48,  2.19it/s]Evaluating:  38%|███▊      | 120/313 [00:52<01:29,  2.17it/s]Evaluating:  24%|██▍       | 76/313 [00:34<01:48,  2.18it/s]Evaluating:  39%|███▊      | 121/313 [00:53<01:28,  2.17it/s]Evaluating:  25%|██▍       | 77/313 [00:35<01:47,  2.19it/s]Evaluating:  39%|███▉      | 122/313 [00:53<01:28,  2.16it/s]Evaluating:  25%|██▍       | 78/313 [00:35<01:47,  2.18it/s]Evaluating:  39%|███▉      | 123/313 [00:54<01:27,  2.16it/s]Evaluating:  25%|██▌       | 79/313 [00:36<01:46,  2.19it/s]Evaluating:  40%|███▉      | 124/313 [00:54<01:27,  2.15it/s]Evaluating:  26%|██▌       | 80/313 [00:36<01:45,  2.20it/s]Evaluating:  40%|███▉      | 125/313 [00:54<01:26,  2.17it/s]Evaluating:  26%|██▌       | 81/313 [00:37<01:46,  2.18it/s]Evaluating:  40%|████      | 126/313 [00:55<01:27,  2.15it/s]Evaluating:  26%|██▌       | 82/313 [00:37<01:45,  2.20it/s]Evaluating:  41%|████      | 127/313 [00:55<01:24,  2.20it/s]Evaluating:  27%|██▋       | 83/313 [00:38<01:45,  2.18it/s]Evaluating:  41%|████      | 128/313 [00:56<01:24,  2.18it/s]Evaluating:  27%|██▋       | 84/313 [00:38<01:44,  2.20it/s]Evaluating:  41%|████      | 129/313 [00:56<01:24,  2.18it/s]Evaluating:  27%|██▋       | 85/313 [00:39<01:44,  2.18it/s]Evaluating:  42%|████▏     | 130/313 [00:57<01:24,  2.16it/s]Evaluating:  27%|██▋       | 86/313 [00:39<01:43,  2.20it/s]Evaluating:  42%|████▏     | 131/313 [00:57<01:23,  2.17it/s]Evaluating:  28%|██▊       | 87/313 [00:39<01:43,  2.18it/s]Evaluating:  42%|████▏     | 132/313 [00:58<01:23,  2.17it/s]Evaluating:  28%|██▊       | 88/313 [00:40<01:42,  2.19it/s]Evaluating:  42%|████▏     | 133/313 [00:58<01:23,  2.16it/s]Evaluating:  28%|██▊       | 89/313 [00:40<01:43,  2.17it/s]Evaluating:  43%|████▎     | 134/313 [00:59<01:23,  2.15it/s]Evaluating:  29%|██▉       | 90/313 [00:41<01:41,  2.19it/s]Evaluating:  43%|████▎     | 135/313 [00:59<01:22,  2.16it/s]Evaluating:  29%|██▉       | 91/313 [00:41<01:41,  2.19it/s]Evaluating:  43%|████▎     | 136/313 [00:59<01:21,  2.18it/s]Evaluating:  29%|██▉       | 92/313 [00:42<01:41,  2.18it/s]Evaluating:  44%|████▍     | 137/313 [01:00<01:21,  2.16it/s]Evaluating:  30%|██▉       | 93/313 [00:42<01:39,  2.22it/s]Evaluating:  44%|████▍     | 138/313 [01:00<01:23,  2.08it/s]Evaluating:  30%|███       | 94/313 [00:43<01:38,  2.22it/s]Evaluating:  44%|████▍     | 139/313 [01:01<01:22,  2.10it/s]Evaluating:  30%|███       | 95/313 [00:43<01:38,  2.22it/s]Evaluating:  45%|████▍     | 140/313 [01:01<01:21,  2.12it/s]Evaluating:  31%|███       | 96/313 [00:44<01:38,  2.20it/s]Evaluating:  45%|████▌     | 141/313 [01:02<01:21,  2.12it/s]Evaluating:  31%|███       | 97/313 [00:44<01:38,  2.20it/s]Evaluating:  45%|████▌     | 142/313 [01:02<01:19,  2.14it/s]Evaluating:  31%|███▏      | 98/313 [00:44<01:38,  2.18it/s]Evaluating:  46%|████▌     | 143/313 [01:03<01:18,  2.16it/s]Evaluating:  32%|███▏      | 99/313 [00:45<01:37,  2.20it/s]Evaluating:  46%|████▌     | 144/313 [01:03<01:18,  2.15it/s]Evaluating:  32%|███▏      | 100/313 [00:45<01:37,  2.18it/s]Evaluating:  46%|████▋     | 145/313 [01:04<01:17,  2.16it/s]Evaluating:  32%|███▏      | 101/313 [00:46<01:36,  2.19it/s]Evaluating:  47%|████▋     | 146/313 [01:04<01:17,  2.15it/s]Evaluating:  33%|███▎      | 102/313 [00:46<01:36,  2.18it/s]Evaluating:  47%|████▋     | 147/313 [01:05<01:16,  2.17it/s]Evaluating:  33%|███▎      | 103/313 [00:47<01:36,  2.18it/s]Evaluating:  47%|████▋     | 148/313 [01:05<01:16,  2.15it/s]Evaluating:  33%|███▎      | 104/313 [00:47<01:36,  2.17it/s]Evaluating:  48%|████▊     | 149/313 [01:06<01:15,  2.17it/s]Evaluating:  34%|███▎      | 105/313 [00:48<01:35,  2.19it/s]Evaluating:  48%|████▊     | 150/313 [01:06<01:15,  2.16it/s]Evaluating:  34%|███▍      | 106/313 [00:48<01:34,  2.20it/s]Evaluating:  48%|████▊     | 151/313 [01:06<01:14,  2.17it/s]Evaluating:  34%|███▍      | 107/313 [00:49<01:34,  2.18it/s]Evaluating:  49%|████▊     | 152/313 [01:07<01:14,  2.15it/s]Evaluating:  35%|███▍      | 108/313 [00:49<01:33,  2.19it/s]Evaluating:  49%|████▉     | 153/313 [01:07<01:13,  2.17it/s]Evaluating:  35%|███▍      | 109/313 [00:50<01:33,  2.18it/s]Evaluating:  49%|████▉     | 154/313 [01:08<01:12,  2.18it/s]Evaluating:  35%|███▌      | 110/313 [00:50<01:32,  2.18it/s]Evaluating:  50%|████▉     | 155/313 [01:08<01:13,  2.16it/s]Evaluating:  35%|███▌      | 111/313 [00:50<01:33,  2.16it/s]Evaluating:  50%|████▉     | 156/313 [01:09<01:12,  2.17it/s]Evaluating:  36%|███▌      | 112/313 [00:51<01:32,  2.17it/s]Evaluating:  50%|█████     | 157/313 [01:09<01:12,  2.16it/s]Evaluating:  36%|███▌      | 113/313 [00:51<01:32,  2.16it/s]Evaluating:  50%|█████     | 158/313 [01:10<01:11,  2.18it/s]Evaluating:  36%|███▋      | 114/313 [00:52<01:31,  2.18it/s]Evaluating:  51%|█████     | 159/313 [01:10<01:11,  2.16it/s]Evaluating:  37%|███▋      | 115/313 [00:52<01:31,  2.17it/s]Evaluating:  51%|█████     | 160/313 [01:11<01:10,  2.17it/s]Evaluating:  37%|███▋      | 116/313 [00:53<01:30,  2.18it/s]Evaluating:  51%|█████▏    | 161/313 [01:11<01:10,  2.16it/s]Evaluating:  37%|███▋      | 117/313 [00:53<01:29,  2.18it/s]Evaluating:  52%|█████▏    | 162/313 [01:12<01:09,  2.17it/s]Evaluating:  38%|███▊      | 118/313 [00:54<01:29,  2.17it/s]Evaluating:  52%|█████▏    | 163/313 [01:12<01:09,  2.16it/s]Evaluating:  38%|███▊      | 119/313 [00:54<01:29,  2.17it/s]Evaluating:  52%|█████▏    | 164/313 [01:12<01:08,  2.17it/s]Evaluating:  38%|███▊      | 120/313 [00:55<01:29,  2.17it/s]Evaluating:  53%|█████▎    | 165/313 [01:13<01:07,  2.18it/s]Evaluating:  39%|███▊      | 121/313 [00:55<01:28,  2.18it/s]Evaluating:  53%|█████▎    | 166/313 [01:13<01:07,  2.17it/s]Evaluating:  39%|███▉      | 122/313 [00:56<01:28,  2.16it/s]Evaluating:  53%|█████▎    | 167/313 [01:14<01:07,  2.18it/s]Evaluating:  39%|███▉      | 123/313 [00:56<01:27,  2.17it/s]Evaluating:  54%|█████▎    | 168/313 [01:14<01:07,  2.16it/s]Evaluating:  40%|███▉      | 124/313 [00:56<01:27,  2.16it/s]Evaluating:  54%|█████▍    | 169/313 [01:15<01:06,  2.18it/s]Evaluating:  40%|███▉      | 125/313 [00:57<01:26,  2.18it/s]Evaluating:  54%|█████▍    | 170/313 [01:15<01:06,  2.16it/s]Evaluating:  40%|████      | 126/313 [00:57<01:26,  2.16it/s]Evaluating:  55%|█████▍    | 171/313 [01:16<01:05,  2.17it/s]Evaluating:  41%|████      | 127/313 [00:58<01:25,  2.17it/s]Evaluating:  55%|█████▍    | 172/313 [01:16<01:05,  2.16it/s]Evaluating:  41%|████      | 128/313 [00:58<01:24,  2.18it/s]Evaluating:  55%|█████▌    | 173/313 [01:17<01:04,  2.17it/s]Evaluating:  41%|████      | 129/313 [00:59<01:24,  2.17it/s]Evaluating:  56%|█████▌    | 174/313 [01:17<01:04,  2.16it/s]Evaluating:  42%|████▏     | 130/313 [00:59<01:23,  2.18it/s]Evaluating:  56%|█████▌    | 175/313 [01:18<01:03,  2.18it/s]Evaluating:  42%|████▏     | 131/313 [01:00<01:23,  2.18it/s]Evaluating:  56%|█████▌    | 176/313 [01:18<01:02,  2.19it/s]Evaluating:  42%|████▏     | 132/313 [01:00<01:22,  2.19it/s]Evaluating:  57%|█████▋    | 177/313 [01:18<01:02,  2.17it/s]Evaluating:  42%|████▏     | 133/313 [01:01<01:23,  2.17it/s]Evaluating:  57%|█████▋    | 178/313 [01:19<01:01,  2.18it/s]Evaluating:  43%|████▎     | 134/313 [01:01<01:22,  2.18it/s]Evaluating:  57%|█████▋    | 179/313 [01:19<01:01,  2.16it/s]Evaluating:  43%|████▎     | 135/313 [01:01<01:22,  2.16it/s]Evaluating:  58%|█████▊    | 180/313 [01:20<01:01,  2.18it/s]Evaluating:  43%|████▎     | 136/313 [01:02<01:21,  2.18it/s]Evaluating:  58%|█████▊    | 181/313 [01:20<00:59,  2.21it/s]Evaluating:  44%|████▍     | 137/313 [01:02<01:24,  2.08it/s]Evaluating:  58%|█████▊    | 182/313 [01:21<00:59,  2.21it/s]Evaluating:  44%|████▍     | 138/313 [01:03<01:22,  2.12it/s]Evaluating:  58%|█████▊    | 183/313 [01:21<00:59,  2.19it/s]Evaluating:  44%|████▍     | 139/313 [01:03<01:21,  2.14it/s]Evaluating:  59%|█████▉    | 184/313 [01:22<00:58,  2.20it/s]Evaluating:  45%|████▍     | 140/313 [01:04<01:20,  2.14it/s]Evaluating:  59%|█████▉    | 185/313 [01:22<00:58,  2.17it/s]Evaluating:  45%|████▌     | 141/313 [01:04<01:19,  2.16it/s]Evaluating:  59%|█████▉    | 186/313 [01:23<00:58,  2.17it/s]Evaluating:  45%|████▌     | 142/313 [01:05<01:19,  2.16it/s]Evaluating:  60%|█████▉    | 187/313 [01:23<00:57,  2.18it/s]Evaluating:  46%|████▌     | 143/313 [01:05<01:18,  2.18it/s]Evaluating:  60%|██████    | 188/313 [01:24<00:58,  2.14it/s]Evaluating:  46%|████▌     | 144/313 [01:06<01:17,  2.18it/s]Evaluating:  60%|██████    | 189/313 [01:24<00:57,  2.16it/s]Evaluating:  46%|████▋     | 145/313 [01:06<01:16,  2.19it/s]Evaluating:  61%|██████    | 190/313 [01:24<00:57,  2.15it/s]Evaluating:  47%|████▋     | 146/313 [01:07<01:16,  2.18it/s]Evaluating:  61%|██████    | 191/313 [01:25<00:56,  2.17it/s]Evaluating:  47%|████▋     | 147/313 [01:07<01:15,  2.19it/s]Evaluating:  61%|██████▏   | 192/313 [01:25<00:56,  2.16it/s]Evaluating:  47%|████▋     | 148/313 [01:08<01:16,  2.17it/s]Evaluating:  62%|██████▏   | 193/313 [01:26<00:55,  2.17it/s]Evaluating:  48%|████▊     | 149/313 [01:08<01:15,  2.18it/s]Evaluating:  62%|██████▏   | 194/313 [01:26<00:55,  2.16it/s]Evaluating:  48%|████▊     | 150/313 [01:08<01:14,  2.19it/s]Evaluating:  62%|██████▏   | 195/313 [01:27<00:54,  2.17it/s]Evaluating:  48%|████▊     | 151/313 [01:09<01:14,  2.17it/s]Evaluating:  63%|██████▎   | 196/313 [01:27<00:54,  2.16it/s]Evaluating:  49%|████▊     | 152/313 [01:09<01:13,  2.18it/s]Evaluating:  63%|██████▎   | 197/313 [01:28<00:53,  2.17it/s]Evaluating:  49%|████▉     | 153/313 [01:10<01:14,  2.16it/s]Evaluating:  63%|██████▎   | 198/313 [01:28<00:52,  2.18it/s]Evaluating:  49%|████▉     | 154/313 [01:10<01:13,  2.18it/s]Evaluating:  64%|██████▎   | 199/313 [01:29<00:52,  2.16it/s]Evaluating:  50%|████▉     | 155/313 [01:11<01:13,  2.16it/s]Evaluating:  64%|██████▍   | 200/313 [01:29<00:51,  2.17it/s]Evaluating:  50%|████▉     | 156/313 [01:11<01:12,  2.17it/s]Evaluating:  64%|██████▍   | 201/313 [01:30<00:51,  2.16it/s]Evaluating:  50%|█████     | 157/313 [01:12<01:12,  2.16it/s]Evaluating:  65%|██████▍   | 202/313 [01:30<00:51,  2.18it/s]Evaluating:  50%|█████     | 158/313 [01:12<01:10,  2.20it/s]Evaluating:  65%|██████▍   | 203/313 [01:30<00:51,  2.12it/s]Evaluating:  51%|█████     | 159/313 [01:13<01:10,  2.17it/s]Evaluating:  65%|██████▌   | 204/313 [01:31<00:50,  2.14it/s]Evaluating:  51%|█████     | 160/313 [01:13<01:10,  2.17it/s]Evaluating:  65%|██████▌   | 205/313 [01:31<00:50,  2.15it/s]Evaluating:  51%|█████▏    | 161/313 [01:13<01:09,  2.19it/s]Evaluating:  66%|██████▌   | 206/313 [01:32<00:49,  2.15it/s]Evaluating:  52%|█████▏    | 162/313 [01:14<01:09,  2.17it/s]Evaluating:  66%|██████▌   | 207/313 [01:32<00:49,  2.15it/s]Evaluating:  52%|█████▏    | 163/313 [01:14<01:08,  2.18it/s]Evaluating:  66%|██████▋   | 208/313 [01:33<00:48,  2.16it/s]Evaluating:  52%|█████▏    | 164/313 [01:15<01:08,  2.17it/s]Evaluating:  67%|██████▋   | 209/313 [01:33<00:47,  2.17it/s]Evaluating:  53%|█████▎    | 165/313 [01:15<01:07,  2.18it/s]Evaluating:  67%|██████▋   | 210/313 [01:34<00:47,  2.16it/s]Evaluating:  53%|█████▎    | 166/313 [01:16<01:08,  2.16it/s]Evaluating:  67%|██████▋   | 211/313 [01:34<00:47,  2.17it/s]Evaluating:  53%|█████▎    | 167/313 [01:16<01:07,  2.17it/s]Evaluating:  68%|██████▊   | 212/313 [01:35<00:46,  2.15it/s]Evaluating:  54%|█████▎    | 168/313 [01:17<01:07,  2.16it/s]Evaluating:  68%|██████▊   | 213/313 [01:35<00:45,  2.18it/s]Evaluating:  54%|█████▍    | 169/313 [01:17<01:05,  2.19it/s]Evaluating:  68%|██████▊   | 214/313 [01:36<00:46,  2.13it/s]Evaluating:  54%|█████▍    | 170/313 [01:18<01:05,  2.17it/s]Evaluating:  69%|██████▊   | 215/313 [01:36<00:45,  2.15it/s]Evaluating:  55%|█████▍    | 171/313 [01:18<01:05,  2.18it/s]Evaluating:  69%|██████▉   | 216/313 [01:37<00:44,  2.16it/s]Evaluating:  55%|█████▍    | 172/313 [01:19<01:04,  2.19it/s]Evaluating:  69%|██████▉   | 217/313 [01:37<00:44,  2.15it/s]Evaluating:  55%|█████▌    | 173/313 [01:19<01:04,  2.17it/s]Evaluating:  70%|██████▉   | 218/313 [01:37<00:43,  2.16it/s]Evaluating:  56%|█████▌    | 174/313 [01:19<01:03,  2.18it/s]Evaluating:  70%|██████▉   | 219/313 [01:38<00:43,  2.15it/s]Evaluating:  56%|█████▌    | 175/313 [01:20<01:03,  2.16it/s]Evaluating:  70%|███████   | 220/313 [01:38<00:42,  2.17it/s]Evaluating:  56%|█████▌    | 176/313 [01:20<01:02,  2.18it/s]Evaluating:  71%|███████   | 221/313 [01:39<00:42,  2.15it/s]Evaluating:  57%|█████▋    | 177/313 [01:21<01:02,  2.16it/s]Evaluating:  71%|███████   | 222/313 [01:39<00:41,  2.17it/s]Evaluating:  57%|█████▋    | 178/313 [01:21<01:02,  2.17it/s]Evaluating:  71%|███████   | 223/313 [01:40<00:41,  2.15it/s]Evaluating:  57%|█████▋    | 179/313 [01:22<01:01,  2.17it/s]Evaluating:  72%|███████▏  | 224/313 [01:40<00:40,  2.18it/s]Evaluating:  58%|█████▊    | 180/313 [01:22<01:01,  2.18it/s]Evaluating:  72%|███████▏  | 225/313 [01:41<00:40,  2.16it/s]Evaluating:  58%|█████▊    | 181/313 [01:23<01:00,  2.16it/s]Evaluating:  72%|███████▏  | 226/313 [01:41<00:39,  2.18it/s]Evaluating:  58%|█████▊    | 182/313 [01:23<01:00,  2.16it/s]Evaluating:  73%|███████▎  | 227/313 [01:42<00:39,  2.19it/s]Evaluating:  58%|█████▊    | 183/313 [01:24<00:59,  2.18it/s]Evaluating:  73%|███████▎  | 228/313 [01:42<00:39,  2.16it/s]Evaluating:  59%|█████▉    | 184/313 [01:24<00:59,  2.16it/s]Evaluating:  73%|███████▎  | 229/313 [01:42<00:38,  2.18it/s]Evaluating:  59%|█████▉    | 185/313 [01:25<00:58,  2.17it/s]Evaluating:  73%|███████▎  | 230/313 [01:43<00:38,  2.16it/s]Evaluating:  59%|█████▉    | 186/313 [01:25<00:58,  2.16it/s]Evaluating:  74%|███████▍  | 231/313 [01:43<00:37,  2.17it/s]Evaluating:  60%|█████▉    | 187/313 [01:25<00:57,  2.17it/s]Evaluating:  74%|███████▍  | 232/313 [01:44<00:37,  2.15it/s]Evaluating:  60%|██████    | 188/313 [01:26<00:57,  2.16it/s]Evaluating:  74%|███████▍  | 233/313 [01:44<00:36,  2.17it/s]Evaluating:  60%|██████    | 189/313 [01:26<00:57,  2.17it/s]Evaluating:  75%|███████▍  | 234/313 [01:45<00:36,  2.15it/s]Evaluating:  61%|██████    | 190/313 [01:27<00:56,  2.17it/s]Evaluating:  75%|███████▌  | 235/313 [01:45<00:35,  2.17it/s]Evaluating:  61%|██████    | 191/313 [01:27<00:56,  2.17it/s]Evaluating:  75%|███████▌  | 236/313 [01:46<00:35,  2.16it/s]Evaluating:  61%|██████▏   | 192/313 [01:28<00:55,  2.17it/s]Evaluating:  76%|███████▌  | 237/313 [01:46<00:35,  2.17it/s]Evaluating:  62%|██████▏   | 193/313 [01:28<00:55,  2.17it/s]Evaluating:  76%|███████▌  | 238/313 [01:47<00:34,  2.18it/s]Evaluating:  62%|██████▏   | 194/313 [01:29<00:54,  2.18it/s]Evaluating:  76%|███████▋  | 239/313 [01:47<00:34,  2.16it/s]Evaluating:  62%|██████▏   | 195/313 [01:29<00:54,  2.16it/s]Evaluating:  77%|███████▋  | 240/313 [01:48<00:33,  2.17it/s]Evaluating:  63%|██████▎   | 196/313 [01:30<00:53,  2.17it/s]Evaluating:  77%|███████▋  | 241/313 [01:48<00:33,  2.15it/s]Evaluating:  63%|██████▎   | 197/313 [01:30<00:53,  2.16it/s]Evaluating:  63%|██████▎   | 198/313 [01:31<00:52,  2.19it/s]Evaluating:  77%|███████▋  | 242/313 [01:49<00:33,  2.11it/s]Evaluating:  64%|██████▎   | 199/313 [01:31<00:52,  2.16it/s]Evaluating:  78%|███████▊  | 243/313 [01:49<00:33,  2.10it/s]Evaluating:  64%|██████▍   | 200/313 [01:31<00:51,  2.18it/s]Evaluating:  78%|███████▊  | 244/313 [01:49<00:32,  2.12it/s]Evaluating:  64%|██████▍   | 201/313 [01:32<00:51,  2.19it/s]Evaluating:  78%|███████▊  | 245/313 [01:50<00:31,  2.14it/s]Evaluating:  65%|██████▍   | 202/313 [01:32<00:51,  2.18it/s]Evaluating:  79%|███████▊  | 246/313 [01:50<00:31,  2.12it/s]Evaluating:  65%|██████▍   | 203/313 [01:33<00:50,  2.19it/s]Evaluating:  79%|███████▉  | 247/313 [01:51<00:30,  2.13it/s]Evaluating:  65%|██████▌   | 204/313 [01:33<00:50,  2.18it/s]Evaluating:  79%|███████▉  | 248/313 [01:51<00:30,  2.13it/s]Evaluating:  65%|██████▌   | 205/313 [01:34<00:49,  2.20it/s]Evaluating:  80%|███████▉  | 249/313 [01:52<00:29,  2.14it/s]Evaluating:  66%|██████▌   | 206/313 [01:34<00:49,  2.18it/s]Evaluating:  80%|███████▉  | 250/313 [01:52<00:29,  2.13it/s]Evaluating:  66%|██████▌   | 207/313 [01:35<00:48,  2.19it/s]Evaluating:  80%|████████  | 251/313 [01:53<00:29,  2.14it/s]Evaluating:  66%|██████▋   | 208/313 [01:35<00:48,  2.18it/s]Evaluating:  81%|████████  | 252/313 [01:53<00:28,  2.14it/s]Evaluating:  67%|██████▋   | 209/313 [01:36<00:47,  2.20it/s]Evaluating:  81%|████████  | 253/313 [01:54<00:28,  2.13it/s]Evaluating:  67%|██████▋   | 210/313 [01:36<00:47,  2.18it/s]Evaluating:  81%|████████  | 254/313 [01:54<00:27,  2.14it/s]Evaluating:  67%|██████▋   | 211/313 [01:36<00:46,  2.19it/s]Evaluating:  81%|████████▏ | 255/313 [01:55<00:27,  2.12it/s]Evaluating:  68%|██████▊   | 212/313 [01:37<00:45,  2.20it/s]Evaluating:  82%|████████▏ | 256/313 [01:55<00:26,  2.14it/s]Evaluating:  68%|██████▊   | 213/313 [01:37<00:45,  2.20it/s]Evaluating:  82%|████████▏ | 257/313 [01:56<00:26,  2.13it/s]Evaluating:  68%|██████▊   | 214/313 [01:38<00:44,  2.20it/s]Evaluating:  82%|████████▏ | 258/313 [01:56<00:25,  2.14it/s]Evaluating:  69%|██████▊   | 215/313 [01:38<00:44,  2.19it/s]Evaluating:  83%|████████▎ | 259/313 [01:57<00:25,  2.12it/s]Evaluating:  69%|██████▉   | 216/313 [01:39<00:44,  2.20it/s]Evaluating:  83%|████████▎ | 260/313 [01:57<00:24,  2.13it/s]Evaluating:  69%|██████▉   | 217/313 [01:39<00:43,  2.18it/s]Evaluating:  83%|████████▎ | 261/313 [01:57<00:24,  2.13it/s]Evaluating:  70%|██████▉   | 218/313 [01:40<00:43,  2.20it/s]Evaluating:  84%|████████▎ | 262/313 [01:58<00:24,  2.12it/s]Evaluating:  70%|██████▉   | 219/313 [01:40<00:43,  2.18it/s]Evaluating:  84%|████████▍ | 263/313 [01:58<00:23,  2.13it/s]Evaluating:  70%|███████   | 220/313 [01:41<00:42,  2.19it/s]Evaluating:  84%|████████▍ | 264/313 [01:59<00:23,  2.12it/s]Evaluating:  71%|███████   | 221/313 [01:41<00:42,  2.18it/s]Evaluating:  85%|████████▍ | 265/313 [01:59<00:22,  2.13it/s]Evaluating:  71%|███████   | 222/313 [01:41<00:41,  2.19it/s]Evaluating:  85%|████████▍ | 266/313 [02:00<00:22,  2.12it/s]Evaluating:  71%|███████   | 223/313 [01:42<00:40,  2.20it/s]Evaluating:  85%|████████▌ | 267/313 [02:00<00:21,  2.13it/s]Evaluating:  72%|███████▏  | 224/313 [01:42<00:40,  2.18it/s]Evaluating:  86%|████████▌ | 268/313 [02:01<00:21,  2.12it/s]Evaluating:  72%|███████▏  | 225/313 [01:43<00:39,  2.20it/s]Evaluating:  86%|████████▌ | 269/313 [02:01<00:20,  2.13it/s]Evaluating:  72%|███████▏  | 226/313 [01:43<00:39,  2.18it/s]Evaluating:  86%|████████▋ | 270/313 [02:02<00:20,  2.14it/s]Evaluating:  73%|███████▎  | 227/313 [01:44<00:39,  2.19it/s]Evaluating:  87%|████████▋ | 271/313 [02:02<00:19,  2.13it/s]Evaluating:  73%|███████▎  | 228/313 [01:44<00:39,  2.18it/s]Evaluating:  87%|████████▋ | 272/313 [02:03<00:19,  2.14it/s]Evaluating:  73%|███████▎  | 229/313 [01:45<00:38,  2.19it/s]Evaluating:  87%|████████▋ | 273/313 [02:03<00:18,  2.12it/s]Evaluating:  73%|███████▎  | 230/313 [01:45<00:38,  2.18it/s]Evaluating:  88%|████████▊ | 274/313 [02:04<00:18,  2.14it/s]Evaluating:  74%|███████▍  | 231/313 [01:46<00:37,  2.19it/s]Evaluating:  88%|████████▊ | 275/313 [02:04<00:17,  2.13it/s]Evaluating:  74%|███████▍  | 232/313 [01:46<00:37,  2.18it/s]Evaluating:  88%|████████▊ | 276/313 [02:05<00:17,  2.14it/s]Evaluating:  74%|███████▍  | 233/313 [01:47<00:36,  2.19it/s]Evaluating:  88%|████████▊ | 277/313 [02:05<00:16,  2.15it/s]Evaluating:  75%|███████▍  | 234/313 [01:47<00:35,  2.20it/s]Evaluating:  89%|████████▉ | 278/313 [02:05<00:16,  2.13it/s]Evaluating:  75%|███████▌  | 235/313 [01:47<00:35,  2.17it/s]Evaluating:  89%|████████▉ | 279/313 [02:06<00:15,  2.14it/s]Evaluating:  75%|███████▌  | 236/313 [01:48<00:35,  2.18it/s]Evaluating:  89%|████████▉ | 280/313 [02:06<00:15,  2.13it/s]Evaluating:  76%|███████▌  | 237/313 [01:48<00:35,  2.16it/s]Evaluating:  90%|████████▉ | 281/313 [02:07<00:14,  2.14it/s]Evaluating:  76%|███████▌  | 238/313 [01:49<00:34,  2.16it/s]Evaluating:  90%|█████████ | 282/313 [02:07<00:14,  2.13it/s]Evaluating:  76%|███████▋  | 239/313 [01:49<00:34,  2.15it/s]Evaluating:  90%|█████████ | 283/313 [02:08<00:13,  2.15it/s]Evaluating:  77%|███████▋  | 240/313 [01:50<00:33,  2.16it/s]Evaluating:  91%|█████████ | 284/313 [02:08<00:13,  2.15it/s]Evaluating:  77%|███████▋  | 241/313 [01:50<00:33,  2.16it/s]Evaluating:  91%|█████████ | 285/313 [02:09<00:13,  2.13it/s]Evaluating:  77%|███████▋  | 242/313 [01:51<00:33,  2.15it/s]Evaluating:  78%|███████▊  | 243/313 [01:51<00:32,  2.15it/s]Evaluating:  91%|█████████▏| 286/313 [02:09<00:12,  2.13it/s]Evaluating:  78%|███████▊  | 244/313 [01:52<00:32,  2.15it/s]Evaluating:  92%|█████████▏| 287/313 [02:10<00:12,  2.11it/s]Evaluating:  78%|███████▊  | 245/313 [01:52<00:31,  2.17it/s]Evaluating:  92%|█████████▏| 288/313 [02:10<00:11,  2.13it/s]Evaluating:  79%|███████▊  | 246/313 [01:53<00:30,  2.16it/s]Evaluating:  92%|█████████▏| 289/313 [02:11<00:11,  2.12it/s]Evaluating:  79%|███████▉  | 247/313 [01:53<00:30,  2.18it/s]Evaluating:  93%|█████████▎| 290/313 [02:11<00:10,  2.12it/s]Evaluating:  79%|███████▉  | 248/313 [01:53<00:29,  2.17it/s]Evaluating:  93%|█████████▎| 291/313 [02:12<00:10,  2.14it/s]Evaluating:  80%|███████▉  | 249/313 [01:54<00:29,  2.18it/s]Evaluating:  93%|█████████▎| 292/313 [02:12<00:09,  2.13it/s]Evaluating:  80%|███████▉  | 250/313 [01:54<00:29,  2.17it/s]Evaluating:  94%|█████████▎| 293/313 [02:12<00:09,  2.14it/s]Evaluating:  80%|████████  | 251/313 [01:55<00:28,  2.17it/s]Evaluating:  94%|█████████▍| 294/313 [02:13<00:08,  2.12it/s]Evaluating:  81%|████████  | 252/313 [01:55<00:27,  2.19it/s]Evaluating:  94%|█████████▍| 295/313 [02:13<00:08,  2.13it/s]Evaluating:  81%|████████  | 253/313 [01:56<00:27,  2.17it/s]Evaluating:  95%|█████████▍| 296/313 [02:14<00:08,  2.12it/s]Evaluating:  81%|████████  | 254/313 [01:56<00:27,  2.18it/s]Evaluating:  95%|█████████▍| 297/313 [02:14<00:07,  2.13it/s]Evaluating:  81%|████████▏ | 255/313 [01:57<00:26,  2.17it/s]Evaluating:  95%|█████████▌| 298/313 [02:15<00:07,  2.13it/s]Evaluating:  82%|████████▏ | 256/313 [01:57<00:26,  2.19it/s]Evaluating:  96%|█████████▌| 299/313 [02:15<00:06,  2.12it/s]Evaluating:  82%|████████▏ | 257/313 [01:58<00:25,  2.17it/s]Evaluating:  96%|█████████▌| 300/313 [02:16<00:06,  2.13it/s]Evaluating:  82%|████████▏ | 258/313 [01:58<00:25,  2.19it/s]Evaluating:  96%|█████████▌| 301/313 [02:16<00:05,  2.11it/s]Evaluating:  83%|████████▎ | 259/313 [01:58<00:24,  2.20it/s]Evaluating:  96%|█████████▋| 302/313 [02:17<00:05,  2.13it/s]Evaluating:  83%|████████▎ | 260/313 [01:59<00:24,  2.18it/s]Evaluating:  97%|█████████▋| 303/313 [02:17<00:04,  2.12it/s]Evaluating:  83%|████████▎ | 261/313 [01:59<00:23,  2.19it/s]Evaluating:  97%|█████████▋| 304/313 [02:18<00:04,  2.13it/s]Evaluating:  84%|████████▎ | 262/313 [02:00<00:23,  2.17it/s]Evaluating:  97%|█████████▋| 305/313 [02:18<00:03,  2.12it/s]Evaluating:  84%|████████▍ | 263/313 [02:00<00:22,  2.19it/s]Evaluating:  98%|█████████▊| 306/313 [02:19<00:03,  2.12it/s]Evaluating:  84%|████████▍ | 264/313 [02:01<00:22,  2.16it/s]Evaluating:  98%|█████████▊| 307/313 [02:19<00:02,  2.14it/s]Evaluating:  85%|████████▍ | 265/313 [02:01<00:22,  2.16it/s]Evaluating:  98%|█████████▊| 308/313 [02:20<00:02,  2.13it/s]Evaluating:  85%|████████▍ | 266/313 [02:02<00:21,  2.16it/s]Evaluating:  99%|█████████▊| 309/313 [02:20<00:01,  2.14it/s]Evaluating:  85%|████████▌ | 267/313 [02:02<00:20,  2.24it/s]Evaluating:  99%|█████████▉| 310/313 [02:21<00:01,  2.02it/s]Evaluating:  86%|████████▌ | 268/313 [02:03<00:20,  2.21it/s]Evaluating:  99%|█████████▉| 311/313 [02:21<00:00,  2.06it/s]Evaluating:  86%|████████▌ | 269/313 [02:03<00:19,  2.21it/s]Evaluating: 100%|█████████▉| 312/313 [02:21<00:00,  2.09it/s]Evaluating:  86%|████████▋ | 270/313 [02:04<00:19,  2.21it/s]Evaluating: 100%|██████████| 313/313 [02:22<00:00,  2.33it/s]Evaluating: 100%|██████████| 313/313 [02:22<00:00,  2.20it/s]Evaluating:  87%|████████▋ | 271/313 [02:04<00:18,  2.31it/s]Evaluating:  87%|████████▋ | 272/313 [02:04<00:16,  2.55it/s]Evaluating:  87%|████████▋ | 273/313 [02:05<00:14,  2.71it/s]Evaluating:  88%|████████▊ | 274/313 [02:05<00:13,  2.88it/s]
10/10/2021 15:37:11 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/10/2021 15:37:11 - INFO - __main__ -     f1 = 0.5337102659347162
10/10/2021 15:37:11 - INFO - __main__ -     loss = 1.4284341504779487
10/10/2021 15:37:11 - INFO - __main__ -     precision = 0.4774732384425362
10/10/2021 15:37:11 - INFO - __main__ -     recall = 0.6049631120053656
10/10/2021 15:37:11 - INFO - __main__ -   Language adapter for bg not found, using ru instead
10/10/2021 15:37:11 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:37:11 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
Evaluating:  88%|████████▊ | 275/313 [02:05<00:12,  2.96it/s]Evaluating:  88%|████████▊ | 276/313 [02:05<00:12,  3.08it/s]Evaluating:  88%|████████▊ | 277/313 [02:06<00:11,  3.17it/s]Evaluating:  89%|████████▉ | 278/313 [02:06<00:10,  3.19it/s]Evaluating:  89%|████████▉ | 279/313 [02:06<00:10,  3.24it/s]10/10/2021 15:37:12 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/10/2021 15:37:12 - INFO - __main__ -     Num examples = 10004
10/10/2021 15:37:12 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  89%|████████▉ | 280/313 [02:07<00:11,  2.91it/s]Evaluating:   0%|          | 1/313 [00:00<02:21,  2.21it/s]Evaluating:  90%|████████▉ | 281/313 [02:07<00:12,  2.65it/s]Evaluating:   1%|          | 2/313 [00:00<02:20,  2.22it/s]Evaluating:  90%|█████████ | 282/313 [02:08<00:12,  2.45it/s]Evaluating:   1%|          | 3/313 [00:01<02:21,  2.19it/s]Evaluating:  90%|█████████ | 283/313 [02:08<00:12,  2.36it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:20,  2.21it/s]Evaluating:  91%|█████████ | 284/313 [02:09<00:12,  2.29it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:18,  2.22it/s]Evaluating:  91%|█████████ | 285/313 [02:09<00:12,  2.24it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:19,  2.20it/s]Evaluating:  91%|█████████▏| 286/313 [02:10<00:12,  2.20it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:18,  2.21it/s]Evaluating:  92%|█████████▏| 287/313 [02:10<00:11,  2.18it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:19,  2.19it/s]Evaluating:  92%|█████████▏| 288/313 [02:10<00:11,  2.17it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:17,  2.21it/s]Evaluating:  92%|█████████▏| 289/313 [02:11<00:11,  2.15it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:18,  2.18it/s]Evaluating:  93%|█████████▎| 290/313 [02:11<00:10,  2.15it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:17,  2.19it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:18,  2.18it/s]Evaluating:  93%|█████████▎| 291/313 [02:12<00:10,  2.14it/s]Evaluating:   4%|▍         | 13/313 [00:05<02:16,  2.20it/s]Evaluating:  93%|█████████▎| 292/313 [02:12<00:09,  2.15it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:16,  2.19it/s]Evaluating:  94%|█████████▎| 293/313 [02:13<00:09,  2.13it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:15,  2.21it/s]Evaluating:  94%|█████████▍| 294/313 [02:13<00:08,  2.14it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:14,  2.21it/s]Evaluating:  94%|█████████▍| 295/313 [02:14<00:08,  2.13it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:14,  2.20it/s]Evaluating:  95%|█████████▍| 296/313 [02:14<00:08,  2.12it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:13,  2.21it/s]Evaluating:  95%|█████████▍| 297/313 [02:15<00:07,  2.13it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:13,  2.20it/s]Evaluating:  95%|█████████▌| 298/313 [02:15<00:07,  2.11it/s]Evaluating:   6%|▋         | 20/313 [00:09<02:12,  2.22it/s]Evaluating:  96%|█████████▌| 299/313 [02:16<00:06,  2.12it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:12,  2.20it/s]Evaluating:  96%|█████████▌| 300/313 [02:16<00:06,  2.11it/s]Evaluating:   7%|▋         | 22/313 [00:09<02:11,  2.21it/s]Evaluating:  96%|█████████▌| 301/313 [02:17<00:05,  2.12it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:11,  2.20it/s]Evaluating:  96%|█████████▋| 302/313 [02:17<00:05,  2.13it/s]Evaluating:   8%|▊         | 24/313 [00:10<02:10,  2.22it/s]Evaluating:  97%|█████████▋| 303/313 [02:18<00:04,  2.12it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:10,  2.20it/s]Evaluating:  97%|█████████▋| 304/313 [02:18<00:04,  2.12it/s]Evaluating:   8%|▊         | 26/313 [00:11<02:09,  2.22it/s]Evaluating:  97%|█████████▋| 305/313 [02:19<00:03,  2.10it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:09,  2.22it/s]Evaluating:  98%|█████████▊| 306/313 [02:19<00:03,  2.12it/s]Evaluating:   9%|▉         | 28/313 [00:12<02:08,  2.21it/s]Evaluating:  98%|█████████▊| 307/313 [02:19<00:02,  2.10it/s]Evaluating:   9%|▉         | 29/313 [00:13<02:08,  2.21it/s]Evaluating:  98%|█████████▊| 308/313 [02:20<00:02,  2.12it/s]Evaluating:  10%|▉         | 30/313 [00:13<02:08,  2.21it/s]Evaluating:  99%|█████████▊| 309/313 [02:20<00:01,  2.12it/s]Evaluating:  10%|▉         | 31/313 [00:14<02:06,  2.22it/s]Evaluating:  99%|█████████▉| 310/313 [02:21<00:01,  2.11it/s]Evaluating:  10%|█         | 32/313 [00:14<02:07,  2.21it/s]Evaluating:  99%|█████████▉| 311/313 [02:21<00:00,  2.12it/s]Evaluating:  11%|█         | 33/313 [00:14<02:06,  2.22it/s]Evaluating: 100%|█████████▉| 312/313 [02:22<00:00,  2.10it/s]Evaluating:  11%|█         | 34/313 [00:15<02:06,  2.21it/s]Evaluating: 100%|██████████| 313/313 [02:22<00:00,  2.37it/s]Evaluating: 100%|██████████| 313/313 [02:22<00:00,  2.19it/s]Evaluating:  11%|█         | 35/313 [00:15<01:59,  2.33it/s]Evaluating:  12%|█▏        | 36/313 [00:16<01:48,  2.55it/s]Evaluating:  12%|█▏        | 37/313 [00:16<01:39,  2.77it/s]Evaluating:  12%|█▏        | 38/313 [00:16<01:34,  2.90it/s]
10/10/2021 15:37:29 - INFO - __main__ -   ***** Evaluation result  in da *****
10/10/2021 15:37:29 - INFO - __main__ -     f1 = 0.7424756105998754
10/10/2021 15:37:29 - INFO - __main__ -     loss = 0.7449887050226474
10/10/2021 15:37:29 - INFO - __main__ -     precision = 0.7056154655444503
10/10/2021 15:37:29 - INFO - __main__ -     recall = 0.7833990363556723
Evaluating:  12%|█▏        | 39/313 [00:16<01:29,  3.06it/s]10/10/2021 15:37:29 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  13%|█▎        | 40/313 [00:17<01:27,  3.11it/s]Evaluating:  13%|█▎        | 41/313 [00:17<01:25,  3.20it/s]Evaluating:  13%|█▎        | 42/313 [00:17<01:22,  3.28it/s]Evaluating:  14%|█▎        | 43/313 [00:18<01:22,  3.27it/s]Evaluating:  14%|█▍        | 44/313 [00:18<01:21,  3.30it/s]Evaluating:  14%|█▍        | 45/313 [00:18<01:21,  3.30it/s]Evaluating:  15%|█▍        | 46/313 [00:19<01:19,  3.35it/s]Evaluating:  15%|█▌        | 47/313 [00:19<01:20,  3.32it/s]Evaluating:  15%|█▌        | 48/313 [00:19<01:19,  3.35it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  16%|█▌        | 49/313 [00:19<01:19,  3.32it/s]Evaluating:  16%|█▌        | 50/313 [00:20<01:18,  3.36it/s]Evaluating:  16%|█▋        | 51/313 [00:20<01:18,  3.32it/s]Evaluating:  17%|█▋        | 52/313 [00:20<01:21,  3.20it/s]Evaluating:  17%|█▋        | 53/313 [00:21<01:19,  3.29it/s]Evaluating:  17%|█▋        | 54/313 [00:21<01:18,  3.28it/s]Evaluating:  18%|█▊        | 55/313 [00:21<01:17,  3.33it/s]Evaluating:  18%|█▊        | 56/313 [00:22<01:17,  3.31it/s]Evaluating:  18%|█▊        | 57/313 [00:22<01:16,  3.36it/s]Evaluating:  19%|█▊        | 58/313 [00:22<01:16,  3.33it/s]Evaluating:  19%|█▉        | 59/313 [00:22<01:15,  3.36it/s]Evaluating:  19%|█▉        | 60/313 [00:23<01:14,  3.39it/s]Evaluating:  19%|█▉        | 61/313 [00:23<01:15,  3.35it/s]Evaluating:  20%|█▉        | 62/313 [00:23<01:15,  3.32it/s]Evaluating:  20%|██        | 63/313 [00:24<01:14,  3.35it/s]Evaluating:  20%|██        | 64/313 [00:24<01:13,  3.38it/s]Evaluating:  21%|██        | 65/313 [00:24<01:14,  3.34it/s]Evaluating:  21%|██        | 66/313 [00:25<01:14,  3.33it/s]Evaluating:  21%|██▏       | 67/313 [00:25<01:13,  3.33it/s]Evaluating:  22%|██▏       | 68/313 [00:25<01:12,  3.37it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  22%|██▏       | 69/313 [00:25<01:13,  3.34it/s]Evaluating:  22%|██▏       | 70/313 [00:26<01:12,  3.36it/s]Evaluating:  23%|██▎       | 71/313 [00:26<01:11,  3.40it/s]Evaluating:  23%|██▎       | 72/313 [00:26<01:11,  3.37it/s]Evaluating:  23%|██▎       | 73/313 [00:27<01:11,  3.34it/s]Evaluating:  24%|██▎       | 74/313 [00:27<01:11,  3.36it/s]Evaluating:  24%|██▍       | 75/313 [00:27<01:10,  3.40it/s]Evaluating:  24%|██▍       | 76/313 [00:28<01:10,  3.35it/s]Evaluating:  25%|██▍       | 77/313 [00:28<01:10,  3.33it/s]Evaluating:  25%|██▍       | 78/313 [00:28<01:10,  3.36it/s]Evaluating:  25%|██▌       | 79/313 [00:28<01:09,  3.38it/s]Evaluating:  26%|██▌       | 80/313 [00:29<01:09,  3.34it/s]Evaluating:  26%|██▌       | 81/313 [00:29<01:09,  3.33it/s]Evaluating:  26%|██▌       | 82/313 [00:29<01:08,  3.39it/s]Evaluating:  27%|██▋       | 83/313 [00:30<01:08,  3.36it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  27%|██▋       | 84/313 [00:30<01:08,  3.33it/s]Evaluating:  27%|██▋       | 85/313 [00:30<01:06,  3.42it/s]Evaluating:  27%|██▋       | 86/313 [00:30<01:05,  3.47it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  28%|██▊       | 87/313 [00:31<01:05,  3.43it/s]Evaluating:  28%|██▊       | 88/313 [00:31<01:06,  3.40it/s]Evaluating:  28%|██▊       | 89/313 [00:31<01:05,  3.42it/s]Evaluating:  29%|██▉       | 90/313 [00:32<01:04,  3.44it/s]Evaluating:  29%|██▉       | 91/313 [00:32<01:05,  3.39it/s]Evaluating:  29%|██▉       | 92/313 [00:32<01:05,  3.35it/s]Evaluating:  30%|██▉       | 93/313 [00:33<01:05,  3.38it/s]Evaluating:  30%|███       | 94/313 [00:33<01:04,  3.41it/s]Evaluating:  30%|███       | 95/313 [00:33<01:04,  3.36it/s]Evaluating:  31%|███       | 96/313 [00:33<01:04,  3.35it/s]Evaluating:  31%|███       | 97/313 [00:34<01:04,  3.34it/s]Evaluating:  31%|███▏      | 98/313 [00:34<01:04,  3.33it/s]Evaluating:  32%|███▏      | 99/313 [00:34<01:04,  3.32it/s]Evaluating:  32%|███▏      | 100/313 [00:35<01:03,  3.37it/s]Evaluating:  32%|███▏      | 101/313 [00:35<01:02,  3.39it/s]Evaluating:  33%|███▎      | 102/313 [00:35<01:02,  3.36it/s]Evaluating:  33%|███▎      | 103/313 [00:36<01:03,  3.33it/s]Evaluating:  33%|███▎      | 104/313 [00:36<01:02,  3.36it/s]Evaluating:  34%|███▎      | 105/313 [00:36<01:01,  3.40it/s]Evaluating:  34%|███▍      | 106/313 [00:36<01:01,  3.35it/s]Evaluating:  34%|███▍      | 107/313 [00:37<01:00,  3.38it/s]Evaluating:  35%|███▍      | 108/313 [00:37<01:01,  3.34it/s]Evaluating:  35%|███▍      | 109/313 [00:37<01:00,  3.38it/s]Evaluating:  35%|███▌      | 110/313 [00:38<01:00,  3.35it/s]Evaluating:  35%|███▌      | 111/313 [00:38<00:59,  3.39it/s]Evaluating:  36%|███▌      | 112/313 [00:38<01:00,  3.35it/s]Evaluating:  36%|███▌      | 113/313 [00:39<00:59,  3.38it/s]Evaluating:  36%|███▋      | 114/313 [00:39<00:59,  3.35it/s]Evaluating:  37%|███▋      | 115/313 [00:39<00:58,  3.37it/s]Evaluating:  37%|███▋      | 116/313 [00:39<00:58,  3.38it/s]Evaluating:  37%|███▋      | 117/313 [00:40<00:58,  3.34it/s]Evaluating:  38%|███▊      | 118/313 [00:40<00:57,  3.37it/s]Evaluating:  38%|███▊      | 119/313 [00:40<00:58,  3.33it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  38%|███▊      | 120/313 [00:41<00:57,  3.37it/s]10/10/2021 15:37:53 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:37:53 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  39%|███▊      | 121/313 [00:41<00:57,  3.33it/s]Evaluating:  39%|███▉      | 122/313 [00:41<00:56,  3.36it/s]Evaluating:  39%|███▉      | 123/313 [00:42<00:57,  3.32it/s]Evaluating:  40%|███▉      | 124/313 [00:42<00:56,  3.37it/s]Evaluating:  40%|███▉      | 125/313 [00:42<00:56,  3.34it/s]Evaluating:  40%|████      | 126/313 [00:42<00:55,  3.38it/s]Evaluating:  41%|████      | 127/313 [00:43<00:55,  3.35it/s]Evaluating:  41%|████      | 128/313 [00:43<00:54,  3.39it/s]Evaluating:  41%|████      | 129/313 [00:43<00:54,  3.41it/s]Evaluating:  42%|████▏     | 130/313 [00:44<00:54,  3.35it/s]Evaluating:  42%|████▏     | 131/313 [00:44<00:53,  3.38it/s]Evaluating:  42%|████▏     | 132/313 [00:44<00:54,  3.35it/s]Evaluating:  42%|████▏     | 133/313 [00:44<00:53,  3.37it/s]Evaluating:  43%|████▎     | 134/313 [00:45<00:53,  3.34it/s]Evaluating:  43%|████▎     | 135/313 [00:45<00:52,  3.37it/s]Evaluating:  43%|████▎     | 136/313 [00:45<00:54,  3.23it/s]Evaluating:  44%|████▍     | 137/313 [00:46<00:54,  3.24it/s]Evaluating:  44%|████▍     | 138/313 [00:46<00:53,  3.26it/s]Evaluating:  44%|████▍     | 139/313 [00:46<00:53,  3.28it/s]Evaluating:  45%|████▍     | 140/313 [00:47<00:51,  3.34it/s]Evaluating:  45%|████▌     | 141/313 [00:47<00:51,  3.31it/s]Evaluating:  45%|████▌     | 142/313 [00:47<00:51,  3.35it/s]Evaluating:  46%|████▌     | 143/313 [00:47<00:51,  3.32it/s]Evaluating:  46%|████▌     | 144/313 [00:48<00:50,  3.36it/s]Evaluating:  46%|████▋     | 145/313 [00:48<00:50,  3.32it/s]Evaluating:  47%|████▋     | 146/313 [00:48<00:49,  3.39it/s]Evaluating:  47%|████▋     | 147/313 [00:49<00:48,  3.40it/s]Evaluating:  47%|████▋     | 148/313 [00:49<00:49,  3.35it/s]Evaluating:  48%|████▊     | 149/313 [00:49<00:49,  3.32it/s]Evaluating:  48%|████▊     | 150/313 [00:50<00:48,  3.36it/s]Evaluating:  48%|████▊     | 151/313 [00:50<00:47,  3.39it/s]Evaluating:  49%|████▊     | 152/313 [00:50<00:48,  3.35it/s]Evaluating:  49%|████▉     | 153/313 [00:50<00:47,  3.34it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  49%|████▉     | 154/313 [00:51<00:47,  3.34it/s]Evaluating:  50%|████▉     | 155/313 [00:51<00:46,  3.37it/s]Evaluating:  50%|████▉     | 156/313 [00:51<00:47,  3.34it/s]Evaluating:  50%|█████     | 157/313 [00:52<00:46,  3.37it/s]Evaluating:  50%|█████     | 158/313 [00:52<00:45,  3.39it/s]Evaluating:  51%|█████     | 159/313 [00:52<00:45,  3.35it/s]Evaluating:  51%|█████     | 160/313 [00:53<00:45,  3.33it/s]Evaluating:  51%|█████▏    | 161/313 [00:53<00:44,  3.40it/s]Evaluating:  52%|█████▏    | 162/313 [00:53<00:44,  3.43it/s]Evaluating:  52%|█████▏    | 163/313 [00:53<00:44,  3.36it/s]Evaluating:  52%|█████▏    | 164/313 [00:54<00:44,  3.33it/s]Evaluating:  53%|█████▎    | 165/313 [00:54<00:43,  3.37it/s]Evaluating:  53%|█████▎    | 166/313 [00:54<00:43,  3.39it/s]Evaluating:  53%|█████▎    | 167/313 [00:55<00:43,  3.33it/s]Evaluating:  54%|█████▎    | 168/313 [00:55<00:43,  3.35it/s]Evaluating:  54%|█████▍    | 169/313 [00:55<00:42,  3.40it/s]Evaluating:  54%|█████▍    | 170/313 [00:56<00:45,  3.15it/s]Evaluating:  55%|█████▍    | 171/313 [00:56<00:43,  3.23it/s]Evaluating:  55%|█████▍    | 172/313 [00:56<00:41,  3.36it/s]Evaluating:  55%|█████▌    | 173/313 [00:56<00:41,  3.39it/s]Evaluating:  56%|█████▌    | 174/313 [00:57<00:40,  3.39it/s]Evaluating:  56%|█████▌    | 175/313 [00:57<00:40,  3.43it/s]Evaluating:  56%|█████▌    | 176/313 [00:57<00:39,  3.50it/s]Evaluating:  57%|█████▋    | 177/313 [00:58<00:39,  3.48it/s]Evaluating:  57%|█████▋    | 178/313 [00:58<00:39,  3.46it/s]Evaluating:  57%|█████▋    | 179/313 [00:58<00:38,  3.45it/s]Evaluating:  58%|█████▊    | 180/313 [00:58<00:38,  3.45it/s]Evaluating:  58%|█████▊    | 181/313 [00:59<00:38,  3.39it/s]Evaluating:  58%|█████▊    | 182/313 [00:59<00:39,  3.35it/s]Evaluating:  58%|█████▊    | 183/313 [00:59<00:38,  3.40it/s]Evaluating:  59%|█████▉    | 184/313 [01:00<00:38,  3.39it/s]Evaluating:  59%|█████▉    | 185/313 [01:00<00:38,  3.34it/s]Evaluating:  59%|█████▉    | 186/313 [01:00<00:38,  3.32it/s]Evaluating:  60%|█████▉    | 187/313 [01:01<00:38,  3.26it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  60%|██████    | 188/313 [01:01<00:37,  3.37it/s]Evaluating:  60%|██████    | 189/313 [01:01<00:36,  3.42it/s]Evaluating:  61%|██████    | 190/313 [01:01<00:35,  3.48it/s]Evaluating:  61%|██████    | 191/313 [01:02<00:34,  3.56it/s]Evaluating:  61%|██████▏   | 192/313 [01:02<00:34,  3.55it/s]Evaluating:  62%|██████▏   | 193/313 [01:02<00:34,  3.51it/s]Evaluating:  62%|██████▏   | 194/313 [01:03<00:33,  3.58it/s]Evaluating:  62%|██████▏   | 195/313 [01:03<00:33,  3.51it/s]Evaluating:  63%|██████▎   | 196/313 [01:03<00:33,  3.48it/s]Evaluating:  63%|██████▎   | 197/313 [01:03<00:33,  3.42it/s]Evaluating:  63%|██████▎   | 198/313 [01:04<00:32,  3.49it/s]Evaluating:  64%|██████▎   | 199/313 [01:04<00:33,  3.42it/s]Evaluating:  64%|██████▍   | 200/313 [01:04<00:33,  3.37it/s]Evaluating:  64%|██████▍   | 201/313 [01:05<00:33,  3.33it/s]Evaluating:  65%|██████▍   | 202/313 [01:05<00:32,  3.42it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  65%|██████▍   | 203/313 [01:05<00:32,  3.39it/s]Evaluating:  65%|██████▌   | 204/313 [01:05<00:32,  3.36it/s]Evaluating:  65%|██████▌   | 205/313 [01:06<00:32,  3.37it/s]Evaluating:  66%|██████▌   | 206/313 [01:06<00:31,  3.41it/s]Evaluating:  66%|██████▌   | 207/313 [01:06<00:31,  3.36it/s]Evaluating:  66%|██████▋   | 208/313 [01:07<00:31,  3.32it/s]Evaluating:  67%|██████▋   | 209/313 [01:07<00:30,  3.41it/s]Evaluating:  67%|██████▋   | 210/313 [01:07<00:30,  3.36it/s]Evaluating:  67%|██████▋   | 211/313 [01:08<00:30,  3.32it/s]Evaluating:  68%|██████▊   | 212/313 [01:08<00:30,  3.30it/s]Evaluating:  68%|██████▊   | 213/313 [01:08<00:29,  3.40it/s]Evaluating:  68%|██████▊   | 214/313 [01:08<00:29,  3.36it/s]Evaluating:  69%|██████▊   | 215/313 [01:09<00:29,  3.32it/s]Evaluating:  69%|██████▉   | 216/313 [01:09<00:29,  3.34it/s]Evaluating:  69%|██████▉   | 217/313 [01:09<00:28,  3.37it/s]Evaluating:  70%|██████▉   | 218/313 [01:10<00:28,  3.33it/s]Evaluating:  70%|██████▉   | 219/313 [01:10<00:28,  3.30it/s]Evaluating:  70%|███████   | 220/313 [01:10<00:27,  3.40it/s]Evaluating:  71%|███████   | 221/313 [01:11<00:27,  3.35it/s]Evaluating:  71%|███████   | 222/313 [01:11<00:27,  3.33it/s]Evaluating:  71%|███████   | 223/313 [01:11<00:27,  3.30it/s]Evaluating:  72%|███████▏  | 224/313 [01:11<00:26,  3.40it/s]Evaluating:  72%|███████▏  | 225/313 [01:12<00:26,  3.35it/s]Evaluating:  72%|███████▏  | 226/313 [01:12<00:26,  3.32it/s]Evaluating:  73%|███████▎  | 227/313 [01:12<00:25,  3.34it/s]Evaluating:  73%|███████▎  | 228/313 [01:13<00:25,  3.37it/s]Evaluating:  73%|███████▎  | 229/313 [01:13<00:25,  3.33it/s]Evaluating:  73%|███████▎  | 230/313 [01:13<00:25,  3.30it/s]Evaluating:  74%|███████▍  | 231/313 [01:14<00:24,  3.39it/s]Evaluating:  74%|███████▍  | 232/313 [01:14<00:24,  3.35it/s]Evaluating:  74%|███████▍  | 233/313 [01:14<00:24,  3.32it/s]Evaluating:  75%|███████▍  | 234/313 [01:14<00:23,  3.29it/s]Evaluating:  75%|███████▌  | 235/313 [01:15<00:22,  3.40it/s]Evaluating:  75%|███████▌  | 236/313 [01:15<00:22,  3.35it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  76%|███████▌  | 237/313 [01:15<00:22,  3.32it/s]Evaluating:  76%|███████▌  | 238/313 [01:16<00:22,  3.36it/s]Evaluating:  76%|███████▋  | 239/313 [01:16<00:22,  3.34it/s]Evaluating:  77%|███████▋  | 240/313 [01:16<00:22,  3.27it/s]Evaluating:  77%|███████▋  | 241/313 [01:17<00:22,  3.25it/s]Evaluating:  77%|███████▋  | 242/313 [01:17<00:21,  3.36it/s]Evaluating:  78%|███████▊  | 243/313 [01:17<00:21,  3.32it/s]Evaluating:  78%|███████▊  | 244/313 [01:17<00:20,  3.30it/s]Evaluating:  78%|███████▊  | 245/313 [01:18<00:20,  3.28it/s]Evaluating:  79%|███████▊  | 246/313 [01:18<00:19,  3.37it/s]Evaluating:  79%|███████▉  | 247/313 [01:18<00:19,  3.32it/s]Evaluating:  79%|███████▉  | 248/313 [01:19<00:19,  3.29it/s]Evaluating:  80%|███████▉  | 249/313 [01:19<00:18,  3.39it/s]Evaluating:  80%|███████▉  | 250/313 [01:19<00:18,  3.34it/s]Evaluating:  80%|████████  | 251/313 [01:20<00:18,  3.30it/s]Evaluating:  81%|████████  | 252/313 [01:20<00:18,  3.29it/s]Evaluating:  81%|████████  | 253/313 [01:20<00:17,  3.39it/s]Evaluating:  81%|████████  | 254/313 [01:20<00:17,  3.34it/s]Evaluating:  81%|████████▏ | 255/313 [01:21<00:17,  3.31it/s]Evaluating:  82%|████████▏ | 256/313 [01:21<00:17,  3.29it/s]Evaluating:  82%|████████▏ | 257/313 [01:21<00:16,  3.39it/s]Evaluating:  82%|████████▏ | 258/313 [01:22<00:16,  3.35it/s]Evaluating:  83%|████████▎ | 259/313 [01:22<00:16,  3.31it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  83%|████████▎ | 260/313 [01:22<00:15,  3.39it/s]10/10/2021 15:38:35 - INFO - __main__ -   Using lang2id = None
Evaluating:  83%|████████▎ | 261/313 [01:23<00:15,  3.33it/s]Evaluating:  84%|████████▎ | 262/313 [01:23<00:15,  3.30it/s]Evaluating:  84%|████████▍ | 263/313 [01:23<00:15,  3.30it/s]Evaluating:  84%|████████▍ | 264/313 [01:23<00:14,  3.42it/s]Evaluating:  85%|████████▍ | 265/313 [01:24<00:14,  3.35it/s]Evaluating:  85%|████████▍ | 266/313 [01:24<00:14,  3.31it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  85%|████████▌ | 267/313 [01:24<00:14,  3.28it/s]10/10/2021 15:38:37 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:38:37 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:38:37 - INFO - __main__ -   Seed = 52
10/10/2021 15:38:37 - INFO - root -   save model
10/10/2021 15:38:37 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:38:37 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  86%|████████▌ | 268/313 [01:25<00:13,  3.33it/s]Evaluating:  86%|████████▌ | 269/313 [01:25<00:13,  3.28it/s]Evaluating:  86%|████████▋ | 270/313 [01:25<00:13,  3.26it/s]Evaluating:  87%|████████▋ | 271/313 [01:26<00:12,  3.35it/s]Evaluating:  87%|████████▋ | 272/313 [01:26<00:12,  3.32it/s]Evaluating:  87%|████████▋ | 273/313 [01:26<00:12,  3.29it/s]Evaluating:  88%|████████▊ | 274/313 [01:27<00:11,  3.27it/s]Evaluating:  88%|████████▊ | 275/313 [01:27<00:11,  3.30it/s]Evaluating:  88%|████████▊ | 276/313 [01:27<00:11,  3.29it/s]Evaluating:  88%|████████▊ | 277/313 [01:27<00:10,  3.28it/s]Evaluating:  89%|████████▉ | 278/313 [01:28<00:10,  3.31it/s]Evaluating:  89%|████████▉ | 279/313 [01:28<00:10,  3.33it/s]Evaluating:  89%|████████▉ | 280/313 [01:28<00:10,  3.29it/s]Evaluating:  90%|████████▉ | 281/313 [01:29<00:09,  3.29it/s]Evaluating:  90%|█████████ | 282/313 [01:29<00:09,  3.32it/s]Evaluating:  90%|█████████ | 283/313 [01:29<00:09,  3.29it/s]Evaluating:  91%|█████████ | 284/313 [01:30<00:08,  3.27it/s]Evaluating:  91%|█████████ | 285/313 [01:30<00:08,  3.30it/s]Evaluating:  91%|█████████▏| 286/313 [01:30<00:08,  3.32it/s]Evaluating:  92%|█████████▏| 287/313 [01:30<00:07,  3.29it/s]Evaluating:  92%|█████████▏| 288/313 [01:31<00:07,  3.32it/s]Evaluating:  92%|█████████▏| 289/313 [01:31<00:07,  3.05it/s]Evaluating:  93%|█████████▎| 290/313 [01:31<00:07,  3.11it/s]Evaluating:  93%|█████████▎| 291/313 [01:32<00:07,  3.13it/s]Evaluating:  93%|█████████▎| 292/313 [01:32<00:06,  3.21it/s]Evaluating:  94%|█████████▎| 293/313 [01:32<00:06,  3.23it/s]Evaluating:  94%|█████████▍| 294/313 [01:33<00:05,  3.22it/s]Evaluating:  94%|█████████▍| 295/313 [01:33<00:05,  3.13it/s]Evaluating:  95%|█████████▍| 296/313 [01:33<00:05,  3.19it/s]Evaluating:  95%|█████████▍| 297/313 [01:34<00:05,  3.17it/s]Evaluating:  95%|█████████▌| 298/313 [01:34<00:04,  3.16it/s]Evaluating:  96%|█████████▌| 299/313 [01:34<00:04,  3.22it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  96%|█████████▌| 300/313 [01:35<00:04,  3.19it/s]Evaluating:  96%|█████████▌| 301/313 [01:35<00:03,  3.19it/s]Evaluating:  96%|█████████▋| 302/313 [01:35<00:03,  3.23it/s]Evaluating:  97%|█████████▋| 303/313 [01:35<00:03,  3.25it/s]Evaluating:  97%|█████████▋| 304/313 [01:36<00:02,  3.21it/s]Evaluating:  97%|█████████▋| 305/313 [01:36<00:02,  3.24it/s]Evaluating:  98%|█████████▊| 306/313 [01:36<00:02,  3.25it/s]Evaluating:  98%|█████████▊| 307/313 [01:37<00:01,  3.24it/s]Evaluating:  98%|█████████▊| 308/313 [01:37<00:01,  3.24it/s]Evaluating:  99%|█████████▊| 309/313 [01:37<00:01,  3.24it/s]Evaluating:  99%|█████████▉| 310/313 [01:38<00:00,  3.26it/s]Evaluating:  99%|█████████▉| 311/313 [01:38<00:00,  3.22it/s]Evaluating: 100%|█████████▉| 312/313 [01:38<00:00,  3.24it/s]Evaluating: 100%|██████████| 313/313 [01:38<00:00,  3.55it/s]Evaluating: 100%|██████████| 313/313 [01:38<00:00,  3.16it/s]
10/10/2021 15:38:52 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/10/2021 15:38:52 - INFO - __main__ -     f1 = 0.6892291686985923
10/10/2021 15:38:52 - INFO - __main__ -     loss = 0.967620432329254
10/10/2021 15:38:52 - INFO - __main__ -     precision = 0.6348065636603186
10/10/2021 15:38:52 - INFO - __main__ -     recall = 0.7538581893179717
10/10/2021 15:38:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:39:17 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:39:17 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:39:17 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/10/2021 15:39:17 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:39:17 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:39:17 - INFO - root -   loading lang adpater is/wiki@ukp
10/10/2021 15:39:17 - INFO - __main__ -   Language = is
10/10/2021 15:39:17 - INFO - __main__ -   Adapter Name = is/wiki@ukp
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/10/2021 15:39:23 - INFO - __main__ -   Language adapter for fo not found, using is instead
10/10/2021 15:39:23 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:39:23 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/10/2021 15:39:23 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/10/2021 15:39:23 - INFO - __main__ -     Num examples = 100
10/10/2021 15:39:23 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  3.61it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  3.76it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  3.73it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  4.93it/s]Evaluating: 100%|██████████| 4/4 [00:01<00:00,  3.97it/s]
10/10/2021 15:39:24 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/10/2021 15:39:24 - INFO - __main__ -     f1 = 0.6126760563380282
10/10/2021 15:39:24 - INFO - __main__ -     loss = 1.1771853864192963
10/10/2021 15:39:24 - INFO - __main__ -     precision = 0.5304878048780488
10/10/2021 15:39:24 - INFO - __main__ -     recall = 0.725
10/10/2021 15:39:24 - INFO - __main__ -   Language adapter for no not found, using is instead
10/10/2021 15:39:24 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:39:24 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/10/2021 15:39:25 - INFO - __main__ -   ***** Running evaluation  in no *****
10/10/2021 15:39:25 - INFO - __main__ -     Num examples = 10000
10/10/2021 15:39:25 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:11,  4.35it/s]Evaluating:   1%|          | 2/313 [00:00<01:16,  4.05it/s]Evaluating:   1%|          | 3/313 [00:00<01:18,  3.94it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:19,  3.89it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:17,  3.98it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:18,  3.92it/s]Evaluating:   2%|▏         | 7/313 [00:01<01:19,  3.87it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:20,  3.81it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:19,  3.84it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:18,  3.87it/s]Evaluating:   4%|▎         | 11/313 [00:02<01:19,  3.81it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:19,  3.78it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:17,  3.85it/s]Evaluating:   4%|▍         | 14/313 [00:03<01:20,  3.70it/s]Evaluating:   5%|▍         | 15/313 [00:03<01:20,  3.70it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:20,  3.68it/s]Evaluating:   5%|▌         | 17/313 [00:04<01:19,  3.72it/s]Evaluating:   6%|▌         | 18/313 [00:04<01:19,  3.70it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:20,  3.64it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:21,  3.58it/s]Evaluating:   7%|▋         | 21/313 [00:05<01:20,  3.61it/s]Evaluating:   7%|▋         | 22/313 [00:05<01:21,  3.55it/s]Evaluating:   7%|▋         | 23/313 [00:06<01:23,  3.47it/s]Evaluating:   8%|▊         | 24/313 [00:06<01:24,  3.40it/s]Evaluating:   8%|▊         | 25/313 [00:06<01:22,  3.47it/s]Evaluating:   8%|▊         | 26/313 [00:07<01:24,  3.41it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:24,  3.37it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:   9%|▉         | 28/313 [00:07<01:24,  3.39it/s]10/10/2021 15:39:33 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:39:33 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:   9%|▉         | 29/313 [00:07<01:22,  3.42it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:24,  3.37it/s]Evaluating:  10%|▉         | 31/313 [00:08<01:24,  3.32it/s]Evaluating:  10%|█         | 32/313 [00:08<01:23,  3.36it/s]Evaluating:  11%|█         | 33/313 [00:09<01:22,  3.38it/s]Evaluating:  11%|█         | 34/313 [00:09<01:23,  3.34it/s]Evaluating:  11%|█         | 35/313 [00:09<01:24,  3.30it/s]Evaluating:  12%|█▏        | 36/313 [00:10<01:21,  3.40it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:22,  3.35it/s]Evaluating:  12%|█▏        | 38/313 [00:10<01:23,  3.31it/s]Evaluating:  12%|█▏        | 39/313 [00:10<01:22,  3.31it/s]Evaluating:  13%|█▎        | 40/313 [00:11<01:21,  3.36it/s]Evaluating:  13%|█▎        | 41/313 [00:11<01:22,  3.31it/s]Evaluating:  13%|█▎        | 42/313 [00:11<01:22,  3.28it/s]Evaluating:  14%|█▎        | 43/313 [00:12<01:20,  3.34it/s]Evaluating:  14%|█▍        | 44/313 [00:12<01:19,  3.36it/s]Evaluating:  14%|█▍        | 45/313 [00:12<01:21,  3.30it/s]Evaluating:  15%|█▍        | 46/313 [00:13<01:21,  3.27it/s]Evaluating:  15%|█▌        | 47/313 [00:13<01:18,  3.38it/s]Evaluating:  15%|█▌        | 48/313 [00:13<01:19,  3.33it/s]Evaluating:  16%|█▌        | 49/313 [00:13<01:19,  3.30it/s]Evaluating:  16%|█▌        | 50/313 [00:14<01:19,  3.32it/s]Evaluating:  16%|█▋        | 51/313 [00:14<01:17,  3.38it/s]Evaluating:  17%|█▋        | 52/313 [00:14<01:18,  3.34it/s]Evaluating:  17%|█▋        | 53/313 [00:15<01:18,  3.31it/s]Evaluating:  17%|█▋        | 54/313 [00:15<01:17,  3.35it/s]Evaluating:  18%|█▊        | 55/313 [00:15<01:16,  3.36it/s]Evaluating:  18%|█▊        | 56/313 [00:16<01:17,  3.32it/s]Evaluating:  18%|█▊        | 57/313 [00:16<01:17,  3.30it/s]Evaluating:  19%|█▊        | 58/313 [00:16<01:15,  3.39it/s]Evaluating:  19%|█▉        | 59/313 [00:16<01:15,  3.35it/s]Evaluating:  19%|█▉        | 60/313 [00:17<01:15,  3.33it/s]Evaluating:  19%|█▉        | 61/313 [00:17<01:16,  3.32it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  20%|█▉        | 62/313 [00:17<01:13,  3.42it/s]Evaluating:  20%|██        | 63/313 [00:18<01:14,  3.37it/s]Evaluating:  20%|██        | 64/313 [00:18<01:14,  3.33it/s]Evaluating:  21%|██        | 65/313 [00:18<01:14,  3.34it/s]Evaluating:  21%|██        | 66/313 [00:19<01:12,  3.38it/s]Evaluating:  21%|██▏       | 67/313 [00:19<01:13,  3.35it/s]Evaluating:  22%|██▏       | 68/313 [00:19<01:13,  3.31it/s]Evaluating:  22%|██▏       | 69/313 [00:19<01:11,  3.41it/s]Evaluating:  22%|██▏       | 70/313 [00:20<01:12,  3.37it/s]Evaluating:  23%|██▎       | 71/313 [00:20<01:12,  3.34it/s]Evaluating:  23%|██▎       | 72/313 [00:20<01:12,  3.31it/s]Evaluating:  23%|██▎       | 73/313 [00:21<01:10,  3.40it/s]Evaluating:  24%|██▎       | 74/313 [00:21<01:11,  3.36it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  24%|██▍       | 75/313 [00:21<01:11,  3.32it/s]Evaluating:  24%|██▍       | 76/313 [00:22<01:11,  3.31it/s]Evaluating:  25%|██▍       | 77/313 [00:22<01:09,  3.40it/s]Evaluating:  25%|██▍       | 78/313 [00:22<01:09,  3.36it/s]Evaluating:  25%|██▌       | 79/313 [00:22<01:10,  3.33it/s]Evaluating:  26%|██▌       | 80/313 [00:23<01:08,  3.38it/s]Evaluating:  26%|██▌       | 81/313 [00:23<01:08,  3.40it/s]Evaluating:  26%|██▌       | 82/313 [00:23<01:16,  3.04it/s]Evaluating:  27%|██▋       | 83/313 [00:24<01:12,  3.15it/s]Evaluating:  27%|██▋       | 84/313 [00:24<01:10,  3.23it/s]Evaluating:  27%|██▋       | 85/313 [00:24<01:10,  3.24it/s]Evaluating:  27%|██▋       | 86/313 [00:25<01:09,  3.25it/s]Evaluating:  28%|██▊       | 87/313 [00:25<01:08,  3.31it/s]Evaluating:  28%|██▊       | 88/313 [00:25<01:06,  3.36it/s]Evaluating:  28%|██▊       | 89/313 [00:25<01:07,  3.34it/s]Evaluating:  29%|██▉       | 90/313 [00:26<01:06,  3.34it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  29%|██▉       | 91/313 [00:26<01:05,  3.37it/s]Evaluating:  29%|██▉       | 92/313 [00:26<01:05,  3.37it/s]Evaluating:  30%|██▉       | 93/313 [00:27<01:06,  3.33it/s]Evaluating:  30%|███       | 94/313 [00:27<01:05,  3.36it/s]Evaluating:  30%|███       | 95/313 [00:27<01:04,  3.39it/s]Evaluating:  31%|███       | 96/313 [00:28<01:04,  3.35it/s]Evaluating:  31%|███       | 97/313 [00:28<01:05,  3.32it/s]Evaluating:  31%|███▏      | 98/313 [00:28<01:07,  3.18it/s]Evaluating:  32%|███▏      | 99/313 [00:29<01:05,  3.25it/s]Evaluating:  32%|███▏      | 100/313 [00:29<01:05,  3.25it/s]Evaluating:  32%|███▏      | 101/313 [00:29<01:04,  3.30it/s]Evaluating:  33%|███▎      | 102/313 [00:29<01:03,  3.34it/s]Evaluating:  33%|███▎      | 103/313 [00:30<01:03,  3.30it/s]Evaluating:  33%|███▎      | 104/313 [00:30<01:03,  3.28it/s]Evaluating:  34%|███▎      | 105/313 [00:30<01:02,  3.32it/s]Evaluating:  34%|███▍      | 106/313 [00:31<01:02,  3.33it/s]Evaluating:  34%|███▍      | 107/313 [00:31<01:02,  3.30it/s]Evaluating:  35%|███▍      | 108/313 [00:31<01:02,  3.28it/s]Evaluating:  35%|███▍      | 109/313 [00:32<01:01,  3.34it/s]Evaluating:  35%|███▌      | 110/313 [00:32<01:00,  3.37it/s]Evaluating:  35%|███▌      | 111/313 [00:32<01:00,  3.32it/s]Evaluating:  36%|███▌      | 112/313 [00:32<01:00,  3.35it/s]Evaluating:  36%|███▌      | 113/313 [00:33<00:59,  3.37it/s]Evaluating:  36%|███▋      | 114/313 [00:33<00:59,  3.32it/s]Evaluating:  37%|███▋      | 115/313 [00:33<01:00,  3.28it/s]Evaluating:  37%|███▋      | 116/313 [00:34<00:59,  3.34it/s]Evaluating:  37%|███▋      | 117/313 [00:34<00:58,  3.36it/s]Evaluating:  38%|███▊      | 118/313 [00:34<00:58,  3.32it/s]Evaluating:  38%|███▊      | 119/313 [00:35<00:59,  3.28it/s]Evaluating:  38%|███▊      | 120/313 [00:35<00:58,  3.32it/s]Evaluating:  39%|███▊      | 121/313 [00:35<00:57,  3.35it/s]Evaluating:  39%|███▉      | 122/313 [00:35<00:57,  3.31it/s]Evaluating:  39%|███▉      | 123/313 [00:36<00:56,  3.35it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  40%|███▉      | 124/313 [00:36<00:56,  3.37it/s]Evaluating:  40%|███▉      | 125/313 [00:36<00:56,  3.33it/s]Evaluating:  40%|████      | 126/313 [00:37<00:56,  3.30it/s]Evaluating:  41%|████      | 127/313 [00:37<00:55,  3.33it/s]Evaluating:  41%|████      | 128/313 [00:37<00:55,  3.36it/s]Evaluating:  41%|████      | 129/313 [00:38<00:55,  3.33it/s]Evaluating:  42%|████▏     | 130/313 [00:38<00:55,  3.31it/s]Evaluating:  42%|████▏     | 131/313 [00:38<00:54,  3.34it/s]Evaluating:  42%|████▏     | 132/313 [00:38<00:53,  3.36it/s]Evaluating:  42%|████▏     | 133/313 [00:39<00:54,  3.32it/s]Evaluating:  43%|████▎     | 134/313 [00:39<00:53,  3.35it/s]Evaluating:  43%|████▎     | 135/313 [00:39<00:53,  3.34it/s]Evaluating:  43%|████▎     | 136/313 [00:40<00:53,  3.33it/s]Evaluating:  44%|████▍     | 137/313 [00:40<00:53,  3.31it/s]Evaluating:  44%|████▍     | 138/313 [00:40<00:52,  3.34it/s]Evaluating:  44%|████▍     | 139/313 [00:41<00:51,  3.37it/s]Evaluating:  45%|████▍     | 140/313 [00:41<00:51,  3.34it/s]Evaluating:  45%|████▌     | 141/313 [00:41<00:51,  3.33it/s]Evaluating:  45%|████▌     | 142/313 [00:41<00:50,  3.36it/s]Evaluating:  46%|████▌     | 143/313 [00:42<00:50,  3.39it/s]Evaluating:  46%|████▌     | 144/313 [00:42<00:50,  3.34it/s]Evaluating:  46%|████▋     | 145/313 [00:42<00:49,  3.36it/s]Evaluating:  47%|████▋     | 146/313 [00:43<00:50,  3.31it/s]Evaluating:  47%|████▋     | 147/313 [00:43<00:49,  3.35it/s]Evaluating:  47%|████▋     | 148/313 [00:43<00:51,  3.23it/s]Evaluating:  48%|████▊     | 149/313 [00:44<00:50,  3.27it/s]Evaluating:  48%|████▊     | 150/313 [00:44<00:49,  3.32it/s]Evaluating:  48%|████▊     | 151/313 [00:44<00:49,  3.29it/s]Evaluating:  49%|████▊     | 152/313 [00:44<00:48,  3.31it/s]Evaluating:  49%|████▉     | 153/313 [00:45<00:48,  3.28it/s]Evaluating:  49%|████▉     | 154/313 [00:45<00:47,  3.32it/s]Evaluating:  50%|████▉     | 155/313 [00:45<00:48,  3.29it/s]Evaluating:  50%|████▉     | 156/313 [00:46<00:47,  3.33it/s]Evaluating:  50%|█████     | 157/313 [00:46<00:46,  3.32it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:40:12 - INFO - __main__ -   Using lang2id = None
Evaluating:  50%|█████     | 158/313 [00:46<00:46,  3.33it/s]Evaluating:  51%|█████     | 159/313 [00:47<00:46,  3.30it/s]Evaluating:  51%|█████     | 160/313 [00:47<00:45,  3.33it/s]Evaluating:  51%|█████▏    | 161/313 [00:47<00:44,  3.39it/s]Evaluating:  52%|█████▏    | 162/313 [00:47<00:45,  3.34it/s]Evaluating:  52%|█████▏    | 163/313 [00:48<00:44,  3.37it/s]Evaluating:  52%|█████▏    | 164/313 [00:48<00:44,  3.38it/s]PyTorch version 1.9.0+cu102 available.
Evaluating:  53%|█████▎    | 165/313 [00:48<00:45,  3.25it/s]10/10/2021 15:40:14 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:40:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/10/2021 15:40:14 - INFO - __main__ -   Seed = 52
10/10/2021 15:40:14 - INFO - root -   save model
10/10/2021 15:40:14 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_ru//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/10/2021 15:40:14 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  53%|█████▎    | 166/313 [00:49<00:45,  3.25it/s]Evaluating:  53%|█████▎    | 167/313 [00:49<00:44,  3.29it/s]Evaluating:  54%|█████▎    | 168/313 [00:49<00:44,  3.27it/s]Evaluating:  54%|█████▍    | 169/313 [00:50<00:43,  3.32it/s]Evaluating:  54%|█████▍    | 170/313 [00:50<00:42,  3.36it/s]Evaluating:  55%|█████▍    | 171/313 [00:50<00:42,  3.32it/s]Evaluating:  55%|█████▍    | 172/313 [00:50<00:42,  3.35it/s]Evaluating:  55%|█████▌    | 173/313 [00:51<00:42,  3.31it/s]Evaluating:  56%|█████▌    | 174/313 [00:51<00:41,  3.34it/s]Evaluating:  56%|█████▌    | 175/313 [00:51<00:41,  3.31it/s]Evaluating:  56%|█████▌    | 176/313 [00:52<00:40,  3.35it/s]Evaluating:  57%|█████▋    | 177/313 [00:52<00:41,  3.31it/s]Evaluating:  57%|█████▋    | 178/313 [00:52<00:40,  3.34it/s]Evaluating:  57%|█████▋    | 179/313 [00:53<00:40,  3.30it/s]Evaluating:  58%|█████▊    | 180/313 [00:53<00:39,  3.33it/s]Evaluating:  58%|█████▊    | 181/313 [00:53<00:39,  3.36it/s]Evaluating:  58%|█████▊    | 182/313 [00:53<00:39,  3.32it/s]Evaluating:  58%|█████▊    | 183/313 [00:54<00:38,  3.34it/s]Evaluating:  59%|█████▉    | 184/313 [00:54<00:38,  3.33it/s]Evaluating:  59%|█████▉    | 185/313 [00:54<00:38,  3.36it/s]Evaluating:  59%|█████▉    | 186/313 [00:55<00:38,  3.31it/s]Evaluating:  60%|█████▉    | 187/313 [00:55<00:37,  3.35it/s]Evaluating:  60%|██████    | 188/313 [00:55<00:37,  3.32it/s]Evaluating:  60%|██████    | 189/313 [00:56<00:37,  3.35it/s]Evaluating:  61%|██████    | 190/313 [00:56<00:37,  3.32it/s]Evaluating:  61%|██████    | 191/313 [00:56<00:36,  3.34it/s]Evaluating:  61%|██████▏   | 192/313 [00:56<00:35,  3.36it/s]Evaluating:  62%|██████▏   | 193/313 [00:57<00:36,  3.32it/s]Evaluating:  62%|██████▏   | 194/313 [00:57<00:35,  3.32it/s]Evaluating:  62%|██████▏   | 195/313 [00:57<00:35,  3.31it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  63%|██████▎   | 196/313 [00:58<00:35,  3.34it/s]Evaluating:  63%|██████▎   | 197/313 [00:58<00:34,  3.31it/s]Evaluating:  63%|██████▎   | 198/313 [00:58<00:40,  2.83it/s]Evaluating:  64%|██████▎   | 199/313 [00:59<00:38,  2.94it/s]Evaluating:  64%|██████▍   | 200/313 [00:59<00:37,  3.00it/s]Evaluating:  64%|██████▍   | 201/313 [00:59<00:36,  3.09it/s]Evaluating:  65%|██████▍   | 202/313 [01:00<00:34,  3.19it/s]Evaluating:  65%|██████▍   | 203/313 [01:00<00:34,  3.21it/s]Evaluating:  65%|██████▌   | 204/313 [01:00<00:33,  3.22it/s]Evaluating:  65%|██████▌   | 205/313 [01:01<00:32,  3.31it/s]Evaluating:  66%|██████▌   | 206/313 [01:01<00:32,  3.28it/s]Evaluating:  66%|██████▌   | 207/313 [01:01<00:32,  3.25it/s]Evaluating:  66%|██████▋   | 208/313 [01:01<00:32,  3.25it/s]Evaluating:  67%|██████▋   | 209/313 [01:02<00:30,  3.36it/s]Evaluating:  67%|██████▋   | 210/313 [01:02<00:31,  3.32it/s]Evaluating:  67%|██████▋   | 211/313 [01:02<00:30,  3.30it/s]Evaluating:  68%|██████▊   | 212/313 [01:03<00:29,  3.38it/s]Evaluating:  68%|██████▊   | 213/313 [01:03<00:29,  3.38it/s]Evaluating:  68%|██████▊   | 214/313 [01:03<00:29,  3.34it/s]Evaluating:  69%|██████▊   | 215/313 [01:04<00:29,  3.31it/s]Evaluating:  69%|██████▉   | 216/313 [01:04<00:28,  3.40it/s]Evaluating:  69%|██████▉   | 217/313 [01:04<00:28,  3.34it/s]Evaluating:  70%|██████▉   | 218/313 [01:04<00:28,  3.31it/s]Evaluating:  70%|██████▉   | 219/313 [01:05<00:28,  3.29it/s]Evaluating:  70%|███████   | 220/313 [01:05<00:27,  3.37it/s]Evaluating:  71%|███████   | 221/313 [01:05<00:27,  3.32it/s]Evaluating:  71%|███████   | 222/313 [01:06<00:27,  3.29it/s]Evaluating:  71%|███████   | 223/313 [01:06<00:27,  3.33it/s]Evaluating:  72%|███████▏  | 224/313 [01:06<00:27,  3.28it/s]Evaluating:  72%|███████▏  | 225/313 [01:07<00:26,  3.27it/s]Evaluating:  72%|███████▏  | 226/313 [01:07<00:26,  3.25it/s]Evaluating:  73%|███████▎  | 227/313 [01:07<00:25,  3.36it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  73%|███████▎  | 228/313 [01:07<00:25,  3.30it/s]Evaluating:  73%|███████▎  | 229/313 [01:08<00:25,  3.28it/s]Evaluating:  73%|███████▎  | 230/313 [01:08<00:25,  3.28it/s]Evaluating:  74%|███████▍  | 231/313 [01:08<00:24,  3.35it/s]Evaluating:  74%|███████▍  | 232/313 [01:09<00:24,  3.31it/s]Evaluating:  74%|███████▍  | 233/313 [01:09<00:24,  3.28it/s]Evaluating:  75%|███████▍  | 234/313 [01:09<00:23,  3.37it/s]Evaluating:  75%|███████▌  | 235/313 [01:10<00:23,  3.32it/s]Evaluating:  75%|███████▌  | 236/313 [01:10<00:23,  3.28it/s]Evaluating:  76%|███████▌  | 237/313 [01:10<00:23,  3.25it/s]Evaluating:  76%|███████▌  | 238/313 [01:10<00:22,  3.37it/s]Evaluating:  76%|███████▋  | 239/313 [01:11<00:22,  3.32it/s]Evaluating:  77%|███████▋  | 240/313 [01:11<00:22,  3.25it/s]Evaluating:  77%|███████▋  | 241/313 [01:11<00:21,  3.28it/s]Evaluating:  77%|███████▋  | 242/313 [01:12<00:21,  3.25it/s]Evaluating:  78%|███████▊  | 243/313 [01:12<00:21,  3.25it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  78%|███████▊  | 244/313 [01:12<00:21,  3.22it/s]Evaluating:  78%|███████▊  | 245/313 [01:13<00:20,  3.31it/s]Evaluating:  79%|███████▊  | 246/313 [01:13<00:20,  3.25it/s]Evaluating:  79%|███████▉  | 247/313 [01:13<00:20,  3.23it/s]Evaluating:  79%|███████▉  | 248/313 [01:14<00:20,  3.24it/s]Evaluating:  80%|███████▉  | 249/313 [01:14<00:19,  3.21it/s]Evaluating:  80%|███████▉  | 250/313 [01:14<00:19,  3.20it/s]Evaluating:  80%|████████  | 251/313 [01:15<00:19,  3.19it/s]Evaluating:  81%|████████  | 252/313 [01:15<00:18,  3.24it/s]Evaluating:  81%|████████  | 253/313 [01:15<00:18,  3.21it/s]Evaluating:  81%|████████  | 254/313 [01:15<00:18,  3.20it/s]Evaluating:  81%|████████▏ | 255/313 [01:16<00:17,  3.25it/s]Evaluating:  82%|████████▏ | 256/313 [01:16<00:17,  3.21it/s]Evaluating:  82%|████████▏ | 257/313 [01:16<00:17,  3.19it/s]Evaluating:  82%|████████▏ | 258/313 [01:17<00:17,  3.21it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  83%|████████▎ | 259/313 [01:17<00:16,  3.25it/s]Evaluating:  83%|████████▎ | 260/313 [01:17<00:16,  3.19it/s]Evaluating:  83%|████████▎ | 261/313 [01:18<00:16,  3.18it/s]Evaluating:  84%|████████▎ | 262/313 [01:18<00:15,  3.28it/s]Evaluating:  84%|████████▍ | 263/313 [01:18<00:16,  3.08it/s]Evaluating:  84%|████████▍ | 264/313 [01:19<00:15,  3.12it/s]Evaluating:  85%|████████▍ | 265/313 [01:19<00:14,  3.21it/s]Evaluating:  85%|████████▍ | 266/313 [01:19<00:14,  3.23it/s]Evaluating:  85%|████████▌ | 267/313 [01:20<00:14,  3.21it/s]Evaluating:  86%|████████▌ | 268/313 [01:20<00:14,  3.21it/s]Evaluating:  86%|████████▌ | 269/313 [01:20<00:13,  3.25it/s]Evaluating:  86%|████████▋ | 270/313 [01:20<00:13,  3.25it/s]Evaluating:  87%|████████▋ | 271/313 [01:21<00:12,  3.25it/s]Evaluating:  87%|████████▋ | 272/313 [01:21<00:12,  3.28it/s]Evaluating:  87%|████████▋ | 273/313 [01:21<00:12,  3.26it/s]Evaluating:  88%|████████▊ | 274/313 [01:22<00:12,  3.22it/s]Evaluating:  88%|████████▊ | 275/313 [01:22<00:11,  3.22it/s]Evaluating:  88%|████████▊ | 276/313 [01:22<00:11,  3.27it/s]Evaluating:  88%|████████▊ | 277/313 [01:23<00:11,  3.23it/s]Evaluating:  89%|████████▉ | 278/313 [01:23<00:10,  3.21it/s]Evaluating:  89%|████████▉ | 279/313 [01:23<00:10,  3.16it/s]Evaluating:  89%|████████▉ | 280/313 [01:24<00:10,  3.20it/s]Evaluating:  90%|████████▉ | 281/313 [01:24<00:09,  3.23it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  90%|█████████ | 282/313 [01:24<00:09,  3.31it/s]10/10/2021 15:40:50 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:40:50 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/10/2021 15:40:50 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/10/2021 15:40:50 - INFO - root -   Trying to decide if add adapter
10/10/2021 15:40:50 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/10/2021 15:40:50 - INFO - root -   loading lang adpater ru/wiki@ukp
10/10/2021 15:40:50 - INFO - __main__ -   Language = ru
10/10/2021 15:40:50 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:  90%|█████████ | 283/313 [01:24<00:08,  3.37it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Evaluating:  91%|█████████ | 284/313 [01:25<00:08,  3.33it/s]Evaluating:  91%|█████████ | 285/313 [01:25<00:08,  3.36it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Evaluating:  91%|█████████▏| 286/313 [01:25<00:08,  3.29it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
Evaluating:  92%|█████████▏| 287/313 [01:26<00:08,  3.23it/s]Evaluating:  92%|█████████▏| 288/313 [01:26<00:07,  3.18it/s]Evaluating:  92%|█████████▏| 289/313 [01:26<00:07,  3.18it/s]Evaluating:  93%|█████████▎| 290/313 [01:27<00:07,  3.19it/s]Evaluating:  93%|█████████▎| 291/313 [01:27<00:06,  3.15it/s]Evaluating:  93%|█████████▎| 292/313 [01:27<00:06,  3.17it/s]Evaluating:  94%|█████████▎| 293/313 [01:28<00:06,  3.20it/s]Evaluating:  94%|█████████▍| 294/313 [01:28<00:06,  3.17it/s]Evaluating:  94%|█████████▍| 295/313 [01:28<00:06,  2.68it/s]Evaluating:  95%|█████████▍| 296/313 [01:29<00:06,  2.83it/s]Evaluating:  95%|█████████▍| 297/313 [01:29<00:05,  2.92it/s]Evaluating:  95%|█████████▌| 298/313 [01:29<00:04,  3.02it/s]Evaluating:  96%|█████████▌| 299/313 [01:30<00:04,  3.09it/s]Evaluating:  96%|█████████▌| 300/313 [01:30<00:04,  3.08it/s]Evaluating:  96%|█████████▌| 301/313 [01:30<00:03,  3.13it/s]Evaluating:  96%|█████████▋| 302/313 [01:31<00:03,  3.17it/s]Evaluating:  97%|█████████▋| 303/313 [01:31<00:03,  3.13it/s]Evaluating:  97%|█████████▋| 304/313 [01:31<00:02,  3.13it/s]Evaluating:  97%|█████████▋| 305/313 [01:32<00:02,  3.12it/s]Evaluating:  98%|█████████▊| 306/313 [01:32<00:02,  3.14it/s]Evaluating:  98%|█████████▊| 307/313 [01:32<00:01,  3.12it/s]Evaluating:  98%|█████████▊| 308/313 [01:32<00:01,  3.15it/s]10/10/2021 15:40:58 - INFO - __main__ -   Language adapter for be not found, using ru instead
10/10/2021 15:40:58 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:40:58 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/10/2021 15:40:58 - INFO - __main__ -   ***** Running evaluation  in be *****
10/10/2021 15:40:58 - INFO - __main__ -     Num examples = 1001
10/10/2021 15:40:58 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:  99%|█████████▊| 309/313 [01:33<00:01,  3.02it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:14,  2.16it/s]Evaluating:  99%|█████████▉| 310/313 [01:33<00:01,  2.65it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:13,  2.16it/s]Evaluating:  99%|█████████▉| 311/313 [01:34<00:00,  2.46it/s]Evaluating:   9%|▉         | 3/32 [00:01<00:13,  2.20it/s]Evaluating: 100%|█████████▉| 312/313 [01:34<00:00,  2.34it/s]Evaluating:  12%|█▎        | 4/32 [00:01<00:12,  2.27it/s]Evaluating: 100%|██████████| 313/313 [01:35<00:00,  2.58it/s]Evaluating: 100%|██████████| 313/313 [01:35<00:00,  3.29it/s]Evaluating:  16%|█▌        | 5/32 [00:02<00:10,  2.47it/s]Evaluating:  19%|█▉        | 6/32 [00:02<00:09,  2.71it/s]Evaluating:  22%|██▏       | 7/32 [00:02<00:08,  2.89it/s]Evaluating:  25%|██▌       | 8/32 [00:03<00:07,  3.06it/s]
10/10/2021 15:41:02 - INFO - __main__ -   ***** Evaluation result  in no *****
10/10/2021 15:41:02 - INFO - __main__ -     f1 = 0.6709737827715356
10/10/2021 15:41:02 - INFO - __main__ -     loss = 0.9821199599546365
10/10/2021 15:41:02 - INFO - __main__ -     precision = 0.6094228370563556
10/10/2021 15:41:02 - INFO - __main__ -     recall = 0.7463546729620886
Evaluating:  28%|██▊       | 9/32 [00:03<00:07,  3.14it/s]10/10/2021 15:41:02 - INFO - __main__ -   Language adapter for da not found, using is instead
10/10/2021 15:41:02 - INFO - __main__ -   Set active language adapter to is
10/10/2021 15:41:02 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
Evaluating:  31%|███▏      | 10/32 [00:03<00:06,  3.24it/s]Evaluating:  34%|███▍      | 11/32 [00:03<00:06,  3.26it/s]Evaluating:  38%|███▊      | 12/32 [00:04<00:06,  3.33it/s]Evaluating:  41%|████      | 13/32 [00:04<00:05,  3.34it/s]10/10/2021 15:41:03 - INFO - __main__ -   ***** Running evaluation  in da *****
10/10/2021 15:41:03 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:41:03 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:  44%|████▍     | 14/32 [00:04<00:05,  3.33it/s]Evaluating:   0%|          | 1/313 [00:00<02:26,  2.14it/s]Evaluating:  47%|████▋     | 15/32 [00:05<00:05,  2.86it/s]Evaluating:   1%|          | 2/313 [00:00<02:22,  2.18it/s]Evaluating:  50%|█████     | 16/32 [00:05<00:06,  2.62it/s]Evaluating:   1%|          | 3/313 [00:01<02:23,  2.16it/s]Evaluating:  53%|█████▎    | 17/32 [00:06<00:06,  2.45it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:21,  2.18it/s]Evaluating:  56%|█████▋    | 18/32 [00:06<00:05,  2.37it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:21,  2.18it/s]Evaluating:  59%|█████▉    | 19/32 [00:07<00:05,  2.33it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:21,  2.17it/s]Evaluating:  62%|██████▎   | 20/32 [00:07<00:05,  2.26it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:21,  2.16it/s]Evaluating:  66%|██████▌   | 21/32 [00:08<00:04,  2.24it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:20,  2.17it/s]Evaluating:  69%|██████▉   | 22/32 [00:08<00:04,  2.21it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:19,  2.18it/s]Evaluating:  72%|███████▏  | 23/32 [00:08<00:04,  2.21it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:19,  2.17it/s]Evaluating:  75%|███████▌  | 24/32 [00:09<00:03,  2.19it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:18,  2.18it/s]Evaluating:  78%|███████▊  | 25/32 [00:09<00:03,  2.19it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:18,  2.17it/s]Evaluating:  81%|████████▏ | 26/32 [00:10<00:02,  2.18it/s]Evaluating:   4%|▍         | 13/313 [00:05<02:17,  2.18it/s]Evaluating:  84%|████████▍ | 27/32 [00:10<00:02,  2.19it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:18,  2.16it/s]Evaluating:  88%|████████▊ | 28/32 [00:11<00:01,  2.17it/s]Evaluating:   5%|▍         | 15/313 [00:06<02:17,  2.17it/s]Evaluating:  91%|█████████ | 29/32 [00:11<00:01,  2.18it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:17,  2.16it/s]Evaluating:  94%|█████████▍| 30/32 [00:12<00:00,  2.19it/s]Evaluating:   5%|▌         | 17/313 [00:07<02:16,  2.17it/s]Evaluating:  97%|█████████▋| 31/32 [00:12<00:00,  2.17it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:08,  2.30it/s]Evaluating: 100%|██████████| 32/32 [00:12<00:00,  2.32it/s]Evaluating: 100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
10/10/2021 15:41:12 - INFO - __main__ -   ***** Evaluation result  in be *****
10/10/2021 15:41:12 - INFO - __main__ -     f1 = 0.585812356979405
10/10/2021 15:41:12 - INFO - __main__ -     loss = 0.8808446652255952
10/10/2021 15:41:12 - INFO - __main__ -     precision = 0.5466192170818506
10/10/2021 15:41:12 - INFO - __main__ -     recall = 0.6310599835661462
10/10/2021 15:41:12 - INFO - __main__ -   Language adapter for uk not found, using ru instead
10/10/2021 15:41:12 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:41:12 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
Evaluating:   6%|▌         | 19/313 [00:08<01:55,  2.54it/s]Evaluating:   6%|▋         | 20/313 [00:08<01:46,  2.76it/s]Evaluating:   7%|▋         | 21/313 [00:09<01:40,  2.90it/s]Evaluating:   7%|▋         | 22/313 [00:09<01:35,  3.04it/s]Evaluating:   7%|▋         | 23/313 [00:09<01:33,  3.10it/s]10/10/2021 15:41:13 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/10/2021 15:41:13 - INFO - __main__ -     Num examples = 10001
10/10/2021 15:41:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   8%|▊         | 24/313 [00:10<01:41,  2.84it/s]Evaluating:   0%|          | 1/313 [00:00<02:20,  2.22it/s]Evaluating:   8%|▊         | 25/313 [00:10<01:51,  2.58it/s]Evaluating:   1%|          | 2/313 [00:00<02:23,  2.17it/s]Evaluating:   8%|▊         | 26/313 [00:11<01:56,  2.46it/s]Evaluating:   1%|          | 3/313 [00:01<02:21,  2.19it/s]Evaluating:   9%|▊         | 27/313 [00:11<02:01,  2.35it/s]Evaluating:   1%|▏         | 4/313 [00:01<02:22,  2.17it/s]Evaluating:   9%|▉         | 28/313 [00:11<02:03,  2.31it/s]Evaluating:   2%|▏         | 5/313 [00:02<02:21,  2.18it/s]Evaluating:   9%|▉         | 29/313 [00:12<02:06,  2.25it/s]Evaluating:   2%|▏         | 6/313 [00:02<02:22,  2.16it/s]Evaluating:  10%|▉         | 30/313 [00:12<02:06,  2.23it/s]Evaluating:   2%|▏         | 7/313 [00:03<02:20,  2.17it/s]Evaluating:  10%|▉         | 31/313 [00:13<02:06,  2.22it/s]Evaluating:   3%|▎         | 8/313 [00:03<02:19,  2.19it/s]Evaluating:  10%|█         | 32/313 [00:13<02:08,  2.19it/s]Evaluating:   3%|▎         | 9/313 [00:04<02:20,  2.17it/s]Evaluating:  11%|█         | 33/313 [00:14<02:08,  2.18it/s]Evaluating:   3%|▎         | 10/313 [00:04<02:18,  2.18it/s]Evaluating:  11%|█         | 34/313 [00:14<02:07,  2.18it/s]Evaluating:   4%|▎         | 11/313 [00:05<02:19,  2.17it/s]Evaluating:  11%|█         | 35/313 [00:15<02:06,  2.20it/s]Evaluating:   4%|▍         | 12/313 [00:05<02:17,  2.18it/s]Evaluating:  12%|█▏        | 36/313 [00:15<02:07,  2.17it/s]Evaluating:   4%|▍         | 13/313 [00:05<02:18,  2.17it/s]Evaluating:  12%|█▏        | 37/313 [00:15<01:54,  2.42it/s]Evaluating:   4%|▍         | 14/313 [00:06<02:37,  1.89it/s]Evaluating:  12%|█▏        | 38/313 [00:16<01:57,  2.34it/s]Evaluating:   5%|▍         | 15/313 [00:07<02:30,  1.98it/s]Evaluating:  12%|█▏        | 39/313 [00:16<01:59,  2.30it/s]Evaluating:   5%|▌         | 16/313 [00:07<02:26,  2.02it/s]Evaluating:  13%|█▎        | 40/313 [00:17<02:01,  2.24it/s]Evaluating:   5%|▌         | 17/313 [00:08<02:22,  2.08it/s]Evaluating:  13%|█▎        | 41/313 [00:17<02:01,  2.23it/s]Evaluating:   6%|▌         | 18/313 [00:08<02:20,  2.10it/s]Evaluating:  13%|█▎        | 42/313 [00:18<02:02,  2.21it/s]Evaluating:   6%|▌         | 19/313 [00:08<02:18,  2.13it/s]Evaluating:  14%|█▎        | 43/313 [00:18<02:02,  2.20it/s]Evaluating:   6%|▋         | 20/313 [00:09<02:17,  2.13it/s]Evaluating:  14%|█▍        | 44/313 [00:19<02:03,  2.18it/s]Evaluating:   7%|▋         | 21/313 [00:09<02:15,  2.16it/s]Evaluating:  14%|█▍        | 45/313 [00:19<02:03,  2.17it/s]Evaluating:   7%|▋         | 22/313 [00:10<02:13,  2.17it/s]Evaluating:  15%|█▍        | 46/313 [00:20<02:02,  2.19it/s]Evaluating:   7%|▋         | 23/313 [00:10<02:14,  2.16it/s]Evaluating:  15%|█▌        | 47/313 [00:20<02:02,  2.17it/s]Evaluating:   8%|▊         | 24/313 [00:11<02:14,  2.15it/s]Evaluating:  15%|█▌        | 48/313 [00:21<02:02,  2.16it/s]Evaluating:   8%|▊         | 25/313 [00:11<02:12,  2.17it/s]Evaluating:  16%|█▌        | 49/313 [00:21<02:01,  2.17it/s]Evaluating:   8%|▊         | 26/313 [00:12<02:11,  2.18it/s]Evaluating:  16%|█▌        | 50/313 [00:21<02:00,  2.17it/s]Evaluating:   9%|▊         | 27/313 [00:12<02:11,  2.17it/s]Evaluating:  16%|█▋        | 51/313 [00:22<02:01,  2.16it/s]Evaluating:   9%|▉         | 28/313 [00:13<02:11,  2.17it/s]Evaluating:  17%|█▋        | 52/313 [00:22<02:00,  2.17it/s]Evaluating:   9%|▉         | 29/313 [00:13<02:10,  2.17it/s]Evaluating:  17%|█▋        | 53/313 [00:23<02:00,  2.16it/s]Evaluating:  10%|▉         | 30/313 [00:14<02:09,  2.18it/s]Evaluating:  17%|█▋        | 54/313 [00:23<01:59,  2.17it/s]Evaluating:  10%|▉         | 31/313 [00:14<02:10,  2.17it/s]Evaluating:  18%|█▊        | 55/313 [00:24<01:59,  2.16it/s]Evaluating:  10%|█         | 32/313 [00:14<02:08,  2.18it/s]Evaluating:  18%|█▊        | 56/313 [00:24<01:58,  2.17it/s]Evaluating:  11%|█         | 33/313 [00:15<02:08,  2.19it/s]Evaluating:  18%|█▊        | 57/313 [00:25<01:57,  2.19it/s]Evaluating:  11%|█         | 34/313 [00:15<02:08,  2.17it/s]Evaluating:  19%|█▊        | 58/313 [00:25<01:57,  2.17it/s]Evaluating:  11%|█         | 35/313 [00:16<02:08,  2.16it/s]Evaluating:  19%|█▉        | 59/313 [00:26<01:57,  2.16it/s]Evaluating:  12%|█▏        | 36/313 [00:16<02:07,  2.17it/s]Evaluating:  19%|█▉        | 60/313 [00:26<01:56,  2.17it/s]Evaluating:  12%|█▏        | 37/313 [00:17<02:06,  2.18it/s]Evaluating:  19%|█▉        | 61/313 [00:27<01:55,  2.18it/s]Evaluating:  12%|█▏        | 38/313 [00:17<02:06,  2.17it/s]Evaluating:  20%|█▉        | 62/313 [00:27<01:56,  2.16it/s]Evaluating:  12%|█▏        | 39/313 [00:18<02:06,  2.17it/s]Evaluating:  20%|██        | 63/313 [00:27<01:55,  2.17it/s]Evaluating:  13%|█▎        | 40/313 [00:18<02:05,  2.17it/s]Evaluating:  20%|██        | 64/313 [00:28<01:55,  2.16it/s]Evaluating:  13%|█▎        | 41/313 [00:19<02:04,  2.18it/s]Evaluating:  21%|██        | 65/313 [00:28<01:54,  2.17it/s]Evaluating:  13%|█▎        | 42/313 [00:19<02:05,  2.16it/s]Evaluating:  21%|██        | 66/313 [00:29<01:54,  2.16it/s]Evaluating:  14%|█▎        | 43/313 [00:19<02:03,  2.18it/s]Evaluating:  21%|██▏       | 67/313 [00:29<01:53,  2.17it/s]Evaluating:  14%|█▍        | 44/313 [00:20<02:03,  2.17it/s]Evaluating:  22%|██▏       | 68/313 [00:30<01:52,  2.18it/s]Evaluating:  14%|█▍        | 45/313 [00:20<02:03,  2.18it/s]Evaluating:  22%|██▏       | 69/313 [00:30<01:52,  2.17it/s]Evaluating:  15%|█▍        | 46/313 [00:21<02:03,  2.16it/s]Evaluating:  22%|██▏       | 70/313 [00:31<01:52,  2.15it/s]Evaluating:  15%|█▌        | 47/313 [00:21<02:02,  2.17it/s]Evaluating:  23%|██▎       | 71/313 [00:31<01:51,  2.17it/s]Evaluating:  15%|█▌        | 48/313 [00:22<02:01,  2.18it/s]Evaluating:  23%|██▎       | 72/313 [00:32<01:50,  2.18it/s]Evaluating:  16%|█▌        | 49/313 [00:22<02:01,  2.17it/s]Evaluating:  23%|██▎       | 73/313 [00:32<01:50,  2.17it/s]Evaluating:  16%|█▌        | 50/313 [00:23<02:01,  2.16it/s]Evaluating:  24%|██▎       | 74/313 [00:33<01:50,  2.17it/s]Evaluating:  16%|█▋        | 51/313 [00:23<02:00,  2.17it/s]Evaluating:  24%|██▍       | 75/313 [00:33<01:50,  2.16it/s]Evaluating:  17%|█▋        | 52/313 [00:24<01:59,  2.18it/s]Evaluating:  24%|██▍       | 76/313 [00:33<01:49,  2.17it/s]Evaluating:  17%|█▋        | 53/313 [00:24<02:00,  2.17it/s]Evaluating:  25%|██▍       | 77/313 [00:34<01:49,  2.16it/s]Evaluating:  17%|█▋        | 54/313 [00:25<01:58,  2.18it/s]Evaluating:  25%|██▍       | 78/313 [00:34<01:48,  2.17it/s]Evaluating:  18%|█▊        | 55/313 [00:25<01:59,  2.16it/s]Evaluating:  25%|██▌       | 79/313 [00:35<01:47,  2.17it/s]Evaluating:  18%|█▊        | 56/313 [00:25<01:57,  2.18it/s]Evaluating:  26%|██▌       | 80/313 [00:35<01:47,  2.16it/s]Evaluating:  18%|█▊        | 57/313 [00:26<01:58,  2.16it/s]Evaluating:  26%|██▌       | 81/313 [00:36<01:47,  2.15it/s]Evaluating:  19%|█▊        | 58/313 [00:26<01:57,  2.17it/s]Evaluating:  26%|██▌       | 82/313 [00:36<01:46,  2.17it/s]Evaluating:  19%|█▉        | 59/313 [00:27<01:56,  2.19it/s]Evaluating:  27%|██▋       | 83/313 [00:37<01:46,  2.17it/s]Evaluating:  19%|█▉        | 60/313 [00:27<01:56,  2.17it/s]Evaluating:  27%|██▋       | 84/313 [00:37<01:46,  2.15it/s]Evaluating:  19%|█▉        | 61/313 [00:28<01:56,  2.16it/s]Evaluating:  27%|██▋       | 85/313 [00:38<01:46,  2.15it/s]Evaluating:  20%|█▉        | 62/313 [00:28<01:55,  2.18it/s]Evaluating:  27%|██▋       | 86/313 [00:38<01:46,  2.14it/s]Evaluating:  20%|██        | 63/313 [00:29<01:54,  2.19it/s]Evaluating:  28%|██▊       | 87/313 [00:39<01:44,  2.16it/s]Evaluating:  20%|██        | 64/313 [00:29<01:54,  2.17it/s]Evaluating:  28%|██▊       | 88/313 [00:39<01:44,  2.15it/s]Evaluating:  21%|██        | 65/313 [00:30<01:54,  2.17it/s]Evaluating:  28%|██▊       | 89/313 [00:39<01:43,  2.16it/s]Evaluating:  21%|██        | 66/313 [00:30<01:54,  2.17it/s]Evaluating:  29%|██▉       | 90/313 [00:40<01:43,  2.16it/s]Evaluating:  21%|██▏       | 67/313 [00:31<01:52,  2.18it/s]Evaluating:  29%|██▉       | 91/313 [00:40<01:42,  2.16it/s]Evaluating:  22%|██▏       | 68/313 [00:31<01:53,  2.16it/s]Evaluating:  29%|██▉       | 92/313 [00:41<01:41,  2.17it/s]Evaluating:  22%|██▏       | 69/313 [00:31<01:53,  2.14it/s]Evaluating:  30%|██▉       | 93/313 [00:41<01:41,  2.17it/s]Evaluating:  22%|██▏       | 70/313 [00:32<01:52,  2.16it/s]Evaluating:  30%|███       | 94/313 [00:42<01:40,  2.18it/s]Evaluating:  23%|██▎       | 71/313 [00:32<01:52,  2.15it/s]Evaluating:  30%|███       | 95/313 [00:42<01:40,  2.16it/s]Evaluating:  23%|██▎       | 72/313 [00:33<01:52,  2.15it/s]Evaluating:  31%|███       | 96/313 [00:43<01:40,  2.16it/s]Evaluating:  23%|██▎       | 73/313 [00:33<01:50,  2.17it/s]Evaluating:  31%|███       | 97/313 [00:43<01:40,  2.15it/s]Evaluating:  24%|██▎       | 74/313 [00:34<01:49,  2.18it/s]Evaluating:  31%|███▏      | 98/313 [00:44<01:38,  2.17it/s]Evaluating:  24%|██▍       | 75/313 [00:34<01:49,  2.17it/s]Evaluating:  32%|███▏      | 99/313 [00:44<01:39,  2.15it/s]Evaluating:  24%|██▍       | 76/313 [00:35<01:49,  2.17it/s]Evaluating:  32%|███▏      | 100/313 [00:45<01:38,  2.17it/s]Evaluating:  25%|██▍       | 77/313 [00:35<01:48,  2.17it/s]Evaluating:  32%|███▏      | 101/313 [00:45<01:37,  2.17it/s]Evaluating:  25%|██▍       | 78/313 [00:36<01:47,  2.18it/s]Evaluating:  33%|███▎      | 102/313 [00:45<01:37,  2.16it/s]Evaluating:  25%|██▌       | 79/313 [00:36<01:47,  2.18it/s]Evaluating:  33%|███▎      | 103/313 [00:46<01:37,  2.16it/s]Evaluating:  26%|██▌       | 80/313 [00:37<01:47,  2.16it/s]Evaluating:  33%|███▎      | 104/313 [00:46<01:36,  2.17it/s]Evaluating:  26%|██▌       | 81/313 [00:37<01:46,  2.17it/s]Evaluating:  34%|███▎      | 105/313 [00:47<01:35,  2.17it/s]Evaluating:  26%|██▌       | 82/313 [00:37<01:46,  2.17it/s]Evaluating:  34%|███▍      | 106/313 [00:47<01:36,  2.15it/s]Evaluating:  27%|██▋       | 83/313 [00:38<01:46,  2.16it/s]Evaluating:  34%|███▍      | 107/313 [00:48<01:35,  2.15it/s]Evaluating:  27%|██▋       | 84/313 [00:38<01:45,  2.18it/s]Evaluating:  35%|███▍      | 108/313 [00:48<01:35,  2.15it/s]Evaluating:  27%|██▋       | 85/313 [00:39<01:44,  2.19it/s]Evaluating:  35%|███▍      | 109/313 [00:49<01:34,  2.16it/s]Evaluating:  27%|██▋       | 86/313 [00:39<01:44,  2.17it/s]Evaluating:  35%|███▌      | 110/313 [00:49<01:34,  2.15it/s]Evaluating:  28%|██▊       | 87/313 [00:40<01:44,  2.17it/s]Evaluating:  35%|███▌      | 111/313 [00:50<01:33,  2.16it/s]Evaluating:  28%|██▊       | 88/313 [00:40<01:43,  2.17it/s]Evaluating:  36%|███▌      | 112/313 [00:50<01:32,  2.16it/s]Evaluating:  28%|██▊       | 89/313 [00:41<01:42,  2.18it/s]Evaluating:  36%|███▌      | 113/313 [00:51<01:32,  2.15it/s]Evaluating:  29%|██▉       | 90/313 [00:41<01:42,  2.17it/s]Evaluating:  36%|███▋      | 114/313 [00:51<01:33,  2.13it/s]Evaluating:  29%|██▉       | 91/313 [00:42<01:41,  2.19it/s]Evaluating:  37%|███▋      | 115/313 [00:52<01:32,  2.14it/s]Evaluating:  29%|██▉       | 92/313 [00:42<01:41,  2.19it/s]Evaluating:  37%|███▋      | 116/313 [00:52<01:31,  2.16it/s]Evaluating:  30%|██▉       | 93/313 [00:43<01:41,  2.18it/s]Evaluating:  37%|███▋      | 117/313 [00:52<01:31,  2.14it/s]Evaluating:  30%|███       | 94/313 [00:43<01:41,  2.17it/s]Evaluating:  38%|███▊      | 118/313 [00:53<01:31,  2.14it/s]Evaluating:  30%|███       | 95/313 [00:43<01:39,  2.18it/s]Evaluating:  38%|███▊      | 119/313 [00:53<01:30,  2.14it/s]Evaluating:  31%|███       | 96/313 [00:44<01:38,  2.19it/s]Evaluating:  38%|███▊      | 120/313 [00:54<01:29,  2.16it/s]Evaluating:  31%|███       | 97/313 [00:44<01:39,  2.18it/s]Evaluating:  39%|███▊      | 121/313 [00:54<01:29,  2.14it/s]Evaluating:  31%|███▏      | 98/313 [00:45<01:39,  2.17it/s]Evaluating:  39%|███▉      | 122/313 [00:55<01:28,  2.16it/s]Evaluating:  32%|███▏      | 99/313 [00:45<01:38,  2.17it/s]Evaluating:  39%|███▉      | 123/313 [00:55<01:27,  2.17it/s]Evaluating:  32%|███▏      | 100/313 [00:46<01:37,  2.18it/s]Evaluating:  40%|███▉      | 124/313 [00:56<01:27,  2.15it/s]Evaluating:  32%|███▏      | 101/313 [00:46<01:37,  2.17it/s]Evaluating:  40%|███▉      | 125/313 [00:56<01:26,  2.18it/s]Evaluating:  33%|███▎      | 102/313 [00:47<01:40,  2.10it/s]Evaluating:  40%|████      | 126/313 [00:57<01:25,  2.18it/s]Evaluating:  33%|███▎      | 103/313 [00:47<01:38,  2.13it/s]Evaluating:  41%|████      | 127/313 [00:57<01:25,  2.19it/s]Evaluating:  33%|███▎      | 104/313 [00:48<01:38,  2.13it/s]Evaluating:  41%|████      | 128/313 [00:58<01:25,  2.16it/s]Evaluating:  34%|███▎      | 105/313 [00:48<01:37,  2.13it/s]Evaluating:  41%|████      | 129/313 [00:58<01:24,  2.17it/s]Evaluating:  34%|███▍      | 106/313 [00:49<01:36,  2.15it/s]Evaluating:  42%|████▏     | 130/313 [00:58<01:24,  2.16it/s]Evaluating:  34%|███▍      | 107/313 [00:49<01:35,  2.17it/s]Evaluating:  42%|████▏     | 131/313 [00:59<01:23,  2.17it/s]Evaluating:  35%|███▍      | 108/313 [00:49<01:35,  2.15it/s]Evaluating:  42%|████▏     | 132/313 [00:59<01:24,  2.15it/s]Evaluating:  35%|███▍      | 109/313 [00:50<01:33,  2.17it/s]Evaluating:  42%|████▏     | 133/313 [01:00<01:23,  2.16it/s]Evaluating:  35%|███▌      | 110/313 [00:50<01:34,  2.16it/s]Evaluating:  43%|████▎     | 134/313 [01:00<01:22,  2.18it/s]Evaluating:  35%|███▌      | 111/313 [00:51<01:33,  2.17it/s]Evaluating:  43%|████▎     | 135/313 [01:01<01:22,  2.16it/s]Evaluating:  36%|███▌      | 112/313 [00:51<01:33,  2.15it/s]Evaluating:  43%|████▎     | 136/313 [01:01<01:22,  2.15it/s]Evaluating:  36%|███▌      | 113/313 [00:52<01:32,  2.17it/s]Evaluating:  44%|████▍     | 137/313 [01:02<01:21,  2.15it/s]Evaluating:  36%|███▋      | 114/313 [00:52<01:31,  2.18it/s]Evaluating:  44%|████▍     | 138/313 [01:02<01:20,  2.17it/s]Evaluating:  37%|███▋      | 115/313 [00:53<01:31,  2.16it/s]Evaluating:  44%|████▍     | 139/313 [01:03<01:20,  2.16it/s]Evaluating:  37%|███▋      | 116/313 [00:53<01:31,  2.15it/s]Evaluating:  45%|████▍     | 140/313 [01:03<01:20,  2.16it/s]Evaluating:  37%|███▋      | 117/313 [00:54<01:30,  2.17it/s]Evaluating:  45%|████▌     | 141/313 [01:04<01:20,  2.15it/s]Evaluating:  38%|███▊      | 118/313 [00:54<01:29,  2.18it/s]Evaluating:  45%|████▌     | 142/313 [01:04<01:19,  2.16it/s]Evaluating:  38%|███▊      | 119/313 [00:55<01:29,  2.17it/s]Evaluating:  46%|████▌     | 143/313 [01:04<01:19,  2.15it/s]Evaluating:  38%|███▊      | 120/313 [00:55<01:28,  2.18it/s]Evaluating:  46%|████▌     | 144/313 [01:05<01:18,  2.16it/s]Evaluating:  39%|███▊      | 121/313 [00:55<01:28,  2.16it/s]Evaluating:  46%|████▋     | 145/313 [01:05<01:17,  2.18it/s]Evaluating:  39%|███▉      | 122/313 [00:56<01:27,  2.18it/s]Evaluating:  47%|████▋     | 146/313 [01:06<01:17,  2.15it/s]Evaluating:  39%|███▉      | 123/313 [00:56<01:27,  2.17it/s]Evaluating:  47%|████▋     | 147/313 [01:06<01:17,  2.14it/s]Evaluating:  40%|███▉      | 124/313 [00:57<01:26,  2.18it/s]Evaluating:  47%|████▋     | 148/313 [01:07<01:16,  2.16it/s]Evaluating:  40%|███▉      | 125/313 [00:57<01:26,  2.17it/s]Evaluating:  48%|████▊     | 149/313 [01:07<01:15,  2.18it/s]Evaluating:  40%|████      | 126/313 [00:58<01:26,  2.16it/s]Evaluating:  48%|████▊     | 150/313 [01:08<01:15,  2.17it/s]Evaluating:  41%|████      | 127/313 [00:58<01:26,  2.15it/s]Evaluating:  48%|████▊     | 151/313 [01:08<01:14,  2.17it/s]Evaluating:  41%|████      | 128/313 [00:59<01:25,  2.16it/s]Evaluating:  49%|████▊     | 152/313 [01:09<01:14,  2.16it/s]Evaluating:  41%|████      | 129/313 [00:59<01:24,  2.17it/s]Evaluating:  49%|████▉     | 153/313 [01:09<01:13,  2.17it/s]Evaluating:  42%|████▏     | 130/313 [01:00<01:24,  2.16it/s]Evaluating:  49%|████▉     | 154/313 [01:10<01:13,  2.16it/s]Evaluating:  42%|████▏     | 131/313 [01:00<01:23,  2.17it/s]Evaluating:  50%|████▉     | 155/313 [01:10<01:12,  2.17it/s]Evaluating:  42%|████▏     | 132/313 [01:01<01:23,  2.16it/s]Evaluating:  50%|████▉     | 156/313 [01:10<01:11,  2.18it/s]Evaluating:  42%|████▏     | 133/313 [01:01<01:22,  2.17it/s]Evaluating:  50%|█████     | 157/313 [01:11<01:12,  2.15it/s]Evaluating:  43%|████▎     | 134/313 [01:01<01:22,  2.16it/s]Evaluating:  50%|█████     | 158/313 [01:11<01:12,  2.15it/s]Evaluating:  43%|████▎     | 135/313 [01:02<01:22,  2.17it/s]Evaluating:  51%|█████     | 159/313 [01:12<01:11,  2.16it/s]Evaluating:  43%|████▎     | 136/313 [01:02<01:21,  2.18it/s]Evaluating:  51%|█████     | 160/313 [01:12<01:10,  2.17it/s]Evaluating:  44%|████▍     | 137/313 [01:03<01:21,  2.17it/s]Evaluating:  51%|█████▏    | 161/313 [01:13<01:10,  2.15it/s]Evaluating:  44%|████▍     | 138/313 [01:03<01:21,  2.15it/s]Evaluating:  52%|█████▏    | 162/313 [01:13<01:09,  2.17it/s]Evaluating:  44%|████▍     | 139/313 [01:04<01:20,  2.17it/s]Evaluating:  52%|█████▏    | 163/313 [01:14<01:09,  2.15it/s]Evaluating:  45%|████▍     | 140/313 [01:04<01:19,  2.18it/s]Evaluating:  52%|█████▏    | 164/313 [01:14<01:08,  2.17it/s]Evaluating:  45%|████▌     | 141/313 [01:05<01:19,  2.16it/s]Evaluating:  53%|█████▎    | 165/313 [01:15<01:08,  2.15it/s]Evaluating:  45%|████▌     | 142/313 [01:05<01:18,  2.17it/s]Evaluating:  53%|█████▎    | 166/313 [01:15<01:07,  2.17it/s]Evaluating:  46%|████▌     | 143/313 [01:06<01:18,  2.16it/s]Evaluating:  53%|█████▎    | 167/313 [01:16<01:07,  2.18it/s]Evaluating:  46%|████▌     | 144/313 [01:06<01:17,  2.18it/s]Evaluating:  54%|█████▎    | 168/313 [01:16<01:07,  2.15it/s]Evaluating:  46%|████▋     | 145/313 [01:07<01:16,  2.20it/s]Evaluating:  54%|█████▍    | 169/313 [01:17<01:09,  2.09it/s]Evaluating:  47%|████▋     | 146/313 [01:07<01:15,  2.21it/s]Evaluating:  54%|█████▍    | 170/313 [01:17<01:08,  2.10it/s]Evaluating:  47%|████▋     | 147/313 [01:07<01:15,  2.20it/s]Evaluating:  55%|█████▍    | 171/313 [01:17<01:06,  2.13it/s]Evaluating:  47%|████▋     | 148/313 [01:08<01:15,  2.18it/s]Evaluating:  55%|█████▍    | 172/313 [01:18<01:06,  2.13it/s]Evaluating:  48%|████▊     | 149/313 [01:08<01:15,  2.16it/s]Evaluating:  55%|█████▌    | 173/313 [01:18<01:05,  2.15it/s]Evaluating:  48%|████▊     | 150/313 [01:09<01:15,  2.17it/s]Evaluating:  56%|█████▌    | 174/313 [01:19<01:04,  2.15it/s]Evaluating:  48%|████▊     | 151/313 [01:09<01:14,  2.18it/s]Evaluating:  56%|█████▌    | 175/313 [01:19<01:04,  2.15it/s]Evaluating:  49%|████▊     | 152/313 [01:10<01:14,  2.16it/s]Evaluating:  56%|█████▌    | 176/313 [01:20<01:04,  2.14it/s]Evaluating:  49%|████▉     | 153/313 [01:10<01:13,  2.17it/s]Evaluating:  57%|█████▋    | 177/313 [01:20<01:03,  2.15it/s]Evaluating:  49%|████▉     | 154/313 [01:11<01:13,  2.16it/s]Evaluating:  57%|█████▋    | 178/313 [01:21<01:02,  2.17it/s]Evaluating:  50%|████▉     | 155/313 [01:11<01:12,  2.17it/s]Evaluating:  57%|█████▋    | 179/313 [01:21<01:02,  2.15it/s]Evaluating:  50%|████▉     | 156/313 [01:12<01:12,  2.16it/s]Evaluating:  58%|█████▊    | 180/313 [01:22<01:01,  2.16it/s]Evaluating:  50%|█████     | 157/313 [01:12<01:11,  2.17it/s]Evaluating:  58%|█████▊    | 181/313 [01:22<01:01,  2.15it/s]Evaluating:  50%|█████     | 158/313 [01:13<01:11,  2.18it/s]Evaluating:  58%|█████▊    | 182/313 [01:23<01:00,  2.17it/s]Evaluating:  51%|█████     | 159/313 [01:13<01:11,  2.16it/s]Evaluating:  58%|█████▊    | 183/313 [01:23<01:00,  2.15it/s]Evaluating:  51%|█████     | 160/313 [01:13<01:11,  2.15it/s]Evaluating:  59%|█████▉    | 184/313 [01:24<00:59,  2.16it/s]Evaluating:  51%|█████▏    | 161/313 [01:14<01:10,  2.16it/s]Evaluating:  59%|█████▉    | 185/313 [01:24<00:58,  2.17it/s]Evaluating:  52%|█████▏    | 162/313 [01:14<01:09,  2.17it/s]Evaluating:  59%|█████▉    | 186/313 [01:24<00:58,  2.16it/s]Evaluating:  52%|█████▏    | 163/313 [01:15<01:09,  2.16it/s]Evaluating:  60%|█████▉    | 187/313 [01:25<00:58,  2.14it/s]Evaluating:  52%|█████▏    | 164/313 [01:15<01:08,  2.17it/s]Evaluating:  60%|██████    | 188/313 [01:25<00:58,  2.15it/s]Evaluating:  53%|█████▎    | 165/313 [01:16<01:08,  2.16it/s]Evaluating:  60%|██████    | 189/313 [01:26<00:57,  2.17it/s]Evaluating:  53%|█████▎    | 166/313 [01:16<01:07,  2.17it/s]Evaluating:  61%|██████    | 190/313 [01:26<00:57,  2.15it/s]Evaluating:  53%|█████▎    | 167/313 [01:17<01:07,  2.16it/s]Evaluating:  61%|██████    | 191/313 [01:27<00:56,  2.17it/s]Evaluating:  54%|█████▎    | 168/313 [01:17<01:06,  2.17it/s]Evaluating:  61%|██████▏   | 192/313 [01:27<00:56,  2.15it/s]Evaluating:  54%|█████▍    | 169/313 [01:18<01:06,  2.18it/s]Evaluating:  62%|██████▏   | 193/313 [01:28<00:55,  2.17it/s]Evaluating:  54%|█████▍    | 170/313 [01:18<01:06,  2.16it/s]Evaluating:  62%|██████▏   | 194/313 [01:28<00:55,  2.15it/s]Evaluating:  55%|█████▍    | 171/313 [01:19<01:06,  2.15it/s]Evaluating:  62%|██████▏   | 195/313 [01:29<00:54,  2.17it/s]Evaluating:  55%|█████▍    | 172/313 [01:19<01:05,  2.17it/s]Evaluating:  63%|██████▎   | 196/313 [01:29<00:54,  2.17it/s]Evaluating:  55%|█████▌    | 173/313 [01:19<01:04,  2.19it/s]Evaluating:  63%|██████▎   | 197/313 [01:30<00:54,  2.15it/s]Evaluating:  56%|█████▌    | 174/313 [01:20<01:04,  2.17it/s]Evaluating:  63%|██████▎   | 198/313 [01:30<00:53,  2.14it/s]Evaluating:  56%|█████▌    | 175/313 [01:20<01:03,  2.18it/s]Evaluating:  64%|██████▎   | 199/313 [01:30<00:52,  2.16it/s]Evaluating:  56%|█████▌    | 176/313 [01:21<01:03,  2.16it/s]Evaluating:  64%|██████▍   | 200/313 [01:31<00:51,  2.18it/s]Evaluating:  57%|█████▋    | 177/313 [01:21<01:02,  2.17it/s]Evaluating:  64%|██████▍   | 201/313 [01:31<00:52,  2.15it/s]Evaluating:  57%|█████▋    | 178/313 [01:22<01:02,  2.16it/s]Evaluating:  65%|██████▍   | 202/313 [01:32<00:51,  2.16it/s]Evaluating:  57%|█████▋    | 179/313 [01:22<01:01,  2.17it/s]Evaluating:  65%|██████▍   | 203/313 [01:32<00:51,  2.15it/s]Evaluating:  58%|█████▊    | 180/313 [01:23<01:00,  2.18it/s]Evaluating:  65%|██████▌   | 204/313 [01:33<00:50,  2.17it/s]Evaluating:  58%|█████▊    | 181/313 [01:23<01:00,  2.16it/s]Evaluating:  65%|██████▌   | 205/313 [01:33<00:50,  2.15it/s]Evaluating:  58%|█████▊    | 182/313 [01:24<01:00,  2.15it/s]Evaluating:  66%|██████▌   | 206/313 [01:34<00:49,  2.17it/s]Evaluating:  58%|█████▊    | 183/313 [01:24<01:00,  2.17it/s]Evaluating:  66%|██████▌   | 207/313 [01:34<00:48,  2.18it/s]Evaluating:  59%|█████▉    | 184/313 [01:25<00:59,  2.18it/s]Evaluating:  66%|██████▋   | 208/313 [01:35<00:48,  2.16it/s]Evaluating:  59%|█████▉    | 185/313 [01:25<00:59,  2.16it/s]Evaluating:  67%|██████▋   | 209/313 [01:35<00:48,  2.15it/s]Evaluating:  59%|█████▉    | 186/313 [01:25<00:58,  2.17it/s]Evaluating:  67%|██████▋   | 210/313 [01:36<00:47,  2.15it/s]Evaluating:  60%|█████▉    | 187/313 [01:26<00:58,  2.16it/s]Evaluating:  67%|██████▋   | 211/313 [01:36<00:47,  2.17it/s]Evaluating:  60%|██████    | 188/313 [01:26<00:57,  2.18it/s]Evaluating:  68%|██████▊   | 212/313 [01:36<00:46,  2.16it/s]Evaluating:  60%|██████    | 189/313 [01:27<00:57,  2.16it/s]Evaluating:  68%|██████▊   | 213/313 [01:37<00:46,  2.17it/s]Evaluating:  61%|██████    | 190/313 [01:27<00:56,  2.17it/s]Evaluating:  68%|██████▊   | 214/313 [01:37<00:46,  2.15it/s]Evaluating:  61%|██████    | 191/313 [01:28<00:55,  2.18it/s]Evaluating:  69%|██████▊   | 215/313 [01:38<00:45,  2.17it/s]Evaluating:  61%|██████▏   | 192/313 [01:28<00:55,  2.16it/s]Evaluating:  69%|██████▉   | 216/313 [01:38<00:44,  2.16it/s]Evaluating:  62%|██████▏   | 193/313 [01:29<00:55,  2.15it/s]Evaluating:  69%|██████▉   | 217/313 [01:39<00:44,  2.17it/s]Evaluating:  62%|██████▏   | 194/313 [01:29<00:54,  2.17it/s]Evaluating:  70%|██████▉   | 218/313 [01:39<00:43,  2.18it/s]Evaluating:  62%|██████▏   | 195/313 [01:30<00:54,  2.18it/s]Evaluating:  70%|██████▉   | 219/313 [01:40<00:43,  2.16it/s]Evaluating:  63%|██████▎   | 196/313 [01:30<00:54,  2.16it/s]Evaluating:  70%|███████   | 220/313 [01:40<00:43,  2.15it/s]Evaluating:  63%|██████▎   | 197/313 [01:31<00:53,  2.17it/s]Evaluating:  71%|███████   | 221/313 [01:41<00:42,  2.17it/s]Evaluating:  63%|██████▎   | 198/313 [01:31<00:53,  2.15it/s]Evaluating:  71%|███████   | 222/313 [01:41<00:41,  2.18it/s]Evaluating:  64%|██████▎   | 199/313 [01:31<00:52,  2.17it/s]Evaluating:  71%|███████   | 223/313 [01:42<00:41,  2.16it/s]Evaluating:  64%|██████▍   | 200/313 [01:32<00:52,  2.15it/s]Evaluating:  72%|███████▏  | 224/313 [01:42<00:40,  2.17it/s]Evaluating:  64%|██████▍   | 201/313 [01:32<00:51,  2.17it/s]Evaluating:  72%|███████▏  | 225/313 [01:42<00:40,  2.16it/s]Evaluating:  65%|██████▍   | 202/313 [01:33<00:51,  2.17it/s]Evaluating:  72%|███████▏  | 226/313 [01:43<00:39,  2.18it/s]Evaluating:  65%|██████▍   | 203/313 [01:33<00:50,  2.16it/s]Evaluating:  73%|███████▎  | 227/313 [01:43<00:39,  2.16it/s]Evaluating:  65%|██████▌   | 204/313 [01:34<00:50,  2.15it/s]Evaluating:  73%|███████▎  | 228/313 [01:44<00:39,  2.16it/s]Evaluating:  65%|██████▌   | 205/313 [01:34<00:49,  2.16it/s]Evaluating:  73%|███████▎  | 229/313 [01:44<00:38,  2.18it/s]Evaluating:  66%|██████▌   | 206/313 [01:35<00:49,  2.18it/s]Evaluating:  73%|███████▎  | 230/313 [01:45<00:38,  2.16it/s]Evaluating:  66%|██████▌   | 207/313 [01:35<00:49,  2.16it/s]Evaluating:  74%|███████▍  | 231/313 [01:45<00:38,  2.15it/s]Evaluating:  66%|██████▋   | 208/313 [01:36<00:48,  2.17it/s]Evaluating:  74%|███████▍  | 232/313 [01:46<00:37,  2.15it/s]Evaluating:  67%|██████▋   | 209/313 [01:36<00:48,  2.16it/s]Evaluating:  74%|███████▍  | 233/313 [01:46<00:36,  2.17it/s]Evaluating:  67%|██████▋   | 210/313 [01:37<00:47,  2.18it/s]Evaluating:  75%|███████▍  | 234/313 [01:47<00:36,  2.15it/s]Evaluating:  67%|██████▋   | 211/313 [01:37<00:46,  2.17it/s]Evaluating:  75%|███████▌  | 235/313 [01:47<00:35,  2.18it/s]Evaluating:  68%|██████▊   | 212/313 [01:37<00:46,  2.19it/s]Evaluating:  75%|███████▌  | 236/313 [01:48<00:35,  2.16it/s]Evaluating:  68%|██████▊   | 213/313 [01:38<00:45,  2.18it/s]Evaluating:  76%|███████▌  | 237/313 [01:48<00:34,  2.18it/s]Evaluating:  68%|██████▊   | 214/313 [01:38<00:45,  2.17it/s]Evaluating:  76%|███████▌  | 238/313 [01:48<00:34,  2.16it/s]Evaluating:  69%|██████▊   | 215/313 [01:39<00:45,  2.16it/s]Evaluating:  76%|███████▋  | 239/313 [01:49<00:34,  2.17it/s]Evaluating:  69%|██████▉   | 216/313 [01:39<00:44,  2.17it/s]Evaluating:  77%|███████▋  | 240/313 [01:49<00:33,  2.17it/s]Evaluating:  69%|██████▉   | 217/313 [01:40<00:43,  2.18it/s]Evaluating:  77%|███████▋  | 241/313 [01:50<00:33,  2.16it/s]Evaluating:  70%|██████▉   | 218/313 [01:40<00:43,  2.18it/s]Evaluating:  77%|███████▋  | 242/313 [01:50<00:33,  2.15it/s]Evaluating:  70%|██████▉   | 219/313 [01:41<00:43,  2.19it/s]Evaluating:  78%|███████▊  | 243/313 [01:51<00:32,  2.16it/s]Evaluating:  70%|███████   | 220/313 [01:41<00:42,  2.17it/s]Evaluating:  78%|███████▊  | 244/313 [01:51<00:31,  2.17it/s]Evaluating:  71%|███████   | 221/313 [01:42<00:41,  2.20it/s]Evaluating:  78%|███████▊  | 245/313 [01:52<00:31,  2.16it/s]Evaluating:  71%|███████   | 222/313 [01:42<00:41,  2.19it/s]Evaluating:  79%|███████▊  | 246/313 [01:52<00:30,  2.16it/s]Evaluating:  71%|███████   | 223/313 [01:42<00:40,  2.21it/s]Evaluating:  79%|███████▉  | 247/313 [01:53<00:30,  2.14it/s]Evaluating:  72%|███████▏  | 224/313 [01:43<00:40,  2.20it/s]Evaluating:  79%|███████▉  | 248/313 [01:53<00:30,  2.16it/s]Evaluating:  72%|███████▏  | 225/313 [01:43<00:40,  2.19it/s]Evaluating:  80%|███████▉  | 249/313 [01:54<00:29,  2.14it/s]Evaluating:  72%|███████▏  | 226/313 [01:44<00:40,  2.17it/s]Evaluating:  80%|███████▉  | 250/313 [01:54<00:29,  2.15it/s]Evaluating:  73%|███████▎  | 227/313 [01:44<00:39,  2.17it/s]Evaluating:  80%|████████  | 251/313 [01:55<00:28,  2.16it/s]Evaluating:  73%|███████▎  | 228/313 [01:45<00:39,  2.18it/s]Evaluating:  81%|████████  | 252/313 [01:55<00:28,  2.15it/s]Evaluating:  73%|███████▎  | 229/313 [01:45<00:39,  2.13it/s]Evaluating:  81%|████████  | 253/313 [01:55<00:27,  2.16it/s]Evaluating:  73%|███████▎  | 230/313 [01:46<00:38,  2.15it/s]Evaluating:  81%|████████  | 254/313 [01:56<00:27,  2.15it/s]Evaluating:  74%|███████▍  | 231/313 [01:46<00:38,  2.15it/s]Evaluating:  81%|████████▏ | 255/313 [01:56<00:26,  2.16it/s]Evaluating:  74%|███████▍  | 232/313 [01:47<00:37,  2.17it/s]Evaluating:  82%|████████▏ | 256/313 [01:57<00:26,  2.15it/s]Evaluating:  74%|███████▍  | 233/313 [01:47<00:36,  2.16it/s]Evaluating:  82%|████████▏ | 257/313 [01:57<00:25,  2.17it/s]Evaluating:  75%|███████▍  | 234/313 [01:48<00:36,  2.17it/s]Evaluating:  82%|████████▏ | 258/313 [01:58<00:25,  2.17it/s]Evaluating:  75%|███████▌  | 235/313 [01:48<00:35,  2.18it/s]Evaluating:  83%|████████▎ | 259/313 [01:58<00:24,  2.16it/s]Evaluating:  75%|███████▌  | 236/313 [01:48<00:35,  2.17it/s]Evaluating:  83%|████████▎ | 260/313 [01:59<00:24,  2.15it/s]Evaluating:  76%|███████▌  | 237/313 [01:49<00:35,  2.15it/s]Evaluating:  83%|████████▎ | 261/313 [01:59<00:24,  2.17it/s]Evaluating:  76%|███████▌  | 238/313 [01:49<00:34,  2.16it/s]Evaluating:  84%|████████▎ | 262/313 [02:00<00:23,  2.17it/s]Evaluating:  76%|███████▋  | 239/313 [01:50<00:34,  2.17it/s]Evaluating:  84%|████████▍ | 263/313 [02:00<00:23,  2.16it/s]Evaluating:  77%|███████▋  | 240/313 [01:50<00:33,  2.16it/s]Evaluating:  84%|████████▍ | 264/313 [02:01<00:22,  2.17it/s]Evaluating:  77%|███████▋  | 241/313 [01:51<00:33,  2.15it/s]Evaluating:  85%|████████▍ | 265/313 [02:01<00:22,  2.16it/s]Evaluating:  77%|███████▋  | 242/313 [01:51<00:33,  2.14it/s]Evaluating:  85%|████████▍ | 266/313 [02:01<00:21,  2.17it/s]Evaluating:  78%|███████▊  | 243/313 [01:52<00:32,  2.15it/s]Evaluating:  85%|████████▌ | 267/313 [02:02<00:21,  2.16it/s]Evaluating:  78%|███████▊  | 244/313 [01:52<00:32,  2.14it/s]Evaluating:  86%|████████▌ | 268/313 [02:02<00:20,  2.17it/s]Evaluating:  78%|███████▊  | 245/313 [01:53<00:31,  2.14it/s]Evaluating:  86%|████████▌ | 269/313 [02:03<00:20,  2.18it/s]Evaluating:  79%|███████▊  | 246/313 [01:53<00:31,  2.16it/s]Evaluating:  86%|████████▋ | 270/313 [02:03<00:19,  2.17it/s]Evaluating:  79%|███████▉  | 247/313 [01:54<00:30,  2.14it/s]Evaluating:  87%|████████▋ | 271/313 [02:04<00:19,  2.15it/s]Evaluating:  79%|███████▉  | 248/313 [01:54<00:30,  2.14it/s]Evaluating:  87%|████████▋ | 272/313 [02:04<00:18,  2.17it/s]Evaluating:  80%|███████▉  | 249/313 [01:55<00:30,  2.13it/s]Evaluating:  87%|████████▋ | 273/313 [02:05<00:18,  2.18it/s]Evaluating:  80%|███████▉  | 250/313 [01:55<00:29,  2.14it/s]Evaluating:  88%|████████▊ | 274/313 [02:05<00:18,  2.17it/s]Evaluating:  80%|████████  | 251/313 [01:55<00:29,  2.13it/s]Evaluating:  88%|████████▊ | 275/313 [02:06<00:17,  2.18it/s]Evaluating:  81%|████████  | 252/313 [01:56<00:28,  2.14it/s]Evaluating:  88%|████████▊ | 276/313 [02:06<00:17,  2.16it/s]Evaluating:  81%|████████  | 253/313 [01:56<00:27,  2.16it/s]Evaluating:  88%|████████▊ | 277/313 [02:07<00:16,  2.18it/s]Evaluating:  81%|████████  | 254/313 [01:57<00:27,  2.14it/s]Evaluating:  89%|████████▉ | 278/313 [02:07<00:16,  2.17it/s]Evaluating:  81%|████████▏ | 255/313 [01:57<00:27,  2.13it/s]Evaluating:  89%|████████▉ | 279/313 [02:07<00:15,  2.20it/s]Evaluating:  82%|████████▏ | 256/313 [01:58<00:26,  2.15it/s]Evaluating:  89%|████████▉ | 280/313 [02:08<00:14,  2.20it/s]Evaluating:  82%|████████▏ | 257/313 [01:58<00:25,  2.16it/s]Evaluating:  90%|████████▉ | 281/313 [02:08<00:14,  2.18it/s]Evaluating:  82%|████████▏ | 258/313 [01:59<00:25,  2.14it/s]Evaluating:  90%|█████████ | 282/313 [02:09<00:14,  2.18it/s]Evaluating:  83%|████████▎ | 259/313 [01:59<00:25,  2.15it/s]Evaluating:  90%|█████████ | 283/313 [02:09<00:13,  2.17it/s]Evaluating:  83%|████████▎ | 260/313 [02:00<00:24,  2.14it/s]Evaluating:  91%|█████████ | 284/313 [02:10<00:13,  2.19it/s]Evaluating:  83%|████████▎ | 261/313 [02:00<00:24,  2.15it/s]Evaluating:  91%|█████████ | 285/313 [02:10<00:12,  2.17it/s]Evaluating:  84%|████████▎ | 262/313 [02:01<00:23,  2.14it/s]Evaluating:  91%|█████████▏| 286/313 [02:11<00:12,  2.18it/s]Evaluating:  84%|████████▍ | 263/313 [02:01<00:23,  2.15it/s]Evaluating:  92%|█████████▏| 287/313 [02:11<00:12,  2.16it/s]Evaluating:  84%|████████▍ | 264/313 [02:02<00:22,  2.16it/s]Evaluating:  92%|█████████▏| 288/313 [02:12<00:11,  2.18it/s]Evaluating:  85%|████████▍ | 265/313 [02:02<00:22,  2.14it/s]Evaluating:  92%|█████████▏| 289/313 [02:12<00:11,  2.16it/s]Evaluating:  85%|████████▍ | 266/313 [02:02<00:20,  2.25it/s]Evaluating:  93%|█████████▎| 290/313 [02:13<00:12,  1.92it/s]Evaluating:  85%|████████▌ | 267/313 [02:03<00:21,  2.11it/s]Evaluating:  93%|█████████▎| 291/313 [02:13<00:11,  1.97it/s]Evaluating:  86%|████████▌ | 268/313 [02:03<00:21,  2.10it/s]Evaluating:  93%|█████████▎| 292/313 [02:14<00:10,  2.02it/s]Evaluating:  86%|████████▌ | 269/313 [02:04<00:21,  2.09it/s]Evaluating:  94%|█████████▎| 293/313 [02:14<00:09,  2.08it/s]Evaluating:  86%|████████▋ | 270/313 [02:04<00:20,  2.12it/s]Evaluating:  94%|█████████▍| 294/313 [02:15<00:09,  2.11it/s]Evaluating:  87%|████████▋ | 271/313 [02:05<00:19,  2.14it/s]Evaluating:  94%|█████████▍| 295/313 [02:15<00:08,  2.10it/s]Evaluating:  87%|████████▋ | 272/313 [02:05<00:19,  2.13it/s]Evaluating:  95%|█████████▍| 296/313 [02:15<00:08,  2.10it/s]Evaluating:  87%|████████▋ | 273/313 [02:06<00:18,  2.13it/s]Evaluating:  95%|█████████▍| 297/313 [02:16<00:07,  2.13it/s]Evaluating:  88%|████████▊ | 274/313 [02:06<00:18,  2.15it/s]Evaluating:  95%|█████████▌| 298/313 [02:16<00:06,  2.15it/s]Evaluating:  88%|████████▊ | 275/313 [02:07<00:17,  2.14it/s]Evaluating:  96%|█████████▌| 299/313 [02:17<00:06,  2.13it/s]Evaluating:  88%|████████▊ | 276/313 [02:07<00:17,  2.13it/s]Evaluating:  96%|█████████▌| 300/313 [02:17<00:06,  2.14it/s]Evaluating:  88%|████████▊ | 277/313 [02:08<00:16,  2.15it/s]Evaluating:  96%|█████████▌| 301/313 [02:18<00:05,  2.16it/s]Evaluating:  89%|████████▉ | 278/313 [02:08<00:16,  2.16it/s]Evaluating:  96%|█████████▋| 302/313 [02:18<00:05,  2.14it/s]Evaluating:  89%|████████▉ | 279/313 [02:09<00:15,  2.14it/s]Evaluating:  97%|█████████▋| 303/313 [02:19<00:04,  2.13it/s]Evaluating:  89%|████████▉ | 280/313 [02:09<00:15,  2.13it/s]Evaluating:  97%|█████████▋| 304/313 [02:19<00:04,  2.14it/s]Evaluating:  90%|████████▉ | 281/313 [02:10<00:14,  2.14it/s]Evaluating:  97%|█████████▋| 305/313 [02:20<00:03,  2.16it/s]Evaluating:  90%|█████████ | 282/313 [02:10<00:14,  2.15it/s]Evaluating:  98%|█████████▊| 306/313 [02:20<00:03,  2.15it/s]Evaluating:  90%|█████████ | 283/313 [02:10<00:14,  2.13it/s]Evaluating:  98%|█████████▊| 307/313 [02:21<00:02,  2.15it/s]Evaluating:  91%|█████████ | 284/313 [02:11<00:13,  2.15it/s]Evaluating:  98%|█████████▊| 308/313 [02:21<00:02,  2.16it/s]Evaluating:  91%|█████████ | 285/313 [02:11<00:12,  2.16it/s]Evaluating:  99%|█████████▊| 309/313 [02:22<00:01,  2.14it/s]Evaluating:  91%|█████████▏| 286/313 [02:12<00:12,  2.15it/s]Evaluating:  99%|█████████▉| 310/313 [02:22<00:01,  2.13it/s]Evaluating:  92%|█████████▏| 287/313 [02:12<00:12,  2.14it/s]Evaluating:  92%|█████████▏| 288/313 [02:13<00:11,  2.14it/s]Evaluating:  99%|█████████▉| 311/313 [02:23<00:00,  2.08it/s]Evaluating:  92%|█████████▏| 289/313 [02:13<00:11,  2.17it/s]Evaluating: 100%|█████████▉| 312/313 [02:23<00:00,  2.11it/s]Evaluating: 100%|██████████| 313/313 [02:23<00:00,  2.31it/s]Evaluating: 100%|██████████| 313/313 [02:23<00:00,  2.18it/s]Evaluating:  93%|█████████▎| 290/313 [02:14<00:10,  2.25it/s]Evaluating:  93%|█████████▎| 291/313 [02:14<00:08,  2.50it/s]Evaluating:  93%|█████████▎| 292/313 [02:14<00:07,  2.72it/s]Evaluating:  94%|█████████▎| 293/313 [02:15<00:07,  2.84it/s]Evaluating:  94%|█████████▍| 294/313 [02:15<00:06,  2.94it/s]
10/10/2021 15:43:28 - INFO - __main__ -   ***** Evaluation result  in da *****
10/10/2021 15:43:28 - INFO - __main__ -     f1 = 0.7413153920087875
10/10/2021 15:43:28 - INFO - __main__ -     loss = 0.7027667125764365
10/10/2021 15:43:28 - INFO - __main__ -     precision = 0.6996242062977841
10/10/2021 15:43:28 - INFO - __main__ -     recall = 0.7882902613520222
10/10/2021 15:43:28 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  94%|█████████▍| 295/313 [02:15<00:05,  3.05it/s]Evaluating:  95%|█████████▍| 296/313 [02:15<00:05,  3.09it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  95%|█████████▍| 297/313 [02:16<00:05,  3.11it/s]Evaluating:  95%|█████████▌| 298/313 [02:16<00:04,  3.19it/s]Evaluating:  96%|█████████▌| 299/313 [02:16<00:04,  3.24it/s]Evaluating:  96%|█████████▌| 300/313 [02:17<00:04,  3.21it/s]Evaluating:  96%|█████████▌| 301/313 [02:17<00:03,  3.20it/s]Evaluating:  96%|█████████▋| 302/313 [02:17<00:03,  3.24it/s]Evaluating:  97%|█████████▋| 303/313 [02:18<00:03,  3.11it/s]Evaluating:  97%|█████████▋| 304/313 [02:18<00:02,  3.15it/s]Evaluating:  97%|█████████▋| 305/313 [02:18<00:02,  3.20it/s]Evaluating:  98%|█████████▊| 306/313 [02:19<00:02,  3.25it/s]Evaluating:  98%|█████████▊| 307/313 [02:19<00:01,  3.25it/s]Evaluating:  98%|█████████▊| 308/313 [02:19<00:01,  3.30it/s]Evaluating:  99%|█████████▊| 309/313 [02:19<00:01,  3.30it/s]Evaluating:  99%|█████████▉| 310/313 [02:20<00:00,  3.41it/s]Evaluating:  99%|█████████▉| 311/313 [02:20<00:00,  3.41it/s]Evaluating: 100%|█████████▉| 312/313 [02:20<00:00,  3.46it/s]Evaluating: 100%|██████████| 313/313 [02:20<00:00,  3.84it/s]Evaluating: 100%|██████████| 313/313 [02:20<00:00,  2.22it/s]
10/10/2021 15:43:35 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/10/2021 15:43:35 - INFO - __main__ -     f1 = 0.5190509938915325
10/10/2021 15:43:35 - INFO - __main__ -     loss = 1.4232891698043568
10/10/2021 15:43:35 - INFO - __main__ -     precision = 0.47433999506538366
10/10/2021 15:43:35 - INFO - __main__ -     recall = 0.5730680378567703
10/10/2021 15:43:35 - INFO - __main__ -   Language adapter for bg not found, using ru instead
10/10/2021 15:43:35 - INFO - __main__ -   Set active language adapter to ru
10/10/2021 15:43:35 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/10/2021 15:43:37 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/10/2021 15:43:37 - INFO - __main__ -     Num examples = 10004
10/10/2021 15:43:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<01:21,  3.81it/s]Evaluating:   1%|          | 2/313 [00:00<01:23,  3.71it/s]Evaluating:   1%|          | 3/313 [00:00<01:26,  3.58it/s]Evaluating:   1%|▏         | 4/313 [00:01<01:27,  3.52it/s]Evaluating:   2%|▏         | 5/313 [00:01<01:24,  3.64it/s]Evaluating:   2%|▏         | 6/313 [00:01<01:26,  3.54it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:   2%|▏         | 7/313 [00:01<01:28,  3.48it/s]Evaluating:   3%|▎         | 8/313 [00:02<01:29,  3.42it/s]Evaluating:   3%|▎         | 9/313 [00:02<01:26,  3.50it/s]Evaluating:   3%|▎         | 10/313 [00:02<01:28,  3.44it/s]Evaluating:   4%|▎         | 11/313 [00:03<01:28,  3.40it/s]Evaluating:   4%|▍         | 12/313 [00:03<01:29,  3.36it/s]Evaluating:   4%|▍         | 13/313 [00:03<01:26,  3.45it/s]Evaluating:   4%|▍         | 14/313 [00:04<01:27,  3.40it/s]Evaluating:   5%|▍         | 15/313 [00:04<01:28,  3.38it/s]Evaluating:   5%|▌         | 16/313 [00:04<01:28,  3.36it/s]Evaluating:   5%|▌         | 17/313 [00:04<01:25,  3.45it/s]Evaluating:   6%|▌         | 18/313 [00:05<01:27,  3.38it/s]Evaluating:   6%|▌         | 19/313 [00:05<01:27,  3.36it/s]Evaluating:   6%|▋         | 20/313 [00:05<01:26,  3.38it/s]Evaluating:   7%|▋         | 21/313 [00:06<01:25,  3.43it/s]Evaluating:   7%|▋         | 22/313 [00:06<01:25,  3.39it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:   7%|▋         | 23/313 [00:06<01:25,  3.38it/s]Evaluating:   8%|▊         | 24/313 [00:06<01:24,  3.43it/s]Evaluating:   8%|▊         | 25/313 [00:07<01:24,  3.40it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   8%|▊         | 26/313 [00:07<01:25,  3.36it/s]Evaluating:   9%|▊         | 27/313 [00:07<01:25,  3.34it/s]Evaluating:   9%|▉         | 28/313 [00:08<01:22,  3.44it/s]Evaluating:   9%|▉         | 29/313 [00:08<01:23,  3.39it/s]Evaluating:  10%|▉         | 30/313 [00:08<01:24,  3.36it/s]Evaluating:  10%|▉         | 31/313 [00:09<01:24,  3.33it/s]Evaluating:  10%|█         | 32/313 [00:09<01:22,  3.42it/s]Evaluating:  11%|█         | 33/313 [00:09<01:23,  3.34it/s]Evaluating:  11%|█         | 34/313 [00:09<01:23,  3.33it/s]Evaluating:  11%|█         | 35/313 [00:10<01:22,  3.36it/s]Evaluating:  12%|█▏        | 36/313 [00:10<01:21,  3.40it/s]Evaluating:  12%|█▏        | 37/313 [00:10<01:22,  3.35it/s]Evaluating:  12%|█▏        | 38/313 [00:11<01:22,  3.32it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  12%|█▏        | 39/313 [00:11<01:20,  3.41it/s]10/10/2021 15:43:48 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:43:48 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  13%|█▎        | 40/313 [00:11<01:20,  3.37it/s]Evaluating:  13%|█▎        | 41/313 [00:12<01:21,  3.33it/s]Evaluating:  13%|█▎        | 42/313 [00:12<01:21,  3.30it/s]Evaluating:  14%|█▎        | 43/313 [00:12<01:19,  3.41it/s]Evaluating:  14%|█▍        | 44/313 [00:12<01:20,  3.36it/s]Evaluating:  14%|█▍        | 45/313 [00:13<01:20,  3.33it/s]Evaluating:  15%|█▍        | 46/313 [00:13<01:19,  3.35it/s]Evaluating:  15%|█▌        | 47/313 [00:13<01:18,  3.39it/s]Evaluating:  15%|█▌        | 48/313 [00:14<01:19,  3.34it/s]Evaluating:  16%|█▌        | 49/313 [00:14<01:19,  3.31it/s]Evaluating:  16%|█▌        | 50/313 [00:14<01:21,  3.23it/s]Evaluating:  16%|█▋        | 51/313 [00:15<01:20,  3.24it/s]Evaluating:  17%|█▋        | 52/313 [00:15<01:20,  3.24it/s]Evaluating:  17%|█▋        | 53/313 [00:15<01:19,  3.28it/s]Evaluating:  17%|█▋        | 54/313 [00:15<01:17,  3.35it/s]Evaluating:  18%|█▊        | 55/313 [00:16<01:17,  3.31it/s]Evaluating:  18%|█▊        | 56/313 [00:16<01:18,  3.29it/s]Evaluating:  18%|█▊        | 57/313 [00:16<01:16,  3.37it/s]Evaluating:  19%|█▊        | 58/313 [00:17<01:16,  3.34it/s]Evaluating:  19%|█▉        | 59/313 [00:17<01:16,  3.31it/s]Evaluating:  19%|█▉        | 60/313 [00:17<01:16,  3.29it/s]Evaluating:  19%|█▉        | 61/313 [00:18<01:14,  3.39it/s]Evaluating:  20%|█▉        | 62/313 [00:18<01:15,  3.34it/s]Evaluating:  20%|██        | 63/313 [00:18<01:15,  3.31it/s]Evaluating:  20%|██        | 64/313 [00:18<01:14,  3.35it/s]Evaluating:  21%|██        | 65/313 [00:19<01:13,  3.37it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  21%|██        | 66/313 [00:19<01:14,  3.33it/s]Evaluating:  21%|██▏       | 67/313 [00:19<01:14,  3.31it/s]Evaluating:  22%|██▏       | 68/313 [00:20<01:12,  3.37it/s]Evaluating:  22%|██▏       | 69/313 [00:20<01:12,  3.35it/s]Evaluating:  22%|██▏       | 70/313 [00:20<01:13,  3.32it/s]Evaluating:  23%|██▎       | 71/313 [00:21<01:13,  3.29it/s]Evaluating:  23%|██▎       | 72/313 [00:21<01:11,  3.39it/s]Evaluating:  23%|██▎       | 73/313 [00:21<01:11,  3.34it/s]Evaluating:  24%|██▎       | 74/313 [00:21<01:12,  3.31it/s]Evaluating:  24%|██▍       | 75/313 [00:22<01:11,  3.35it/s]Evaluating:  24%|██▍       | 76/313 [00:22<01:10,  3.38it/s]Evaluating:  25%|██▍       | 77/313 [00:22<01:10,  3.34it/s]Evaluating:  25%|██▍       | 78/313 [00:23<01:11,  3.31it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  25%|██▌       | 79/313 [00:23<01:09,  3.38it/s]Evaluating:  26%|██▌       | 80/313 [00:23<01:09,  3.36it/s]Evaluating:  26%|██▌       | 81/313 [00:24<01:09,  3.33it/s]Evaluating:  26%|██▌       | 82/313 [00:24<01:09,  3.31it/s]Evaluating:  27%|██▋       | 83/313 [00:24<01:07,  3.41it/s]Evaluating:  27%|██▋       | 84/313 [00:24<01:08,  3.36it/s]Evaluating:  27%|██▋       | 85/313 [00:25<01:08,  3.33it/s]Evaluating:  27%|██▋       | 86/313 [00:25<01:07,  3.38it/s]Evaluating:  28%|██▊       | 87/313 [00:25<01:06,  3.41it/s]Evaluating:  28%|██▊       | 88/313 [00:26<01:07,  3.36it/s]Evaluating:  28%|██▊       | 89/313 [00:26<01:07,  3.33it/s]Evaluating:  29%|██▉       | 90/313 [00:26<01:06,  3.36it/s]Evaluating:  29%|██▉       | 91/313 [00:27<01:05,  3.40it/s]Evaluating:  29%|██▉       | 92/313 [00:27<01:05,  3.35it/s]Evaluating:  30%|██▉       | 93/313 [00:27<01:06,  3.33it/s]Evaluating:  30%|███       | 94/313 [00:27<01:04,  3.39it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  30%|███       | 95/313 [00:28<01:04,  3.37it/s]Evaluating:  31%|███       | 96/313 [00:28<01:05,  3.33it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  31%|███       | 97/313 [00:28<01:04,  3.36it/s]Evaluating:  31%|███▏      | 98/313 [00:29<01:03,  3.39it/s]Evaluating:  32%|███▏      | 99/313 [00:29<01:03,  3.35it/s]Evaluating:  32%|███▏      | 100/313 [00:29<01:04,  3.32it/s]Evaluating:  32%|███▏      | 101/313 [00:29<01:03,  3.36it/s]Evaluating:  33%|███▎      | 102/313 [00:30<01:02,  3.38it/s]Evaluating:  33%|███▎      | 103/313 [00:30<01:03,  3.33it/s]Evaluating:  33%|███▎      | 104/313 [00:30<01:03,  3.32it/s]Evaluating:  34%|███▎      | 105/313 [00:31<01:01,  3.36it/s]Evaluating:  34%|███▍      | 106/313 [00:31<01:01,  3.39it/s]Evaluating:  34%|███▍      | 107/313 [00:31<01:01,  3.36it/s]Evaluating:  35%|███▍      | 108/313 [00:32<01:00,  3.37it/s]Evaluating:  35%|███▍      | 109/313 [00:32<01:00,  3.39it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  35%|███▌      | 110/313 [00:32<01:00,  3.34it/s]10/10/2021 15:44:09 - INFO - __main__ -   Using lang2id = None
Evaluating:  35%|███▌      | 111/313 [00:32<01:00,  3.32it/s]Evaluating:  36%|███▌      | 112/313 [00:33<00:59,  3.36it/s]Evaluating:  36%|███▌      | 113/313 [00:33<00:59,  3.39it/s]Evaluating:  36%|███▋      | 114/313 [00:33<00:59,  3.34it/s]Evaluating:  37%|███▋      | 115/313 [00:34<00:59,  3.31it/s]Evaluating:  37%|███▋      | 116/313 [00:34<00:58,  3.35it/s]Evaluating:  37%|███▋      | 117/313 [00:34<00:58,  3.37it/s]Evaluating:  38%|███▊      | 118/313 [00:35<00:58,  3.33it/s]Evaluating:  38%|███▊      | 119/313 [00:35<00:57,  3.36it/s]Evaluating:  38%|███▊      | 120/313 [00:35<00:57,  3.35it/s]Evaluating:  39%|███▊      | 121/313 [00:35<00:57,  3.35it/s]Evaluating:  39%|███▉      | 122/313 [00:36<00:57,  3.34it/s]Evaluating:  39%|███▉      | 123/313 [00:36<00:56,  3.38it/s]Evaluating:  40%|███▉      | 124/313 [00:36<00:55,  3.40it/s]Evaluating:  40%|███▉      | 125/313 [00:37<00:55,  3.37it/s]Evaluating:  40%|████      | 126/313 [00:37<00:55,  3.35it/s]Evaluating:  41%|████      | 127/313 [00:37<00:55,  3.37it/s]Evaluating:  41%|████      | 128/313 [00:38<00:54,  3.40it/s]Evaluating:  41%|████      | 129/313 [00:38<00:54,  3.35it/s]Evaluating:  42%|████▏     | 130/313 [00:38<00:54,  3.38it/s]Evaluating:  42%|████▏     | 131/313 [00:38<00:54,  3.34it/s]Evaluating:  42%|████▏     | 132/313 [00:39<00:53,  3.38it/s]Evaluating:  42%|████▏     | 133/313 [00:39<00:53,  3.34it/s]Evaluating:  43%|████▎     | 134/313 [00:39<00:52,  3.38it/s]Evaluating:  43%|████▎     | 135/313 [00:40<00:52,  3.37it/s]Evaluating:  43%|████▎     | 136/313 [00:40<00:52,  3.35it/s]Evaluating:  44%|████▍     | 137/313 [00:40<00:53,  3.32it/s]Evaluating:  44%|████▍     | 138/313 [00:41<00:52,  3.36it/s]Evaluating:  44%|████▍     | 139/313 [00:41<00:51,  3.39it/s]Evaluating:  45%|████▍     | 140/313 [00:41<00:51,  3.34it/s]Evaluating:  45%|████▌     | 141/313 [00:41<00:51,  3.37it/s]Evaluating:  45%|████▌     | 142/313 [00:42<00:51,  3.34it/s]Evaluating:  46%|████▌     | 143/313 [00:42<00:50,  3.38it/s]Evaluating:  46%|████▌     | 144/313 [00:42<00:50,  3.35it/s]Evaluating:  46%|████▋     | 145/313 [00:43<00:49,  3.39it/s]Evaluating:  47%|████▋     | 146/313 [00:43<00:49,  3.35it/s]Evaluating:  47%|████▋     | 147/313 [00:43<00:49,  3.37it/s]Evaluating:  47%|████▋     | 148/313 [00:43<00:49,  3.34it/s]Evaluating:  48%|████▊     | 149/313 [00:44<00:48,  3.38it/s]Evaluating:  48%|████▊     | 150/313 [00:44<00:48,  3.39it/s]Evaluating:  48%|████▊     | 151/313 [00:44<00:48,  3.34it/s]Evaluating:  49%|████▊     | 152/313 [00:45<00:47,  3.37it/s]Evaluating:  49%|████▉     | 153/313 [00:45<00:48,  3.33it/s]Evaluating:  49%|████▉     | 154/313 [00:45<00:47,  3.37it/s]Evaluating:  50%|████▉     | 155/313 [00:46<00:47,  3.33it/s]Evaluating:  50%|████▉     | 156/313 [00:46<00:46,  3.36it/s]Evaluating:  50%|█████     | 157/313 [00:46<00:47,  3.32it/s]Evaluating:  50%|█████     | 158/313 [00:46<00:46,  3.36it/s]Evaluating:  51%|█████     | 159/313 [00:47<00:46,  3.32it/s]Evaluating:  51%|█████     | 160/313 [00:47<00:45,  3.35it/s]Evaluating:  51%|█████▏    | 161/313 [00:47<00:45,  3.35it/s]Evaluating:  52%|█████▏    | 162/313 [00:48<00:45,  3.33it/s]Evaluating:  52%|█████▏    | 163/313 [00:48<00:44,  3.35it/s]Evaluating:  52%|█████▏    | 164/313 [00:48<00:44,  3.32it/s]Evaluating:  53%|█████▎    | 165/313 [00:49<00:44,  3.36it/s]Evaluating:  53%|█████▎    | 166/313 [00:49<00:44,  3.31it/s]Evaluating:  53%|█████▎    | 167/313 [00:49<00:43,  3.33it/s]Evaluating:  54%|█████▎    | 168/313 [00:49<00:43,  3.30it/s]Evaluating:  54%|█████▍    | 169/313 [00:50<00:45,  3.15it/s]Evaluating:  54%|█████▍    | 170/313 [00:50<00:43,  3.32it/s]Evaluating:  55%|█████▍    | 171/313 [00:50<00:41,  3.41it/s]Evaluating:  55%|█████▍    | 172/313 [00:51<00:40,  3.49it/s]Evaluating:  55%|█████▌    | 173/313 [00:51<00:39,  3.51it/s]Evaluating:  56%|█████▌    | 174/313 [00:51<00:39,  3.54it/s]Evaluating:  56%|█████▌    | 175/313 [00:51<00:39,  3.51it/s]Evaluating:  56%|█████▌    | 176/313 [00:52<00:38,  3.52it/s]Evaluating:  57%|█████▋    | 177/313 [00:52<00:39,  3.44it/s]Evaluating:  57%|█████▋    | 178/313 [00:52<00:39,  3.40it/s]Evaluating:  57%|█████▋    | 179/313 [00:53<00:39,  3.35it/s]Evaluating:  58%|█████▊    | 180/313 [00:53<00:39,  3.37it/s]Evaluating:  58%|█████▊    | 181/313 [00:53<00:38,  3.39it/s]Evaluating:  58%|█████▊    | 182/313 [00:54<00:39,  3.34it/s]Evaluating:  58%|█████▊    | 183/313 [00:54<00:39,  3.29it/s]Evaluating:  59%|█████▉    | 184/313 [00:54<00:38,  3.34it/s]Evaluating:  59%|█████▉    | 185/313 [00:54<00:38,  3.35it/s]Evaluating:  59%|█████▉    | 186/313 [00:55<00:38,  3.31it/s]Evaluating:  60%|█████▉    | 187/313 [00:55<00:37,  3.34it/s]Evaluating:  60%|██████    | 188/313 [00:55<00:37,  3.36it/s]Evaluating:  60%|██████    | 189/313 [00:56<00:37,  3.34it/s]Evaluating:  61%|██████    | 190/313 [00:56<00:37,  3.32it/s]Evaluating:  61%|██████    | 191/313 [00:56<00:36,  3.36it/s]Evaluating:  61%|██████▏   | 192/313 [00:57<00:35,  3.38it/s]Evaluating:  62%|██████▏   | 193/313 [00:57<00:35,  3.34it/s]Evaluating:  62%|██████▏   | 194/313 [00:57<00:36,  3.30it/s]Evaluating:  62%|██████▏   | 195/313 [00:57<00:35,  3.35it/s]Evaluating:  63%|██████▎   | 196/313 [00:58<00:34,  3.37it/s]Evaluating:  63%|██████▎   | 197/313 [00:58<00:35,  3.28it/s]Evaluating:  63%|██████▎   | 198/313 [00:58<00:34,  3.32it/s]Evaluating:  64%|██████▎   | 199/313 [00:59<00:34,  3.34it/s]Evaluating:  64%|██████▍   | 200/313 [00:59<00:34,  3.31it/s]Evaluating:  64%|██████▍   | 201/313 [00:59<00:34,  3.28it/s]Evaluating:  65%|██████▍   | 202/313 [01:00<00:33,  3.33it/s]Evaluating:  65%|██████▍   | 203/313 [01:00<00:35,  3.13it/s]Evaluating:  65%|██████▌   | 204/313 [01:00<00:34,  3.20it/s]Evaluating:  65%|██████▌   | 205/313 [01:01<00:32,  3.31it/s]Evaluating:  66%|██████▌   | 206/313 [01:01<00:31,  3.38it/s]Evaluating:  66%|██████▌   | 207/313 [01:01<00:31,  3.35it/s]Evaluating:  66%|██████▋   | 208/313 [01:01<00:31,  3.34it/s]Evaluating:  67%|██████▋   | 209/313 [01:02<00:30,  3.38it/s]Evaluating:  67%|██████▋   | 210/313 [01:02<00:30,  3.39it/s]Evaluating:  67%|██████▋   | 211/313 [01:02<00:30,  3.34it/s]Evaluating:  68%|██████▊   | 212/313 [01:03<00:30,  3.30it/s]Evaluating:  68%|██████▊   | 213/313 [01:03<00:29,  3.39it/s]Evaluating:  68%|██████▊   | 214/313 [01:03<00:29,  3.34it/s]Evaluating:  69%|██████▊   | 215/313 [01:04<00:29,  3.31it/s]Evaluating:  69%|██████▉   | 216/313 [01:04<00:29,  3.32it/s]Evaluating:  69%|██████▉   | 217/313 [01:04<00:28,  3.33it/s]Evaluating:  70%|██████▉   | 218/313 [01:04<00:28,  3.29it/s]Evaluating:  70%|██████▉   | 219/313 [01:05<00:28,  3.28it/s]Evaluating:  70%|███████   | 220/313 [01:05<00:28,  3.30it/s]Evaluating:  71%|███████   | 221/313 [01:05<00:28,  3.28it/s]Evaluating:  71%|███████   | 222/313 [01:06<00:27,  3.27it/s]Evaluating:  71%|███████   | 223/313 [01:06<00:27,  3.27it/s]Evaluating:  72%|███████▏  | 224/313 [01:06<00:26,  3.39it/s]Evaluating:  72%|███████▏  | 225/313 [01:07<00:26,  3.33it/s]Evaluating:  72%|███████▏  | 226/313 [01:07<00:26,  3.30it/s]Evaluating:  73%|███████▎  | 227/313 [01:07<00:25,  3.34it/s]Evaluating:  73%|███████▎  | 228/313 [01:07<00:25,  3.33it/s]Evaluating:  73%|███████▎  | 229/313 [01:08<00:25,  3.30it/s]Evaluating:  73%|███████▎  | 230/313 [01:08<00:25,  3.28it/s]Evaluating:  74%|███████▍  | 231/313 [01:08<00:24,  3.38it/s]Evaluating:  74%|███████▍  | 232/313 [01:09<00:24,  3.36it/s]Evaluating:  74%|███████▍  | 233/313 [01:09<00:23,  3.34it/s]Evaluating:  75%|███████▍  | 234/313 [01:09<00:23,  3.32it/s]Evaluating:  75%|███████▌  | 235/313 [01:10<00:22,  3.40it/s]Evaluating:  75%|███████▌  | 236/313 [01:10<00:23,  3.34it/s]Evaluating:  76%|███████▌  | 237/313 [01:10<00:23,  3.30it/s]Evaluating:  76%|███████▌  | 238/313 [01:10<00:22,  3.39it/s]Evaluating:  76%|███████▋  | 239/313 [01:11<00:22,  3.34it/s]Evaluating:  77%|███████▋  | 240/313 [01:11<00:22,  3.30it/s]Evaluating:  77%|███████▋  | 241/313 [01:11<00:22,  3.27it/s]Evaluating:  77%|███████▋  | 242/313 [01:12<00:21,  3.36it/s]Evaluating:  78%|███████▊  | 243/313 [01:12<00:21,  3.32it/s]Evaluating:  78%|███████▊  | 244/313 [01:12<00:20,  3.29it/s]Evaluating:  78%|███████▊  | 245/313 [01:13<00:20,  3.32it/s]Evaluating:  79%|███████▊  | 246/313 [01:13<00:20,  3.35it/s]Evaluating:  79%|███████▉  | 247/313 [01:13<00:19,  3.32it/s]Evaluating:  79%|███████▉  | 248/313 [01:13<00:19,  3.29it/s]Evaluating:  80%|███████▉  | 249/313 [01:14<00:18,  3.39it/s]Evaluating:  80%|███████▉  | 250/313 [01:14<00:18,  3.34it/s]Evaluating:  80%|████████  | 251/313 [01:14<00:18,  3.30it/s]Evaluating:  81%|████████  | 252/313 [01:15<00:18,  3.30it/s]Evaluating:  81%|████████  | 253/313 [01:15<00:17,  3.41it/s]Evaluating:  81%|████████  | 254/313 [01:15<00:17,  3.36it/s]Evaluating:  81%|████████▏ | 255/313 [01:16<00:17,  3.32it/s]Evaluating:  82%|████████▏ | 256/313 [01:16<00:17,  3.34it/s]Evaluating:  82%|████████▏ | 257/313 [01:16<00:16,  3.37it/s]Evaluating:  82%|████████▏ | 258/313 [01:16<00:16,  3.31it/s]Evaluating:  83%|████████▎ | 259/313 [01:17<00:16,  3.30it/s]Evaluating:  83%|████████▎ | 260/313 [01:17<00:15,  3.38it/s]Evaluating:  83%|████████▎ | 261/313 [01:17<00:15,  3.32it/s]Evaluating:  84%|████████▎ | 262/313 [01:18<00:15,  3.29it/s]Evaluating:  84%|████████▍ | 263/313 [01:18<00:15,  3.30it/s]Evaluating:  84%|████████▍ | 264/313 [01:18<00:14,  3.33it/s]Evaluating:  85%|████████▍ | 265/313 [01:19<00:14,  3.30it/s]Evaluating:  85%|████████▍ | 266/313 [01:19<00:14,  3.29it/s]Evaluating:  85%|████████▌ | 267/313 [01:19<00:13,  3.33it/s]Evaluating:  86%|████████▌ | 268/313 [01:19<00:13,  3.36it/s]Evaluating:  86%|████████▌ | 269/313 [01:20<00:13,  3.32it/s]Evaluating:  86%|████████▋ | 270/313 [01:20<00:13,  3.29it/s]Evaluating:  87%|████████▋ | 271/313 [01:20<00:12,  3.35it/s]Evaluating:  87%|████████▋ | 272/313 [01:21<00:12,  3.31it/s]Evaluating:  87%|████████▋ | 273/313 [01:21<00:12,  3.28it/s]Evaluating:  88%|████████▊ | 274/313 [01:21<00:11,  3.31it/s]Evaluating:  88%|████████▊ | 275/313 [01:22<00:11,  3.35it/s]Evaluating:  88%|████████▊ | 276/313 [01:22<00:11,  3.31it/s]Evaluating:  88%|████████▊ | 277/313 [01:22<00:10,  3.28it/s]Evaluating:  89%|████████▉ | 278/313 [01:22<00:10,  3.27it/s]Evaluating:  89%|████████▉ | 279/313 [01:23<00:10,  3.33it/s]Evaluating:  89%|████████▉ | 280/313 [01:23<00:10,  3.28it/s]Evaluating:  90%|████████▉ | 281/313 [01:23<00:09,  3.32it/s]Evaluating:  90%|█████████ | 282/313 [01:24<00:09,  3.34it/s]Evaluating:  90%|█████████ | 283/313 [01:24<00:09,  3.32it/s]Evaluating:  91%|█████████ | 284/313 [01:24<00:08,  3.29it/s]Evaluating:  91%|█████████ | 285/313 [01:25<00:08,  3.33it/s]Evaluating:  91%|█████████▏| 286/313 [01:25<00:07,  3.40it/s]Evaluating:  92%|█████████▏| 287/313 [01:25<00:07,  3.34it/s]Evaluating:  92%|█████████▏| 288/313 [01:25<00:07,  3.34it/s]Evaluating:  92%|█████████▏| 289/313 [01:26<00:07,  3.31it/s]Evaluating:  93%|█████████▎| 290/313 [01:26<00:06,  3.33it/s]Evaluating:  93%|█████████▎| 291/313 [01:26<00:06,  3.28it/s]Evaluating:  93%|█████████▎| 292/313 [01:27<00:06,  3.31it/s]Evaluating:  94%|█████████▎| 293/313 [01:27<00:05,  3.35it/s]Evaluating:  94%|█████████▍| 294/313 [01:27<00:05,  3.28it/s]Evaluating:  94%|█████████▍| 295/313 [01:28<00:05,  3.30it/s]Evaluating:  95%|█████████▍| 296/313 [01:28<00:05,  3.27it/s]Evaluating:  95%|█████████▍| 297/313 [01:28<00:04,  3.30it/s]Evaluating:  95%|█████████▌| 298/313 [01:29<00:04,  3.27it/s]Evaluating:  96%|█████████▌| 299/313 [01:29<00:04,  3.28it/s]Evaluating:  96%|█████████▌| 300/313 [01:29<00:04,  3.25it/s]Evaluating:  96%|█████████▌| 301/313 [01:29<00:03,  3.21it/s]Evaluating:  96%|█████████▋| 302/313 [01:30<00:03,  3.24it/s]Evaluating:  97%|█████████▋| 303/313 [01:30<00:03,  3.21it/s]Evaluating:  97%|█████████▋| 304/313 [01:30<00:02,  3.26it/s]Evaluating:  97%|█████████▋| 305/313 [01:31<00:02,  3.28it/s]Evaluating:  98%|█████████▊| 306/313 [01:31<00:02,  3.24it/s]Evaluating:  98%|█████████▊| 307/313 [01:31<00:01,  3.23it/s]Evaluating:  98%|█████████▊| 308/313 [01:32<00:01,  3.19it/s]Evaluating:  99%|█████████▊| 309/313 [01:32<00:01,  3.22it/s]Evaluating:  99%|█████████▉| 310/313 [01:32<00:00,  3.19it/s]Evaluating:  99%|█████████▉| 311/313 [01:33<00:00,  3.23it/s]Evaluating: 100%|█████████▉| 312/313 [01:33<00:00,  3.26it/s]Evaluating: 100%|██████████| 313/313 [01:33<00:00,  3.54it/s]Evaluating: 100%|██████████| 313/313 [01:33<00:00,  3.34it/s]
10/10/2021 15:45:11 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/10/2021 15:45:11 - INFO - __main__ -     f1 = 0.6544281974331141
10/10/2021 15:45:11 - INFO - __main__ -     loss = 0.9670069812776182
10/10/2021 15:45:11 - INFO - __main__ -     precision = 0.5937662939574764
10/10/2021 15:45:11 - INFO - __main__ -     recall = 0.7288955266339521
10/10/2021 15:45:11 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:45:28 - INFO - __main__ -   Using lang2id = None
10/10/2021 15:45:28 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/10/2021 15:45:50 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 15:53:22 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 15:53:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 15:53:22 - INFO - __main__ -   Seed = 1
10/14/2021 15:53:22 - INFO - root -   save model
10/14/2021 15:53:22 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 15:53:22 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:53:34 - INFO - __main__ -   Using lang2id = None
10/14/2021 15:53:34 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 15:53:34 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
10/14/2021 15:53:34 - INFO - root -   Trying to decide if add adapter
10/14/2021 15:53:34 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 15:53:34 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/14/2021 15:53:34 - INFO - __main__ -   Language = en
10/14/2021 15:53:34 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 15:53:36 - INFO - __main__ -   Language = hi
10/14/2021 15:53:36 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/14/2021 15:53:38 - INFO - __main__ -   Language = ar
10/14/2021 15:53:38 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/14/2021 15:53:59 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/14/2021 15:53:59 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/14/2021 15:53:59 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:53:59 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.37it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.69it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  5.93it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.04it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.11it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.15it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.18it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.19it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.20it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.19it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.18it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.18it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.19it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.19it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.20it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.19it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.18it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.19it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.19it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.19it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.18it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.17it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.18it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.18it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.18it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.17it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.18it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.18it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.15it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.16it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.17it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.29it/s]
10/14/2021 15:54:04 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/14/2021 15:54:04 - INFO - __main__ -     f1 = 0.5046659201194476
10/14/2021 15:54:04 - INFO - __main__ -     loss = 2.088470995426178
10/14/2021 15:54:04 - INFO - __main__ -     precision = 0.4777385159010601
10/14/2021 15:54:04 - INFO - __main__ -     recall = 0.5348101265822784
10/14/2021 15:54:04 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/14/2021 15:54:04 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/14/2021 15:54:04 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:54:04 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.24it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.22it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  5.95it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.04it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.09it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.12it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.14it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.15it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.16it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.16it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.16it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.15it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.16it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.16it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.15it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.14it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.15it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.16it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.17it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.17it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.16it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.14it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.14it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.15it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.14it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.13it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.14it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.14it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.14it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.15it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.13it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.28it/s]
10/14/2021 15:54:10 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/14/2021 15:54:10 - INFO - __main__ -     f1 = 0.4337921214997627
10/14/2021 15:54:10 - INFO - __main__ -     loss = 4.738484933972359
10/14/2021 15:54:10 - INFO - __main__ -     precision = 0.4489194499017682
10/14/2021 15:54:10 - INFO - __main__ -     recall = 0.4196510560146924
10/14/2021 15:54:10 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/14/2021 15:54:10 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/14/2021 15:54:10 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:54:10 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  6.18it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.17it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.17it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.16it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.16it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.15it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.17it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.16it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.16it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.17it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.16it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.16it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.14it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.15it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.14it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.14it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.14it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.14it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.14it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.14it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.13it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.13it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.13it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.13it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.12it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.11it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.12it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.13it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.13it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.14it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  4.84it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.10it/s]
10/14/2021 15:54:15 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/14/2021 15:54:15 - INFO - __main__ -     f1 = 0.22264150943396227
10/14/2021 15:54:15 - INFO - __main__ -     loss = 4.114778518676758
10/14/2021 15:54:15 - INFO - __main__ -     precision = 0.2684869169510808
10/14/2021 15:54:15 - INFO - __main__ -     recall = 0.19016921837228043
10/14/2021 15:54:15 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:54:27 - INFO - __main__ -   Using lang2id = None
10/14/2021 15:54:27 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:54:49 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 15:54:51 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 15:54:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 15:54:51 - INFO - __main__ -   Seed = 2
10/14/2021 15:54:51 - INFO - root -   save model
10/14/2021 15:54:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 15:54:51 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:55:04 - INFO - __main__ -   Using lang2id = None
10/14/2021 15:55:04 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 15:55:04 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
10/14/2021 15:55:04 - INFO - root -   Trying to decide if add adapter
10/14/2021 15:55:04 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 15:55:04 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/14/2021 15:55:04 - INFO - __main__ -   Language = en
10/14/2021 15:55:04 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 15:55:05 - INFO - __main__ -   Language = hi
10/14/2021 15:55:05 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/14/2021 15:55:06 - INFO - __main__ -   Language = ar
10/14/2021 15:55:06 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/14/2021 15:55:10 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/14/2021 15:55:10 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/14/2021 15:55:10 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:55:10 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.27it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.67it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  5.92it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.04it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.11it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.14it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.16it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.17it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.17it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.19it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.19it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.19it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.20it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.20it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.17it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.17it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.17it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.17it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.18it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.17it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.17it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.15it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.17it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.18it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.18it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.18it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.19it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  5.50it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  5.69it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  5.83it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  5.92it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.20it/s]
10/14/2021 15:55:16 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/14/2021 15:55:16 - INFO - __main__ -     f1 = 0.5241180705543557
10/14/2021 15:55:16 - INFO - __main__ -     loss = 2.172150271013379
10/14/2021 15:55:16 - INFO - __main__ -     precision = 0.4808454425363276
10/14/2021 15:55:16 - INFO - __main__ -     recall = 0.5759493670886076
10/14/2021 15:55:16 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/14/2021 15:55:16 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/14/2021 15:55:16 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:55:16 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  6.19it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.19it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.20it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.20it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.20it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.19it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.19it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.19it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.19it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.19it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.18it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.18it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.18it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.18it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.18it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.18it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.17it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.17it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.17it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.16it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.16it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.16it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.16it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.17it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.16it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.16it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.16it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.15it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.15it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.15it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.15it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.31it/s]
10/14/2021 15:55:21 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/14/2021 15:55:21 - INFO - __main__ -     f1 = 0.5927272727272728
10/14/2021 15:55:21 - INFO - __main__ -     loss = 3.865293651819229
10/14/2021 15:55:21 - INFO - __main__ -     precision = 0.5868586858685868
10/14/2021 15:55:21 - INFO - __main__ -     recall = 0.5987144168962351
10/14/2021 15:55:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/14/2021 15:55:21 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/14/2021 15:55:21 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:55:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.24it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.18it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.16it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.16it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.16it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.16it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.16it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.16it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.16it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.16it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.15it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.14it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.14it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.15it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.15it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.14it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.13it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.13it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.14it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.13it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.13it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.13it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.14it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.13it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.13it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.13it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.12it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.12it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.11it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.08it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.08it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.27it/s]
10/14/2021 15:55:26 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/14/2021 15:55:26 - INFO - __main__ -     f1 = 0.3483709273182957
10/14/2021 15:55:26 - INFO - __main__ -     loss = 3.421078100800514
10/14/2021 15:55:26 - INFO - __main__ -     precision = 0.3616652211621856
10/14/2021 15:55:26 - INFO - __main__ -     recall = 0.33601933924254634
10/14/2021 15:55:26 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:55:38 - INFO - __main__ -   Using lang2id = None
10/14/2021 15:55:38 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:55:55 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 15:55:57 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 15:55:57 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 15:55:57 - INFO - __main__ -   Seed = 3
10/14/2021 15:55:57 - INFO - root -   save model
10/14/2021 15:55:57 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 15:55:57 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:56:14 - INFO - __main__ -   Using lang2id = None
10/14/2021 15:56:14 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 15:56:14 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
10/14/2021 15:56:14 - INFO - root -   Trying to decide if add adapter
10/14/2021 15:56:14 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 15:56:14 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/14/2021 15:56:14 - INFO - __main__ -   Language = en
10/14/2021 15:56:14 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 15:56:15 - INFO - __main__ -   Language = hi
10/14/2021 15:56:15 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/14/2021 15:56:16 - INFO - __main__ -   Language = ar
10/14/2021 15:56:16 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/14/2021 15:56:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/14/2021 15:56:21 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/14/2021 15:56:21 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:56:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.25it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.63it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  5.88it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.01it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.06it/s]Evaluating:  19%|█▉        | 6/32 [00:01<00:04,  6.08it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.10it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.12it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.13it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.15it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.15it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.15it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.15it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.16it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.15it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.15it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.15it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.15it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.13it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.13it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.13it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.11it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.09it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.11it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  6.13it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.12it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.13it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.13it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.13it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.13it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.13it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.23it/s]
10/14/2021 15:56:26 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/14/2021 15:56:26 - INFO - __main__ -     f1 = 0.5233309404163675
10/14/2021 15:56:26 - INFO - __main__ -     loss = 2.1998152434825897
10/14/2021 15:56:26 - INFO - __main__ -     precision = 0.47897503285151116
10/14/2021 15:56:26 - INFO - __main__ -     recall = 0.5767405063291139
10/14/2021 15:56:26 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/14/2021 15:56:26 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/14/2021 15:56:26 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:56:26 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.23it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.19it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.18it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.16it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.15it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.14it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.13it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.14it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.13it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.14it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.14it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.13it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.13it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.14it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.13it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.14it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.12it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.12it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.12it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.12it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.12it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.11it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  5.71it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  5.83it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  5.90it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:01,  5.95it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.00it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.03it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.05it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.07it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  6.09it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.21it/s]
10/14/2021 15:56:31 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/14/2021 15:56:31 - INFO - __main__ -     f1 = 0.5176362803481447
10/14/2021 15:56:31 - INFO - __main__ -     loss = 4.001183874905109
10/14/2021 15:56:31 - INFO - __main__ -     precision = 0.5164533820840951
10/14/2021 15:56:31 - INFO - __main__ -     recall = 0.5188246097337006
10/14/2021 15:56:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/14/2021 15:56:32 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/14/2021 15:56:32 - INFO - __main__ -     Num examples = 1000
10/14/2021 15:56:32 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  6.20it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.16it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.15it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.14it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.13it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.13it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:04,  6.11it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.11it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.12it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.11it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.08it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.05it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.02it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:03,  6.00it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  5.98it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  5.97it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  5.94it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  5.92it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  5.93it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:02,  5.91it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  5.90it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  5.89it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  5.88it/s]Evaluating:  75%|███████▌  | 24/32 [00:04<00:01,  5.86it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  5.86it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:01,  5.86it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  5.86it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  5.85it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  5.85it/s]Evaluating:  94%|█████████▍| 30/32 [00:05<00:00,  5.82it/s]Evaluating:  97%|█████████▋| 31/32 [00:05<00:00,  5.81it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.09it/s]
10/14/2021 15:56:37 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/14/2021 15:56:37 - INFO - __main__ -     f1 = 0.2885189927443449
10/14/2021 15:56:37 - INFO - __main__ -     loss = 3.584737040102482
10/14/2021 15:56:37 - INFO - __main__ -     precision = 0.30671506352087113
10/14/2021 15:56:37 - INFO - __main__ -     recall = 0.2723609991941982
10/14/2021 15:56:37 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:56:49 - INFO - __main__ -   Using lang2id = None
10/14/2021 15:56:49 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 15:57:05 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 16:08:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:08:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 16:08:29 - INFO - __main__ -   Seed = 1
10/14/2021 16:08:29 - INFO - root -   save model
10/14/2021 16:08:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:08:29 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:08:42 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:08:42 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 16:08:42 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
10/14/2021 16:08:42 - INFO - root -   Trying to decide if add adapter
10/14/2021 16:08:42 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 16:08:42 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/14/2021 16:08:42 - INFO - __main__ -   Language = en
10/14/2021 16:08:42 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 16:08:43 - INFO - __main__ -   Language = is
10/14/2021 16:08:43 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/14/2021 16:08:45 - INFO - __main__ -   Language = de
10/14/2021 16:08:45 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/14/2021 16:08:50 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/14/2021 16:08:50 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/14/2021 16:08:50 - INFO - __main__ -     Num examples = 100
10/14/2021 16:08:50 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  5.25it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  5.66it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  5.85it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  7.28it/s]
10/14/2021 16:08:51 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/14/2021 16:08:51 - INFO - __main__ -     f1 = 0.654275092936803
10/14/2021 16:08:51 - INFO - __main__ -     loss = 1.169814482331276
10/14/2021 16:08:51 - INFO - __main__ -     precision = 0.5906040268456376
10/14/2021 16:08:51 - INFO - __main__ -     recall = 0.7333333333333333
10/14/2021 16:08:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/14/2021 16:08:52 - INFO - __main__ -   ***** Running evaluation  in no *****
10/14/2021 16:08:52 - INFO - __main__ -     Num examples = 10000
10/14/2021 16:08:52 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:50,  6.13it/s]Evaluating:   1%|          | 2/313 [00:00<00:50,  6.10it/s]Evaluating:   1%|          | 3/313 [00:00<00:50,  6.11it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:50,  6.12it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:50,  6.11it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:50,  6.10it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.10it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.10it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:49,  6.09it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:49,  6.10it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:49,  6.09it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:49,  6.07it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:49,  6.07it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.08it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  6.08it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:48,  6.08it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:48,  6.07it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:48,  6.06it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.07it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.07it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  6.06it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:47,  6.07it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:47,  6.06it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:47,  6.06it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  6.05it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  6.05it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  6.05it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:46,  6.05it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:46,  6.03it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:46,  6.04it/s]Evaluating:  10%|█         | 32/313 [00:05<00:46,  6.04it/s]Evaluating:  11%|█         | 33/313 [00:05<00:46,  6.02it/s]Evaluating:  11%|█         | 34/313 [00:05<00:46,  6.02it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  6.02it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:46,  6.02it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:45,  6.02it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  6.03it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.01it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  6.02it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.02it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:45,  6.02it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:44,  6.02it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  6.03it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:44,  6.03it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:44,  6.02it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:43,  6.00it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:43,  6.01it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  6.01it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  6.00it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  6.00it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:43,  6.00it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:43,  5.99it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:42,  5.99it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  5.97it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:44,  5.71it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:43,  5.79it/s]Evaluating:  20%|██        | 63/313 [00:10<00:42,  5.85it/s]Evaluating:  20%|██        | 64/313 [00:10<00:42,  5.88it/s]Evaluating:  21%|██        | 65/313 [00:10<00:41,  5.91it/s]Evaluating:  21%|██        | 66/313 [00:10<00:41,  5.93it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:41,  5.94it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:41,  5.95it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:40,  5.95it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  5.96it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  5.96it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:40,  5.95it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  5.92it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:40,  5.92it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:40,  5.93it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:39,  5.94it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  5.94it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:39,  5.94it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.90it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:39,  5.89it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:39,  5.90it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:39,  5.92it/s]Evaluating:  27%|██▋       | 83/313 [00:13<00:38,  5.92it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:38,  5.92it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:38,  5.92it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:38,  5.93it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:38,  5.93it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:37,  5.93it/s]Evaluating:  28%|██▊       | 89/313 [00:14<00:37,  5.92it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:52,  4.27it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:47,  4.66it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:44,  4.98it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:42,  5.23it/s]Evaluating:  30%|███       | 94/313 [00:15<00:40,  5.35it/s]Evaluating:  30%|███       | 95/313 [00:16<00:39,  5.51it/s]Evaluating:  31%|███       | 96/313 [00:16<00:38,  5.62it/s]Evaluating:  31%|███       | 97/313 [00:16<00:37,  5.71it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:37,  5.77it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:36,  5.81it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:36,  5.84it/s]Evaluating:  32%|███▏      | 101/313 [00:17<00:36,  5.85it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:35,  5.86it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:35,  5.87it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:35,  5.83it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:35,  5.85it/s]Evaluating:  34%|███▍      | 106/313 [00:17<00:35,  5.86it/s]Evaluating:  34%|███▍      | 107/313 [00:18<00:35,  5.87it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:34,  5.87it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:34,  5.87it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:34,  5.88it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:34,  5.88it/s]Evaluating:  36%|███▌      | 112/313 [00:18<00:34,  5.88it/s]Evaluating:  36%|███▌      | 113/313 [00:19<00:33,  5.89it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:33,  5.89it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:33,  5.89it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:33,  5.89it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:33,  5.88it/s]Evaluating:  38%|███▊      | 118/313 [00:19<00:33,  5.88it/s]Evaluating:  38%|███▊      | 119/313 [00:20<00:33,  5.88it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:32,  5.87it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:32,  5.87it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:32,  5.86it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:32,  5.80it/s]Evaluating:  40%|███▉      | 124/313 [00:21<00:32,  5.81it/s]Evaluating:  40%|███▉      | 125/313 [00:21<00:32,  5.82it/s]Evaluating:  40%|████      | 126/313 [00:21<00:32,  5.83it/s]Evaluating:  41%|████      | 127/313 [00:21<00:31,  5.83it/s]Evaluating:  41%|████      | 128/313 [00:21<00:31,  5.84it/s]Evaluating:  41%|████      | 129/313 [00:21<00:31,  5.84it/s]Evaluating:  42%|████▏     | 130/313 [00:22<00:31,  5.84it/s]Evaluating:  42%|████▏     | 131/313 [00:22<00:31,  5.84it/s]Evaluating:  42%|████▏     | 132/313 [00:22<00:30,  5.84it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:30,  5.85it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:30,  5.86it/s]Evaluating:  43%|████▎     | 135/313 [00:22<00:30,  5.84it/s]Evaluating:  43%|████▎     | 136/313 [00:23<00:30,  5.84it/s]Evaluating:  44%|████▍     | 137/313 [00:23<00:30,  5.84it/s]Evaluating:  44%|████▍     | 138/313 [00:23<00:29,  5.84it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:29,  5.84it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:29,  5.83it/s]Evaluating:  45%|████▌     | 141/313 [00:23<00:29,  5.83it/s]Evaluating:  45%|████▌     | 142/313 [00:24<00:29,  5.83it/s]Evaluating:  46%|████▌     | 143/313 [00:24<00:29,  5.83it/s]Evaluating:  46%|████▌     | 144/313 [00:24<00:28,  5.83it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:28,  5.83it/s]Evaluating:  47%|████▋     | 146/313 [00:24<00:28,  5.83it/s]Evaluating:  47%|████▋     | 147/313 [00:24<00:28,  5.83it/s]Evaluating:  47%|████▋     | 148/313 [00:25<00:28,  5.83it/s]Evaluating:  48%|████▊     | 149/313 [00:25<00:28,  5.78it/s]Evaluating:  48%|████▊     | 150/313 [00:25<00:28,  5.79it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:27,  5.80it/s]Evaluating:  49%|████▊     | 152/313 [00:25<00:27,  5.81it/s]Evaluating:  49%|████▉     | 153/313 [00:25<00:27,  5.81it/s]Evaluating:  49%|████▉     | 154/313 [00:26<00:27,  5.80it/s]Evaluating:  50%|████▉     | 155/313 [00:26<00:27,  5.75it/s]Evaluating:  50%|████▉     | 156/313 [00:26<00:27,  5.69it/s]Evaluating:  50%|█████     | 157/313 [00:26<00:27,  5.68it/s]Evaluating:  50%|█████     | 158/313 [00:26<00:27,  5.71it/s]Evaluating:  51%|█████     | 159/313 [00:27<00:26,  5.74it/s]Evaluating:  51%|█████     | 160/313 [00:27<00:26,  5.77it/s]Evaluating:  51%|█████▏    | 161/313 [00:27<00:26,  5.73it/s]Evaluating:  52%|█████▏    | 162/313 [00:27<00:26,  5.72it/s]Evaluating:  52%|█████▏    | 163/313 [00:27<00:26,  5.74it/s]Evaluating:  52%|█████▏    | 164/313 [00:27<00:25,  5.75it/s]Evaluating:  53%|█████▎    | 165/313 [00:28<00:25,  5.77it/s]Evaluating:  53%|█████▎    | 166/313 [00:28<00:25,  5.78it/s]Evaluating:  53%|█████▎    | 167/313 [00:28<00:25,  5.78it/s]Evaluating:  54%|█████▎    | 168/313 [00:28<00:25,  5.77it/s]Evaluating:  54%|█████▍    | 169/313 [00:28<00:24,  5.78it/s]Evaluating:  54%|█████▍    | 170/313 [00:28<00:24,  5.78it/s]Evaluating:  55%|█████▍    | 171/313 [00:29<00:24,  5.78it/s]Evaluating:  55%|█████▍    | 172/313 [00:29<00:24,  5.79it/s]Evaluating:  55%|█████▌    | 173/313 [00:29<00:24,  5.79it/s]Evaluating:  56%|█████▌    | 174/313 [00:29<00:24,  5.78it/s]Evaluating:  56%|█████▌    | 175/313 [00:29<00:23,  5.78it/s]Evaluating:  56%|█████▌    | 176/313 [00:29<00:23,  5.78it/s]Evaluating:  57%|█████▋    | 177/313 [00:30<00:23,  5.77it/s]Evaluating:  57%|█████▋    | 178/313 [00:30<00:23,  5.78it/s]Evaluating:  57%|█████▋    | 179/313 [00:30<00:28,  4.74it/s]Evaluating:  58%|█████▊    | 180/313 [00:30<00:26,  5.01it/s]Evaluating:  58%|█████▊    | 181/313 [00:30<00:25,  5.22it/s]Evaluating:  58%|█████▊    | 182/313 [00:31<00:24,  5.37it/s]Evaluating:  58%|█████▊    | 183/313 [00:31<00:23,  5.49it/s]Evaluating:  59%|█████▉    | 184/313 [00:31<00:23,  5.57it/s]Evaluating:  59%|█████▉    | 185/313 [00:31<00:22,  5.62it/s]Evaluating:  59%|█████▉    | 186/313 [00:31<00:22,  5.66it/s]Evaluating:  60%|█████▉    | 187/313 [00:32<00:22,  5.70it/s]Evaluating:  60%|██████    | 188/313 [00:32<00:21,  5.72it/s]Evaluating:  60%|██████    | 189/313 [00:32<00:21,  5.73it/s]Evaluating:  61%|██████    | 190/313 [00:32<00:21,  5.74it/s]Evaluating:  61%|██████    | 191/313 [00:32<00:21,  5.74it/s]Evaluating:  61%|██████▏   | 192/313 [00:32<00:21,  5.69it/s]Evaluating:  62%|██████▏   | 193/313 [00:33<00:21,  5.63it/s]Evaluating:  62%|██████▏   | 194/313 [00:33<00:21,  5.58it/s]Evaluating:  62%|██████▏   | 195/313 [00:33<00:21,  5.55it/s]Evaluating:  63%|██████▎   | 196/313 [00:33<00:21,  5.52it/s]Evaluating:  63%|██████▎   | 197/313 [00:33<00:21,  5.49it/s]Evaluating:  63%|██████▎   | 198/313 [00:33<00:20,  5.48it/s]Evaluating:  64%|██████▎   | 199/313 [00:34<00:20,  5.47it/s]Evaluating:  64%|██████▍   | 200/313 [00:34<00:20,  5.46it/s]Evaluating:  64%|██████▍   | 201/313 [00:34<00:20,  5.40it/s]Evaluating:  65%|██████▍   | 202/313 [00:34<00:20,  5.35it/s]Evaluating:  65%|██████▍   | 203/313 [00:34<00:20,  5.33it/s]Evaluating:  65%|██████▌   | 204/313 [00:35<00:20,  5.35it/s]Evaluating:  65%|██████▌   | 205/313 [00:35<00:20,  5.38it/s]Evaluating:  66%|██████▌   | 206/313 [00:35<00:19,  5.43it/s]Evaluating:  66%|██████▌   | 207/313 [00:35<00:19,  5.47it/s]Evaluating:  66%|██████▋   | 208/313 [00:35<00:19,  5.50it/s]Evaluating:  67%|██████▋   | 209/313 [00:36<00:18,  5.53it/s]Evaluating:  67%|██████▋   | 210/313 [00:36<00:18,  5.55it/s]Evaluating:  67%|██████▋   | 211/313 [00:36<00:18,  5.52it/s]Evaluating:  68%|██████▊   | 212/313 [00:36<00:18,  5.49it/s]Evaluating:  68%|██████▊   | 213/313 [00:36<00:18,  5.47it/s]Evaluating:  68%|██████▊   | 214/313 [00:36<00:18,  5.45it/s]Evaluating:  69%|██████▊   | 215/313 [00:37<00:18,  5.44it/s]Evaluating:  69%|██████▉   | 216/313 [00:37<00:17,  5.44it/s]Evaluating:  69%|██████▉   | 217/313 [00:37<00:17,  5.46it/s]Evaluating:  70%|██████▉   | 218/313 [00:37<00:17,  5.46it/s]Evaluating:  70%|██████▉   | 219/313 [00:37<00:17,  5.48it/s]Evaluating:  70%|███████   | 220/313 [00:38<00:16,  5.50it/s]Evaluating:  71%|███████   | 221/313 [00:38<00:16,  5.49it/s]Evaluating:  71%|███████   | 222/313 [00:38<00:16,  5.47it/s]Evaluating:  71%|███████   | 223/313 [00:38<00:16,  5.48it/s]Evaluating:  72%|███████▏  | 224/313 [00:38<00:16,  5.50it/s]Evaluating:  72%|███████▏  | 225/313 [00:38<00:15,  5.55it/s]Evaluating:  72%|███████▏  | 226/313 [00:39<00:15,  5.58it/s]Evaluating:  73%|███████▎  | 227/313 [00:39<00:15,  5.62it/s]Evaluating:  73%|███████▎  | 228/313 [00:39<00:15,  5.58it/s]Evaluating:  73%|███████▎  | 229/313 [00:39<00:15,  5.56it/s]Evaluating:  73%|███████▎  | 230/313 [00:39<00:14,  5.56it/s]Evaluating:  74%|███████▍  | 231/313 [00:40<00:14,  5.57it/s]Evaluating:  74%|███████▍  | 232/313 [00:40<00:14,  5.57it/s]Evaluating:  74%|███████▍  | 233/313 [00:40<00:14,  5.55it/s]Evaluating:  75%|███████▍  | 234/313 [00:40<00:14,  5.58it/s]Evaluating:  75%|███████▌  | 235/313 [00:40<00:13,  5.63it/s]Evaluating:  75%|███████▌  | 236/313 [00:40<00:13,  5.66it/s]Evaluating:  76%|███████▌  | 237/313 [00:41<00:14,  5.42it/s]Evaluating:  76%|███████▌  | 238/313 [00:41<00:13,  5.51it/s]Evaluating:  76%|███████▋  | 239/313 [00:41<00:13,  5.57it/s]Evaluating:  77%|███████▋  | 240/313 [00:41<00:12,  5.62it/s]Evaluating:  77%|███████▋  | 241/313 [00:41<00:12,  5.65it/s]Evaluating:  77%|███████▋  | 242/313 [00:41<00:12,  5.67it/s]Evaluating:  78%|███████▊  | 243/313 [00:42<00:12,  5.68it/s]Evaluating:  78%|███████▊  | 244/313 [00:42<00:12,  5.64it/s]Evaluating:  78%|███████▊  | 245/313 [00:42<00:12,  5.61it/s]Evaluating:  79%|███████▊  | 246/313 [00:42<00:11,  5.60it/s]Evaluating:  79%|███████▉  | 247/313 [00:42<00:11,  5.59it/s]Evaluating:  79%|███████▉  | 248/313 [00:43<00:11,  5.56it/s]Evaluating:  80%|███████▉  | 249/313 [00:43<00:11,  5.54it/s]Evaluating:  80%|███████▉  | 250/313 [00:43<00:11,  5.52it/s]Evaluating:  80%|████████  | 251/313 [00:43<00:11,  5.49it/s]Evaluating:  81%|████████  | 252/313 [00:43<00:11,  5.43it/s]Evaluating:  81%|████████  | 253/313 [00:43<00:11,  5.37it/s]Evaluating:  81%|████████  | 254/313 [00:44<00:11,  5.33it/s]Evaluating:  81%|████████▏ | 255/313 [00:44<00:10,  5.34it/s]Evaluating:  82%|████████▏ | 256/313 [00:44<00:10,  5.33it/s]Evaluating:  82%|████████▏ | 257/313 [00:44<00:10,  5.33it/s]Evaluating:  82%|████████▏ | 258/313 [00:44<00:10,  5.26it/s]Evaluating:  83%|████████▎ | 259/313 [00:45<00:10,  5.21it/s]Evaluating:  83%|████████▎ | 260/313 [00:45<00:10,  5.21it/s]Evaluating:  83%|████████▎ | 261/313 [00:45<00:09,  5.27it/s]Evaluating:  84%|████████▎ | 262/313 [00:45<00:09,  5.33it/s]Evaluating:  84%|████████▍ | 263/313 [00:45<00:09,  5.39it/s]Evaluating:  84%|████████▍ | 264/313 [00:46<00:11,  4.44it/s]Evaluating:  85%|████████▍ | 265/313 [00:46<00:10,  4.71it/s]Evaluating:  85%|████████▍ | 266/313 [00:46<00:09,  4.92it/s]Evaluating:  85%|████████▌ | 267/313 [00:46<00:09,  5.09it/s]Evaluating:  86%|████████▌ | 268/313 [00:46<00:08,  5.20it/s]Evaluating:  86%|████████▌ | 269/313 [00:47<00:08,  5.28it/s]Evaluating:  86%|████████▋ | 270/313 [00:47<00:08,  5.34it/s]Evaluating:  87%|████████▋ | 271/313 [00:47<00:07,  5.39it/s]Evaluating:  87%|████████▋ | 272/313 [00:47<00:07,  5.42it/s]Evaluating:  87%|████████▋ | 273/313 [00:47<00:07,  5.44it/s]Evaluating:  88%|████████▊ | 274/313 [00:48<00:07,  5.45it/s]Evaluating:  88%|████████▊ | 275/313 [00:48<00:06,  5.47it/s]Evaluating:  88%|████████▊ | 276/313 [00:48<00:06,  5.47it/s]Evaluating:  88%|████████▊ | 277/313 [00:48<00:06,  5.48it/s]Evaluating:  89%|████████▉ | 278/313 [00:48<00:06,  5.48it/s]Evaluating:  89%|████████▉ | 279/313 [00:48<00:06,  5.22it/s]Evaluating:  89%|████████▉ | 280/313 [00:49<00:06,  4.96it/s]Evaluating:  90%|████████▉ | 281/313 [00:49<00:06,  4.81it/s]Evaluating:  90%|█████████ | 282/313 [00:49<00:06,  4.78it/s]Evaluating:  90%|█████████ | 283/313 [00:49<00:06,  4.81it/s]Evaluating:  91%|█████████ | 284/313 [00:50<00:06,  4.82it/s]Evaluating:  91%|█████████ | 285/313 [00:50<00:05,  4.85it/s]Evaluating:  91%|█████████▏| 286/313 [00:50<00:05,  4.89it/s]Evaluating:  92%|█████████▏| 287/313 [00:50<00:05,  4.93it/s]Evaluating:  92%|█████████▏| 288/313 [00:50<00:05,  4.97it/s]Evaluating:  92%|█████████▏| 289/313 [00:51<00:04,  5.02it/s]Evaluating:  93%|█████████▎| 290/313 [00:51<00:04,  5.05it/s]Evaluating:  93%|█████████▎| 291/313 [00:51<00:05,  3.80it/s]Evaluating:  93%|█████████▎| 292/313 [00:51<00:05,  4.11it/s]Evaluating:  94%|█████████▎| 293/313 [00:52<00:04,  4.42it/s]Evaluating:  94%|█████████▍| 294/313 [00:52<00:04,  4.65it/s]Evaluating:  94%|█████████▍| 295/313 [00:52<00:03,  4.76it/s]Evaluating:  95%|█████████▍| 296/313 [00:52<00:03,  4.84it/s]Evaluating:  95%|█████████▍| 297/313 [00:52<00:03,  4.90it/s]Evaluating:  95%|█████████▌| 298/313 [00:52<00:03,  4.93it/s]Evaluating:  96%|█████████▌| 299/313 [00:53<00:02,  4.98it/s]Evaluating:  96%|█████████▌| 300/313 [00:53<00:02,  5.02it/s]Evaluating:  96%|█████████▌| 301/313 [00:53<00:02,  5.08it/s]Evaluating:  96%|█████████▋| 302/313 [00:53<00:02,  5.14it/s]Evaluating:  97%|█████████▋| 303/313 [00:53<00:01,  5.21it/s]Evaluating:  97%|█████████▋| 304/313 [00:54<00:01,  5.28it/s]Evaluating:  97%|█████████▋| 305/313 [00:54<00:01,  5.33it/s]Evaluating:  98%|█████████▊| 306/313 [00:54<00:01,  5.35it/s]Evaluating:  98%|█████████▊| 307/313 [00:54<00:01,  5.39it/s]Evaluating:  98%|█████████▊| 308/313 [00:54<00:00,  5.38it/s]Evaluating:  99%|█████████▊| 309/313 [00:55<00:00,  5.28it/s]Evaluating:  99%|█████████▉| 310/313 [00:55<00:00,  5.22it/s]Evaluating:  99%|█████████▉| 311/313 [00:55<00:00,  5.16it/s]Evaluating: 100%|█████████▉| 312/313 [00:55<00:00,  5.11it/s]Evaluating: 100%|██████████| 313/313 [00:55<00:00,  5.87it/s]Evaluating: 100%|██████████| 313/313 [00:55<00:00,  5.61it/s]
10/14/2021 16:09:49 - INFO - __main__ -   ***** Evaluation result  in no *****
10/14/2021 16:09:49 - INFO - __main__ -     f1 = 0.7261794634597595
10/14/2021 16:09:49 - INFO - __main__ -     loss = 1.0188681848894674
10/14/2021 16:09:49 - INFO - __main__ -     precision = 0.6926761628639859
10/14/2021 16:09:49 - INFO - __main__ -     recall = 0.76308845993612
10/14/2021 16:09:50 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/14/2021 16:09:51 - INFO - __main__ -   ***** Running evaluation  in da *****
10/14/2021 16:09:51 - INFO - __main__ -     Num examples = 10001
10/14/2021 16:09:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:57,  5.44it/s]Evaluating:   1%|          | 2/313 [00:00<00:57,  5.44it/s]Evaluating:   1%|          | 3/313 [00:00<00:56,  5.45it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:56,  5.45it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:56,  5.45it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:56,  5.44it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:56,  5.44it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:56,  5.44it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:55,  5.43it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:55,  5.44it/s]Evaluating:   4%|▎         | 11/313 [00:02<00:55,  5.44it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:55,  5.46it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:54,  5.47it/s]Evaluating:   4%|▍         | 14/313 [00:02<01:12,  4.13it/s]Evaluating:   5%|▍         | 15/313 [00:02<01:07,  4.45it/s]Evaluating:   5%|▌         | 16/313 [00:03<01:03,  4.70it/s]Evaluating:   5%|▌         | 17/313 [00:03<01:00,  4.90it/s]Evaluating:   6%|▌         | 18/313 [00:03<00:58,  5.04it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:57,  5.16it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:56,  5.23it/s]Evaluating:   7%|▋         | 21/313 [00:04<00:55,  5.28it/s]Evaluating:   7%|▋         | 22/313 [00:04<00:54,  5.31it/s]Evaluating:   7%|▋         | 23/313 [00:04<00:54,  5.34it/s]Evaluating:   8%|▊         | 24/313 [00:04<00:53,  5.36it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:53,  5.38it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:53,  5.39it/s]Evaluating:   9%|▊         | 27/313 [00:05<00:52,  5.40it/s]Evaluating:   9%|▉         | 28/313 [00:05<00:52,  5.41it/s]Evaluating:   9%|▉         | 29/313 [00:05<00:52,  5.41it/s]Evaluating:  10%|▉         | 30/313 [00:05<00:52,  5.41it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:52,  5.41it/s]Evaluating:  10%|█         | 32/313 [00:06<00:51,  5.42it/s]Evaluating:  11%|█         | 33/313 [00:06<00:51,  5.42it/s]Evaluating:  11%|█         | 34/313 [00:06<00:51,  5.41it/s]Evaluating:  11%|█         | 35/313 [00:06<00:51,  5.41it/s]Evaluating:  12%|█▏        | 36/313 [00:06<00:51,  5.41it/s]Evaluating:  12%|█▏        | 37/313 [00:07<00:50,  5.41it/s]Evaluating:  12%|█▏        | 38/313 [00:07<00:50,  5.41it/s]Evaluating:  12%|█▏        | 39/313 [00:07<00:50,  5.41it/s]Evaluating:  13%|█▎        | 40/313 [00:07<01:12,  3.79it/s]Evaluating:  13%|█▎        | 41/313 [00:08<01:05,  4.16it/s]Evaluating:  13%|█▎        | 42/313 [00:08<01:00,  4.47it/s]Evaluating:  14%|█▎        | 43/313 [00:08<00:57,  4.72it/s]Evaluating:  14%|█▍        | 44/313 [00:08<00:54,  4.91it/s]Evaluating:  14%|█▍        | 45/313 [00:08<00:52,  5.06it/s]Evaluating:  15%|█▍        | 46/313 [00:08<00:51,  5.14it/s]Evaluating:  15%|█▌        | 47/313 [00:09<00:50,  5.22it/s]Evaluating:  15%|█▌        | 48/313 [00:09<00:50,  5.28it/s]Evaluating:  16%|█▌        | 49/313 [00:09<00:49,  5.33it/s]Evaluating:  16%|█▌        | 50/313 [00:09<00:48,  5.37it/s]Evaluating:  16%|█▋        | 51/313 [00:09<00:48,  5.38it/s]Evaluating:  17%|█▋        | 52/313 [00:10<00:48,  5.36it/s]Evaluating:  17%|█▋        | 53/313 [00:10<00:48,  5.39it/s]Evaluating:  17%|█▋        | 54/313 [00:10<00:47,  5.43it/s]Evaluating:  18%|█▊        | 55/313 [00:10<00:47,  5.49it/s]Evaluating:  18%|█▊        | 56/313 [00:10<00:46,  5.53it/s]Evaluating:  18%|█▊        | 57/313 [00:10<00:46,  5.56it/s]Evaluating:  19%|█▊        | 58/313 [00:11<00:45,  5.57it/s]Evaluating:  19%|█▉        | 59/313 [00:11<00:45,  5.59it/s]Evaluating:  19%|█▉        | 60/313 [00:11<00:45,  5.57it/s]Evaluating:  19%|█▉        | 61/313 [00:11<00:45,  5.53it/s]Evaluating:  20%|█▉        | 62/313 [00:11<00:45,  5.52it/s]Evaluating:  20%|██        | 63/313 [00:12<00:45,  5.51it/s]Evaluating:  20%|██        | 64/313 [00:12<00:45,  5.50it/s]Evaluating:  21%|██        | 65/313 [00:12<00:45,  5.49it/s]Evaluating:  21%|██        | 66/313 [00:12<00:55,  4.45it/s]Evaluating:  21%|██▏       | 67/313 [00:12<00:51,  4.81it/s]Evaluating:  22%|██▏       | 68/313 [00:13<00:48,  5.08it/s]Evaluating:  22%|██▏       | 69/313 [00:13<00:46,  5.30it/s]Evaluating:  22%|██▏       | 70/313 [00:13<00:44,  5.46it/s]Evaluating:  23%|██▎       | 71/313 [00:13<00:43,  5.56it/s]Evaluating:  23%|██▎       | 72/313 [00:13<00:42,  5.64it/s]Evaluating:  23%|██▎       | 73/313 [00:13<00:42,  5.66it/s]Evaluating:  24%|██▎       | 74/313 [00:14<00:42,  5.66it/s]Evaluating:  24%|██▍       | 75/313 [00:14<00:42,  5.66it/s]Evaluating:  24%|██▍       | 76/313 [00:14<00:41,  5.66it/s]Evaluating:  25%|██▍       | 77/313 [00:14<00:41,  5.65it/s]Evaluating:  25%|██▍       | 78/313 [00:14<00:41,  5.65it/s]Evaluating:  25%|██▌       | 79/313 [00:14<00:41,  5.65it/s]Evaluating:  26%|██▌       | 80/313 [00:15<00:41,  5.60it/s]Evaluating:  26%|██▌       | 81/313 [00:15<00:41,  5.57it/s]Evaluating:  26%|██▌       | 82/313 [00:15<00:41,  5.53it/s]Evaluating:  27%|██▋       | 83/313 [00:15<00:41,  5.50it/s]Evaluating:  27%|██▋       | 84/313 [00:15<00:41,  5.47it/s]Evaluating:  27%|██▋       | 85/313 [00:16<00:41,  5.45it/s]Evaluating:  27%|██▋       | 86/313 [00:16<00:41,  5.43it/s]Evaluating:  28%|██▊       | 87/313 [00:16<00:41,  5.41it/s]Evaluating:  28%|██▊       | 88/313 [00:16<00:41,  5.40it/s]Evaluating:  28%|██▊       | 89/313 [00:16<00:41,  5.38it/s]Evaluating:  29%|██▉       | 90/313 [00:17<00:41,  5.38it/s]Evaluating:  29%|██▉       | 91/313 [00:17<00:41,  5.37it/s]Evaluating:  29%|██▉       | 92/313 [00:17<00:41,  5.37it/s]Evaluating:  30%|██▉       | 93/313 [00:17<00:44,  4.99it/s]Evaluating:  30%|███       | 94/313 [00:17<00:42,  5.19it/s]Evaluating:  30%|███       | 95/313 [00:17<00:40,  5.32it/s]Evaluating:  31%|███       | 96/313 [00:18<00:39,  5.43it/s]Evaluating:  31%|███       | 97/313 [00:18<00:39,  5.51it/s]Evaluating:  31%|███▏      | 98/313 [00:18<00:38,  5.56it/s]Evaluating:  32%|███▏      | 99/313 [00:18<00:38,  5.59it/s]Evaluating:  32%|███▏      | 100/313 [00:18<00:38,  5.58it/s]Evaluating:  32%|███▏      | 101/313 [00:19<00:38,  5.55it/s]Evaluating:  33%|███▎      | 102/313 [00:19<00:38,  5.53it/s]Evaluating:  33%|███▎      | 103/313 [00:19<00:38,  5.51it/s]Evaluating:  33%|███▎      | 104/313 [00:19<00:38,  5.47it/s]Evaluating:  34%|███▎      | 105/313 [00:19<00:38,  5.44it/s]Evaluating:  34%|███▍      | 106/313 [00:19<00:38,  5.41it/s]Evaluating:  34%|███▍      | 107/313 [00:20<00:38,  5.40it/s]Evaluating:  35%|███▍      | 108/313 [00:20<00:38,  5.39it/s]Evaluating:  35%|███▍      | 109/313 [00:20<00:37,  5.37it/s]Evaluating:  35%|███▌      | 110/313 [00:20<00:37,  5.35it/s]Evaluating:  35%|███▌      | 111/313 [00:20<00:37,  5.36it/s]Evaluating:  36%|███▌      | 112/313 [00:21<00:37,  5.37it/s]Evaluating:  36%|███▌      | 113/313 [00:21<00:37,  5.39it/s]Evaluating:  36%|███▋      | 114/313 [00:21<00:36,  5.45it/s]Evaluating:  37%|███▋      | 115/313 [00:21<00:36,  5.49it/s]Evaluating:  37%|███▋      | 116/313 [00:21<00:35,  5.53it/s]Evaluating:  37%|███▋      | 117/313 [00:21<00:35,  5.55it/s]Evaluating:  38%|███▊      | 118/313 [00:22<00:35,  5.55it/s]Evaluating:  38%|███▊      | 119/313 [00:22<00:35,  5.52it/s]Evaluating:  38%|███▊      | 120/313 [00:22<00:35,  5.48it/s]Evaluating:  39%|███▊      | 121/313 [00:22<00:35,  5.47it/s]Evaluating:  39%|███▉      | 122/313 [00:22<00:34,  5.47it/s]Evaluating:  39%|███▉      | 123/313 [00:23<00:34,  5.47it/s]Evaluating:  40%|███▉      | 124/313 [00:23<00:34,  5.45it/s]Evaluating:  40%|███▉      | 125/313 [00:23<00:34,  5.45it/s]Evaluating:  40%|████      | 126/313 [00:23<00:34,  5.46it/s]Evaluating:  41%|████      | 127/313 [00:23<00:34,  5.45it/s]Evaluating:  41%|████      | 128/313 [00:24<00:34,  5.41it/s]Evaluating:  41%|████      | 129/313 [00:24<00:34,  5.38it/s]Evaluating:  42%|████▏     | 130/313 [00:24<00:34,  5.38it/s]Evaluating:  42%|████▏     | 131/313 [00:24<00:33,  5.39it/s]Evaluating:  42%|████▏     | 132/313 [00:24<00:33,  5.42it/s]Evaluating:  42%|████▏     | 133/313 [00:24<00:33,  5.44it/s]Evaluating:  43%|████▎     | 134/313 [00:25<00:32,  5.48it/s]Evaluating:  43%|████▎     | 135/313 [00:25<00:32,  5.51it/s]Evaluating:  43%|████▎     | 136/313 [00:25<00:31,  5.53it/s]Evaluating:  44%|████▍     | 137/313 [00:25<00:31,  5.53it/s]Evaluating:  44%|████▍     | 138/313 [00:25<00:31,  5.49it/s]Evaluating:  44%|████▍     | 139/313 [00:26<00:31,  5.45it/s]Evaluating:  45%|████▍     | 140/313 [00:26<00:31,  5.42it/s]Evaluating:  45%|████▌     | 141/313 [00:26<00:31,  5.42it/s]Evaluating:  45%|████▌     | 142/313 [00:26<00:31,  5.41it/s]Evaluating:  46%|████▌     | 143/313 [00:26<00:31,  5.40it/s]Evaluating:  46%|████▌     | 144/313 [00:26<00:31,  5.40it/s]Evaluating:  46%|████▋     | 145/313 [00:27<00:30,  5.44it/s]Evaluating:  47%|████▋     | 146/313 [00:27<00:30,  5.47it/s]Evaluating:  47%|████▋     | 147/313 [00:27<00:30,  5.50it/s]Evaluating:  47%|████▋     | 148/313 [00:27<00:40,  4.05it/s]Evaluating:  48%|████▊     | 149/313 [00:28<00:37,  4.39it/s]Evaluating:  48%|████▊     | 150/313 [00:28<00:34,  4.68it/s]Evaluating:  48%|████▊     | 151/313 [00:28<00:32,  4.92it/s]Evaluating:  49%|████▊     | 152/313 [00:28<00:31,  5.10it/s]Evaluating:  49%|████▉     | 153/313 [00:28<00:30,  5.25it/s]Evaluating:  49%|████▉     | 154/313 [00:28<00:29,  5.36it/s]Evaluating:  50%|████▉     | 155/313 [00:29<00:29,  5.43it/s]Evaluating:  50%|████▉     | 156/313 [00:29<00:28,  5.47it/s]Evaluating:  50%|█████     | 157/313 [00:29<00:28,  5.46it/s]Evaluating:  50%|█████     | 158/313 [00:29<00:28,  5.43it/s]Evaluating:  51%|█████     | 159/313 [00:29<00:28,  5.40it/s]Evaluating:  51%|█████     | 160/313 [00:30<00:28,  5.42it/s]Evaluating:  51%|█████▏    | 161/313 [00:30<00:27,  5.45it/s]Evaluating:  52%|█████▏    | 162/313 [00:30<00:27,  5.47it/s]Evaluating:  52%|█████▏    | 163/313 [00:30<00:27,  5.46it/s]Evaluating:  52%|█████▏    | 164/313 [00:30<00:27,  5.45it/s]Evaluating:  53%|█████▎    | 165/313 [00:30<00:27,  5.42it/s]Evaluating:  53%|█████▎    | 166/313 [00:31<00:27,  5.38it/s]Evaluating:  53%|█████▎    | 167/313 [00:31<00:27,  5.36it/s]Evaluating:  54%|█████▎    | 168/313 [00:31<00:27,  5.35it/s]Evaluating:  54%|█████▍    | 169/313 [00:31<00:26,  5.36it/s]Evaluating:  54%|█████▍    | 170/313 [00:31<00:26,  5.36it/s]Evaluating:  55%|█████▍    | 171/313 [00:32<00:26,  5.35it/s]Evaluating:  55%|█████▍    | 172/313 [00:32<00:26,  5.35it/s]Evaluating:  55%|█████▌    | 173/313 [00:32<00:26,  5.34it/s]Evaluating:  56%|█████▌    | 174/313 [00:32<00:26,  5.34it/s]Evaluating:  56%|█████▌    | 175/313 [00:32<00:25,  5.35it/s]Evaluating:  56%|█████▌    | 176/313 [00:33<00:25,  5.36it/s]Evaluating:  57%|█████▋    | 177/313 [00:33<00:25,  5.33it/s]Evaluating:  57%|█████▋    | 178/313 [00:33<00:25,  5.32it/s]Evaluating:  57%|█████▋    | 179/313 [00:33<00:25,  5.30it/s]Evaluating:  58%|█████▊    | 180/313 [00:33<00:25,  5.31it/s]Evaluating:  58%|█████▊    | 181/313 [00:33<00:24,  5.33it/s]Evaluating:  58%|█████▊    | 182/313 [00:34<00:24,  5.37it/s]Evaluating:  58%|█████▊    | 183/313 [00:34<00:24,  5.40it/s]Evaluating:  59%|█████▉    | 184/313 [00:34<00:23,  5.43it/s]Evaluating:  59%|█████▉    | 185/313 [00:34<00:23,  5.43it/s]Evaluating:  59%|█████▉    | 186/313 [00:34<00:23,  5.42it/s]Evaluating:  60%|█████▉    | 187/313 [00:35<00:23,  5.41it/s]Evaluating:  60%|██████    | 188/313 [00:35<00:23,  5.39it/s]Evaluating:  60%|██████    | 189/313 [00:35<00:23,  5.35it/s]Evaluating:  61%|██████    | 190/313 [00:35<00:23,  5.33it/s]Evaluating:  61%|██████    | 191/313 [00:35<00:22,  5.32it/s]Evaluating:  61%|██████▏   | 192/313 [00:36<00:22,  5.31it/s]Evaluating:  62%|██████▏   | 193/313 [00:36<00:22,  5.32it/s]Evaluating:  62%|██████▏   | 194/313 [00:36<00:22,  5.34it/s]Evaluating:  62%|██████▏   | 195/313 [00:36<00:21,  5.38it/s]Evaluating:  63%|██████▎   | 196/313 [00:36<00:21,  5.39it/s]Evaluating:  63%|██████▎   | 197/313 [00:36<00:21,  5.43it/s]Evaluating:  63%|██████▎   | 198/313 [00:37<00:21,  5.46it/s]Evaluating:  64%|██████▎   | 199/313 [00:37<00:20,  5.48it/s]Evaluating:  64%|██████▍   | 200/313 [00:37<00:20,  5.45it/s]Evaluating:  64%|██████▍   | 201/313 [00:37<00:21,  5.27it/s]Evaluating:  65%|██████▍   | 202/313 [00:37<00:20,  5.33it/s]Evaluating:  65%|██████▍   | 203/313 [00:38<00:20,  5.37it/s]Evaluating:  65%|██████▌   | 204/313 [00:38<00:20,  5.41it/s]Evaluating:  65%|██████▌   | 205/313 [00:38<00:19,  5.44it/s]Evaluating:  66%|██████▌   | 206/313 [00:38<00:19,  5.48it/s]Evaluating:  66%|██████▌   | 207/313 [00:38<00:19,  5.53it/s]Evaluating:  66%|██████▋   | 208/313 [00:38<00:18,  5.56it/s]Evaluating:  67%|██████▋   | 209/313 [00:39<00:18,  5.52it/s]Evaluating:  67%|██████▋   | 210/313 [00:39<00:18,  5.49it/s]Evaluating:  67%|██████▋   | 211/313 [00:39<00:18,  5.46it/s]Evaluating:  68%|██████▊   | 212/313 [00:39<00:18,  5.46it/s]Evaluating:  68%|██████▊   | 213/313 [00:39<00:18,  5.46it/s]Evaluating:  68%|██████▊   | 214/313 [00:40<00:18,  5.49it/s]Evaluating:  69%|██████▊   | 215/313 [00:40<00:17,  5.51it/s]Evaluating:  69%|██████▉   | 216/313 [00:40<00:17,  5.57it/s]Evaluating:  69%|██████▉   | 217/313 [00:40<00:17,  5.62it/s]Evaluating:  70%|██████▉   | 218/313 [00:40<00:16,  5.65it/s]Evaluating:  70%|██████▉   | 219/313 [00:40<00:16,  5.67it/s]Evaluating:  70%|███████   | 220/313 [00:41<00:16,  5.69it/s]Evaluating:  71%|███████   | 221/313 [00:41<00:16,  5.70it/s]Evaluating:  71%|███████   | 222/313 [00:41<00:15,  5.71it/s]Evaluating:  71%|███████   | 223/313 [00:41<00:15,  5.72it/s]Evaluating:  72%|███████▏  | 224/313 [00:41<00:15,  5.71it/s]Evaluating:  72%|███████▏  | 225/313 [00:41<00:15,  5.65it/s]Evaluating:  72%|███████▏  | 226/313 [00:42<00:15,  5.60it/s]Evaluating:  73%|███████▎  | 227/313 [00:42<00:15,  5.59it/s]Evaluating:  73%|███████▎  | 228/313 [00:42<00:21,  3.97it/s]Evaluating:  73%|███████▎  | 229/313 [00:42<00:19,  4.38it/s]Evaluating:  73%|███████▎  | 230/313 [00:43<00:17,  4.71it/s]Evaluating:  74%|███████▍  | 231/313 [00:43<00:16,  4.97it/s]Evaluating:  74%|███████▍  | 232/313 [00:43<00:15,  5.18it/s]Evaluating:  74%|███████▍  | 233/313 [00:43<00:15,  5.33it/s]Evaluating:  75%|███████▍  | 234/313 [00:43<00:14,  5.41it/s]Evaluating:  75%|███████▌  | 235/313 [00:44<00:14,  5.46it/s]Evaluating:  75%|███████▌  | 236/313 [00:44<00:13,  5.51it/s]Evaluating:  76%|███████▌  | 237/313 [00:44<00:13,  5.56it/s]Evaluating:  76%|███████▌  | 238/313 [00:44<00:13,  5.59it/s]Evaluating:  76%|███████▋  | 239/313 [00:44<00:13,  5.62it/s]Evaluating:  77%|███████▋  | 240/313 [00:44<00:12,  5.63it/s]Evaluating:  77%|███████▋  | 241/313 [00:45<00:12,  5.61it/s]Evaluating:  77%|███████▋  | 242/313 [00:45<00:12,  5.61it/s]Evaluating:  78%|███████▊  | 243/313 [00:45<00:12,  5.61it/s]Evaluating:  78%|███████▊  | 244/313 [00:45<00:12,  5.61it/s]Evaluating:  78%|███████▊  | 245/313 [00:45<00:12,  5.61it/s]Evaluating:  79%|███████▊  | 246/313 [00:45<00:11,  5.62it/s]Evaluating:  79%|███████▉  | 247/313 [00:46<00:11,  5.64it/s]Evaluating:  79%|███████▉  | 248/313 [00:46<00:11,  5.65it/s]Evaluating:  80%|███████▉  | 249/313 [00:46<00:11,  5.63it/s]Evaluating:  80%|███████▉  | 250/313 [00:46<00:11,  5.61it/s]Evaluating:  80%|████████  | 251/313 [00:46<00:11,  5.61it/s]Evaluating:  81%|████████  | 252/313 [00:47<00:10,  5.60it/s]Evaluating:  81%|████████  | 253/313 [00:47<00:10,  5.59it/s]Evaluating:  81%|████████  | 254/313 [00:47<00:10,  5.58it/s]Evaluating:  81%|████████▏ | 255/313 [00:47<00:11,  5.25it/s]Evaluating:  82%|████████▏ | 256/313 [00:47<00:10,  5.34it/s]Evaluating:  82%|████████▏ | 257/313 [00:47<00:10,  5.38it/s]Evaluating:  82%|████████▏ | 258/313 [00:48<00:10,  5.38it/s]Evaluating:  83%|████████▎ | 259/313 [00:48<00:10,  5.38it/s]Evaluating:  83%|████████▎ | 260/313 [00:48<00:09,  5.39it/s]Evaluating:  83%|████████▎ | 261/313 [00:48<00:09,  5.41it/s]Evaluating:  84%|████████▎ | 262/313 [00:48<00:09,  5.47it/s]Evaluating:  84%|████████▍ | 263/313 [00:49<00:09,  5.53it/s]Evaluating:  84%|████████▍ | 264/313 [00:49<00:08,  5.57it/s]Evaluating:  85%|████████▍ | 265/313 [00:49<00:08,  5.59it/s]Evaluating:  85%|████████▍ | 266/313 [00:49<00:08,  5.61it/s]Evaluating:  85%|████████▌ | 267/313 [00:49<00:08,  5.59it/s]Evaluating:  86%|████████▌ | 268/313 [00:49<00:08,  5.56it/s]Evaluating:  86%|████████▌ | 269/313 [00:50<00:07,  5.55it/s]Evaluating:  86%|████████▋ | 270/313 [00:50<00:07,  5.54it/s]Evaluating:  87%|████████▋ | 271/313 [00:50<00:07,  5.53it/s]Evaluating:  87%|████████▋ | 272/313 [00:50<00:07,  5.52it/s]Evaluating:  87%|████████▋ | 273/313 [00:50<00:07,  5.54it/s]Evaluating:  88%|████████▊ | 274/313 [00:51<00:07,  5.56it/s]Evaluating:  88%|████████▊ | 275/313 [00:51<00:06,  5.52it/s]Evaluating:  88%|████████▊ | 276/313 [00:51<00:06,  5.48it/s]Evaluating:  88%|████████▊ | 277/313 [00:51<00:06,  5.46it/s]Evaluating:  89%|████████▉ | 278/313 [00:51<00:06,  5.46it/s]Evaluating:  89%|████████▉ | 279/313 [00:51<00:06,  5.45it/s]Evaluating:  89%|████████▉ | 280/313 [00:52<00:06,  5.47it/s]Evaluating:  90%|████████▉ | 281/313 [00:52<00:05,  5.49it/s]Evaluating:  90%|█████████ | 282/313 [00:52<00:05,  5.53it/s]Evaluating:  90%|█████████ | 283/313 [00:52<00:05,  5.56it/s]Evaluating:  91%|█████████ | 284/313 [00:52<00:05,  5.53it/s]Evaluating:  91%|█████████ | 285/313 [00:53<00:05,  5.50it/s]Evaluating:  91%|█████████▏| 286/313 [00:53<00:04,  5.49it/s]Evaluating:  92%|█████████▏| 287/313 [00:53<00:04,  5.52it/s]Evaluating:  92%|█████████▏| 288/313 [00:53<00:04,  5.56it/s]Evaluating:  92%|█████████▏| 289/313 [00:53<00:04,  5.59it/s]Evaluating:  93%|█████████▎| 290/313 [00:53<00:04,  5.58it/s]Evaluating:  93%|█████████▎| 291/313 [00:54<00:04,  5.15it/s]Evaluating:  93%|█████████▎| 292/313 [00:54<00:04,  4.83it/s]Evaluating:  94%|█████████▎| 293/313 [00:54<00:04,  4.56it/s]Evaluating:  94%|█████████▍| 294/313 [00:54<00:04,  4.52it/s]Evaluating:  94%|█████████▍| 295/313 [00:55<00:03,  4.62it/s]Evaluating:  95%|█████████▍| 296/313 [00:55<00:03,  4.71it/s]Evaluating:  95%|█████████▍| 297/313 [00:55<00:03,  4.78it/s]Evaluating:  95%|█████████▌| 298/313 [00:55<00:03,  4.85it/s]Evaluating:  96%|█████████▌| 299/313 [00:55<00:02,  4.90it/s]Evaluating:  96%|█████████▌| 300/313 [00:56<00:02,  4.94it/s]Evaluating:  96%|█████████▌| 301/313 [00:56<00:02,  5.00it/s]Evaluating:  96%|█████████▋| 302/313 [00:56<00:02,  5.05it/s]Evaluating:  97%|█████████▋| 303/313 [00:56<00:01,  5.10it/s]Evaluating:  97%|█████████▋| 304/313 [00:56<00:01,  5.13it/s]Evaluating:  97%|█████████▋| 305/313 [00:57<00:01,  5.18it/s]Evaluating:  98%|█████████▊| 306/313 [00:57<00:01,  5.22it/s]Evaluating:  98%|█████████▊| 307/313 [00:57<00:01,  5.27it/s]Evaluating:  98%|█████████▊| 308/313 [00:57<00:00,  5.20it/s]Evaluating:  99%|█████████▊| 309/313 [00:57<00:00,  5.13it/s]Evaluating:  99%|█████████▉| 310/313 [00:58<00:00,  5.09it/s]Evaluating:  99%|█████████▉| 311/313 [00:58<00:00,  5.06it/s]Evaluating: 100%|█████████▉| 312/313 [00:58<00:00,  5.05it/s]Evaluating: 100%|██████████| 313/313 [00:58<00:00,  5.75it/s]Evaluating: 100%|██████████| 313/313 [00:58<00:00,  5.35it/s]
10/14/2021 16:10:51 - INFO - __main__ -   ***** Evaluation result  in da *****
10/14/2021 16:10:51 - INFO - __main__ -     f1 = 0.7939487323154502
10/14/2021 16:10:51 - INFO - __main__ -     loss = 0.7669835780708554
10/14/2021 16:10:51 - INFO - __main__ -     precision = 0.7629559833086552
10/14/2021 16:10:51 - INFO - __main__ -     recall = 0.8275660680391298
10/14/2021 16:10:51 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:11:08 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:11:08 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:11:29 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 16:11:31 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:11:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 16:11:31 - INFO - __main__ -   Seed = 2
10/14/2021 16:11:31 - INFO - root -   save model
10/14/2021 16:11:31 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:11:31 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:11:43 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:11:43 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 16:11:43 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
10/14/2021 16:11:43 - INFO - root -   Trying to decide if add adapter
10/14/2021 16:11:43 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 16:11:43 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/14/2021 16:11:43 - INFO - __main__ -   Language = en
10/14/2021 16:11:43 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 16:11:44 - INFO - __main__ -   Language = is
10/14/2021 16:11:44 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/14/2021 16:11:45 - INFO - __main__ -   Language = de
10/14/2021 16:11:45 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/14/2021 16:11:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/14/2021 16:11:49 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/14/2021 16:11:49 - INFO - __main__ -     Num examples = 100
10/14/2021 16:11:49 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  5.37it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  5.60it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  5.79it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  7.24it/s]
10/14/2021 16:11:50 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/14/2021 16:11:50 - INFO - __main__ -     f1 = 0.6409266409266409
10/14/2021 16:11:50 - INFO - __main__ -     loss = 1.2338898181915283
10/14/2021 16:11:50 - INFO - __main__ -     precision = 0.5971223021582733
10/14/2021 16:11:50 - INFO - __main__ -     recall = 0.6916666666666667
10/14/2021 16:11:50 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/14/2021 16:11:51 - INFO - __main__ -   ***** Running evaluation  in no *****
10/14/2021 16:11:51 - INFO - __main__ -     Num examples = 10000
10/14/2021 16:11:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:50,  6.13it/s]Evaluating:   1%|          | 2/313 [00:00<00:50,  6.10it/s]Evaluating:   1%|          | 3/313 [00:00<00:50,  6.09it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:50,  6.09it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:50,  6.08it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:50,  6.08it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.08it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.07it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:50,  6.07it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:49,  6.07it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:49,  6.07it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:49,  6.07it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:49,  6.06it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:48,  6.05it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:48,  6.05it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.05it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  6.03it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  6.02it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:48,  6.02it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:47,  6.02it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:47,  6.02it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  6.02it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  6.01it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  6.02it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:47,  6.02it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:46,  6.02it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:46,  6.03it/s]Evaluating:  10%|█         | 32/313 [00:05<00:46,  6.03it/s]Evaluating:  11%|█         | 33/313 [00:05<00:46,  6.03it/s]Evaluating:  11%|█         | 34/313 [00:05<00:46,  6.03it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  6.02it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:45,  6.02it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:45,  6.02it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  6.03it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.03it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  6.03it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.02it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:45,  6.01it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:44,  6.00it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  6.00it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:45,  5.95it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  5.97it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  5.98it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:44,  5.99it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:44,  5.99it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:43,  5.99it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  5.97it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  5.97it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  5.97it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:43,  5.98it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:43,  5.97it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:43,  5.97it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  5.97it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  5.97it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  5.97it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:42,  5.98it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:42,  5.97it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:42,  5.94it/s]Evaluating:  20%|██        | 63/313 [00:10<00:42,  5.94it/s]Evaluating:  20%|██        | 64/313 [00:10<00:42,  5.92it/s]Evaluating:  21%|██        | 65/313 [00:10<00:41,  5.92it/s]Evaluating:  21%|██        | 66/313 [00:10<00:41,  5.94it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:41,  5.95it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:41,  5.94it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:41,  5.94it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  5.94it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  5.95it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:40,  5.96it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  5.96it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:40,  5.96it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:40,  5.95it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:40,  5.90it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  5.90it/s]Evaluating:  25%|██▍       | 78/313 [00:13<00:39,  5.91it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.91it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:39,  5.90it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:39,  5.91it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:39,  5.92it/s]Evaluating:  27%|██▋       | 83/313 [00:13<00:38,  5.92it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:38,  5.92it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:38,  5.88it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:38,  5.86it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:38,  5.87it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:38,  5.88it/s]Evaluating:  28%|██▊       | 89/313 [00:14<00:38,  5.87it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:37,  5.88it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:37,  5.86it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:37,  5.85it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:37,  5.84it/s]Evaluating:  30%|███       | 94/313 [00:15<00:37,  5.85it/s]Evaluating:  30%|███       | 95/313 [00:15<00:37,  5.85it/s]Evaluating:  31%|███       | 96/313 [00:16<00:37,  5.86it/s]Evaluating:  31%|███       | 97/313 [00:16<00:37,  5.83it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:36,  5.82it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:36,  5.82it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:36,  5.84it/s]Evaluating:  32%|███▏      | 101/313 [00:16<00:36,  5.85it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:36,  5.86it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:35,  5.86it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:36,  5.72it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:36,  5.76it/s]Evaluating:  34%|███▍      | 106/313 [00:17<00:35,  5.80it/s]Evaluating:  34%|███▍      | 107/313 [00:17<00:35,  5.81it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:35,  5.83it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:34,  5.84it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:34,  5.84it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:34,  5.85it/s]Evaluating:  36%|███▌      | 112/313 [00:18<00:34,  5.86it/s]Evaluating:  36%|███▌      | 113/313 [00:18<00:34,  5.85it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:33,  5.85it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:33,  5.86it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:33,  5.85it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:33,  5.84it/s]Evaluating:  38%|███▊      | 118/313 [00:19<00:33,  5.86it/s]Evaluating:  38%|███▊      | 119/313 [00:20<00:33,  5.85it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:33,  5.85it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:32,  5.85it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:32,  5.85it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:32,  5.84it/s]Evaluating:  40%|███▉      | 124/313 [00:20<00:32,  5.84it/s]Evaluating:  40%|███▉      | 125/313 [00:21<00:32,  5.83it/s]Evaluating:  40%|████      | 126/313 [00:21<00:32,  5.84it/s]Evaluating:  41%|████      | 127/313 [00:21<00:31,  5.84it/s]Evaluating:  41%|████      | 128/313 [00:21<00:31,  5.82it/s]Evaluating:  41%|████      | 129/313 [00:21<00:31,  5.83it/s]Evaluating:  42%|████▏     | 130/313 [00:21<00:31,  5.84it/s]Evaluating:  42%|████▏     | 131/313 [00:22<00:31,  5.83it/s]Evaluating:  42%|████▏     | 132/313 [00:22<00:31,  5.83it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:50,  3.57it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:44,  4.04it/s]Evaluating:  43%|████▎     | 135/313 [00:23<00:40,  4.44it/s]Evaluating:  43%|████▎     | 136/313 [00:23<00:36,  4.78it/s]Evaluating:  44%|████▍     | 137/313 [00:23<00:34,  5.04it/s]Evaluating:  44%|████▍     | 138/313 [00:23<00:33,  5.25it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:32,  5.37it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:31,  5.45it/s]Evaluating:  45%|████▌     | 141/313 [00:24<00:31,  5.52it/s]Evaluating:  45%|████▌     | 142/313 [00:24<00:30,  5.57it/s]Evaluating:  46%|████▌     | 143/313 [00:24<00:30,  5.60it/s]Evaluating:  46%|████▌     | 144/313 [00:24<00:30,  5.62it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:29,  5.63it/s]Evaluating:  47%|████▋     | 146/313 [00:25<00:29,  5.61it/s]Evaluating:  47%|████▋     | 147/313 [00:25<00:29,  5.62it/s]Evaluating:  47%|████▋     | 148/313 [00:25<00:29,  5.63it/s]Evaluating:  48%|████▊     | 149/313 [00:25<00:29,  5.62it/s]Evaluating:  48%|████▊     | 150/313 [00:25<00:29,  5.60it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:28,  5.59it/s]Evaluating:  49%|████▊     | 152/313 [00:26<00:28,  5.59it/s]Evaluating:  49%|████▉     | 153/313 [00:26<00:28,  5.53it/s]Evaluating:  49%|████▉     | 154/313 [00:26<00:29,  5.47it/s]Evaluating:  50%|████▉     | 155/313 [00:26<00:29,  5.43it/s]Evaluating:  50%|████▉     | 156/313 [00:26<00:28,  5.43it/s]Evaluating:  50%|█████     | 157/313 [00:27<00:28,  5.38it/s]Evaluating:  50%|█████     | 158/313 [00:27<00:28,  5.36it/s]Evaluating:  51%|█████     | 159/313 [00:27<00:31,  4.97it/s]Evaluating:  51%|█████     | 160/313 [00:27<00:29,  5.18it/s]Evaluating:  51%|█████▏    | 161/313 [00:27<00:28,  5.33it/s]Evaluating:  52%|█████▏    | 162/313 [00:28<00:27,  5.46it/s]Evaluating:  52%|█████▏    | 163/313 [00:28<00:27,  5.54it/s]Evaluating:  52%|█████▏    | 164/313 [00:28<00:26,  5.60it/s]Evaluating:  53%|█████▎    | 165/313 [00:28<00:26,  5.64it/s]Evaluating:  53%|█████▎    | 166/313 [00:28<00:25,  5.67it/s]Evaluating:  53%|█████▎    | 167/313 [00:28<00:25,  5.69it/s]Evaluating:  54%|█████▎    | 168/313 [00:29<00:25,  5.72it/s]Evaluating:  54%|█████▍    | 169/313 [00:29<00:25,  5.70it/s]Evaluating:  54%|█████▍    | 170/313 [00:29<00:25,  5.68it/s]Evaluating:  55%|█████▍    | 171/313 [00:29<00:25,  5.63it/s]Evaluating:  55%|█████▍    | 172/313 [00:29<00:25,  5.58it/s]Evaluating:  55%|█████▌    | 173/313 [00:29<00:25,  5.54it/s]Evaluating:  56%|█████▌    | 174/313 [00:30<00:25,  5.50it/s]Evaluating:  56%|█████▌    | 175/313 [00:30<00:25,  5.47it/s]Evaluating:  56%|█████▌    | 176/313 [00:30<00:25,  5.45it/s]Evaluating:  57%|█████▋    | 177/313 [00:30<00:24,  5.44it/s]Evaluating:  57%|█████▋    | 178/313 [00:30<00:24,  5.45it/s]Evaluating:  57%|█████▋    | 179/313 [00:31<00:24,  5.42it/s]Evaluating:  58%|█████▊    | 180/313 [00:31<00:24,  5.43it/s]Evaluating:  58%|█████▊    | 181/313 [00:31<00:24,  5.43it/s]Evaluating:  58%|█████▊    | 182/313 [00:31<00:24,  5.41it/s]Evaluating:  58%|█████▊    | 183/313 [00:31<00:24,  5.40it/s]Evaluating:  59%|█████▉    | 184/313 [00:31<00:23,  5.40it/s]Evaluating:  59%|█████▉    | 185/313 [00:32<00:23,  5.42it/s]Evaluating:  59%|█████▉    | 186/313 [00:32<00:23,  5.43it/s]Evaluating:  60%|█████▉    | 187/313 [00:32<00:26,  4.80it/s]Evaluating:  60%|██████    | 188/313 [00:32<00:24,  5.04it/s]Evaluating:  60%|██████    | 189/313 [00:32<00:23,  5.23it/s]Evaluating:  61%|██████    | 190/313 [00:33<00:22,  5.39it/s]Evaluating:  61%|██████    | 191/313 [00:33<00:22,  5.48it/s]Evaluating:  61%|██████▏   | 192/313 [00:33<00:22,  5.45it/s]Evaluating:  62%|██████▏   | 193/313 [00:33<00:22,  5.41it/s]Evaluating:  62%|██████▏   | 194/313 [00:33<00:22,  5.39it/s]Evaluating:  62%|██████▏   | 195/313 [00:34<00:21,  5.37it/s]Evaluating:  63%|██████▎   | 196/313 [00:34<00:21,  5.35it/s]Evaluating:  63%|██████▎   | 197/313 [00:34<00:21,  5.34it/s]Evaluating:  63%|██████▎   | 198/313 [00:34<00:21,  5.35it/s]Evaluating:  64%|██████▎   | 199/313 [00:34<00:21,  5.34it/s]Evaluating:  64%|██████▍   | 200/313 [00:34<00:21,  5.35it/s]Evaluating:  64%|██████▍   | 201/313 [00:35<00:20,  5.35it/s]Evaluating:  65%|██████▍   | 202/313 [00:35<00:20,  5.35it/s]Evaluating:  65%|██████▍   | 203/313 [00:35<00:20,  5.35it/s]Evaluating:  65%|██████▌   | 204/313 [00:35<00:20,  5.35it/s]Evaluating:  65%|██████▌   | 205/313 [00:35<00:20,  5.33it/s]Evaluating:  66%|██████▌   | 206/313 [00:36<00:20,  5.31it/s]Evaluating:  66%|██████▌   | 207/313 [00:36<00:19,  5.31it/s]Evaluating:  66%|██████▋   | 208/313 [00:36<00:19,  5.30it/s]Evaluating:  67%|██████▋   | 209/313 [00:36<00:19,  5.29it/s]Evaluating:  67%|██████▋   | 210/313 [00:36<00:19,  5.30it/s]Evaluating:  67%|██████▋   | 211/313 [00:37<00:19,  5.32it/s]Evaluating:  68%|██████▊   | 212/313 [00:37<00:18,  5.37it/s]Evaluating:  68%|██████▊   | 213/313 [00:37<00:23,  4.18it/s]Evaluating:  68%|██████▊   | 214/313 [00:37<00:22,  4.46it/s]Evaluating:  69%|██████▊   | 215/313 [00:37<00:20,  4.68it/s]Evaluating:  69%|██████▉   | 216/313 [00:38<00:20,  4.84it/s]Evaluating:  69%|██████▉   | 217/313 [00:38<00:19,  4.96it/s]Evaluating:  70%|██████▉   | 218/313 [00:38<00:18,  5.05it/s]Evaluating:  70%|██████▉   | 219/313 [00:38<00:18,  5.11it/s]Evaluating:  70%|███████   | 220/313 [00:38<00:17,  5.18it/s]Evaluating:  71%|███████   | 221/313 [00:39<00:17,  5.26it/s]Evaluating:  71%|███████   | 222/313 [00:39<00:17,  5.32it/s]Evaluating:  71%|███████   | 223/313 [00:39<00:16,  5.37it/s]Evaluating:  72%|███████▏  | 224/313 [00:39<00:16,  5.39it/s]Evaluating:  72%|███████▏  | 225/313 [00:39<00:16,  5.42it/s]Evaluating:  72%|███████▏  | 226/313 [00:40<00:16,  5.43it/s]Evaluating:  73%|███████▎  | 227/313 [00:40<00:15,  5.41it/s]Evaluating:  73%|███████▎  | 228/313 [00:40<00:15,  5.39it/s]Evaluating:  73%|███████▎  | 229/313 [00:40<00:15,  5.39it/s]Evaluating:  73%|███████▎  | 230/313 [00:40<00:15,  5.39it/s]Evaluating:  74%|███████▍  | 231/313 [00:40<00:15,  5.41it/s]Evaluating:  74%|███████▍  | 232/313 [00:41<00:14,  5.44it/s]Evaluating:  74%|███████▍  | 233/313 [00:41<00:14,  5.47it/s]Evaluating:  75%|███████▍  | 234/313 [00:41<00:14,  5.50it/s]Evaluating:  75%|███████▌  | 235/313 [00:41<00:14,  5.52it/s]Evaluating:  75%|███████▌  | 236/313 [00:41<00:14,  5.45it/s]Evaluating:  76%|███████▌  | 237/313 [00:42<00:14,  5.43it/s]Evaluating:  76%|███████▌  | 238/313 [00:42<00:13,  5.41it/s]Evaluating:  76%|███████▋  | 239/313 [00:42<00:15,  4.81it/s]Evaluating:  77%|███████▋  | 240/313 [00:42<00:14,  4.94it/s]Evaluating:  77%|███████▋  | 241/313 [00:42<00:14,  5.03it/s]Evaluating:  77%|███████▋  | 242/313 [00:43<00:13,  5.11it/s]Evaluating:  78%|███████▊  | 243/313 [00:43<00:13,  5.16it/s]Evaluating:  78%|███████▊  | 244/313 [00:43<00:13,  5.22it/s]Evaluating:  78%|███████▊  | 245/313 [00:43<00:12,  5.30it/s]Evaluating:  79%|███████▊  | 246/313 [00:43<00:12,  5.36it/s]Evaluating:  79%|███████▉  | 247/313 [00:43<00:12,  5.43it/s]Evaluating:  79%|███████▉  | 248/313 [00:44<00:11,  5.47it/s]Evaluating:  80%|███████▉  | 249/313 [00:44<00:11,  5.50it/s]Evaluating:  80%|███████▉  | 250/313 [00:44<00:11,  5.48it/s]Evaluating:  80%|████████  | 251/313 [00:44<00:11,  5.44it/s]Evaluating:  81%|████████  | 252/313 [00:44<00:11,  5.41it/s]Evaluating:  81%|████████  | 253/313 [00:45<00:11,  5.41it/s]Evaluating:  81%|████████  | 254/313 [00:45<00:10,  5.41it/s]Evaluating:  81%|████████▏ | 255/313 [00:45<00:10,  5.41it/s]Evaluating:  82%|████████▏ | 256/313 [00:45<00:10,  5.42it/s]Evaluating:  82%|████████▏ | 257/313 [00:45<00:10,  5.45it/s]Evaluating:  82%|████████▏ | 258/313 [00:46<00:10,  5.46it/s]Evaluating:  83%|████████▎ | 259/313 [00:46<00:09,  5.47it/s]Evaluating:  83%|████████▎ | 260/313 [00:46<00:09,  5.42it/s]Evaluating:  83%|████████▎ | 261/313 [00:46<00:09,  5.37it/s]Evaluating:  84%|████████▎ | 262/313 [00:46<00:09,  5.34it/s]Evaluating:  84%|████████▍ | 263/313 [00:46<00:09,  5.33it/s]Evaluating:  84%|████████▍ | 264/313 [00:47<00:09,  5.32it/s]Evaluating:  85%|████████▍ | 265/313 [00:47<00:08,  5.34it/s]Evaluating:  85%|████████▍ | 266/313 [00:47<00:10,  4.35it/s]Evaluating:  85%|████████▌ | 267/313 [00:47<00:10,  4.58it/s]Evaluating:  86%|████████▌ | 268/313 [00:48<00:09,  4.76it/s]Evaluating:  86%|████████▌ | 269/313 [00:48<00:08,  4.89it/s]Evaluating:  86%|████████▋ | 270/313 [00:48<00:08,  4.99it/s]Evaluating:  87%|████████▋ | 271/313 [00:48<00:08,  5.06it/s]Evaluating:  87%|████████▋ | 272/313 [00:48<00:08,  5.12it/s]Evaluating:  87%|████████▋ | 273/313 [00:48<00:07,  5.17it/s]Evaluating:  88%|████████▊ | 274/313 [00:49<00:07,  5.22it/s]Evaluating:  88%|████████▊ | 275/313 [00:49<00:07,  5.28it/s]Evaluating:  88%|████████▊ | 276/313 [00:49<00:06,  5.32it/s]Evaluating:  88%|████████▊ | 277/313 [00:49<00:06,  5.35it/s]Evaluating:  89%|████████▉ | 278/313 [00:49<00:06,  5.36it/s]Evaluating:  89%|████████▉ | 279/313 [00:50<00:06,  5.38it/s]Evaluating:  89%|████████▉ | 280/313 [00:50<00:06,  5.41it/s]Evaluating:  90%|████████▉ | 281/313 [00:50<00:05,  5.41it/s]Evaluating:  90%|█████████ | 282/313 [00:50<00:05,  5.42it/s]Evaluating:  90%|█████████ | 283/313 [00:50<00:05,  5.35it/s]Evaluating:  91%|█████████ | 284/313 [00:51<00:05,  5.34it/s]Evaluating:  91%|█████████ | 285/313 [00:51<00:05,  5.38it/s]Evaluating:  91%|█████████▏| 286/313 [00:51<00:04,  5.41it/s]Evaluating:  92%|█████████▏| 287/313 [00:51<00:04,  5.43it/s]Evaluating:  92%|█████████▏| 288/313 [00:51<00:04,  5.45it/s]Evaluating:  92%|█████████▏| 289/313 [00:51<00:04,  5.46it/s]Evaluating:  93%|█████████▎| 290/313 [00:52<00:04,  5.47it/s]Evaluating:  93%|█████████▎| 291/313 [00:52<00:04,  5.42it/s]Evaluating:  93%|█████████▎| 292/313 [00:52<00:03,  5.40it/s]Evaluating:  94%|█████████▎| 293/313 [00:52<00:03,  5.42it/s]Evaluating:  94%|█████████▍| 294/313 [00:52<00:03,  5.44it/s]Evaluating:  94%|█████████▍| 295/313 [00:53<00:03,  5.44it/s]Evaluating:  95%|█████████▍| 296/313 [00:53<00:03,  5.44it/s]Evaluating:  95%|█████████▍| 297/313 [00:53<00:02,  5.45it/s]Evaluating:  95%|█████████▌| 298/313 [00:53<00:02,  5.46it/s]Evaluating:  96%|█████████▌| 299/313 [00:53<00:02,  5.46it/s]Evaluating:  96%|█████████▌| 300/313 [00:53<00:02,  5.40it/s]Evaluating:  96%|█████████▌| 301/313 [00:54<00:02,  5.35it/s]Evaluating:  96%|█████████▋| 302/313 [00:54<00:02,  5.33it/s]Evaluating:  97%|█████████▋| 303/313 [00:54<00:01,  5.32it/s]Evaluating:  97%|█████████▋| 304/313 [00:54<00:01,  5.35it/s]Evaluating:  97%|█████████▋| 305/313 [00:54<00:01,  5.38it/s]Evaluating:  98%|█████████▊| 306/313 [00:55<00:01,  5.40it/s]Evaluating:  98%|█████████▊| 307/313 [00:55<00:01,  5.41it/s]Evaluating:  98%|█████████▊| 308/313 [00:55<00:00,  5.37it/s]Evaluating:  99%|█████████▊| 309/313 [00:55<00:00,  5.36it/s]Evaluating:  99%|█████████▉| 310/313 [00:55<00:00,  5.33it/s]Evaluating:  99%|█████████▉| 311/313 [00:56<00:00,  5.24it/s]Evaluating: 100%|█████████▉| 312/313 [00:56<00:00,  5.20it/s]Evaluating: 100%|██████████| 313/313 [00:56<00:00,  5.93it/s]Evaluating: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]
10/14/2021 16:12:49 - INFO - __main__ -   ***** Evaluation result  in no *****
10/14/2021 16:12:49 - INFO - __main__ -     f1 = 0.7451948393891522
10/14/2021 16:12:49 - INFO - __main__ -     loss = 0.8540041556183141
10/14/2021 16:12:49 - INFO - __main__ -     precision = 0.7083594043298711
10/14/2021 16:12:49 - INFO - __main__ -     recall = 0.7860713789751423
10/14/2021 16:12:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/14/2021 16:12:50 - INFO - __main__ -   ***** Running evaluation  in da *****
10/14/2021 16:12:50 - INFO - __main__ -     Num examples = 10001
10/14/2021 16:12:50 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:57,  5.45it/s]Evaluating:   1%|          | 2/313 [00:00<00:57,  5.45it/s]Evaluating:   1%|          | 3/313 [00:00<00:56,  5.45it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:56,  5.45it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:56,  5.44it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:56,  5.44it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:56,  5.44it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:56,  5.45it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:55,  5.45it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:55,  5.45it/s]Evaluating:   4%|▎         | 11/313 [00:02<00:55,  5.45it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:55,  5.44it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:55,  5.44it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:54,  5.44it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:54,  5.45it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:54,  5.44it/s]Evaluating:   5%|▌         | 17/313 [00:03<00:54,  5.45it/s]Evaluating:   6%|▌         | 18/313 [00:03<00:54,  5.45it/s]Evaluating:   6%|▌         | 19/313 [00:03<01:04,  4.54it/s]Evaluating:   6%|▋         | 20/313 [00:03<01:01,  4.78it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:58,  4.95it/s]Evaluating:   7%|▋         | 22/313 [00:04<00:57,  5.09it/s]Evaluating:   7%|▋         | 23/313 [00:04<00:55,  5.19it/s]Evaluating:   8%|▊         | 24/313 [00:04<00:54,  5.26it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:54,  5.31it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:53,  5.35it/s]Evaluating:   9%|▊         | 27/313 [00:05<00:53,  5.38it/s]Evaluating:   9%|▉         | 28/313 [00:05<00:52,  5.40it/s]Evaluating:   9%|▉         | 29/313 [00:05<00:52,  5.41it/s]Evaluating:  10%|▉         | 30/313 [00:05<00:52,  5.42it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:51,  5.43it/s]Evaluating:  10%|█         | 32/313 [00:06<00:51,  5.43it/s]Evaluating:  11%|█         | 33/313 [00:06<00:51,  5.44it/s]Evaluating:  11%|█         | 34/313 [00:06<00:51,  5.44it/s]Evaluating:  11%|█         | 35/313 [00:06<00:51,  5.44it/s]Evaluating:  12%|█▏        | 36/313 [00:06<00:51,  5.42it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:50,  5.42it/s]Evaluating:  12%|█▏        | 38/313 [00:07<00:50,  5.42it/s]Evaluating:  12%|█▏        | 39/313 [00:07<00:50,  5.43it/s]Evaluating:  13%|█▎        | 40/313 [00:07<00:50,  5.43it/s]Evaluating:  13%|█▎        | 41/313 [00:07<00:50,  5.43it/s]Evaluating:  13%|█▎        | 42/313 [00:07<00:49,  5.44it/s]Evaluating:  14%|█▎        | 43/313 [00:08<00:49,  5.44it/s]Evaluating:  14%|█▍        | 44/313 [00:08<00:49,  5.43it/s]Evaluating:  14%|█▍        | 45/313 [00:08<00:49,  5.43it/s]Evaluating:  15%|█▍        | 46/313 [00:08<00:49,  5.43it/s]Evaluating:  15%|█▌        | 47/313 [00:08<00:49,  5.43it/s]Evaluating:  15%|█▌        | 48/313 [00:08<00:48,  5.42it/s]Evaluating:  16%|█▌        | 49/313 [00:09<00:48,  5.42it/s]Evaluating:  16%|█▌        | 50/313 [00:09<00:48,  5.42it/s]Evaluating:  16%|█▋        | 51/313 [00:09<00:48,  5.42it/s]Evaluating:  17%|█▋        | 52/313 [00:09<00:48,  5.42it/s]Evaluating:  17%|█▋        | 53/313 [00:09<00:48,  5.41it/s]Evaluating:  17%|█▋        | 54/313 [00:10<00:47,  5.41it/s]Evaluating:  18%|█▊        | 55/313 [00:10<00:47,  5.42it/s]Evaluating:  18%|█▊        | 56/313 [00:10<00:47,  5.42it/s]Evaluating:  18%|█▊        | 57/313 [00:10<00:47,  5.42it/s]Evaluating:  19%|█▊        | 58/313 [00:10<00:47,  5.42it/s]Evaluating:  19%|█▉        | 59/313 [00:10<00:46,  5.42it/s]Evaluating:  19%|█▉        | 60/313 [00:11<00:46,  5.41it/s]Evaluating:  19%|█▉        | 61/313 [00:11<00:46,  5.41it/s]Evaluating:  20%|█▉        | 62/313 [00:11<00:46,  5.41it/s]Evaluating:  20%|██        | 63/313 [00:11<00:46,  5.41it/s]Evaluating:  20%|██        | 64/313 [00:11<00:46,  5.41it/s]Evaluating:  21%|██        | 65/313 [00:12<00:45,  5.41it/s]Evaluating:  21%|██        | 66/313 [00:12<00:45,  5.41it/s]Evaluating:  21%|██▏       | 67/313 [00:12<00:45,  5.41it/s]Evaluating:  22%|██▏       | 68/313 [00:12<00:45,  5.41it/s]Evaluating:  22%|██▏       | 69/313 [00:12<00:45,  5.41it/s]Evaluating:  22%|██▏       | 70/313 [00:13<00:44,  5.40it/s]Evaluating:  23%|██▎       | 71/313 [00:13<00:44,  5.40it/s]Evaluating:  23%|██▎       | 72/313 [00:13<00:44,  5.40it/s]Evaluating:  23%|██▎       | 73/313 [00:13<00:44,  5.39it/s]Evaluating:  24%|██▎       | 74/313 [00:13<00:44,  5.40it/s]Evaluating:  24%|██▍       | 75/313 [00:13<00:44,  5.40it/s]Evaluating:  24%|██▍       | 76/313 [00:14<00:43,  5.40it/s]Evaluating:  25%|██▍       | 77/313 [00:14<00:43,  5.40it/s]Evaluating:  25%|██▍       | 78/313 [00:14<00:43,  5.40it/s]Evaluating:  25%|██▌       | 79/313 [00:14<00:43,  5.39it/s]Evaluating:  26%|██▌       | 80/313 [00:14<00:43,  5.39it/s]Evaluating:  26%|██▌       | 81/313 [00:15<00:43,  5.39it/s]Evaluating:  26%|██▌       | 82/313 [00:15<00:42,  5.39it/s]Evaluating:  27%|██▋       | 83/313 [00:15<00:42,  5.40it/s]Evaluating:  27%|██▋       | 84/313 [00:15<00:42,  5.41it/s]Evaluating:  27%|██▋       | 85/313 [00:15<00:42,  5.41it/s]Evaluating:  27%|██▋       | 86/313 [00:15<00:42,  5.40it/s]Evaluating:  28%|██▊       | 87/313 [00:16<00:41,  5.39it/s]Evaluating:  28%|██▊       | 88/313 [00:16<00:41,  5.39it/s]Evaluating:  28%|██▊       | 89/313 [00:16<00:41,  5.40it/s]Evaluating:  29%|██▉       | 90/313 [00:16<00:41,  5.39it/s]Evaluating:  29%|██▉       | 91/313 [00:16<00:41,  5.39it/s]Evaluating:  29%|██▉       | 92/313 [00:17<00:41,  5.38it/s]Evaluating:  30%|██▉       | 93/313 [00:17<00:40,  5.38it/s]Evaluating:  30%|███       | 94/313 [00:17<00:40,  5.39it/s]Evaluating:  30%|███       | 95/313 [00:17<00:40,  5.40it/s]Evaluating:  31%|███       | 96/313 [00:17<00:40,  5.42it/s]Evaluating:  31%|███       | 97/313 [00:18<00:39,  5.44it/s]Evaluating:  31%|███▏      | 98/313 [00:18<00:39,  5.46it/s]Evaluating:  32%|███▏      | 99/313 [00:18<00:38,  5.49it/s]Evaluating:  32%|███▏      | 100/313 [00:18<00:38,  5.50it/s]Evaluating:  32%|███▏      | 101/313 [00:18<00:38,  5.47it/s]Evaluating:  33%|███▎      | 102/313 [00:18<00:38,  5.44it/s]Evaluating:  33%|███▎      | 103/313 [00:19<00:38,  5.42it/s]Evaluating:  33%|███▎      | 104/313 [00:19<00:38,  5.40it/s]Evaluating:  34%|███▎      | 105/313 [00:19<00:38,  5.41it/s]Evaluating:  34%|███▍      | 106/313 [00:19<00:38,  5.40it/s]Evaluating:  34%|███▍      | 107/313 [00:19<00:38,  5.39it/s]Evaluating:  35%|███▍      | 108/313 [00:20<00:38,  5.38it/s]Evaluating:  35%|███▍      | 109/313 [00:20<00:37,  5.39it/s]Evaluating:  35%|███▌      | 110/313 [00:20<00:37,  5.39it/s]Evaluating:  35%|███▌      | 111/313 [00:20<00:37,  5.40it/s]Evaluating:  36%|███▌      | 112/313 [00:20<00:37,  5.39it/s]Evaluating:  36%|███▌      | 113/313 [00:20<00:37,  5.37it/s]Evaluating:  36%|███▋      | 114/313 [00:21<00:37,  5.37it/s]Evaluating:  37%|███▋      | 115/313 [00:21<00:36,  5.37it/s]Evaluating:  37%|███▋      | 116/313 [00:21<00:36,  5.36it/s]Evaluating:  37%|███▋      | 117/313 [00:21<00:36,  5.36it/s]Evaluating:  38%|███▊      | 118/313 [00:21<00:36,  5.37it/s]Evaluating:  38%|███▊      | 119/313 [00:22<00:36,  5.37it/s]Evaluating:  38%|███▊      | 120/313 [00:22<00:35,  5.40it/s]Evaluating:  39%|███▊      | 121/313 [00:22<00:35,  5.46it/s]Evaluating:  39%|███▉      | 122/313 [00:22<00:34,  5.50it/s]Evaluating:  39%|███▉      | 123/313 [00:22<00:34,  5.53it/s]Evaluating:  40%|███▉      | 124/313 [00:22<00:34,  5.52it/s]Evaluating:  40%|███▉      | 125/313 [00:23<00:34,  5.51it/s]Evaluating:  40%|████      | 126/313 [00:23<00:34,  5.48it/s]Evaluating:  41%|████      | 127/313 [00:23<00:39,  4.66it/s]Evaluating:  41%|████      | 128/313 [00:23<00:38,  4.84it/s]Evaluating:  41%|████      | 129/313 [00:24<00:36,  5.00it/s]Evaluating:  42%|████▏     | 130/313 [00:24<00:35,  5.12it/s]Evaluating:  42%|████▏     | 131/313 [00:24<00:34,  5.23it/s]Evaluating:  42%|████▏     | 132/313 [00:24<00:34,  5.31it/s]Evaluating:  42%|████▏     | 133/313 [00:24<00:33,  5.37it/s]Evaluating:  43%|████▎     | 134/313 [00:24<00:33,  5.39it/s]Evaluating:  43%|████▎     | 135/313 [00:25<00:33,  5.39it/s]Evaluating:  43%|████▎     | 136/313 [00:25<00:32,  5.38it/s]Evaluating:  44%|████▍     | 137/313 [00:25<00:32,  5.36it/s]Evaluating:  44%|████▍     | 138/313 [00:25<00:32,  5.35it/s]Evaluating:  44%|████▍     | 139/313 [00:25<00:32,  5.35it/s]Evaluating:  45%|████▍     | 140/313 [00:26<00:32,  5.36it/s]Evaluating:  45%|████▌     | 141/313 [00:26<00:32,  5.36it/s]Evaluating:  45%|████▌     | 142/313 [00:26<00:31,  5.40it/s]Evaluating:  46%|████▌     | 143/313 [00:26<00:31,  5.42it/s]Evaluating:  46%|████▌     | 144/313 [00:26<00:31,  5.45it/s]Evaluating:  46%|████▋     | 145/313 [00:26<00:30,  5.46it/s]Evaluating:  47%|████▋     | 146/313 [00:27<00:30,  5.45it/s]Evaluating:  47%|████▋     | 147/313 [00:27<00:30,  5.43it/s]Evaluating:  47%|████▋     | 148/313 [00:27<00:30,  5.40it/s]Evaluating:  48%|████▊     | 149/313 [00:27<00:30,  5.37it/s]Evaluating:  48%|████▊     | 150/313 [00:27<00:30,  5.36it/s]Evaluating:  48%|████▊     | 151/313 [00:28<00:30,  5.35it/s]Evaluating:  49%|████▊     | 152/313 [00:28<00:30,  5.35it/s]Evaluating:  49%|████▉     | 153/313 [00:28<00:31,  5.13it/s]Evaluating:  49%|████▉     | 154/313 [00:28<00:29,  5.31it/s]Evaluating:  50%|████▉     | 155/313 [00:28<00:29,  5.44it/s]Evaluating:  50%|████▉     | 156/313 [00:29<00:28,  5.55it/s]Evaluating:  50%|█████     | 157/313 [00:29<00:27,  5.61it/s]Evaluating:  50%|█████     | 158/313 [00:29<00:27,  5.61it/s]Evaluating:  51%|█████     | 159/313 [00:29<00:27,  5.59it/s]Evaluating:  51%|█████     | 160/313 [00:29<00:27,  5.58it/s]Evaluating:  51%|█████▏    | 161/313 [00:29<00:27,  5.54it/s]Evaluating:  52%|█████▏    | 162/313 [00:30<00:27,  5.53it/s]Evaluating:  52%|█████▏    | 163/313 [00:30<00:27,  5.53it/s]Evaluating:  52%|█████▏    | 164/313 [00:30<00:26,  5.53it/s]Evaluating:  53%|█████▎    | 165/313 [00:30<00:26,  5.51it/s]Evaluating:  53%|█████▎    | 166/313 [00:30<00:26,  5.45it/s]Evaluating:  53%|█████▎    | 167/313 [00:31<00:27,  5.40it/s]Evaluating:  54%|█████▎    | 168/313 [00:31<00:26,  5.38it/s]Evaluating:  54%|█████▍    | 169/313 [00:31<00:26,  5.38it/s]Evaluating:  54%|█████▍    | 170/313 [00:31<00:26,  5.40it/s]Evaluating:  55%|█████▍    | 171/313 [00:31<00:26,  5.44it/s]Evaluating:  55%|█████▍    | 172/313 [00:31<00:25,  5.47it/s]Evaluating:  55%|█████▌    | 173/313 [00:32<00:25,  5.49it/s]Evaluating:  56%|█████▌    | 174/313 [00:32<00:25,  5.54it/s]Evaluating:  56%|█████▌    | 175/313 [00:32<00:24,  5.56it/s]Evaluating:  56%|█████▌    | 176/313 [00:32<00:24,  5.53it/s]Evaluating:  57%|█████▋    | 177/313 [00:32<00:24,  5.51it/s]Evaluating:  57%|█████▋    | 178/313 [00:33<00:24,  5.49it/s]Evaluating:  57%|█████▋    | 179/313 [00:33<00:24,  5.45it/s]Evaluating:  58%|█████▊    | 180/313 [00:33<00:24,  5.43it/s]Evaluating:  58%|█████▊    | 181/313 [00:33<00:24,  5.42it/s]Evaluating:  58%|█████▊    | 182/313 [00:33<00:24,  5.41it/s]Evaluating:  58%|█████▊    | 183/313 [00:33<00:23,  5.42it/s]Evaluating:  59%|█████▉    | 184/313 [00:34<00:23,  5.45it/s]Evaluating:  59%|█████▉    | 185/313 [00:34<00:23,  5.44it/s]Evaluating:  59%|█████▉    | 186/313 [00:34<00:23,  5.39it/s]Evaluating:  60%|█████▉    | 187/313 [00:34<00:23,  5.36it/s]Evaluating:  60%|██████    | 188/313 [00:34<00:23,  5.34it/s]Evaluating:  60%|██████    | 189/313 [00:35<00:23,  5.33it/s]Evaluating:  61%|██████    | 190/313 [00:35<00:22,  5.36it/s]Evaluating:  61%|██████    | 191/313 [00:35<00:22,  5.38it/s]Evaluating:  61%|██████▏   | 192/313 [00:35<00:22,  5.39it/s]Evaluating:  62%|██████▏   | 193/313 [00:35<00:22,  5.38it/s]Evaluating:  62%|██████▏   | 194/313 [00:35<00:22,  5.39it/s]Evaluating:  62%|██████▏   | 195/313 [00:36<00:21,  5.39it/s]Evaluating:  63%|██████▎   | 196/313 [00:36<00:21,  5.39it/s]Evaluating:  63%|██████▎   | 197/313 [00:36<00:21,  5.35it/s]Evaluating:  63%|██████▎   | 198/313 [00:36<00:21,  5.34it/s]Evaluating:  64%|██████▎   | 199/313 [00:36<00:21,  5.32it/s]Evaluating:  64%|██████▍   | 200/313 [00:37<00:21,  5.31it/s]Evaluating:  64%|██████▍   | 201/313 [00:37<00:21,  5.33it/s]Evaluating:  65%|██████▍   | 202/313 [00:37<00:20,  5.38it/s]Evaluating:  65%|██████▍   | 203/313 [00:37<00:20,  5.42it/s]Evaluating:  65%|██████▌   | 204/313 [00:37<00:19,  5.48it/s]Evaluating:  65%|██████▌   | 205/313 [00:38<00:19,  5.52it/s]Evaluating:  66%|██████▌   | 206/313 [00:38<00:19,  5.53it/s]Evaluating:  66%|██████▌   | 207/313 [00:38<00:19,  5.48it/s]Evaluating:  66%|██████▋   | 208/313 [00:38<00:19,  5.43it/s]Evaluating:  67%|██████▋   | 209/313 [00:38<00:19,  5.40it/s]Evaluating:  67%|██████▋   | 210/313 [00:38<00:19,  5.38it/s]Evaluating:  67%|██████▋   | 211/313 [00:39<00:18,  5.38it/s]Evaluating:  68%|██████▊   | 212/313 [00:39<00:18,  5.38it/s]Evaluating:  68%|██████▊   | 213/313 [00:39<00:18,  5.39it/s]Evaluating:  68%|██████▊   | 214/313 [00:39<00:18,  5.38it/s]Evaluating:  69%|██████▊   | 215/313 [00:39<00:18,  5.38it/s]Evaluating:  69%|██████▉   | 216/313 [00:40<00:18,  5.36it/s]Evaluating:  69%|██████▉   | 217/313 [00:40<00:17,  5.34it/s]Evaluating:  70%|██████▉   | 218/313 [00:40<00:17,  5.31it/s]Evaluating:  70%|██████▉   | 219/313 [00:40<00:17,  5.30it/s]Evaluating:  70%|███████   | 220/313 [00:40<00:17,  5.29it/s]Evaluating:  71%|███████   | 221/313 [00:41<00:17,  5.30it/s]Evaluating:  71%|███████   | 222/313 [00:41<00:17,  5.33it/s]Evaluating:  71%|███████   | 223/313 [00:41<00:16,  5.37it/s]Evaluating:  72%|███████▏  | 224/313 [00:41<00:16,  5.39it/s]Evaluating:  72%|███████▏  | 225/313 [00:41<00:16,  5.41it/s]Evaluating:  72%|███████▏  | 226/313 [00:41<00:15,  5.44it/s]Evaluating:  73%|███████▎  | 227/313 [00:42<00:15,  5.48it/s]Evaluating:  73%|███████▎  | 228/313 [00:42<00:15,  5.48it/s]Evaluating:  73%|███████▎  | 229/313 [00:42<00:15,  5.47it/s]Evaluating:  73%|███████▎  | 230/313 [00:42<00:15,  5.46it/s]Evaluating:  74%|███████▍  | 231/313 [00:42<00:15,  5.45it/s]Evaluating:  74%|███████▍  | 232/313 [00:43<00:14,  5.47it/s]Evaluating:  74%|███████▍  | 233/313 [00:43<00:14,  5.51it/s]Evaluating:  75%|███████▍  | 234/313 [00:43<00:14,  5.56it/s]Evaluating:  75%|███████▌  | 235/313 [00:43<00:13,  5.61it/s]Evaluating:  75%|███████▌  | 236/313 [00:43<00:13,  5.65it/s]Evaluating:  76%|███████▌  | 237/313 [00:43<00:13,  5.62it/s]Evaluating:  76%|███████▌  | 238/313 [00:44<00:13,  5.61it/s]Evaluating:  76%|███████▋  | 239/313 [00:44<00:13,  5.61it/s]Evaluating:  77%|███████▋  | 240/313 [00:44<00:13,  5.60it/s]Evaluating:  77%|███████▋  | 241/313 [00:44<00:12,  5.63it/s]Evaluating:  77%|███████▋  | 242/313 [00:44<00:12,  5.66it/s]Evaluating:  78%|███████▊  | 243/313 [00:44<00:12,  5.67it/s]Evaluating:  78%|███████▊  | 244/313 [00:45<00:12,  5.67it/s]Evaluating:  78%|███████▊  | 245/313 [00:45<00:12,  5.66it/s]Evaluating:  79%|███████▊  | 246/313 [00:45<00:11,  5.65it/s]Evaluating:  79%|███████▉  | 247/313 [00:45<00:11,  5.62it/s]Evaluating:  79%|███████▉  | 248/313 [00:45<00:11,  5.60it/s]Evaluating:  80%|███████▉  | 249/313 [00:46<00:11,  5.60it/s]Evaluating:  80%|███████▉  | 250/313 [00:46<00:11,  5.60it/s]Evaluating:  80%|████████  | 251/313 [00:46<00:11,  5.58it/s]Evaluating:  81%|████████  | 252/313 [00:46<00:10,  5.55it/s]Evaluating:  81%|████████  | 253/313 [00:46<00:10,  5.51it/s]Evaluating:  81%|████████  | 254/313 [00:46<00:10,  5.48it/s]Evaluating:  81%|████████▏ | 255/313 [00:47<00:10,  5.45it/s]Evaluating:  82%|████████▏ | 256/313 [00:47<00:10,  5.41it/s]Evaluating:  82%|████████▏ | 257/313 [00:47<00:10,  5.42it/s]Evaluating:  82%|████████▏ | 258/313 [00:47<00:10,  5.46it/s]Evaluating:  83%|████████▎ | 259/313 [00:47<00:09,  5.52it/s]Evaluating:  83%|████████▎ | 260/313 [00:48<00:09,  5.57it/s]Evaluating:  83%|████████▎ | 261/313 [00:48<00:09,  5.53it/s]Evaluating:  84%|████████▎ | 262/313 [00:48<00:09,  5.50it/s]Evaluating:  84%|████████▍ | 263/313 [00:48<00:09,  5.47it/s]Evaluating:  84%|████████▍ | 264/313 [00:48<00:08,  5.48it/s]Evaluating:  85%|████████▍ | 265/313 [00:48<00:08,  5.39it/s]Evaluating:  85%|████████▍ | 266/313 [00:49<00:08,  5.47it/s]Evaluating:  85%|████████▌ | 267/313 [00:49<00:08,  5.54it/s]Evaluating:  86%|████████▌ | 268/313 [00:49<00:08,  5.58it/s]Evaluating:  86%|████████▌ | 269/313 [00:49<00:07,  5.62it/s]Evaluating:  86%|████████▋ | 270/313 [00:49<00:07,  5.65it/s]Evaluating:  87%|████████▋ | 271/313 [00:50<00:07,  5.65it/s]Evaluating:  87%|████████▋ | 272/313 [00:50<00:07,  5.66it/s]Evaluating:  87%|████████▋ | 273/313 [00:50<00:07,  5.67it/s]Evaluating:  88%|████████▊ | 274/313 [00:50<00:06,  5.68it/s]Evaluating:  88%|████████▊ | 275/313 [00:50<00:06,  5.69it/s]Evaluating:  88%|████████▊ | 276/313 [00:50<00:06,  5.68it/s]Evaluating:  88%|████████▊ | 277/313 [00:51<00:06,  5.68it/s]Evaluating:  89%|████████▉ | 278/313 [00:51<00:06,  5.65it/s]Evaluating:  89%|████████▉ | 279/313 [00:51<00:06,  5.63it/s]Evaluating:  89%|████████▉ | 280/313 [00:51<00:05,  5.63it/s]Evaluating:  90%|████████▉ | 281/313 [00:51<00:05,  5.63it/s]Evaluating:  90%|█████████ | 282/313 [00:51<00:05,  5.64it/s]Evaluating:  90%|█████████ | 283/313 [00:52<00:05,  5.66it/s]Evaluating:  91%|█████████ | 284/313 [00:52<00:05,  5.66it/s]Evaluating:  91%|█████████ | 285/313 [00:52<00:04,  5.61it/s]Evaluating:  91%|█████████▏| 286/313 [00:52<00:04,  5.57it/s]Evaluating:  92%|█████████▏| 287/313 [00:52<00:04,  5.56it/s]Evaluating:  92%|█████████▏| 288/313 [00:53<00:04,  5.57it/s]Evaluating:  92%|█████████▏| 289/313 [00:53<00:04,  5.59it/s]Evaluating:  93%|█████████▎| 290/313 [00:53<00:04,  5.61it/s]Evaluating:  93%|█████████▎| 291/313 [00:53<00:03,  5.63it/s]Evaluating:  93%|█████████▎| 292/313 [00:53<00:03,  5.62it/s]Evaluating:  94%|█████████▎| 293/313 [00:53<00:03,  5.63it/s]Evaluating:  94%|█████████▍| 294/313 [00:54<00:03,  5.55it/s]Evaluating:  94%|█████████▍| 295/313 [00:54<00:03,  5.47it/s]Evaluating:  95%|█████████▍| 296/313 [00:54<00:03,  5.43it/s]Evaluating:  95%|█████████▍| 297/313 [00:54<00:02,  5.40it/s]Evaluating:  95%|█████████▌| 298/313 [00:54<00:02,  5.38it/s]Evaluating:  96%|█████████▌| 299/313 [00:55<00:02,  5.34it/s]Evaluating:  96%|█████████▌| 300/313 [00:55<00:02,  5.31it/s]Evaluating:  96%|█████████▌| 301/313 [00:55<00:02,  5.31it/s]Evaluating:  96%|█████████▋| 302/313 [00:55<00:02,  5.28it/s]Evaluating:  97%|█████████▋| 303/313 [00:55<00:01,  5.20it/s]Evaluating:  97%|█████████▋| 304/313 [00:56<00:01,  5.16it/s]Evaluating:  97%|█████████▋| 305/313 [00:56<00:01,  5.13it/s]Evaluating:  98%|█████████▊| 306/313 [00:56<00:01,  5.11it/s]Evaluating:  98%|█████████▊| 307/313 [00:56<00:01,  5.11it/s]Evaluating:  98%|█████████▊| 308/313 [00:56<00:00,  5.11it/s]Evaluating:  99%|█████████▊| 309/313 [00:56<00:00,  5.15it/s]Evaluating:  99%|█████████▉| 310/313 [00:57<00:00,  5.18it/s]Evaluating:  99%|█████████▉| 311/313 [00:57<00:00,  5.24it/s]Evaluating: 100%|█████████▉| 312/313 [00:57<00:00,  5.30it/s]Evaluating: 100%|██████████| 313/313 [00:57<00:00,  6.08it/s]Evaluating: 100%|██████████| 313/313 [00:57<00:00,  5.43it/s]
10/14/2021 16:13:49 - INFO - __main__ -   ***** Evaluation result  in da *****
10/14/2021 16:13:49 - INFO - __main__ -     f1 = 0.7986942328618063
10/14/2021 16:13:49 - INFO - __main__ -     loss = 0.7118737544602575
10/14/2021 16:13:49 - INFO - __main__ -     precision = 0.7691839632208776
10/14/2021 16:13:49 - INFO - __main__ -     recall = 0.8305592057234633
10/14/2021 16:13:49 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:14:07 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:14:07 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:14:24 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 16:14:26 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:14:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 16:14:26 - INFO - __main__ -   Seed = 3
10/14/2021 16:14:26 - INFO - root -   save model
10/14/2021 16:14:26 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:14:26 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:14:44 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:14:44 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 16:14:44 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
10/14/2021 16:14:44 - INFO - root -   Trying to decide if add adapter
10/14/2021 16:14:44 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 16:14:44 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/14/2021 16:14:44 - INFO - __main__ -   Language = en
10/14/2021 16:14:44 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 16:14:46 - INFO - __main__ -   Language = is
10/14/2021 16:14:46 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/14/2021 16:14:47 - INFO - __main__ -   Language = de
10/14/2021 16:14:47 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/14/2021 16:14:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/14/2021 16:14:51 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/14/2021 16:14:51 - INFO - __main__ -     Num examples = 100
10/14/2021 16:14:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]Evaluating:  25%|██▌       | 1/4 [00:00<00:00,  5.39it/s]Evaluating:  50%|█████     | 2/4 [00:00<00:00,  5.63it/s]Evaluating:  75%|███████▌  | 3/4 [00:00<00:00,  5.81it/s]Evaluating: 100%|██████████| 4/4 [00:00<00:00,  7.10it/s]
10/14/2021 16:14:52 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/14/2021 16:14:52 - INFO - __main__ -     f1 = 0.6029411764705882
10/14/2021 16:14:52 - INFO - __main__ -     loss = 1.07160381693393
10/14/2021 16:14:52 - INFO - __main__ -     precision = 0.5394736842105263
10/14/2021 16:14:52 - INFO - __main__ -     recall = 0.6833333333333333
10/14/2021 16:14:52 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/14/2021 16:14:53 - INFO - __main__ -   ***** Running evaluation  in no *****
10/14/2021 16:14:53 - INFO - __main__ -     Num examples = 10000
10/14/2021 16:14:53 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.06it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|▏         | 4/313 [00:00<01:07,  4.60it/s]Evaluating:   2%|▏         | 5/313 [00:00<01:01,  5.03it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:57,  5.34it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:55,  5.56it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:53,  5.70it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:52,  5.80it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:51,  5.88it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:51,  5.92it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:50,  5.95it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:50,  5.97it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:50,  5.98it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  5.99it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  6.01it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:49,  6.02it/s]Evaluating:   6%|▌         | 18/313 [00:03<00:49,  6.02it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.02it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.00it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  5.99it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  5.98it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:48,  5.99it/s]Evaluating:   8%|▊         | 24/313 [00:04<00:48,  5.99it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:48,  5.99it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  5.99it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  5.99it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  5.99it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:47,  5.99it/s]Evaluating:  10%|▉         | 30/313 [00:05<00:47,  5.99it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:47,  5.99it/s]Evaluating:  10%|█         | 32/313 [00:05<00:46,  5.99it/s]Evaluating:  11%|█         | 33/313 [00:05<00:46,  5.99it/s]Evaluating:  11%|█         | 34/313 [00:05<00:46,  5.99it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  5.99it/s]Evaluating:  12%|█▏        | 36/313 [00:06<00:46,  6.00it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:46,  6.00it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  6.00it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.00it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  6.01it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.00it/s]Evaluating:  13%|█▎        | 42/313 [00:07<00:45,  6.00it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:44,  6.00it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  5.99it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:45,  5.95it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  5.96it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  5.96it/s]Evaluating:  15%|█▌        | 48/313 [00:08<00:44,  5.96it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:44,  5.96it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:44,  5.94it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:44,  5.94it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  5.96it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  5.96it/s]Evaluating:  17%|█▋        | 54/313 [00:09<00:43,  5.96it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:43,  5.97it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:43,  5.97it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  5.97it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  5.96it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  5.96it/s]Evaluating:  19%|█▉        | 60/313 [00:10<00:42,  5.95it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:42,  5.95it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:42,  5.95it/s]Evaluating:  20%|██        | 63/313 [00:10<00:42,  5.92it/s]Evaluating:  20%|██        | 64/313 [00:10<00:42,  5.92it/s]Evaluating:  21%|██        | 65/313 [00:10<00:41,  5.92it/s]Evaluating:  21%|██        | 66/313 [00:11<00:41,  5.91it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:41,  5.91it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:41,  5.91it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:41,  5.90it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:41,  5.90it/s]Evaluating:  23%|██▎       | 71/313 [00:12<00:40,  5.90it/s]Evaluating:  23%|██▎       | 72/313 [00:12<00:40,  5.90it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  5.90it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:40,  5.90it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:40,  5.89it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:40,  5.89it/s]Evaluating:  25%|██▍       | 77/313 [00:13<00:40,  5.90it/s]Evaluating:  25%|██▍       | 78/313 [00:13<00:39,  5.89it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.89it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:39,  5.89it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:39,  5.89it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:39,  5.89it/s]Evaluating:  27%|██▋       | 83/313 [00:14<00:39,  5.89it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:38,  5.88it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:38,  5.85it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:38,  5.83it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:38,  5.82it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:38,  5.80it/s]Evaluating:  28%|██▊       | 89/313 [00:15<00:38,  5.78it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:39,  5.70it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:39,  5.67it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:44,  4.98it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:43,  5.09it/s]Evaluating:  30%|███       | 94/313 [00:16<00:42,  5.17it/s]Evaluating:  30%|███       | 95/313 [00:16<00:41,  5.23it/s]Evaluating:  31%|███       | 96/313 [00:16<00:41,  5.27it/s]Evaluating:  31%|███       | 97/313 [00:16<00:40,  5.31it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:40,  5.33it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:40,  5.34it/s]Evaluating:  32%|███▏      | 100/313 [00:17<00:39,  5.36it/s]Evaluating:  32%|███▏      | 101/313 [00:17<00:39,  5.39it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:38,  5.42it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:38,  5.48it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:38,  5.49it/s]Evaluating:  34%|███▎      | 105/313 [00:18<00:37,  5.48it/s]Evaluating:  34%|███▍      | 106/313 [00:18<00:37,  5.46it/s]Evaluating:  34%|███▍      | 107/313 [00:18<00:37,  5.44it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:37,  5.43it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:37,  5.44it/s]Evaluating:  35%|███▌      | 110/313 [00:19<00:37,  5.45it/s]Evaluating:  35%|███▌      | 111/313 [00:19<00:36,  5.47it/s]Evaluating:  36%|███▌      | 112/313 [00:19<00:36,  5.48it/s]Evaluating:  36%|███▌      | 113/313 [00:19<00:36,  5.48it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:36,  5.44it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:36,  5.42it/s]Evaluating:  37%|███▋      | 116/313 [00:20<00:36,  5.43it/s]Evaluating:  37%|███▋      | 117/313 [00:20<00:36,  5.44it/s]Evaluating:  38%|███▊      | 118/313 [00:20<00:35,  5.44it/s]Evaluating:  38%|███▊      | 119/313 [00:20<00:35,  5.43it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:35,  5.43it/s]Evaluating:  39%|███▊      | 121/313 [00:21<00:35,  5.43it/s]Evaluating:  39%|███▉      | 122/313 [00:21<00:35,  5.42it/s]Evaluating:  39%|███▉      | 123/313 [00:21<00:35,  5.40it/s]Evaluating:  40%|███▉      | 124/313 [00:21<00:35,  5.38it/s]Evaluating:  40%|███▉      | 125/313 [00:21<00:35,  5.37it/s]Evaluating:  40%|████      | 126/313 [00:21<00:34,  5.36it/s]Evaluating:  41%|████      | 127/313 [00:22<00:34,  5.37it/s]Evaluating:  41%|████      | 128/313 [00:22<00:34,  5.38it/s]Evaluating:  41%|████      | 129/313 [00:22<00:34,  5.38it/s]Evaluating:  42%|████▏     | 130/313 [00:22<00:34,  5.37it/s]Evaluating:  42%|████▏     | 131/313 [00:22<00:33,  5.36it/s]Evaluating:  42%|████▏     | 132/313 [00:23<00:33,  5.36it/s]Evaluating:  42%|████▏     | 133/313 [00:23<00:33,  5.36it/s]Evaluating:  43%|████▎     | 134/313 [00:23<00:33,  5.35it/s]Evaluating:  43%|████▎     | 135/313 [00:23<00:33,  5.35it/s]Evaluating:  43%|████▎     | 136/313 [00:23<00:33,  5.34it/s]Evaluating:  44%|████▍     | 137/313 [00:24<00:32,  5.36it/s]Evaluating:  44%|████▍     | 138/313 [00:24<00:32,  5.37it/s]Evaluating:  44%|████▍     | 139/313 [00:24<00:32,  5.37it/s]Evaluating:  45%|████▍     | 140/313 [00:24<00:32,  5.36it/s]Evaluating:  45%|████▌     | 141/313 [00:24<00:32,  5.35it/s]Evaluating:  45%|████▌     | 142/313 [00:24<00:32,  5.34it/s]Evaluating:  46%|████▌     | 143/313 [00:25<00:31,  5.35it/s]Evaluating:  46%|████▌     | 144/313 [00:25<00:31,  5.34it/s]Evaluating:  46%|████▋     | 145/313 [00:25<00:31,  5.34it/s]Evaluating:  47%|████▋     | 146/313 [00:25<00:39,  4.22it/s]Evaluating:  47%|████▋     | 147/313 [00:26<00:36,  4.59it/s]Evaluating:  47%|████▋     | 148/313 [00:26<00:33,  4.89it/s]Evaluating:  48%|████▊     | 149/313 [00:26<00:31,  5.13it/s]Evaluating:  48%|████▊     | 150/313 [00:26<00:30,  5.31it/s]Evaluating:  48%|████▊     | 151/313 [00:26<00:29,  5.46it/s]Evaluating:  49%|████▊     | 152/313 [00:26<00:29,  5.51it/s]Evaluating:  49%|████▉     | 153/313 [00:27<00:28,  5.56it/s]Evaluating:  49%|████▉     | 154/313 [00:27<00:28,  5.58it/s]Evaluating:  50%|████▉     | 155/313 [00:27<00:28,  5.59it/s]Evaluating:  50%|████▉     | 156/313 [00:27<00:28,  5.59it/s]Evaluating:  50%|█████     | 157/313 [00:27<00:27,  5.59it/s]Evaluating:  50%|█████     | 158/313 [00:27<00:27,  5.58it/s]Evaluating:  51%|█████     | 159/313 [00:28<00:27,  5.58it/s]Evaluating:  51%|█████     | 160/313 [00:28<00:27,  5.55it/s]Evaluating:  51%|█████▏    | 161/313 [00:28<00:27,  5.51it/s]Evaluating:  52%|█████▏    | 162/313 [00:28<00:27,  5.50it/s]Evaluating:  52%|█████▏    | 163/313 [00:28<00:27,  5.49it/s]Evaluating:  52%|█████▏    | 164/313 [00:29<00:27,  5.48it/s]Evaluating:  53%|█████▎    | 165/313 [00:29<00:27,  5.47it/s]Evaluating:  53%|█████▎    | 166/313 [00:29<00:26,  5.48it/s]Evaluating:  53%|█████▎    | 167/313 [00:29<00:26,  5.48it/s]Evaluating:  54%|█████▎    | 168/313 [00:29<00:26,  5.48it/s]Evaluating:  54%|█████▍    | 169/313 [00:29<00:26,  5.44it/s]Evaluating:  54%|█████▍    | 170/313 [00:30<00:26,  5.40it/s]Evaluating:  55%|█████▍    | 171/313 [00:30<00:26,  5.38it/s]Evaluating:  55%|█████▍    | 172/313 [00:30<00:26,  5.36it/s]Evaluating:  55%|█████▌    | 173/313 [00:30<00:26,  5.38it/s]Evaluating:  56%|█████▌    | 174/313 [00:30<00:26,  5.29it/s]Evaluating:  56%|█████▌    | 175/313 [00:31<00:25,  5.39it/s]Evaluating:  56%|█████▌    | 176/313 [00:31<00:25,  5.46it/s]Evaluating:  57%|█████▋    | 177/313 [00:31<00:24,  5.52it/s]Evaluating:  57%|█████▋    | 178/313 [00:31<00:24,  5.56it/s]Evaluating:  57%|█████▋    | 179/313 [00:31<00:23,  5.59it/s]Evaluating:  58%|█████▊    | 180/313 [00:32<00:23,  5.59it/s]Evaluating:  58%|█████▊    | 181/313 [00:32<00:23,  5.59it/s]Evaluating:  58%|█████▊    | 182/313 [00:32<00:23,  5.59it/s]Evaluating:  58%|█████▊    | 183/313 [00:32<00:23,  5.60it/s]Evaluating:  59%|█████▉    | 184/313 [00:32<00:22,  5.61it/s]Evaluating:  59%|█████▉    | 185/313 [00:32<00:22,  5.63it/s]Evaluating:  59%|█████▉    | 186/313 [00:33<00:22,  5.62it/s]Evaluating:  60%|█████▉    | 187/313 [00:33<00:22,  5.58it/s]Evaluating:  60%|██████    | 188/313 [00:33<00:22,  5.55it/s]Evaluating:  60%|██████    | 189/313 [00:33<00:22,  5.53it/s]Evaluating:  61%|██████    | 190/313 [00:33<00:22,  5.51it/s]Evaluating:  61%|██████    | 191/313 [00:33<00:22,  5.50it/s]Evaluating:  61%|██████▏   | 192/313 [00:34<00:21,  5.50it/s]Evaluating:  62%|██████▏   | 193/313 [00:34<00:21,  5.52it/s]Evaluating:  62%|██████▏   | 194/313 [00:34<00:21,  5.52it/s]Evaluating:  62%|██████▏   | 195/313 [00:34<00:21,  5.50it/s]Evaluating:  63%|██████▎   | 196/313 [00:34<00:21,  5.42it/s]Evaluating:  63%|██████▎   | 197/313 [00:35<00:21,  5.39it/s]Evaluating:  63%|██████▎   | 198/313 [00:35<00:21,  5.37it/s]Evaluating:  64%|██████▎   | 199/313 [00:35<00:21,  5.37it/s]Evaluating:  64%|██████▍   | 200/313 [00:35<00:21,  5.32it/s]Evaluating:  64%|██████▍   | 201/313 [00:35<00:20,  5.35it/s]Evaluating:  65%|██████▍   | 202/313 [00:36<00:20,  5.35it/s]Evaluating:  65%|██████▍   | 203/313 [00:36<00:20,  5.36it/s]Evaluating:  65%|██████▌   | 204/313 [00:36<00:20,  5.35it/s]Evaluating:  65%|██████▌   | 205/313 [00:36<00:20,  5.36it/s]Evaluating:  66%|██████▌   | 206/313 [00:36<00:19,  5.36it/s]Evaluating:  66%|██████▌   | 207/313 [00:36<00:19,  5.34it/s]Evaluating:  66%|██████▋   | 208/313 [00:37<00:19,  5.32it/s]Evaluating:  67%|██████▋   | 209/313 [00:37<00:19,  5.32it/s]Evaluating:  67%|██████▋   | 210/313 [00:37<00:19,  5.35it/s]Evaluating:  67%|██████▋   | 211/313 [00:37<00:18,  5.39it/s]Evaluating:  68%|██████▊   | 212/313 [00:37<00:18,  5.43it/s]Evaluating:  68%|██████▊   | 213/313 [00:38<00:18,  5.46it/s]Evaluating:  68%|██████▊   | 214/313 [00:38<00:18,  5.48it/s]Evaluating:  69%|██████▊   | 215/313 [00:38<00:17,  5.52it/s]Evaluating:  69%|██████▉   | 216/313 [00:38<00:17,  5.53it/s]Evaluating:  69%|██████▉   | 217/313 [00:38<00:17,  5.45it/s]Evaluating:  70%|██████▉   | 218/313 [00:38<00:17,  5.39it/s]Evaluating:  70%|██████▉   | 219/313 [00:39<00:17,  5.35it/s]Evaluating:  70%|███████   | 220/313 [00:39<00:17,  5.33it/s]Evaluating:  71%|███████   | 221/313 [00:39<00:17,  5.31it/s]Evaluating:  71%|███████   | 222/313 [00:39<00:17,  5.31it/s]Evaluating:  71%|███████   | 223/313 [00:39<00:16,  5.33it/s]Evaluating:  72%|███████▏  | 224/313 [00:40<00:16,  5.35it/s]Evaluating:  72%|███████▏  | 225/313 [00:40<00:16,  5.37it/s]Evaluating:  72%|███████▏  | 226/313 [00:40<00:16,  5.40it/s]Evaluating:  73%|███████▎  | 227/313 [00:40<00:18,  4.75it/s]Evaluating:  73%|███████▎  | 228/313 [00:40<00:16,  5.00it/s]Evaluating:  73%|███████▎  | 229/313 [00:41<00:16,  5.20it/s]Evaluating:  73%|███████▎  | 230/313 [00:41<00:15,  5.25it/s]Evaluating:  74%|███████▍  | 231/313 [00:41<00:15,  5.26it/s]Evaluating:  74%|███████▍  | 232/313 [00:41<00:15,  5.26it/s]Evaluating:  74%|███████▍  | 233/313 [00:41<00:15,  5.26it/s]Evaluating:  75%|███████▍  | 234/313 [00:42<00:14,  5.29it/s]Evaluating:  75%|███████▌  | 235/313 [00:42<00:14,  5.28it/s]Evaluating:  75%|███████▌  | 236/313 [00:42<00:14,  5.27it/s]Evaluating:  76%|███████▌  | 237/313 [00:42<00:14,  5.27it/s]Evaluating:  76%|███████▌  | 238/313 [00:42<00:14,  5.29it/s]Evaluating:  76%|███████▋  | 239/313 [00:42<00:13,  5.32it/s]Evaluating:  77%|███████▋  | 240/313 [00:43<00:13,  5.36it/s]Evaluating:  77%|███████▋  | 241/313 [00:43<00:13,  5.40it/s]Evaluating:  77%|███████▋  | 242/313 [00:43<00:13,  5.41it/s]Evaluating:  78%|███████▊  | 243/313 [00:43<00:13,  5.37it/s]Evaluating:  78%|███████▊  | 244/313 [00:43<00:12,  5.33it/s]Evaluating:  78%|███████▊  | 245/313 [00:44<00:12,  5.32it/s]Evaluating:  79%|███████▊  | 246/313 [00:44<00:12,  5.34it/s]Evaluating:  79%|███████▉  | 247/313 [00:44<00:12,  5.34it/s]Evaluating:  79%|███████▉  | 248/313 [00:44<00:12,  5.35it/s]Evaluating:  80%|███████▉  | 249/313 [00:44<00:11,  5.38it/s]Evaluating:  80%|███████▉  | 250/313 [00:45<00:11,  5.43it/s]Evaluating:  80%|████████  | 251/313 [00:45<00:11,  5.47it/s]Evaluating:  81%|████████  | 252/313 [00:45<00:11,  5.49it/s]Evaluating:  81%|████████  | 253/313 [00:45<00:10,  5.50it/s]Evaluating:  81%|████████  | 254/313 [00:45<00:10,  5.42it/s]Evaluating:  81%|████████▏ | 255/313 [00:45<00:10,  5.36it/s]Evaluating:  82%|████████▏ | 256/313 [00:46<00:10,  5.32it/s]Evaluating:  82%|████████▏ | 257/313 [00:46<00:10,  5.31it/s]Evaluating:  82%|████████▏ | 258/313 [00:46<00:10,  5.32it/s]Evaluating:  83%|████████▎ | 259/313 [00:46<00:10,  5.33it/s]Evaluating:  83%|████████▎ | 260/313 [00:46<00:09,  5.35it/s]Evaluating:  83%|████████▎ | 261/313 [00:47<00:09,  5.38it/s]Evaluating:  84%|████████▎ | 262/313 [00:47<00:09,  5.43it/s]Evaluating:  84%|████████▍ | 263/313 [00:47<00:09,  5.46it/s]Evaluating:  84%|████████▍ | 264/313 [00:47<00:09,  5.40it/s]Evaluating:  85%|████████▍ | 265/313 [00:47<00:08,  5.35it/s]Evaluating:  85%|████████▍ | 266/313 [00:48<00:08,  5.32it/s]Evaluating:  85%|████████▌ | 267/313 [00:48<00:08,  5.30it/s]Evaluating:  86%|████████▌ | 268/313 [00:48<00:08,  5.32it/s]Evaluating:  86%|████████▌ | 269/313 [00:48<00:08,  5.37it/s]Evaluating:  86%|████████▋ | 270/313 [00:48<00:07,  5.40it/s]Evaluating:  87%|████████▋ | 271/313 [00:48<00:07,  5.43it/s]Evaluating:  87%|████████▋ | 272/313 [00:49<00:07,  5.45it/s]Evaluating:  87%|████████▋ | 273/313 [00:49<00:07,  5.47it/s]Evaluating:  88%|████████▊ | 274/313 [00:49<00:07,  5.51it/s]Evaluating:  88%|████████▊ | 275/313 [00:49<00:06,  5.55it/s]Evaluating:  88%|████████▊ | 276/313 [00:49<00:06,  5.56it/s]Evaluating:  88%|████████▊ | 277/313 [00:50<00:06,  5.58it/s]Evaluating:  89%|████████▉ | 278/313 [00:50<00:06,  5.60it/s]Evaluating:  89%|████████▉ | 279/313 [00:50<00:06,  5.54it/s]Evaluating:  89%|████████▉ | 280/313 [00:50<00:06,  5.47it/s]Evaluating:  90%|████████▉ | 281/313 [00:50<00:05,  5.43it/s]Evaluating:  90%|█████████ | 282/313 [00:50<00:05,  5.43it/s]Evaluating:  90%|█████████ | 283/313 [00:51<00:05,  5.42it/s]Evaluating:  91%|█████████ | 284/313 [00:51<00:05,  5.33it/s]Evaluating:  91%|█████████ | 285/313 [00:51<00:05,  5.30it/s]Evaluating:  91%|█████████▏| 286/313 [00:51<00:05,  5.31it/s]Evaluating:  92%|█████████▏| 287/313 [00:51<00:04,  5.31it/s]Evaluating:  92%|█████████▏| 288/313 [00:52<00:04,  5.36it/s]Evaluating:  92%|█████████▏| 289/313 [00:52<00:04,  5.39it/s]Evaluating:  93%|█████████▎| 290/313 [00:52<00:04,  5.38it/s]Evaluating:  93%|█████████▎| 291/313 [00:52<00:04,  5.33it/s]Evaluating:  93%|█████████▎| 292/313 [00:52<00:03,  5.28it/s]Evaluating:  94%|█████████▎| 293/313 [00:53<00:03,  5.24it/s]Evaluating:  94%|█████████▍| 294/313 [00:53<00:03,  5.22it/s]Evaluating:  94%|█████████▍| 295/313 [00:53<00:03,  5.23it/s]Evaluating:  95%|█████████▍| 296/313 [00:53<00:03,  5.24it/s]Evaluating:  95%|█████████▍| 297/313 [00:53<00:03,  5.19it/s]Evaluating:  95%|█████████▌| 298/313 [00:53<00:02,  5.14it/s]Evaluating:  96%|█████████▌| 299/313 [00:54<00:02,  5.14it/s]Evaluating:  96%|█████████▌| 300/313 [00:54<00:02,  5.11it/s]Evaluating:  96%|█████████▌| 301/313 [00:54<00:02,  5.09it/s]Evaluating:  96%|█████████▋| 302/313 [00:54<00:02,  5.10it/s]Evaluating:  97%|█████████▋| 303/313 [00:54<00:01,  5.15it/s]Evaluating:  97%|█████████▋| 304/313 [00:55<00:01,  5.20it/s]Evaluating:  97%|█████████▋| 305/313 [00:55<00:01,  5.27it/s]Evaluating:  98%|█████████▊| 306/313 [00:55<00:01,  5.33it/s]Evaluating:  98%|█████████▊| 307/313 [00:55<00:01,  5.36it/s]Evaluating:  98%|█████████▊| 308/313 [00:55<00:00,  5.36it/s]Evaluating:  99%|█████████▊| 309/313 [00:56<00:00,  5.35it/s]Evaluating:  99%|█████████▉| 310/313 [00:56<00:00,  5.37it/s]Evaluating:  99%|█████████▉| 311/313 [00:56<00:00,  5.38it/s]Evaluating: 100%|█████████▉| 312/313 [00:56<00:00,  5.38it/s]Evaluating: 100%|██████████| 313/313 [00:56<00:00,  6.17it/s]Evaluating: 100%|██████████| 313/313 [00:56<00:00,  5.52it/s]
10/14/2021 16:15:51 - INFO - __main__ -   ***** Evaluation result  in no *****
10/14/2021 16:15:51 - INFO - __main__ -     f1 = 0.7454184434100453
10/14/2021 16:15:51 - INFO - __main__ -     loss = 0.9434496553275532
10/14/2021 16:15:51 - INFO - __main__ -     precision = 0.7016361296648079
10/14/2021 16:15:51 - INFO - __main__ -     recall = 0.7950284682682961
10/14/2021 16:15:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/14/2021 16:15:53 - INFO - __main__ -   ***** Running evaluation  in da *****
10/14/2021 16:15:53 - INFO - __main__ -     Num examples = 10001
10/14/2021 16:15:53 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:57,  5.47it/s]Evaluating:   1%|          | 2/313 [00:00<00:56,  5.46it/s]Evaluating:   1%|          | 3/313 [00:00<00:56,  5.46it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:56,  5.47it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:56,  5.47it/s]Evaluating:   2%|▏         | 6/313 [00:01<00:56,  5.46it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:56,  5.46it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:55,  5.46it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:55,  5.46it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:59,  5.10it/s]Evaluating:   4%|▎         | 11/313 [00:02<00:57,  5.26it/s]Evaluating:   4%|▍         | 12/313 [00:02<00:56,  5.37it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:55,  5.43it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:54,  5.47it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:54,  5.49it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:54,  5.50it/s]Evaluating:   5%|▌         | 17/313 [00:03<00:53,  5.49it/s]Evaluating:   6%|▌         | 18/313 [00:03<00:53,  5.47it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:53,  5.47it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:53,  5.47it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:53,  5.45it/s]Evaluating:   7%|▋         | 22/313 [00:04<00:53,  5.44it/s]Evaluating:   7%|▋         | 23/313 [00:04<00:53,  5.45it/s]Evaluating:   8%|▊         | 24/313 [00:04<00:53,  5.45it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:52,  5.45it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:52,  5.45it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:52,  5.44it/s]Evaluating:   9%|▉         | 28/313 [00:05<00:52,  5.44it/s]Evaluating:   9%|▉         | 29/313 [00:05<00:52,  5.44it/s]Evaluating:  10%|▉         | 30/313 [00:05<00:52,  5.44it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:51,  5.44it/s]Evaluating:  10%|█         | 32/313 [00:05<00:51,  5.43it/s]Evaluating:  11%|█         | 33/313 [00:06<00:51,  5.43it/s]Evaluating:  11%|█         | 34/313 [00:06<00:51,  5.44it/s]Evaluating:  11%|█         | 35/313 [00:06<00:51,  5.44it/s]Evaluating:  12%|█▏        | 36/313 [00:06<00:50,  5.44it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:50,  5.44it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:50,  5.44it/s]Evaluating:  12%|█▏        | 39/313 [00:07<01:04,  4.26it/s]Evaluating:  13%|█▎        | 40/313 [00:07<00:59,  4.57it/s]Evaluating:  13%|█▎        | 41/313 [00:07<00:56,  4.82it/s]Evaluating:  13%|█▎        | 42/313 [00:07<00:54,  5.00it/s]Evaluating:  14%|█▎        | 43/313 [00:08<00:52,  5.12it/s]Evaluating:  14%|█▍        | 44/313 [00:08<00:51,  5.21it/s]Evaluating:  14%|█▍        | 45/313 [00:08<00:50,  5.27it/s]Evaluating:  15%|█▍        | 46/313 [00:08<00:50,  5.31it/s]Evaluating:  15%|█▌        | 47/313 [00:08<00:49,  5.35it/s]Evaluating:  15%|█▌        | 48/313 [00:08<00:49,  5.37it/s]Evaluating:  16%|█▌        | 49/313 [00:09<00:49,  5.39it/s]Evaluating:  16%|█▌        | 50/313 [00:09<00:48,  5.38it/s]Evaluating:  16%|█▋        | 51/313 [00:09<00:48,  5.40it/s]Evaluating:  17%|█▋        | 52/313 [00:09<00:48,  5.41it/s]Evaluating:  17%|█▋        | 53/313 [00:09<00:48,  5.41it/s]Evaluating:  17%|█▋        | 54/313 [00:10<00:47,  5.41it/s]Evaluating:  18%|█▊        | 55/313 [00:10<00:47,  5.41it/s]Evaluating:  18%|█▊        | 56/313 [00:10<00:47,  5.41it/s]Evaluating:  18%|█▊        | 57/313 [00:10<00:47,  5.41it/s]Evaluating:  19%|█▊        | 58/313 [00:10<00:47,  5.41it/s]Evaluating:  19%|█▉        | 59/313 [00:11<00:46,  5.42it/s]Evaluating:  19%|█▉        | 60/313 [00:11<00:46,  5.41it/s]Evaluating:  19%|█▉        | 61/313 [00:11<00:46,  5.42it/s]Evaluating:  20%|█▉        | 62/313 [00:11<00:46,  5.41it/s]Evaluating:  20%|██        | 63/313 [00:11<00:46,  5.40it/s]Evaluating:  20%|██        | 64/313 [00:11<00:46,  5.39it/s]Evaluating:  21%|██        | 65/313 [00:12<01:12,  3.41it/s]Evaluating:  21%|██        | 66/313 [00:12<01:04,  3.83it/s]Evaluating:  21%|██▏       | 67/313 [00:12<00:58,  4.20it/s]Evaluating:  22%|██▏       | 68/313 [00:13<00:54,  4.50it/s]Evaluating:  22%|██▏       | 69/313 [00:13<00:51,  4.74it/s]Evaluating:  22%|██▏       | 70/313 [00:13<00:49,  4.93it/s]Evaluating:  23%|██▎       | 71/313 [00:13<00:47,  5.08it/s]Evaluating:  23%|██▎       | 72/313 [00:13<00:46,  5.18it/s]Evaluating:  23%|██▎       | 73/313 [00:13<00:45,  5.24it/s]Evaluating:  24%|██▎       | 74/313 [00:14<00:45,  5.29it/s]Evaluating:  24%|██▍       | 75/313 [00:14<00:44,  5.32it/s]Evaluating:  24%|██▍       | 76/313 [00:14<00:44,  5.34it/s]Evaluating:  25%|██▍       | 77/313 [00:14<00:44,  5.36it/s]Evaluating:  25%|██▍       | 78/313 [00:14<00:43,  5.39it/s]Evaluating:  25%|██▌       | 79/313 [00:15<00:43,  5.40it/s]Evaluating:  26%|██▌       | 80/313 [00:15<00:43,  5.40it/s]Evaluating:  26%|██▌       | 81/313 [00:15<00:42,  5.40it/s]Evaluating:  26%|██▌       | 82/313 [00:15<00:42,  5.39it/s]Evaluating:  27%|██▋       | 83/313 [00:15<00:42,  5.39it/s]Evaluating:  27%|██▋       | 84/313 [00:16<00:42,  5.39it/s]Evaluating:  27%|██▋       | 85/313 [00:16<00:42,  5.39it/s]Evaluating:  27%|██▋       | 86/313 [00:16<00:42,  5.38it/s]Evaluating:  28%|██▊       | 87/313 [00:16<00:41,  5.39it/s]Evaluating:  28%|██▊       | 88/313 [00:16<00:41,  5.39it/s]Evaluating:  28%|██▊       | 89/313 [00:16<00:41,  5.41it/s]Evaluating:  29%|██▉       | 90/313 [00:17<00:40,  5.45it/s]Evaluating:  29%|██▉       | 91/313 [00:17<00:41,  5.39it/s]Evaluating:  29%|██▉       | 92/313 [00:17<00:40,  5.49it/s]Evaluating:  30%|██▉       | 93/313 [00:17<00:39,  5.56it/s]Evaluating:  30%|███       | 94/313 [00:17<00:39,  5.59it/s]Evaluating:  30%|███       | 95/313 [00:18<00:38,  5.61it/s]Evaluating:  31%|███       | 96/313 [00:18<00:38,  5.64it/s]Evaluating:  31%|███       | 97/313 [00:18<00:38,  5.64it/s]Evaluating:  31%|███▏      | 98/313 [00:18<00:38,  5.66it/s]Evaluating:  32%|███▏      | 99/313 [00:18<00:37,  5.68it/s]Evaluating:  32%|███▏      | 100/313 [00:18<00:37,  5.68it/s]Evaluating:  32%|███▏      | 101/313 [00:19<00:37,  5.65it/s]Evaluating:  33%|███▎      | 102/313 [00:19<00:37,  5.61it/s]Evaluating:  33%|███▎      | 103/313 [00:19<00:37,  5.57it/s]Evaluating:  33%|███▎      | 104/313 [00:19<00:37,  5.52it/s]Evaluating:  34%|███▎      | 105/313 [00:19<00:37,  5.48it/s]Evaluating:  34%|███▍      | 106/313 [00:19<00:38,  5.44it/s]Evaluating:  34%|███▍      | 107/313 [00:20<00:38,  5.42it/s]Evaluating:  35%|███▍      | 108/313 [00:20<00:37,  5.41it/s]Evaluating:  35%|███▍      | 109/313 [00:20<00:37,  5.39it/s]Evaluating:  35%|███▌      | 110/313 [00:20<00:37,  5.38it/s]Evaluating:  35%|███▌      | 111/313 [00:20<00:37,  5.38it/s]Evaluating:  36%|███▌      | 112/313 [00:21<00:37,  5.37it/s]Evaluating:  36%|███▌      | 113/313 [00:21<00:37,  5.37it/s]Evaluating:  36%|███▋      | 114/313 [00:21<00:37,  5.37it/s]Evaluating:  37%|███▋      | 115/313 [00:21<00:36,  5.36it/s]Evaluating:  37%|███▋      | 116/313 [00:21<00:36,  5.36it/s]Evaluating:  37%|███▋      | 117/313 [00:22<00:36,  5.36it/s]Evaluating:  38%|███▊      | 118/313 [00:22<00:36,  5.36it/s]Evaluating:  38%|███▊      | 119/313 [00:22<00:36,  5.37it/s]Evaluating:  38%|███▊      | 120/313 [00:22<00:35,  5.37it/s]Evaluating:  39%|███▊      | 121/313 [00:22<00:35,  5.36it/s]Evaluating:  39%|███▉      | 122/313 [00:22<00:35,  5.36it/s]Evaluating:  39%|███▉      | 123/313 [00:23<00:35,  5.37it/s]Evaluating:  40%|███▉      | 124/313 [00:23<00:35,  5.39it/s]Evaluating:  40%|███▉      | 125/313 [00:23<00:34,  5.41it/s]Evaluating:  40%|████      | 126/313 [00:23<00:34,  5.40it/s]Evaluating:  41%|████      | 127/313 [00:23<00:34,  5.38it/s]Evaluating:  41%|████      | 128/313 [00:24<00:34,  5.37it/s]Evaluating:  41%|████      | 129/313 [00:24<00:34,  5.38it/s]Evaluating:  42%|████▏     | 130/313 [00:24<00:34,  5.38it/s]Evaluating:  42%|████▏     | 131/313 [00:24<00:33,  5.37it/s]Evaluating:  42%|████▏     | 132/313 [00:24<00:33,  5.37it/s]Evaluating:  42%|████▏     | 133/313 [00:25<00:33,  5.36it/s]Evaluating:  43%|████▎     | 134/313 [00:25<00:33,  5.36it/s]Evaluating:  43%|████▎     | 135/313 [00:25<00:33,  5.37it/s]Evaluating:  43%|████▎     | 136/313 [00:25<00:32,  5.37it/s]Evaluating:  44%|████▍     | 137/313 [00:25<00:32,  5.38it/s]Evaluating:  44%|████▍     | 138/313 [00:25<00:32,  5.39it/s]Evaluating:  44%|████▍     | 139/313 [00:26<00:32,  5.37it/s]Evaluating:  45%|████▍     | 140/313 [00:26<00:32,  5.38it/s]Evaluating:  45%|████▌     | 141/313 [00:26<00:31,  5.39it/s]Evaluating:  45%|████▌     | 142/313 [00:26<00:31,  5.38it/s]Evaluating:  46%|████▌     | 143/313 [00:26<00:31,  5.37it/s]Evaluating:  46%|████▌     | 144/313 [00:27<00:31,  5.36it/s]Evaluating:  46%|████▋     | 145/313 [00:27<00:41,  4.02it/s]Evaluating:  47%|████▋     | 146/313 [00:27<00:38,  4.39it/s]Evaluating:  47%|████▋     | 147/313 [00:27<00:35,  4.72it/s]Evaluating:  47%|████▋     | 148/313 [00:27<00:33,  4.98it/s]Evaluating:  48%|████▊     | 149/313 [00:28<00:31,  5.20it/s]Evaluating:  48%|████▊     | 150/313 [00:28<00:30,  5.36it/s]Evaluating:  48%|████▊     | 151/313 [00:28<00:29,  5.47it/s]Evaluating:  49%|████▊     | 152/313 [00:28<00:29,  5.54it/s]Evaluating:  49%|████▉     | 153/313 [00:28<00:28,  5.58it/s]Evaluating:  49%|████▉     | 154/313 [00:29<00:28,  5.60it/s]Evaluating:  50%|████▉     | 155/313 [00:29<00:28,  5.62it/s]Evaluating:  50%|████▉     | 156/313 [00:29<00:27,  5.64it/s]Evaluating:  50%|█████     | 157/313 [00:29<00:27,  5.65it/s]Evaluating:  50%|█████     | 158/313 [00:29<00:27,  5.62it/s]Evaluating:  51%|█████     | 159/313 [00:29<00:27,  5.54it/s]Evaluating:  51%|█████     | 160/313 [00:30<00:28,  5.46it/s]Evaluating:  51%|█████▏    | 161/313 [00:30<00:28,  5.43it/s]Evaluating:  52%|█████▏    | 162/313 [00:30<00:28,  5.39it/s]Evaluating:  52%|█████▏    | 163/313 [00:30<00:27,  5.37it/s]Evaluating:  52%|█████▏    | 164/313 [00:30<00:27,  5.36it/s]Evaluating:  53%|█████▎    | 165/313 [00:31<00:27,  5.39it/s]Evaluating:  53%|█████▎    | 166/313 [00:31<00:27,  5.39it/s]Evaluating:  53%|█████▎    | 167/313 [00:31<00:26,  5.41it/s]Evaluating:  54%|█████▎    | 168/313 [00:31<00:26,  5.39it/s]Evaluating:  54%|█████▍    | 169/313 [00:31<00:26,  5.36it/s]Evaluating:  54%|█████▍    | 170/313 [00:31<00:26,  5.35it/s]Evaluating:  55%|█████▍    | 171/313 [00:32<00:29,  4.84it/s]Evaluating:  55%|█████▍    | 172/313 [00:32<00:28,  4.97it/s]Evaluating:  55%|█████▌    | 173/313 [00:32<00:27,  5.07it/s]Evaluating:  56%|█████▌    | 174/313 [00:32<00:27,  5.14it/s]Evaluating:  56%|█████▌    | 175/313 [00:32<00:26,  5.18it/s]Evaluating:  56%|█████▌    | 176/313 [00:33<00:26,  5.24it/s]Evaluating:  57%|█████▋    | 177/313 [00:33<00:25,  5.28it/s]Evaluating:  57%|█████▋    | 178/313 [00:33<00:25,  5.32it/s]Evaluating:  57%|█████▋    | 179/313 [00:33<00:25,  5.35it/s]Evaluating:  58%|█████▊    | 180/313 [00:33<00:24,  5.38it/s]Evaluating:  58%|█████▊    | 181/313 [00:34<00:24,  5.42it/s]Evaluating:  58%|█████▊    | 182/313 [00:34<00:24,  5.43it/s]Evaluating:  58%|█████▊    | 183/313 [00:34<00:24,  5.39it/s]Evaluating:  59%|█████▉    | 184/313 [00:34<00:24,  5.35it/s]Evaluating:  59%|█████▉    | 185/313 [00:34<00:23,  5.34it/s]Evaluating:  59%|█████▉    | 186/313 [00:35<00:23,  5.32it/s]Evaluating:  60%|█████▉    | 187/313 [00:35<00:23,  5.31it/s]Evaluating:  60%|██████    | 188/313 [00:35<00:23,  5.30it/s]Evaluating:  60%|██████    | 189/313 [00:35<00:23,  5.30it/s]Evaluating:  61%|██████    | 190/313 [00:35<00:23,  5.32it/s]Evaluating:  61%|██████    | 191/313 [00:35<00:22,  5.34it/s]Evaluating:  61%|██████▏   | 192/313 [00:36<00:22,  5.36it/s]Evaluating:  62%|██████▏   | 193/313 [00:36<00:22,  5.38it/s]Evaluating:  62%|██████▏   | 194/313 [00:36<00:22,  5.39it/s]Evaluating:  62%|██████▏   | 195/313 [00:36<00:22,  5.36it/s]Evaluating:  63%|██████▎   | 196/313 [00:36<00:21,  5.33it/s]Evaluating:  63%|██████▎   | 197/313 [00:37<00:21,  5.36it/s]Evaluating:  63%|██████▎   | 198/313 [00:37<00:27,  4.11it/s]Evaluating:  64%|██████▎   | 199/313 [00:37<00:25,  4.41it/s]Evaluating:  64%|██████▍   | 200/313 [00:37<00:24,  4.67it/s]Evaluating:  64%|██████▍   | 201/313 [00:38<00:22,  4.88it/s]Evaluating:  65%|██████▍   | 202/313 [00:38<00:22,  5.02it/s]Evaluating:  65%|██████▍   | 203/313 [00:38<00:21,  5.13it/s]Evaluating:  65%|██████▌   | 204/313 [00:38<00:20,  5.20it/s]Evaluating:  65%|██████▌   | 205/313 [00:38<00:20,  5.24it/s]Evaluating:  66%|██████▌   | 206/313 [00:38<00:20,  5.25it/s]Evaluating:  66%|██████▌   | 207/313 [00:39<00:20,  5.26it/s]Evaluating:  66%|██████▋   | 208/313 [00:39<00:19,  5.28it/s]Evaluating:  67%|██████▋   | 209/313 [00:39<00:19,  5.30it/s]Evaluating:  67%|██████▋   | 210/313 [00:39<00:19,  5.33it/s]Evaluating:  67%|██████▋   | 211/313 [00:39<00:19,  5.34it/s]Evaluating:  68%|██████▊   | 212/313 [00:40<00:18,  5.34it/s]Evaluating:  68%|██████▊   | 213/313 [00:40<00:18,  5.35it/s]Evaluating:  68%|██████▊   | 214/313 [00:40<00:18,  5.35it/s]Evaluating:  69%|██████▊   | 215/313 [00:40<00:18,  5.36it/s]Evaluating:  69%|██████▉   | 216/313 [00:40<00:18,  5.36it/s]Evaluating:  69%|██████▉   | 217/313 [00:41<00:17,  5.38it/s]Evaluating:  70%|██████▉   | 218/313 [00:41<00:17,  5.38it/s]Evaluating:  70%|██████▉   | 219/313 [00:41<00:17,  5.37it/s]Evaluating:  70%|███████   | 220/313 [00:41<00:17,  5.37it/s]Evaluating:  71%|███████   | 221/313 [00:41<00:17,  5.38it/s]Evaluating:  71%|███████   | 222/313 [00:41<00:16,  5.40it/s]Evaluating:  71%|███████   | 223/313 [00:42<00:16,  5.41it/s]Evaluating:  72%|███████▏  | 224/313 [00:42<00:16,  5.42it/s]Evaluating:  72%|███████▏  | 225/313 [00:42<00:16,  5.41it/s]Evaluating:  72%|███████▏  | 226/313 [00:42<00:16,  5.41it/s]Evaluating:  73%|███████▎  | 227/313 [00:42<00:15,  5.39it/s]Evaluating:  73%|███████▎  | 228/313 [00:43<00:15,  5.35it/s]Evaluating:  73%|███████▎  | 229/313 [00:43<00:15,  5.33it/s]Evaluating:  73%|███████▎  | 230/313 [00:43<00:15,  5.33it/s]Evaluating:  74%|███████▍  | 231/313 [00:43<00:15,  5.35it/s]Evaluating:  74%|███████▍  | 232/313 [00:43<00:15,  5.37it/s]Evaluating:  74%|███████▍  | 233/313 [00:43<00:14,  5.43it/s]Evaluating:  75%|███████▍  | 234/313 [00:44<00:14,  5.47it/s]Evaluating:  75%|███████▌  | 235/313 [00:44<00:14,  5.54it/s]Evaluating:  75%|███████▌  | 236/313 [00:44<00:13,  5.60it/s]Evaluating:  76%|███████▌  | 237/313 [00:44<00:13,  5.65it/s]Evaluating:  76%|███████▌  | 238/313 [00:44<00:13,  5.61it/s]Evaluating:  76%|███████▋  | 239/313 [00:45<00:13,  5.58it/s]Evaluating:  77%|███████▋  | 240/313 [00:45<00:13,  5.57it/s]Evaluating:  77%|███████▋  | 241/313 [00:45<00:12,  5.59it/s]Evaluating:  77%|███████▋  | 242/313 [00:45<00:12,  5.62it/s]Evaluating:  78%|███████▊  | 243/313 [00:45<00:12,  5.65it/s]Evaluating:  78%|███████▊  | 244/313 [00:45<00:12,  5.67it/s]Evaluating:  78%|███████▊  | 245/313 [00:46<00:12,  5.64it/s]Evaluating:  79%|███████▊  | 246/313 [00:46<00:11,  5.60it/s]Evaluating:  79%|███████▉  | 247/313 [00:46<00:11,  5.59it/s]Evaluating:  79%|███████▉  | 248/313 [00:46<00:11,  5.60it/s]Evaluating:  80%|███████▉  | 249/313 [00:46<00:11,  5.63it/s]Evaluating:  80%|███████▉  | 250/313 [00:46<00:11,  5.66it/s]Evaluating:  80%|████████  | 251/313 [00:47<00:12,  4.87it/s]Evaluating:  81%|████████  | 252/313 [00:47<00:11,  5.09it/s]Evaluating:  81%|████████  | 253/313 [00:47<00:11,  5.27it/s]Evaluating:  81%|████████  | 254/313 [00:47<00:10,  5.40it/s]Evaluating:  81%|████████▏ | 255/313 [00:47<00:10,  5.49it/s]Evaluating:  82%|████████▏ | 256/313 [00:48<00:10,  5.54it/s]Evaluating:  82%|████████▏ | 257/313 [00:48<00:10,  5.58it/s]Evaluating:  82%|████████▏ | 258/313 [00:48<00:09,  5.61it/s]Evaluating:  83%|████████▎ | 259/313 [00:48<00:09,  5.64it/s]Evaluating:  83%|████████▎ | 260/313 [00:48<00:09,  5.66it/s]Evaluating:  83%|████████▎ | 261/313 [00:49<00:09,  5.68it/s]Evaluating:  84%|████████▎ | 262/313 [00:49<00:08,  5.68it/s]Evaluating:  84%|████████▍ | 263/313 [00:49<00:08,  5.64it/s]Evaluating:  84%|████████▍ | 264/313 [00:49<00:08,  5.60it/s]Evaluating:  85%|████████▍ | 265/313 [00:49<00:08,  5.58it/s]Evaluating:  85%|████████▍ | 266/313 [00:49<00:08,  5.55it/s]Evaluating:  85%|████████▌ | 267/313 [00:50<00:08,  5.56it/s]Evaluating:  86%|████████▌ | 268/313 [00:50<00:08,  5.59it/s]Evaluating:  86%|████████▌ | 269/313 [00:50<00:07,  5.61it/s]Evaluating:  86%|████████▋ | 270/313 [00:50<00:07,  5.63it/s]Evaluating:  87%|████████▋ | 271/313 [00:50<00:07,  5.64it/s]Evaluating:  87%|████████▋ | 272/313 [00:50<00:07,  5.65it/s]Evaluating:  87%|████████▋ | 273/313 [00:51<00:07,  5.67it/s]Evaluating:  88%|████████▊ | 274/313 [00:51<00:06,  5.67it/s]Evaluating:  88%|████████▊ | 275/313 [00:51<00:06,  5.69it/s]Evaluating:  88%|████████▊ | 276/313 [00:51<00:06,  5.50it/s]Evaluating:  88%|████████▊ | 277/313 [00:51<00:06,  5.53it/s]Evaluating:  89%|████████▉ | 278/313 [00:52<00:06,  5.44it/s]Evaluating:  89%|████████▉ | 279/313 [00:52<00:07,  4.39it/s]Evaluating:  89%|████████▉ | 280/313 [00:52<00:07,  4.61it/s]Evaluating:  90%|████████▉ | 281/313 [00:52<00:06,  4.79it/s]Evaluating:  90%|█████████ | 282/313 [00:52<00:06,  4.93it/s]Evaluating:  90%|█████████ | 283/313 [00:53<00:05,  5.06it/s]Evaluating:  91%|█████████ | 284/313 [00:53<00:05,  5.17it/s]Evaluating:  91%|█████████ | 285/313 [00:53<00:05,  5.25it/s]Evaluating:  91%|█████████▏| 286/313 [00:53<00:05,  5.26it/s]Evaluating:  92%|█████████▏| 287/313 [00:53<00:04,  5.26it/s]Evaluating:  92%|█████████▏| 288/313 [00:54<00:04,  5.24it/s]Evaluating:  92%|█████████▏| 289/313 [00:54<00:04,  5.25it/s]Evaluating:  93%|█████████▎| 290/313 [00:54<00:04,  5.24it/s]Evaluating:  93%|█████████▎| 291/313 [00:54<00:04,  5.26it/s]Evaluating:  93%|█████████▎| 292/313 [00:54<00:03,  5.28it/s]Evaluating:  94%|█████████▎| 293/313 [00:55<00:03,  5.30it/s]Evaluating:  94%|█████████▍| 294/313 [00:55<00:03,  5.26it/s]Evaluating:  94%|█████████▍| 295/313 [00:55<00:03,  5.31it/s]Evaluating:  95%|█████████▍| 296/313 [00:55<00:03,  5.36it/s]Evaluating:  95%|█████████▍| 297/313 [00:55<00:02,  5.40it/s]Evaluating:  95%|█████████▌| 298/313 [00:55<00:02,  5.41it/s]Evaluating:  96%|█████████▌| 299/313 [00:56<00:02,  5.36it/s]Evaluating:  96%|█████████▌| 300/313 [00:56<00:02,  5.31it/s]Evaluating:  96%|█████████▌| 301/313 [00:56<00:02,  5.31it/s]Evaluating:  96%|█████████▋| 302/313 [00:56<00:02,  5.32it/s]Evaluating:  97%|█████████▋| 303/313 [00:56<00:01,  5.36it/s]Evaluating:  97%|█████████▋| 304/313 [00:57<00:01,  5.39it/s]Evaluating:  97%|█████████▋| 305/313 [00:57<00:01,  5.41it/s]Evaluating:  98%|█████████▊| 306/313 [00:57<00:01,  5.43it/s]Evaluating:  98%|█████████▊| 307/313 [00:57<00:01,  5.44it/s]Evaluating:  98%|█████████▊| 308/313 [00:57<00:00,  5.45it/s]Evaluating:  99%|█████████▊| 309/313 [00:58<00:00,  5.43it/s]Evaluating:  99%|█████████▉| 310/313 [00:58<00:00,  5.41it/s]Evaluating:  99%|█████████▉| 311/313 [00:58<00:00,  5.43it/s]Evaluating: 100%|█████████▉| 312/313 [00:58<00:00,  5.44it/s]Evaluating: 100%|██████████| 313/313 [00:58<00:00,  6.22it/s]Evaluating: 100%|██████████| 313/313 [00:58<00:00,  5.33it/s]
10/14/2021 16:16:53 - INFO - __main__ -   ***** Evaluation result  in da *****
10/14/2021 16:16:53 - INFO - __main__ -     f1 = 0.7952988629646371
10/14/2021 16:16:53 - INFO - __main__ -     loss = 0.7532412073911189
10/14/2021 16:16:53 - INFO - __main__ -     precision = 0.7593121306686142
10/14/2021 16:16:53 - INFO - __main__ -     recall = 0.8348664038545773
10/14/2021 16:16:53 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:17:09 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:17:09 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:17:38 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 16:20:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:20:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 16:20:32 - INFO - __main__ -   Seed = 1
10/14/2021 16:20:32 - INFO - root -   save model
10/14/2021 16:20:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:20:32 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:20:44 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:20:44 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 16:20:44 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
10/14/2021 16:20:44 - INFO - root -   Trying to decide if add adapter
10/14/2021 16:20:44 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 16:20:44 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/14/2021 16:20:44 - INFO - __main__ -   Language = en
10/14/2021 16:20:44 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 16:20:46 - INFO - __main__ -   Language = ru
10/14/2021 16:20:46 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/14/2021 16:20:51 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/14/2021 16:20:51 - INFO - __main__ -   ***** Running evaluation  in be *****
10/14/2021 16:20:51 - INFO - __main__ -     Num examples = 1001
10/14/2021 16:20:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.85it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.17it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.42it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.55it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.62it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.67it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.69it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.72it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.73it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.74it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.75it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.74it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.74it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.73it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.73it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.72it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.72it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.73it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.73it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.74it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:02,  4.11it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:02,  4.65it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  5.12it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  5.52it/s]Evaluating:  78%|███████▊  | 25/32 [00:04<00:01,  5.83it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.07it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.25it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.39it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.48it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.54it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.58it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.39it/s]
10/14/2021 16:20:56 - INFO - __main__ -   ***** Evaluation result  in be *****
10/14/2021 16:20:56 - INFO - __main__ -     f1 = 0.6511075949367088
10/14/2021 16:20:56 - INFO - __main__ -     loss = 1.2652716166339815
10/14/2021 16:20:56 - INFO - __main__ -     precision = 0.627765064836003
10/14/2021 16:20:56 - INFO - __main__ -     recall = 0.676253081347576
10/14/2021 16:20:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/14/2021 16:20:58 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/14/2021 16:20:58 - INFO - __main__ -     Num examples = 10001
10/14/2021 16:20:58 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:45,  6.79it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.74it/s]Evaluating:   1%|          | 3/313 [00:00<00:46,  6.73it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.72it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.73it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.73it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.73it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.73it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.72it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.73it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.74it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.74it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.73it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.73it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.73it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.72it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.70it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:43,  6.71it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.72it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.73it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.73it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.72it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.72it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.72it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:42,  6.72it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.71it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.71it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.70it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.70it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.71it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.70it/s]Evaluating:  10%|█         | 32/313 [00:04<00:41,  6.70it/s]Evaluating:  11%|█         | 33/313 [00:04<00:41,  6.70it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.69it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.69it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.69it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.67it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.66it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:41,  6.68it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:40,  6.68it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:40,  6.68it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.67it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.67it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.67it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:55,  4.84it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:50,  5.28it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:47,  5.62it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:44,  5.89it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:43,  6.11it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:41,  6.27it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:41,  6.38it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:40,  6.45it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:39,  6.52it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:39,  6.56it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:39,  6.58it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:39,  6.57it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.59it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.60it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:38,  6.62it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:38,  6.63it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:37,  6.64it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:37,  6.64it/s]Evaluating:  20%|██        | 63/313 [00:09<00:37,  6.64it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.65it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.64it/s]Evaluating:  21%|██        | 66/313 [00:10<00:37,  6.63it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:37,  6.63it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:36,  6.63it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:36,  6.62it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:36,  6.63it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.62it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.62it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:36,  6.60it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:36,  6.60it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:36,  6.60it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:35,  6.61it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:44,  5.33it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:41,  5.66it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:39,  5.91it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:38,  6.11it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:37,  6.25it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:36,  6.35it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:35,  6.43it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:35,  6.49it/s]Evaluating:  27%|██▋       | 85/313 [00:13<00:34,  6.53it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:34,  6.54it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:34,  6.55it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:34,  6.56it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:34,  6.57it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:33,  6.57it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:33,  6.58it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:33,  6.58it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:33,  6.57it/s]Evaluating:  30%|███       | 94/313 [00:14<00:33,  6.57it/s]Evaluating:  30%|███       | 95/313 [00:14<00:33,  6.58it/s]Evaluating:  31%|███       | 96/313 [00:14<00:33,  6.57it/s]Evaluating:  31%|███       | 97/313 [00:14<00:32,  6.57it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:32,  6.57it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:32,  6.57it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:32,  6.58it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:32,  6.58it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:32,  6.58it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:31,  6.58it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:31,  6.58it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:31,  6.57it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:31,  6.57it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:31,  6.57it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:31,  6.55it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:34,  5.87it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:33,  6.07it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:32,  6.20it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:31,  6.30it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:31,  6.37it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:31,  6.40it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:30,  6.44it/s]Evaluating:  37%|███▋      | 116/313 [00:17<00:30,  6.46it/s]Evaluating:  37%|███▋      | 117/313 [00:17<00:30,  6.47it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:30,  6.48it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:29,  6.50it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:29,  6.50it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:29,  6.50it/s]Evaluating:  39%|███▉      | 122/313 [00:18<00:29,  6.50it/s]Evaluating:  39%|███▉      | 123/313 [00:18<00:29,  6.44it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:29,  6.43it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:29,  6.46it/s]Evaluating:  40%|████      | 126/313 [00:19<00:28,  6.47it/s]Evaluating:  41%|████      | 127/313 [00:19<00:28,  6.49it/s]Evaluating:  41%|████      | 128/313 [00:19<00:28,  6.49it/s]Evaluating:  41%|████      | 129/313 [00:19<00:28,  6.50it/s]Evaluating:  42%|████▏     | 130/313 [00:19<00:28,  6.50it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:27,  6.50it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:27,  6.50it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:27,  6.49it/s]Evaluating:  43%|████▎     | 134/313 [00:20<00:27,  6.50it/s]Evaluating:  43%|████▎     | 135/313 [00:20<00:27,  6.49it/s]Evaluating:  43%|████▎     | 136/313 [00:20<00:27,  6.48it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:27,  6.49it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:26,  6.49it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:26,  6.49it/s]Evaluating:  45%|████▍     | 140/313 [00:21<00:26,  6.49it/s]Evaluating:  45%|████▌     | 141/313 [00:21<00:34,  4.94it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:32,  5.30it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:30,  5.59it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:28,  5.83it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:27,  6.01it/s]Evaluating:  47%|████▋     | 146/313 [00:22<00:27,  6.14it/s]Evaluating:  47%|████▋     | 147/313 [00:22<00:26,  6.23it/s]Evaluating:  47%|████▋     | 148/313 [00:22<00:26,  6.30it/s]Evaluating:  48%|████▊     | 149/313 [00:23<00:25,  6.34it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:25,  6.37it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:25,  6.40it/s]Evaluating:  49%|████▊     | 152/313 [00:23<00:25,  6.42it/s]Evaluating:  49%|████▉     | 153/313 [00:23<00:24,  6.43it/s]Evaluating:  49%|████▉     | 154/313 [00:23<00:24,  6.43it/s]Evaluating:  50%|████▉     | 155/313 [00:24<00:24,  6.43it/s]Evaluating:  50%|████▉     | 156/313 [00:24<00:24,  6.44it/s]Evaluating:  50%|█████     | 157/313 [00:24<00:24,  6.45it/s]Evaluating:  50%|█████     | 158/313 [00:24<00:24,  6.44it/s]Evaluating:  51%|█████     | 159/313 [00:24<00:23,  6.45it/s]Evaluating:  51%|█████     | 160/313 [00:24<00:23,  6.45it/s]Evaluating:  51%|█████▏    | 161/313 [00:24<00:23,  6.45it/s]Evaluating:  52%|█████▏    | 162/313 [00:25<00:23,  6.45it/s]Evaluating:  52%|█████▏    | 163/313 [00:25<00:23,  6.42it/s]Evaluating:  52%|█████▏    | 164/313 [00:25<00:23,  6.42it/s]Evaluating:  53%|█████▎    | 165/313 [00:25<00:23,  6.42it/s]Evaluating:  53%|█████▎    | 166/313 [00:25<00:22,  6.42it/s]Evaluating:  53%|█████▎    | 167/313 [00:25<00:22,  6.39it/s]Evaluating:  54%|█████▎    | 168/313 [00:26<00:22,  6.36it/s]Evaluating:  54%|█████▍    | 169/313 [00:26<00:22,  6.33it/s]Evaluating:  54%|█████▍    | 170/313 [00:26<00:22,  6.32it/s]Evaluating:  55%|█████▍    | 171/313 [00:26<00:22,  6.32it/s]Evaluating:  55%|█████▍    | 172/313 [00:26<00:25,  5.55it/s]Evaluating:  55%|█████▌    | 173/313 [00:26<00:24,  5.64it/s]Evaluating:  56%|█████▌    | 174/313 [00:27<00:24,  5.71it/s]Evaluating:  56%|█████▌    | 175/313 [00:27<00:23,  5.76it/s]Evaluating:  56%|█████▌    | 176/313 [00:27<00:23,  5.82it/s]Evaluating:  57%|█████▋    | 177/313 [00:27<00:23,  5.89it/s]Evaluating:  57%|█████▋    | 178/313 [00:27<00:22,  5.95it/s]Evaluating:  57%|█████▋    | 179/313 [00:27<00:22,  6.01it/s]Evaluating:  58%|█████▊    | 180/313 [00:28<00:21,  6.07it/s]Evaluating:  58%|█████▊    | 181/313 [00:28<00:21,  6.12it/s]Evaluating:  58%|█████▊    | 182/313 [00:28<00:21,  6.18it/s]Evaluating:  58%|█████▊    | 183/313 [00:28<00:20,  6.22it/s]Evaluating:  59%|█████▉    | 184/313 [00:28<00:20,  6.24it/s]Evaluating:  59%|█████▉    | 185/313 [00:28<00:20,  6.26it/s]Evaluating:  59%|█████▉    | 186/313 [00:29<00:20,  6.25it/s]Evaluating:  60%|█████▉    | 187/313 [00:29<00:20,  6.23it/s]Evaluating:  60%|██████    | 188/313 [00:29<00:20,  6.19it/s]Evaluating:  60%|██████    | 189/313 [00:29<00:20,  6.15it/s]Evaluating:  61%|██████    | 190/313 [00:29<00:20,  6.11it/s]Evaluating:  61%|██████    | 191/313 [00:29<00:19,  6.11it/s]Evaluating:  61%|██████▏   | 192/313 [00:30<00:19,  6.09it/s]Evaluating:  62%|██████▏   | 193/313 [00:30<00:19,  6.06it/s]Evaluating:  62%|██████▏   | 194/313 [00:30<00:19,  6.05it/s]Evaluating:  62%|██████▏   | 195/313 [00:30<00:19,  6.04it/s]Evaluating:  63%|██████▎   | 196/313 [00:30<00:19,  6.03it/s]Evaluating:  63%|██████▎   | 197/313 [00:30<00:19,  6.00it/s]Evaluating:  63%|██████▎   | 198/313 [00:31<00:19,  5.95it/s]Evaluating:  64%|██████▎   | 199/313 [00:31<00:19,  5.92it/s]Evaluating:  64%|██████▍   | 200/313 [00:31<00:19,  5.93it/s]Evaluating:  64%|██████▍   | 201/313 [00:31<00:18,  5.95it/s]Evaluating:  65%|██████▍   | 202/313 [00:31<00:24,  4.53it/s]Evaluating:  65%|██████▍   | 203/313 [00:32<00:22,  4.86it/s]Evaluating:  65%|██████▌   | 204/313 [00:32<00:21,  5.12it/s]Evaluating:  65%|██████▌   | 205/313 [00:32<00:20,  5.31it/s]Evaluating:  66%|██████▌   | 206/313 [00:32<00:19,  5.47it/s]Evaluating:  66%|██████▌   | 207/313 [00:32<00:18,  5.62it/s]Evaluating:  66%|██████▋   | 208/313 [00:32<00:18,  5.72it/s]Evaluating:  67%|██████▋   | 209/313 [00:33<00:17,  5.80it/s]Evaluating:  67%|██████▋   | 210/313 [00:33<00:17,  5.85it/s]Evaluating:  67%|██████▋   | 211/313 [00:33<00:17,  5.89it/s]Evaluating:  68%|██████▊   | 212/313 [00:33<00:17,  5.92it/s]Evaluating:  68%|██████▊   | 213/313 [00:33<00:16,  5.96it/s]Evaluating:  68%|██████▊   | 214/313 [00:33<00:16,  6.00it/s]Evaluating:  69%|██████▊   | 215/313 [00:34<00:16,  6.03it/s]Evaluating:  69%|██████▉   | 216/313 [00:34<00:16,  6.06it/s]Evaluating:  69%|██████▉   | 217/313 [00:34<00:15,  6.07it/s]Evaluating:  70%|██████▉   | 218/313 [00:34<00:15,  6.10it/s]Evaluating:  70%|██████▉   | 219/313 [00:34<00:15,  6.12it/s]Evaluating:  70%|███████   | 220/313 [00:34<00:15,  6.12it/s]Evaluating:  71%|███████   | 221/313 [00:35<00:15,  6.13it/s]Evaluating:  71%|███████   | 222/313 [00:35<00:14,  6.15it/s]Evaluating:  71%|███████   | 223/313 [00:35<00:14,  6.16it/s]Evaluating:  72%|███████▏  | 224/313 [00:35<00:14,  6.08it/s]Evaluating:  72%|███████▏  | 225/313 [00:35<00:14,  6.02it/s]Evaluating:  72%|███████▏  | 226/313 [00:35<00:14,  5.99it/s]Evaluating:  73%|███████▎  | 227/313 [00:36<00:14,  5.98it/s]Evaluating:  73%|███████▎  | 228/313 [00:36<00:14,  5.97it/s]Evaluating:  73%|███████▎  | 229/313 [00:36<00:14,  5.95it/s]Evaluating:  73%|███████▎  | 230/313 [00:36<00:14,  5.90it/s]Evaluating:  74%|███████▍  | 231/313 [00:36<00:13,  5.88it/s]Evaluating:  74%|███████▍  | 232/313 [00:36<00:13,  5.88it/s]Evaluating:  74%|███████▍  | 233/313 [00:37<00:13,  5.90it/s]Evaluating:  75%|███████▍  | 234/313 [00:37<00:13,  5.89it/s]Evaluating:  75%|███████▌  | 235/313 [00:37<00:13,  5.87it/s]Evaluating:  75%|███████▌  | 236/313 [00:37<00:13,  5.84it/s]Evaluating:  76%|███████▌  | 237/313 [00:37<00:13,  5.83it/s]Evaluating:  76%|███████▌  | 238/313 [00:37<00:12,  5.82it/s]Evaluating:  76%|███████▋  | 239/313 [00:38<00:12,  5.84it/s]Evaluating:  77%|███████▋  | 240/313 [00:38<00:12,  5.85it/s]Evaluating:  77%|███████▋  | 241/313 [00:38<00:12,  5.88it/s]Evaluating:  77%|███████▋  | 242/313 [00:38<00:12,  5.85it/s]Evaluating:  78%|███████▊  | 243/313 [00:38<00:11,  5.84it/s]Evaluating:  78%|███████▊  | 244/313 [00:38<00:11,  5.82it/s]Evaluating:  78%|███████▊  | 245/313 [00:39<00:11,  5.81it/s]Evaluating:  79%|███████▊  | 246/313 [00:39<00:11,  5.80it/s]Evaluating:  79%|███████▉  | 247/313 [00:39<00:11,  5.80it/s]Evaluating:  79%|███████▉  | 248/313 [00:39<00:11,  5.79it/s]Evaluating:  80%|███████▉  | 249/313 [00:39<00:11,  5.79it/s]Evaluating:  80%|███████▉  | 250/313 [00:39<00:10,  5.79it/s]Evaluating:  80%|████████  | 251/313 [00:40<00:10,  5.80it/s]Evaluating:  81%|████████  | 252/313 [00:40<00:10,  5.79it/s]Evaluating:  81%|████████  | 253/313 [00:40<00:10,  5.81it/s]Evaluating:  81%|████████  | 254/313 [00:40<00:10,  5.79it/s]Evaluating:  81%|████████▏ | 255/313 [00:40<00:10,  5.79it/s]Evaluating:  82%|████████▏ | 256/313 [00:41<00:09,  5.78it/s]Evaluating:  82%|████████▏ | 257/313 [00:41<00:09,  5.79it/s]Evaluating:  82%|████████▏ | 258/313 [00:41<00:09,  5.74it/s]Evaluating:  83%|████████▎ | 259/313 [00:41<00:09,  5.72it/s]Evaluating:  83%|████████▎ | 260/313 [00:41<00:09,  5.70it/s]Evaluating:  83%|████████▎ | 261/313 [00:41<00:09,  5.68it/s]Evaluating:  84%|████████▎ | 262/313 [00:42<00:09,  5.67it/s]Evaluating:  84%|████████▍ | 263/313 [00:42<00:08,  5.68it/s]Evaluating:  84%|████████▍ | 264/313 [00:42<00:08,  5.69it/s]Evaluating:  85%|████████▍ | 265/313 [00:42<00:08,  5.74it/s]Evaluating:  85%|████████▍ | 266/313 [00:42<00:08,  5.78it/s]Evaluating:  85%|████████▌ | 267/313 [00:42<00:07,  5.81it/s]Evaluating:  86%|████████▌ | 268/313 [00:43<00:07,  5.75it/s]Evaluating:  86%|████████▌ | 269/313 [00:43<00:07,  5.70it/s]Evaluating:  86%|████████▋ | 270/313 [00:43<00:07,  5.68it/s]Evaluating:  87%|████████▋ | 271/313 [00:43<00:07,  5.70it/s]Evaluating:  87%|████████▋ | 272/313 [00:43<00:07,  5.74it/s]Evaluating:  87%|████████▋ | 273/313 [00:43<00:06,  5.81it/s]Evaluating:  88%|████████▊ | 274/313 [00:44<00:06,  5.88it/s]Evaluating:  88%|████████▊ | 275/313 [00:44<00:06,  5.92it/s]Evaluating:  88%|████████▊ | 276/313 [00:44<00:06,  5.94it/s]Evaluating:  88%|████████▊ | 277/313 [00:44<00:06,  5.97it/s]Evaluating:  89%|████████▉ | 278/313 [00:44<00:05,  6.00it/s]Evaluating:  89%|████████▉ | 279/313 [00:44<00:05,  6.03it/s]Evaluating:  89%|████████▉ | 280/313 [00:45<00:05,  6.05it/s]Evaluating:  90%|████████▉ | 281/313 [00:45<00:05,  6.05it/s]Evaluating:  90%|█████████ | 282/313 [00:45<00:05,  6.06it/s]Evaluating:  90%|█████████ | 283/313 [00:45<00:04,  6.06it/s]Evaluating:  91%|█████████ | 284/313 [00:45<00:04,  6.05it/s]Evaluating:  91%|█████████ | 285/313 [00:45<00:04,  5.96it/s]Evaluating:  91%|█████████▏| 286/313 [00:46<00:04,  5.93it/s]Evaluating:  92%|█████████▏| 287/313 [00:46<00:04,  5.96it/s]Evaluating:  92%|█████████▏| 288/313 [00:46<00:04,  5.99it/s]Evaluating:  92%|█████████▏| 289/313 [00:46<00:03,  6.00it/s]Evaluating:  93%|█████████▎| 290/313 [00:46<00:03,  6.01it/s]Evaluating:  93%|█████████▎| 291/313 [00:46<00:03,  6.00it/s]Evaluating:  93%|█████████▎| 292/313 [00:47<00:03,  5.42it/s]Evaluating:  94%|█████████▎| 293/313 [00:47<00:03,  5.59it/s]Evaluating:  94%|█████████▍| 294/313 [00:47<00:03,  5.73it/s]Evaluating:  94%|█████████▍| 295/313 [00:47<00:03,  5.82it/s]Evaluating:  95%|█████████▍| 296/313 [00:47<00:02,  5.89it/s]Evaluating:  95%|█████████▍| 297/313 [00:48<00:02,  5.93it/s]Evaluating:  95%|█████████▌| 298/313 [00:48<00:02,  5.94it/s]Evaluating:  96%|█████████▌| 299/313 [00:48<00:02,  5.94it/s]Evaluating:  96%|█████████▌| 300/313 [00:48<00:02,  5.96it/s]Evaluating:  96%|█████████▌| 301/313 [00:48<00:02,  5.95it/s]Evaluating:  96%|█████████▋| 302/313 [00:48<00:01,  5.83it/s]Evaluating:  97%|█████████▋| 303/313 [00:49<00:01,  5.76it/s]Evaluating:  97%|█████████▋| 304/313 [00:49<00:01,  5.75it/s]Evaluating:  97%|█████████▋| 305/313 [00:49<00:01,  5.77it/s]Evaluating:  98%|█████████▊| 306/313 [00:49<00:01,  5.81it/s]Evaluating:  98%|█████████▊| 307/313 [00:49<00:01,  5.86it/s]Evaluating:  98%|█████████▊| 308/313 [00:49<00:00,  5.87it/s]Evaluating:  99%|█████████▊| 309/313 [00:50<00:00,  5.79it/s]Evaluating:  99%|█████████▉| 310/313 [00:50<00:00,  5.76it/s]Evaluating:  99%|█████████▉| 311/313 [00:50<00:00,  5.77it/s]Evaluating: 100%|█████████▉| 312/313 [00:50<00:00,  5.80it/s]Evaluating: 100%|██████████| 313/313 [00:50<00:00,  6.17it/s]
10/14/2021 16:21:49 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/14/2021 16:21:49 - INFO - __main__ -     f1 = 0.5782348990916482
10/14/2021 16:21:49 - INFO - __main__ -     loss = 1.7934469123617909
10/14/2021 16:21:49 - INFO - __main__ -     precision = 0.5499529380126396
10/14/2021 16:21:49 - INFO - __main__ -     recall = 0.609583426484835
10/14/2021 16:21:50 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/14/2021 16:21:51 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/14/2021 16:21:51 - INFO - __main__ -     Num examples = 10004
10/14/2021 16:21:51 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.04it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.04it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  6.04it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:51,  6.04it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:50,  6.04it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:50,  6.05it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.05it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.06it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:50,  6.05it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:50,  6.06it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:49,  6.05it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:49,  6.05it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:49,  6.05it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:49,  6.04it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:48,  6.04it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.04it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.04it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  6.04it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  6.04it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:47,  6.05it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:47,  6.04it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:47,  6.05it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  6.05it/s]Evaluating:   9%|▊         | 27/313 [00:04<01:02,  4.54it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:58,  4.91it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:54,  5.20it/s]Evaluating:  10%|▉         | 30/313 [00:05<00:52,  5.42it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:50,  5.59it/s]Evaluating:  10%|█         | 32/313 [00:05<00:49,  5.72it/s]Evaluating:  11%|█         | 33/313 [00:05<00:48,  5.81it/s]Evaluating:  11%|█         | 34/313 [00:05<00:47,  5.88it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  5.93it/s]Evaluating:  12%|█▏        | 36/313 [00:06<00:46,  5.96it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:46,  5.98it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  5.99it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.00it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  6.01it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.01it/s]Evaluating:  13%|█▎        | 42/313 [00:07<00:45,  6.02it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:44,  6.02it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  6.02it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▌        | 48/313 [00:08<00:44,  6.02it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:43,  6.02it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:43,  6.02it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  6.03it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  6.02it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  6.02it/s]Evaluating:  17%|█▋        | 54/313 [00:09<00:43,  6.02it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:42,  6.01it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:42,  6.01it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  6.02it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  6.03it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  6.04it/s]Evaluating:  19%|█▉        | 60/313 [00:10<00:41,  6.05it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:41,  6.07it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:41,  6.08it/s]Evaluating:  20%|██        | 63/313 [00:10<00:41,  6.08it/s]Evaluating:  20%|██        | 64/313 [00:10<00:40,  6.09it/s]Evaluating:  21%|██        | 65/313 [00:10<00:40,  6.09it/s]Evaluating:  21%|██        | 66/313 [00:11<00:40,  6.06it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:40,  6.04it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:40,  6.02it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:40,  6.02it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  6.01it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  6.00it/s]Evaluating:  23%|██▎       | 72/313 [00:12<00:40,  6.00it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  6.00it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:39,  6.00it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:39,  6.02it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:39,  6.03it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  6.03it/s]Evaluating:  25%|██▍       | 78/313 [00:13<00:39,  6.02it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:38,  6.03it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:38,  6.04it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:38,  6.04it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:38,  6.03it/s]Evaluating:  27%|██▋       | 83/313 [00:13<00:38,  6.01it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:38,  6.02it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:37,  6.03it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:37,  6.02it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:37,  6.01it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:37,  6.00it/s]Evaluating:  28%|██▊       | 89/313 [00:14<00:37,  5.99it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:37,  5.98it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:37,  5.98it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:36,  5.98it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:36,  5.97it/s]Evaluating:  30%|███       | 94/313 [00:15<00:36,  5.96it/s]Evaluating:  30%|███       | 95/313 [00:15<00:36,  5.99it/s]Evaluating:  31%|███       | 96/313 [00:16<00:36,  5.99it/s]Evaluating:  31%|███       | 97/313 [00:16<00:35,  6.01it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:35,  6.01it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:35,  5.99it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:35,  5.98it/s]Evaluating:  32%|███▏      | 101/313 [00:16<00:35,  5.99it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:35,  5.98it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:35,  5.97it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:34,  5.98it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:34,  5.96it/s]Evaluating:  34%|███▍      | 106/313 [00:17<00:34,  5.97it/s]Evaluating:  34%|███▍      | 107/313 [00:17<00:34,  5.98it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:34,  5.98it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:34,  5.97it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:34,  5.96it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:33,  5.98it/s]Evaluating:  36%|███▌      | 112/313 [00:18<00:33,  5.99it/s]Evaluating:  36%|███▌      | 113/313 [00:18<00:33,  5.99it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:33,  5.97it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:33,  5.96it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:34,  5.65it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:33,  5.87it/s]Evaluating:  38%|███▊      | 118/313 [00:19<00:32,  6.05it/s]Evaluating:  38%|███▊      | 119/313 [00:19<00:31,  6.18it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:30,  6.23it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:30,  6.27it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:30,  6.31it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:29,  6.34it/s]Evaluating:  40%|███▉      | 124/313 [00:20<00:29,  6.33it/s]Evaluating:  40%|███▉      | 125/313 [00:20<00:29,  6.31it/s]Evaluating:  40%|████      | 126/313 [00:21<00:29,  6.28it/s]Evaluating:  41%|████      | 127/313 [00:21<00:29,  6.24it/s]Evaluating:  41%|████      | 128/313 [00:21<00:29,  6.19it/s]Evaluating:  41%|████      | 129/313 [00:21<00:29,  6.14it/s]Evaluating:  42%|████▏     | 130/313 [00:21<00:29,  6.10it/s]Evaluating:  42%|████▏     | 131/313 [00:21<00:29,  6.10it/s]Evaluating:  42%|████▏     | 132/313 [00:22<00:29,  6.10it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:29,  6.12it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:29,  6.15it/s]Evaluating:  43%|████▎     | 135/313 [00:22<00:28,  6.18it/s]Evaluating:  43%|████▎     | 136/313 [00:22<00:28,  6.20it/s]Evaluating:  44%|████▍     | 137/313 [00:22<00:28,  6.23it/s]Evaluating:  44%|████▍     | 138/313 [00:22<00:27,  6.25it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:27,  6.26it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:27,  6.28it/s]Evaluating:  45%|████▌     | 141/313 [00:23<00:27,  6.30it/s]Evaluating:  45%|████▌     | 142/313 [00:23<00:27,  6.31it/s]Evaluating:  46%|████▌     | 143/313 [00:23<00:26,  6.31it/s]Evaluating:  46%|████▌     | 144/313 [00:23<00:26,  6.31it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:26,  6.31it/s]Evaluating:  47%|████▋     | 146/313 [00:24<00:26,  6.30it/s]Evaluating:  47%|████▋     | 147/313 [00:24<00:27,  6.13it/s]Evaluating:  47%|████▋     | 148/313 [00:24<00:26,  6.19it/s]Evaluating:  48%|████▊     | 149/313 [00:24<00:26,  6.23it/s]Evaluating:  48%|████▊     | 150/313 [00:24<00:26,  6.26it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:25,  6.28it/s]Evaluating:  49%|████▊     | 152/313 [00:25<00:25,  6.27it/s]Evaluating:  49%|████▉     | 153/313 [00:25<00:25,  6.25it/s]Evaluating:  49%|████▉     | 154/313 [00:25<00:25,  6.23it/s]Evaluating:  50%|████▉     | 155/313 [00:25<00:25,  6.20it/s]Evaluating:  50%|████▉     | 156/313 [00:25<00:25,  6.19it/s]Evaluating:  50%|█████     | 157/313 [00:26<00:25,  6.19it/s]Evaluating:  50%|█████     | 158/313 [00:26<00:25,  6.20it/s]Evaluating:  51%|█████     | 159/313 [00:26<00:24,  6.18it/s]Evaluating:  51%|█████     | 160/313 [00:26<00:24,  6.15it/s]Evaluating:  51%|█████▏    | 161/313 [00:26<00:24,  6.09it/s]Evaluating:  52%|█████▏    | 162/313 [00:26<00:24,  6.05it/s]Evaluating:  52%|█████▏    | 163/313 [00:27<00:24,  6.03it/s]Evaluating:  52%|█████▏    | 164/313 [00:27<00:24,  6.02it/s]Evaluating:  53%|█████▎    | 165/313 [00:27<00:24,  6.01it/s]Evaluating:  53%|█████▎    | 166/313 [00:27<00:24,  5.98it/s]Evaluating:  53%|█████▎    | 167/313 [00:27<00:24,  5.95it/s]Evaluating:  54%|█████▎    | 168/313 [00:27<00:24,  5.93it/s]Evaluating:  54%|█████▍    | 169/313 [00:28<00:24,  5.93it/s]Evaluating:  54%|█████▍    | 170/313 [00:28<00:24,  5.92it/s]Evaluating:  55%|█████▍    | 171/313 [00:28<00:23,  5.92it/s]Evaluating:  55%|█████▍    | 172/313 [00:28<00:23,  5.93it/s]Evaluating:  55%|█████▌    | 173/313 [00:28<00:23,  5.94it/s]Evaluating:  56%|█████▌    | 174/313 [00:28<00:23,  5.95it/s]Evaluating:  56%|█████▌    | 175/313 [00:29<00:23,  5.94it/s]Evaluating:  56%|█████▌    | 176/313 [00:29<00:23,  5.95it/s]Evaluating:  57%|█████▋    | 177/313 [00:29<00:27,  4.91it/s]Evaluating:  57%|█████▋    | 178/313 [00:29<00:26,  5.18it/s]Evaluating:  57%|█████▋    | 179/313 [00:29<00:24,  5.42it/s]Evaluating:  58%|█████▊    | 180/313 [00:30<00:23,  5.60it/s]Evaluating:  58%|█████▊    | 181/313 [00:30<00:23,  5.73it/s]Evaluating:  58%|█████▊    | 182/313 [00:30<00:22,  5.81it/s]Evaluating:  58%|█████▊    | 183/313 [00:30<00:22,  5.88it/s]Evaluating:  59%|█████▉    | 184/313 [00:30<00:21,  5.94it/s]Evaluating:  59%|█████▉    | 185/313 [00:30<00:21,  6.00it/s]Evaluating:  59%|█████▉    | 186/313 [00:30<00:20,  6.05it/s]Evaluating:  60%|█████▉    | 187/313 [00:31<00:20,  6.09it/s]Evaluating:  60%|██████    | 188/313 [00:31<00:20,  6.11it/s]Evaluating:  60%|██████    | 189/313 [00:31<00:20,  6.11it/s]Evaluating:  61%|██████    | 190/313 [00:31<00:20,  6.11it/s]Evaluating:  61%|██████    | 191/313 [00:31<00:20,  6.09it/s]Evaluating:  61%|██████▏   | 192/313 [00:31<00:19,  6.08it/s]Evaluating:  62%|██████▏   | 193/313 [00:32<00:19,  6.07it/s]Evaluating:  62%|██████▏   | 194/313 [00:32<00:19,  6.06it/s]Evaluating:  62%|██████▏   | 195/313 [00:32<00:19,  6.05it/s]Evaluating:  63%|██████▎   | 196/313 [00:32<00:19,  6.04it/s]Evaluating:  63%|██████▎   | 197/313 [00:32<00:19,  6.06it/s]Evaluating:  63%|██████▎   | 198/313 [00:32<00:18,  6.09it/s]Evaluating:  64%|██████▎   | 199/313 [00:33<00:18,  6.13it/s]Evaluating:  64%|██████▍   | 200/313 [00:33<00:18,  6.18it/s]Evaluating:  64%|██████▍   | 201/313 [00:33<00:18,  6.22it/s]Evaluating:  65%|██████▍   | 202/313 [00:33<00:17,  6.23it/s]Evaluating:  65%|██████▍   | 203/313 [00:33<00:17,  6.22it/s]Evaluating:  65%|██████▌   | 204/313 [00:33<00:17,  6.22it/s]Evaluating:  65%|██████▌   | 205/313 [00:34<00:17,  6.21it/s]Evaluating:  66%|██████▌   | 206/313 [00:34<00:17,  6.19it/s]Evaluating:  66%|██████▌   | 207/313 [00:34<00:23,  4.59it/s]Evaluating:  66%|██████▋   | 208/313 [00:34<00:21,  4.91it/s]Evaluating:  67%|██████▋   | 209/313 [00:34<00:20,  5.20it/s]Evaluating:  67%|██████▋   | 210/313 [00:35<00:18,  5.44it/s]Evaluating:  67%|██████▋   | 211/313 [00:35<00:18,  5.61it/s]Evaluating:  68%|██████▊   | 212/313 [00:35<00:17,  5.73it/s]Evaluating:  68%|██████▊   | 213/313 [00:35<00:17,  5.84it/s]Evaluating:  68%|██████▊   | 214/313 [00:35<00:16,  5.92it/s]Evaluating:  69%|██████▊   | 215/313 [00:35<00:16,  5.87it/s]Evaluating:  69%|██████▉   | 216/313 [00:36<00:16,  5.94it/s]Evaluating:  69%|██████▉   | 217/313 [00:36<00:15,  6.03it/s]Evaluating:  70%|██████▉   | 218/313 [00:36<00:15,  6.07it/s]Evaluating:  70%|██████▉   | 219/313 [00:36<00:15,  6.08it/s]Evaluating:  70%|███████   | 220/313 [00:36<00:15,  6.05it/s]Evaluating:  71%|███████   | 221/313 [00:36<00:15,  5.98it/s]Evaluating:  71%|███████   | 222/313 [00:37<00:15,  5.93it/s]Evaluating:  71%|███████   | 223/313 [00:37<00:15,  5.90it/s]Evaluating:  72%|███████▏  | 224/313 [00:37<00:15,  5.87it/s]Evaluating:  72%|███████▏  | 225/313 [00:37<00:14,  5.87it/s]Evaluating:  72%|███████▏  | 226/313 [00:37<00:14,  5.85it/s]Evaluating:  73%|███████▎  | 227/313 [00:37<00:14,  5.84it/s]Evaluating:  73%|███████▎  | 228/313 [00:38<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 229/313 [00:38<00:14,  5.85it/s]Evaluating:  73%|███████▎  | 230/313 [00:38<00:14,  5.84it/s]Evaluating:  74%|███████▍  | 231/313 [00:38<00:14,  5.84it/s]Evaluating:  74%|███████▍  | 232/313 [00:38<00:13,  5.84it/s]Evaluating:  74%|███████▍  | 233/313 [00:38<00:13,  5.84it/s]Evaluating:  75%|███████▍  | 234/313 [00:39<00:13,  5.83it/s]Evaluating:  75%|███████▌  | 235/313 [00:39<00:13,  5.83it/s]Evaluating:  75%|███████▌  | 236/313 [00:39<00:13,  5.85it/s]Evaluating:  76%|███████▌  | 237/313 [00:39<00:12,  5.87it/s]Evaluating:  76%|███████▌  | 238/313 [00:39<00:12,  5.86it/s]Evaluating:  76%|███████▋  | 239/313 [00:40<00:12,  5.84it/s]Evaluating:  77%|███████▋  | 240/313 [00:40<00:12,  5.83it/s]Evaluating:  77%|███████▋  | 241/313 [00:40<00:12,  5.84it/s]Evaluating:  77%|███████▋  | 242/313 [00:40<00:12,  5.84it/s]Evaluating:  78%|███████▊  | 243/313 [00:40<00:11,  5.85it/s]Evaluating:  78%|███████▊  | 244/313 [00:40<00:11,  5.85it/s]Evaluating:  78%|███████▊  | 245/313 [00:41<00:11,  5.84it/s]Evaluating:  79%|███████▊  | 246/313 [00:41<00:11,  5.83it/s]Evaluating:  79%|███████▉  | 247/313 [00:41<00:11,  5.85it/s]Evaluating:  79%|███████▉  | 248/313 [00:41<00:11,  5.85it/s]Evaluating:  80%|███████▉  | 249/313 [00:41<00:10,  5.88it/s]Evaluating:  80%|███████▉  | 250/313 [00:41<00:10,  5.91it/s]Evaluating:  80%|████████  | 251/313 [00:42<00:10,  5.93it/s]Evaluating:  81%|████████  | 252/313 [00:42<00:10,  5.96it/s]Evaluating:  81%|████████  | 253/313 [00:42<00:10,  5.99it/s]Evaluating:  81%|████████  | 254/313 [00:42<00:09,  6.06it/s]Evaluating:  81%|████████▏ | 255/313 [00:42<00:09,  6.12it/s]Evaluating:  82%|████████▏ | 256/313 [00:42<00:09,  6.06it/s]Evaluating:  82%|████████▏ | 257/313 [00:43<00:09,  6.04it/s]Evaluating:  82%|████████▏ | 258/313 [00:43<00:09,  6.04it/s]Evaluating:  83%|████████▎ | 259/313 [00:43<00:08,  6.05it/s]Evaluating:  83%|████████▎ | 260/313 [00:43<00:08,  6.05it/s]Evaluating:  83%|████████▎ | 261/313 [00:43<00:08,  5.98it/s]Evaluating:  84%|████████▎ | 262/313 [00:43<00:08,  5.92it/s]Evaluating:  84%|████████▍ | 263/313 [00:44<00:08,  5.89it/s]Evaluating:  84%|████████▍ | 264/313 [00:44<00:08,  5.91it/s]Evaluating:  85%|████████▍ | 265/313 [00:44<00:08,  5.53it/s]Evaluating:  85%|████████▍ | 266/313 [00:44<00:08,  5.74it/s]Evaluating:  85%|████████▌ | 267/313 [00:44<00:07,  5.91it/s]Evaluating:  86%|████████▌ | 268/313 [00:44<00:07,  6.01it/s]Evaluating:  86%|████████▌ | 269/313 [00:45<00:07,  6.09it/s]Evaluating:  86%|████████▋ | 270/313 [00:45<00:07,  6.13it/s]Evaluating:  87%|████████▋ | 271/313 [00:45<00:06,  6.16it/s]Evaluating:  87%|████████▋ | 272/313 [00:45<00:06,  6.10it/s]Evaluating:  87%|████████▋ | 273/313 [00:45<00:06,  6.03it/s]Evaluating:  88%|████████▊ | 274/313 [00:45<00:06,  5.98it/s]Evaluating:  88%|████████▊ | 275/313 [00:46<00:06,  5.96it/s]Evaluating:  88%|████████▊ | 276/313 [00:46<00:06,  5.97it/s]Evaluating:  88%|████████▊ | 277/313 [00:46<00:06,  5.95it/s]Evaluating:  89%|████████▉ | 278/313 [00:46<00:05,  5.92it/s]Evaluating:  89%|████████▉ | 279/313 [00:46<00:05,  5.90it/s]Evaluating:  89%|████████▉ | 280/313 [00:46<00:05,  5.89it/s]Evaluating:  90%|████████▉ | 281/313 [00:47<00:05,  5.89it/s]Evaluating:  90%|█████████ | 282/313 [00:47<00:05,  5.88it/s]Evaluating:  90%|█████████ | 283/313 [00:47<00:05,  5.87it/s]Evaluating:  91%|█████████ | 284/313 [00:47<00:04,  5.84it/s]Evaluating:  91%|█████████ | 285/313 [00:47<00:04,  5.82it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:04,  5.80it/s]Evaluating:  92%|█████████▏| 287/313 [00:48<00:04,  5.79it/s]Evaluating:  92%|█████████▏| 288/313 [00:48<00:04,  5.78it/s]Evaluating:  92%|█████████▏| 289/313 [00:48<00:04,  5.78it/s]Evaluating:  93%|█████████▎| 290/313 [00:48<00:03,  5.77it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:03,  5.77it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  5.79it/s]Evaluating:  94%|█████████▎| 293/313 [00:49<00:03,  5.80it/s]Evaluating:  94%|█████████▍| 294/313 [00:49<00:03,  5.74it/s]Evaluating:  94%|█████████▍| 295/313 [00:49<00:03,  5.71it/s]Evaluating:  95%|█████████▍| 296/313 [00:49<00:02,  5.69it/s]Evaluating:  95%|█████████▍| 297/313 [00:49<00:02,  5.65it/s]Evaluating:  95%|█████████▌| 298/313 [00:50<00:02,  5.62it/s]Evaluating:  96%|█████████▌| 299/313 [00:50<00:02,  5.63it/s]Evaluating:  96%|█████████▌| 300/313 [00:50<00:02,  5.66it/s]Evaluating:  96%|█████████▌| 301/313 [00:50<00:02,  5.71it/s]Evaluating:  96%|█████████▋| 302/313 [00:50<00:01,  5.75it/s]Evaluating:  97%|█████████▋| 303/313 [00:50<00:01,  5.80it/s]Evaluating:  97%|█████████▋| 304/313 [00:51<00:01,  5.73it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  5.70it/s]Evaluating:  98%|█████████▊| 306/313 [00:51<00:01,  5.69it/s]Evaluating:  98%|█████████▊| 307/313 [00:51<00:01,  5.71it/s]Evaluating:  98%|█████████▊| 308/313 [00:51<00:00,  5.72it/s]Evaluating:  99%|█████████▊| 309/313 [00:51<00:00,  5.76it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.78it/s]Evaluating:  99%|█████████▉| 311/313 [00:52<00:00,  5.71it/s]Evaluating: 100%|█████████▉| 312/313 [00:52<00:00,  5.66it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  6.22it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  5.95it/s]
10/14/2021 16:22:45 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/14/2021 16:22:45 - INFO - __main__ -     f1 = 0.7230495142693871
10/14/2021 16:22:45 - INFO - __main__ -     loss = 1.1565013021325912
10/14/2021 16:22:45 - INFO - __main__ -     precision = 0.6855558388577256
10/14/2021 16:22:45 - INFO - __main__ -     recall = 0.7648815873693194
10/14/2021 16:22:45 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:22:57 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:22:57 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:23:15 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 16:23:17 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:23:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 16:23:17 - INFO - __main__ -   Seed = 2
10/14/2021 16:23:17 - INFO - root -   save model
10/14/2021 16:23:17 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:23:17 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:23:31 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:23:31 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 16:23:31 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
10/14/2021 16:23:31 - INFO - root -   Trying to decide if add adapter
10/14/2021 16:23:31 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 16:23:31 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/14/2021 16:23:31 - INFO - __main__ -   Language = en
10/14/2021 16:23:31 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 16:23:32 - INFO - __main__ -   Language = ru
10/14/2021 16:23:32 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/14/2021 16:23:37 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/14/2021 16:23:37 - INFO - __main__ -   ***** Running evaluation  in be *****
10/14/2021 16:23:37 - INFO - __main__ -     Num examples = 1001
10/14/2021 16:23:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.75it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.15it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.38it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.52it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.60it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.64it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.68it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.70it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.71it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.71it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.72it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.72it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.72it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.72it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.71it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.71it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.72it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.72it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.72it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.71it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.71it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.71it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.70it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.70it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.70it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.70it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.71it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.71it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.70it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.69it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.68it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.81it/s]
10/14/2021 16:23:42 - INFO - __main__ -   ***** Evaluation result  in be *****
10/14/2021 16:23:42 - INFO - __main__ -     f1 = 0.6702912621359223
10/14/2021 16:23:42 - INFO - __main__ -     loss = 1.2751046363264322
10/14/2021 16:23:42 - INFO - __main__ -     precision = 0.6354933726067746
10/14/2021 16:23:42 - INFO - __main__ -     recall = 0.7091207888249794
10/14/2021 16:23:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/14/2021 16:23:43 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/14/2021 16:23:43 - INFO - __main__ -     Num examples = 10001
10/14/2021 16:23:43 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.76it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.73it/s]Evaluating:   1%|          | 3/313 [00:00<00:46,  6.73it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:45,  6.72it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.72it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.72it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.71it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.71it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.70it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.71it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:44,  6.71it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.71it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.70it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.70it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.70it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.69it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.70it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:44,  6.68it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.69it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.70it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.69it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.69it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.70it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.69it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:43,  6.69it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:42,  6.69it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.68it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.68it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.67it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.66it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.66it/s]Evaluating:  10%|█         | 32/313 [00:04<00:42,  6.65it/s]Evaluating:  11%|█         | 33/313 [00:04<00:42,  6.65it/s]Evaluating:  11%|█         | 34/313 [00:05<00:41,  6.65it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.65it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.65it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.65it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.65it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:41,  6.64it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:41,  6.64it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:41,  6.63it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:40,  6.62it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.63it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.57it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.59it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:40,  6.61it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.62it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:40,  6.62it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:41,  6.34it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:40,  6.42it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:40,  6.46it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:40,  6.48it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:39,  6.51it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:39,  6.54it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:39,  6.55it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:39,  6.56it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:38,  6.57it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:38,  6.57it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:38,  6.58it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:38,  6.58it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.57it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.57it/s]Evaluating:  20%|██        | 63/313 [00:09<00:38,  6.56it/s]Evaluating:  20%|██        | 64/313 [00:09<00:37,  6.56it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.56it/s]Evaluating:  21%|██        | 66/313 [00:09<00:37,  6.57it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:37,  6.58it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:37,  6.58it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:37,  6.57it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:37,  6.56it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:36,  6.55it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:36,  6.56it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:36,  6.55it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:36,  6.56it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:36,  6.55it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:36,  6.55it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:36,  6.51it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:36,  6.48it/s]Evaluating:  25%|██▌       | 79/313 [00:11<00:36,  6.45it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:36,  6.45it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:36,  6.43it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:35,  6.42it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:35,  6.42it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:35,  6.40it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:35,  6.35it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:35,  6.33it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:35,  6.35it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:35,  6.37it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:35,  6.39it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:34,  6.38it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:34,  6.36it/s]Evaluating:  29%|██▉       | 92/313 [00:13<00:34,  6.37it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:34,  6.35it/s]Evaluating:  30%|███       | 94/313 [00:14<00:34,  6.31it/s]Evaluating:  30%|███       | 95/313 [00:14<00:34,  6.26it/s]Evaluating:  31%|███       | 96/313 [00:14<00:34,  6.21it/s]Evaluating:  31%|███       | 97/313 [00:14<00:35,  6.16it/s]Evaluating:  31%|███▏      | 98/313 [00:14<00:35,  6.11it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:35,  6.06it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:35,  6.03it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:35,  6.01it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:35,  6.02it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:35,  6.00it/s]Evaluating:  33%|███▎      | 104/313 [00:15<00:34,  5.98it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:34,  5.97it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:34,  5.97it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:34,  5.96it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:34,  5.96it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:34,  5.95it/s]Evaluating:  35%|███▌      | 110/313 [00:16<00:34,  5.95it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:33,  5.95it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:35,  5.62it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:34,  5.79it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:33,  5.89it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:33,  5.95it/s]Evaluating:  37%|███▋      | 116/313 [00:18<00:33,  5.95it/s]Evaluating:  37%|███▋      | 117/313 [00:18<00:32,  5.96it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:32,  5.97it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:32,  6.00it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:32,  6.02it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:32,  5.99it/s]Evaluating:  39%|███▉      | 122/313 [00:19<00:31,  5.97it/s]Evaluating:  39%|███▉      | 123/313 [00:19<00:31,  5.96it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:31,  5.95it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:31,  5.95it/s]Evaluating:  40%|████      | 126/313 [00:19<00:31,  5.94it/s]Evaluating:  41%|████      | 127/313 [00:19<00:31,  5.93it/s]Evaluating:  41%|████      | 128/313 [00:20<00:31,  5.93it/s]Evaluating:  41%|████      | 129/313 [00:20<00:31,  5.92it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:30,  5.92it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:30,  5.92it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:30,  5.92it/s]Evaluating:  42%|████▏     | 133/313 [00:20<00:30,  5.92it/s]Evaluating:  43%|████▎     | 134/313 [00:21<00:30,  5.93it/s]Evaluating:  43%|████▎     | 135/313 [00:21<00:29,  5.95it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:29,  5.97it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:29,  5.98it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:29,  5.96it/s]Evaluating:  44%|████▍     | 139/313 [00:21<00:29,  5.94it/s]Evaluating:  45%|████▍     | 140/313 [00:22<00:29,  5.93it/s]Evaluating:  45%|████▌     | 141/313 [00:22<00:29,  5.92it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:29,  5.84it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:28,  5.88it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:28,  5.89it/s]Evaluating:  46%|████▋     | 145/313 [00:22<00:28,  5.89it/s]Evaluating:  47%|████▋     | 146/313 [00:23<00:28,  5.90it/s]Evaluating:  47%|████▋     | 147/313 [00:23<00:28,  5.91it/s]Evaluating:  47%|████▋     | 148/313 [00:23<00:27,  5.91it/s]Evaluating:  48%|████▊     | 149/313 [00:23<00:27,  5.91it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:27,  5.90it/s]Evaluating:  48%|████▊     | 151/313 [00:23<00:27,  5.90it/s]Evaluating:  49%|████▊     | 152/313 [00:24<00:27,  5.91it/s]Evaluating:  49%|████▉     | 153/313 [00:24<00:27,  5.92it/s]Evaluating:  49%|████▉     | 154/313 [00:24<00:26,  5.91it/s]Evaluating:  50%|████▉     | 155/313 [00:24<00:26,  5.90it/s]Evaluating:  50%|████▉     | 156/313 [00:24<00:26,  5.90it/s]Evaluating:  50%|█████     | 157/313 [00:24<00:26,  5.90it/s]Evaluating:  50%|█████     | 158/313 [00:25<00:26,  5.91it/s]Evaluating:  51%|█████     | 159/313 [00:25<00:26,  5.90it/s]Evaluating:  51%|█████     | 160/313 [00:25<00:25,  5.89it/s]Evaluating:  51%|█████▏    | 161/313 [00:25<00:25,  5.89it/s]Evaluating:  52%|█████▏    | 162/313 [00:25<00:25,  5.89it/s]Evaluating:  52%|█████▏    | 163/313 [00:25<00:25,  5.89it/s]Evaluating:  52%|█████▏    | 164/313 [00:26<00:25,  5.89it/s]Evaluating:  53%|█████▎    | 165/313 [00:26<00:25,  5.88it/s]Evaluating:  53%|█████▎    | 166/313 [00:26<00:24,  5.89it/s]Evaluating:  53%|█████▎    | 167/313 [00:26<00:24,  5.88it/s]Evaluating:  54%|█████▎    | 168/313 [00:26<00:24,  5.88it/s]Evaluating:  54%|█████▍    | 169/313 [00:26<00:24,  5.88it/s]Evaluating:  54%|█████▍    | 170/313 [00:27<00:24,  5.88it/s]Evaluating:  55%|█████▍    | 171/313 [00:27<00:25,  5.53it/s]Evaluating:  55%|█████▍    | 172/313 [00:27<00:24,  5.75it/s]Evaluating:  55%|█████▌    | 173/313 [00:27<00:23,  5.90it/s]Evaluating:  56%|█████▌    | 174/313 [00:27<00:23,  6.00it/s]Evaluating:  56%|█████▌    | 175/313 [00:27<00:22,  6.05it/s]Evaluating:  56%|█████▌    | 176/313 [00:28<00:22,  6.09it/s]Evaluating:  57%|█████▋    | 177/313 [00:28<00:22,  6.08it/s]Evaluating:  57%|█████▋    | 178/313 [00:28<00:22,  6.07it/s]Evaluating:  57%|█████▋    | 179/313 [00:28<00:22,  6.06it/s]Evaluating:  58%|█████▊    | 180/313 [00:28<00:21,  6.05it/s]Evaluating:  58%|█████▊    | 181/313 [00:28<00:21,  6.05it/s]Evaluating:  58%|█████▊    | 182/313 [00:29<00:21,  6.05it/s]Evaluating:  58%|█████▊    | 183/313 [00:29<00:21,  6.05it/s]Evaluating:  59%|█████▉    | 184/313 [00:29<00:21,  6.05it/s]Evaluating:  59%|█████▉    | 185/313 [00:29<00:21,  6.05it/s]Evaluating:  59%|█████▉    | 186/313 [00:29<00:21,  6.04it/s]Evaluating:  60%|█████▉    | 187/313 [00:29<00:20,  6.07it/s]Evaluating:  60%|██████    | 188/313 [00:30<00:20,  6.10it/s]Evaluating:  60%|██████    | 189/313 [00:30<00:20,  6.03it/s]Evaluating:  61%|██████    | 190/313 [00:30<00:20,  5.99it/s]Evaluating:  61%|██████    | 191/313 [00:30<00:20,  5.96it/s]Evaluating:  61%|██████▏   | 192/313 [00:30<00:20,  5.98it/s]Evaluating:  62%|██████▏   | 193/313 [00:30<00:20,  5.98it/s]Evaluating:  62%|██████▏   | 194/313 [00:31<00:19,  5.96it/s]Evaluating:  62%|██████▏   | 195/313 [00:31<00:19,  5.94it/s]Evaluating:  63%|██████▎   | 196/313 [00:31<00:19,  5.95it/s]Evaluating:  63%|██████▎   | 197/313 [00:31<00:19,  5.98it/s]Evaluating:  63%|██████▎   | 198/313 [00:31<00:19,  6.03it/s]Evaluating:  64%|██████▎   | 199/313 [00:31<00:18,  6.05it/s]Evaluating:  64%|██████▍   | 200/313 [00:32<00:18,  6.03it/s]Evaluating:  64%|██████▍   | 201/313 [00:32<00:18,  6.01it/s]Evaluating:  65%|██████▍   | 202/313 [00:32<00:18,  5.99it/s]Evaluating:  65%|██████▍   | 203/313 [00:32<00:18,  5.97it/s]Evaluating:  65%|██████▌   | 204/313 [00:32<00:18,  5.96it/s]Evaluating:  65%|██████▌   | 205/313 [00:32<00:18,  5.92it/s]Evaluating:  66%|██████▌   | 206/313 [00:33<00:18,  5.90it/s]Evaluating:  66%|██████▌   | 207/313 [00:33<00:18,  5.88it/s]Evaluating:  66%|██████▋   | 208/313 [00:33<00:17,  5.87it/s]Evaluating:  67%|██████▋   | 209/313 [00:33<00:17,  5.86it/s]Evaluating:  67%|██████▋   | 210/313 [00:33<00:17,  5.85it/s]Evaluating:  67%|██████▋   | 211/313 [00:34<00:17,  5.84it/s]Evaluating:  68%|██████▊   | 212/313 [00:34<00:17,  5.84it/s]Evaluating:  68%|██████▊   | 213/313 [00:34<00:17,  5.84it/s]Evaluating:  68%|██████▊   | 214/313 [00:34<00:16,  5.85it/s]Evaluating:  69%|██████▊   | 215/313 [00:34<00:16,  5.85it/s]Evaluating:  69%|██████▉   | 216/313 [00:34<00:16,  5.84it/s]Evaluating:  69%|██████▉   | 217/313 [00:35<00:16,  5.84it/s]Evaluating:  70%|██████▉   | 218/313 [00:35<00:16,  5.83it/s]Evaluating:  70%|██████▉   | 219/313 [00:35<00:16,  5.83it/s]Evaluating:  70%|███████   | 220/313 [00:35<00:15,  5.86it/s]Evaluating:  71%|███████   | 221/313 [00:35<00:15,  5.87it/s]Evaluating:  71%|███████   | 222/313 [00:35<00:15,  5.89it/s]Evaluating:  71%|███████   | 223/313 [00:36<00:15,  5.86it/s]Evaluating:  72%|███████▏  | 224/313 [00:36<00:15,  5.85it/s]Evaluating:  72%|███████▏  | 225/313 [00:36<00:15,  5.84it/s]Evaluating:  72%|███████▏  | 226/313 [00:36<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 227/313 [00:36<00:14,  5.82it/s]Evaluating:  73%|███████▎  | 228/313 [00:36<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 229/313 [00:37<00:14,  5.83it/s]Evaluating:  73%|███████▎  | 230/313 [00:37<00:14,  5.83it/s]Evaluating:  74%|███████▍  | 231/313 [00:37<00:14,  5.83it/s]Evaluating:  74%|███████▍  | 232/313 [00:37<00:13,  5.83it/s]Evaluating:  74%|███████▍  | 233/313 [00:37<00:13,  5.83it/s]Evaluating:  75%|███████▍  | 234/313 [00:37<00:13,  5.85it/s]Evaluating:  75%|███████▌  | 235/313 [00:38<00:13,  5.87it/s]Evaluating:  75%|███████▌  | 236/313 [00:38<00:13,  5.90it/s]Evaluating:  76%|███████▌  | 237/313 [00:38<00:12,  5.93it/s]Evaluating:  76%|███████▌  | 238/313 [00:38<00:12,  5.96it/s]Evaluating:  76%|███████▋  | 239/313 [00:38<00:12,  5.97it/s]Evaluating:  77%|███████▋  | 240/313 [00:38<00:12,  5.99it/s]Evaluating:  77%|███████▋  | 241/313 [00:39<00:12,  5.93it/s]Evaluating:  77%|███████▋  | 242/313 [00:39<00:12,  5.90it/s]Evaluating:  78%|███████▊  | 243/313 [00:39<00:11,  5.89it/s]Evaluating:  78%|███████▊  | 244/313 [00:39<00:11,  5.92it/s]Evaluating:  78%|███████▊  | 245/313 [00:39<00:11,  5.96it/s]Evaluating:  79%|███████▊  | 246/313 [00:39<00:11,  5.99it/s]Evaluating:  79%|███████▉  | 247/313 [00:40<00:11,  5.99it/s]Evaluating:  79%|███████▉  | 248/313 [00:40<00:10,  5.99it/s]Evaluating:  80%|███████▉  | 249/313 [00:40<00:10,  5.99it/s]Evaluating:  80%|███████▉  | 250/313 [00:40<00:10,  6.00it/s]Evaluating:  80%|████████  | 251/313 [00:40<00:10,  5.77it/s]Evaluating:  81%|████████  | 252/313 [00:40<00:10,  5.79it/s]Evaluating:  81%|████████  | 253/313 [00:41<00:10,  5.80it/s]Evaluating:  81%|████████  | 254/313 [00:41<00:10,  5.81it/s]Evaluating:  81%|████████▏ | 255/313 [00:41<00:09,  5.84it/s]Evaluating:  82%|████████▏ | 256/313 [00:41<00:09,  5.86it/s]Evaluating:  82%|████████▏ | 257/313 [00:41<00:09,  5.87it/s]Evaluating:  82%|████████▏ | 258/313 [00:42<00:09,  5.85it/s]Evaluating:  83%|████████▎ | 259/313 [00:42<00:09,  5.75it/s]Evaluating:  83%|████████▎ | 260/313 [00:42<00:09,  5.71it/s]Evaluating:  83%|████████▎ | 261/313 [00:42<00:09,  5.72it/s]Evaluating:  84%|████████▎ | 262/313 [00:42<00:08,  5.76it/s]Evaluating:  84%|████████▍ | 263/313 [00:42<00:08,  5.81it/s]Evaluating:  84%|████████▍ | 264/313 [00:43<00:08,  5.88it/s]Evaluating:  85%|████████▍ | 265/313 [00:43<00:08,  5.88it/s]Evaluating:  85%|████████▍ | 266/313 [00:43<00:07,  5.88it/s]Evaluating:  85%|████████▌ | 267/313 [00:43<00:07,  5.90it/s]Evaluating:  86%|████████▌ | 268/313 [00:43<00:07,  5.95it/s]Evaluating:  86%|████████▌ | 269/313 [00:43<00:07,  5.97it/s]Evaluating:  86%|████████▋ | 270/313 [00:44<00:07,  5.97it/s]Evaluating:  87%|████████▋ | 271/313 [00:44<00:07,  6.00it/s]Evaluating:  87%|████████▋ | 272/313 [00:44<00:06,  6.02it/s]Evaluating:  87%|████████▋ | 273/313 [00:44<00:06,  6.03it/s]Evaluating:  88%|████████▊ | 274/313 [00:44<00:06,  6.04it/s]Evaluating:  88%|████████▊ | 275/313 [00:44<00:06,  6.05it/s]Evaluating:  88%|████████▊ | 276/313 [00:45<00:06,  6.06it/s]Evaluating:  88%|████████▊ | 277/313 [00:45<00:05,  6.07it/s]Evaluating:  89%|████████▉ | 278/313 [00:45<00:05,  6.08it/s]Evaluating:  89%|████████▉ | 279/313 [00:45<00:05,  6.06it/s]Evaluating:  89%|████████▉ | 280/313 [00:45<00:06,  5.31it/s]Evaluating:  90%|████████▉ | 281/313 [00:46<00:06,  4.66it/s]Evaluating:  90%|█████████ | 282/313 [00:46<00:07,  4.37it/s]Evaluating:  90%|█████████ | 283/313 [00:46<00:06,  4.41it/s]Evaluating:  91%|█████████ | 284/313 [00:46<00:06,  4.54it/s]Evaluating:  91%|█████████ | 285/313 [00:46<00:06,  4.63it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:05,  4.78it/s]Evaluating:  92%|█████████▏| 287/313 [00:47<00:05,  4.93it/s]Evaluating:  92%|█████████▏| 288/313 [00:47<00:04,  5.07it/s]Evaluating:  92%|█████████▏| 289/313 [00:47<00:04,  5.19it/s]Evaluating:  93%|█████████▎| 290/313 [00:47<00:04,  5.27it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:04,  5.35it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  5.41it/s]Evaluating:  94%|█████████▎| 293/313 [00:48<00:03,  5.47it/s]Evaluating:  94%|█████████▍| 294/313 [00:48<00:03,  5.51it/s]Evaluating:  94%|█████████▍| 295/313 [00:48<00:03,  5.53it/s]Evaluating:  95%|█████████▍| 296/313 [00:48<00:03,  5.54it/s]Evaluating:  95%|█████████▍| 297/313 [00:49<00:02,  5.56it/s]Evaluating:  95%|█████████▌| 298/313 [00:49<00:02,  5.61it/s]Evaluating:  96%|█████████▌| 299/313 [00:49<00:02,  5.65it/s]Evaluating:  96%|█████████▌| 300/313 [00:49<00:02,  5.70it/s]Evaluating:  96%|█████████▌| 301/313 [00:49<00:02,  5.75it/s]Evaluating:  96%|█████████▋| 302/313 [00:49<00:01,  5.80it/s]Evaluating:  97%|█████████▋| 303/313 [00:50<00:01,  5.86it/s]Evaluating:  97%|█████████▋| 304/313 [00:50<00:01,  5.91it/s]Evaluating:  97%|█████████▋| 305/313 [00:50<00:01,  5.93it/s]Evaluating:  98%|█████████▊| 306/313 [00:50<00:01,  5.95it/s]Evaluating:  98%|█████████▊| 307/313 [00:50<00:01,  5.96it/s]Evaluating:  98%|█████████▊| 308/313 [00:50<00:00,  5.89it/s]Evaluating:  99%|█████████▊| 309/313 [00:51<00:00,  5.80it/s]Evaluating:  99%|█████████▉| 310/313 [00:51<00:00,  5.79it/s]Evaluating:  99%|█████████▉| 311/313 [00:51<00:00,  5.76it/s]Evaluating: 100%|█████████▉| 312/313 [00:51<00:00,  5.78it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.61it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.04it/s]
10/14/2021 16:24:36 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/14/2021 16:24:36 - INFO - __main__ -     f1 = 0.593852946170827
10/14/2021 16:24:36 - INFO - __main__ -     loss = 1.7632446290966801
10/14/2021 16:24:36 - INFO - __main__ -     precision = 0.5455509796580557
10/14/2021 16:24:36 - INFO - __main__ -     recall = 0.6515388628064684
10/14/2021 16:24:36 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/14/2021 16:24:37 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/14/2021 16:24:37 - INFO - __main__ -     Num examples = 10004
10/14/2021 16:24:37 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.04it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  6.05it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:51,  6.05it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:50,  6.05it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:50,  6.05it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.06it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.05it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:50,  6.05it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:50,  6.06it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:49,  6.05it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:49,  6.05it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:49,  6.05it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  6.05it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:48,  6.04it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:48,  6.04it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.04it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:47,  6.05it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:47,  6.05it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:47,  6.05it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  6.05it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  6.04it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  6.04it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:47,  6.04it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:46,  6.04it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:46,  6.04it/s]Evaluating:  10%|█         | 32/313 [00:05<00:46,  6.04it/s]Evaluating:  11%|█         | 33/313 [00:05<00:46,  6.04it/s]Evaluating:  11%|█         | 34/313 [00:05<00:46,  6.04it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  6.04it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:45,  6.04it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:45,  6.04it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  6.04it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.04it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  6.04it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.03it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:44,  6.03it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:44,  6.03it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  6.03it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:44,  6.03it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  6.03it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  6.03it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:43,  6.03it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:43,  6.03it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:43,  6.04it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  6.02it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  6.02it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  6.02it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:43,  6.01it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:42,  6.01it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:42,  6.01it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  6.01it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  6.01it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  6.02it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:42,  6.02it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:41,  6.03it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:41,  6.04it/s]Evaluating:  20%|██        | 63/313 [00:10<00:41,  6.03it/s]Evaluating:  20%|██        | 64/313 [00:10<00:41,  6.02it/s]Evaluating:  21%|██        | 65/313 [00:10<00:41,  6.02it/s]Evaluating:  21%|██        | 66/313 [00:10<00:41,  6.01it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:40,  6.00it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:40,  6.00it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:40,  6.00it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  6.01it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  6.01it/s]Evaluating:  23%|██▎       | 72/313 [00:11<00:40,  6.02it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:39,  6.01it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:39,  6.00it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:39,  6.00it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:39,  6.00it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  6.00it/s]Evaluating:  25%|██▍       | 78/313 [00:12<00:39,  6.00it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.99it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:38,  5.99it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:38,  5.99it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:38,  5.98it/s]Evaluating:  27%|██▋       | 83/313 [00:13<00:38,  5.99it/s]Evaluating:  27%|██▋       | 84/313 [00:13<00:38,  5.98it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:38,  5.98it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:37,  5.98it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:37,  6.00it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:37,  6.01it/s]Evaluating:  28%|██▊       | 89/313 [00:14<00:37,  6.00it/s]Evaluating:  29%|██▉       | 90/313 [00:14<00:37,  6.00it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:37,  5.99it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:36,  5.98it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:36,  5.97it/s]Evaluating:  30%|███       | 94/313 [00:15<00:36,  5.99it/s]Evaluating:  30%|███       | 95/313 [00:15<00:36,  5.98it/s]Evaluating:  31%|███       | 96/313 [00:15<00:36,  5.98it/s]Evaluating:  31%|███       | 97/313 [00:16<00:36,  5.98it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:35,  5.97it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:35,  5.98it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:35,  5.97it/s]Evaluating:  32%|███▏      | 101/313 [00:16<00:35,  5.97it/s]Evaluating:  33%|███▎      | 102/313 [00:16<00:35,  5.96it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:35,  5.96it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:35,  5.96it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:34,  5.97it/s]Evaluating:  34%|███▍      | 106/313 [00:17<00:34,  5.96it/s]Evaluating:  34%|███▍      | 107/313 [00:17<00:34,  5.96it/s]Evaluating:  35%|███▍      | 108/313 [00:17<00:34,  5.95it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:34,  5.95it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:34,  5.95it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:33,  5.95it/s]Evaluating:  36%|███▌      | 112/313 [00:18<00:33,  5.94it/s]Evaluating:  36%|███▌      | 113/313 [00:18<00:33,  5.96it/s]Evaluating:  36%|███▋      | 114/313 [00:18<00:34,  5.81it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:33,  6.00it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:32,  6.13it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:31,  6.23it/s]Evaluating:  38%|███▊      | 118/313 [00:19<00:30,  6.29it/s]Evaluating:  38%|███▊      | 119/313 [00:19<00:30,  6.31it/s]Evaluating:  38%|███▊      | 120/313 [00:19<00:30,  6.31it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:30,  6.28it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:30,  6.25it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:30,  6.25it/s]Evaluating:  40%|███▉      | 124/313 [00:20<00:30,  6.22it/s]Evaluating:  40%|███▉      | 125/313 [00:20<00:30,  6.18it/s]Evaluating:  40%|████      | 126/313 [00:20<00:30,  6.12it/s]Evaluating:  41%|████      | 127/313 [00:21<00:30,  6.08it/s]Evaluating:  41%|████      | 128/313 [00:21<00:30,  6.06it/s]Evaluating:  41%|████      | 129/313 [00:21<00:30,  6.05it/s]Evaluating:  42%|████▏     | 130/313 [00:21<00:30,  6.04it/s]Evaluating:  42%|████▏     | 131/313 [00:21<00:30,  6.00it/s]Evaluating:  42%|████▏     | 132/313 [00:21<00:30,  5.97it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:30,  5.96it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:30,  5.95it/s]Evaluating:  43%|████▎     | 135/313 [00:22<00:29,  5.94it/s]Evaluating:  43%|████▎     | 136/313 [00:22<00:29,  5.94it/s]Evaluating:  44%|████▍     | 137/313 [00:22<00:29,  5.93it/s]Evaluating:  44%|████▍     | 138/313 [00:22<00:29,  5.94it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:29,  5.99it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:28,  6.06it/s]Evaluating:  45%|████▌     | 141/313 [00:23<00:28,  6.10it/s]Evaluating:  45%|████▌     | 142/313 [00:23<00:28,  6.10it/s]Evaluating:  46%|████▌     | 143/313 [00:23<00:28,  6.07it/s]Evaluating:  46%|████▌     | 144/313 [00:23<00:29,  5.64it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:28,  5.86it/s]Evaluating:  47%|████▋     | 146/313 [00:24<00:27,  6.02it/s]Evaluating:  47%|████▋     | 147/313 [00:24<00:27,  6.14it/s]Evaluating:  47%|████▋     | 148/313 [00:24<00:26,  6.23it/s]Evaluating:  48%|████▊     | 149/313 [00:24<00:26,  6.29it/s]Evaluating:  48%|████▊     | 150/313 [00:24<00:25,  6.33it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:25,  6.34it/s]Evaluating:  49%|████▊     | 152/313 [00:25<00:25,  6.32it/s]Evaluating:  49%|████▉     | 153/313 [00:25<00:25,  6.31it/s]Evaluating:  49%|████▉     | 154/313 [00:25<00:25,  6.31it/s]Evaluating:  50%|████▉     | 155/313 [00:25<00:24,  6.34it/s]Evaluating:  50%|████▉     | 156/313 [00:25<00:24,  6.34it/s]Evaluating:  50%|█████     | 157/313 [00:25<00:24,  6.35it/s]Evaluating:  50%|█████     | 158/313 [00:26<00:24,  6.37it/s]Evaluating:  51%|█████     | 159/313 [00:26<00:24,  6.38it/s]Evaluating:  51%|█████     | 160/313 [00:26<00:23,  6.39it/s]Evaluating:  51%|█████▏    | 161/313 [00:26<00:23,  6.40it/s]Evaluating:  52%|█████▏    | 162/313 [00:26<00:23,  6.38it/s]Evaluating:  52%|█████▏    | 163/313 [00:26<00:23,  6.37it/s]Evaluating:  52%|█████▏    | 164/313 [00:27<00:23,  6.36it/s]Evaluating:  53%|█████▎    | 165/313 [00:27<00:23,  6.34it/s]Evaluating:  53%|█████▎    | 166/313 [00:27<00:23,  6.33it/s]Evaluating:  53%|█████▎    | 167/313 [00:27<00:23,  6.33it/s]Evaluating:  54%|█████▎    | 168/313 [00:27<00:22,  6.31it/s]Evaluating:  54%|█████▍    | 169/313 [00:27<00:23,  6.24it/s]Evaluating:  54%|█████▍    | 170/313 [00:28<00:22,  6.22it/s]Evaluating:  55%|█████▍    | 171/313 [00:28<00:22,  6.22it/s]Evaluating:  55%|█████▍    | 172/313 [00:28<00:22,  6.21it/s]Evaluating:  55%|█████▌    | 173/313 [00:28<00:22,  6.21it/s]Evaluating:  56%|█████▌    | 174/313 [00:28<00:22,  6.19it/s]Evaluating:  56%|█████▌    | 175/313 [00:28<00:23,  5.76it/s]Evaluating:  56%|█████▌    | 176/313 [00:29<00:23,  5.94it/s]Evaluating:  57%|█████▋    | 177/313 [00:29<00:22,  6.07it/s]Evaluating:  57%|█████▋    | 178/313 [00:29<00:21,  6.16it/s]Evaluating:  57%|█████▋    | 179/313 [00:29<00:21,  6.24it/s]Evaluating:  58%|█████▊    | 180/313 [00:29<00:21,  6.28it/s]Evaluating:  58%|█████▊    | 181/313 [00:29<00:20,  6.32it/s]Evaluating:  58%|█████▊    | 182/313 [00:29<00:20,  6.33it/s]Evaluating:  58%|█████▊    | 183/313 [00:30<00:20,  6.35it/s]Evaluating:  59%|█████▉    | 184/313 [00:30<00:20,  6.26it/s]Evaluating:  59%|█████▉    | 185/313 [00:30<00:20,  6.29it/s]Evaluating:  59%|█████▉    | 186/313 [00:30<00:20,  6.31it/s]Evaluating:  60%|█████▉    | 187/313 [00:30<00:19,  6.32it/s]Evaluating:  60%|██████    | 188/313 [00:30<00:19,  6.30it/s]Evaluating:  60%|██████    | 189/313 [00:31<00:19,  6.30it/s]Evaluating:  61%|██████    | 190/313 [00:31<00:19,  6.31it/s]Evaluating:  61%|██████    | 191/313 [00:31<00:19,  6.33it/s]Evaluating:  61%|██████▏   | 192/313 [00:31<00:19,  6.35it/s]Evaluating:  62%|██████▏   | 193/313 [00:31<00:18,  6.35it/s]Evaluating:  62%|██████▏   | 194/313 [00:31<00:18,  6.36it/s]Evaluating:  62%|██████▏   | 195/313 [00:32<00:18,  6.37it/s]Evaluating:  63%|██████▎   | 196/313 [00:32<00:18,  6.36it/s]Evaluating:  63%|██████▎   | 197/313 [00:32<00:18,  6.36it/s]Evaluating:  63%|██████▎   | 198/313 [00:32<00:18,  6.37it/s]Evaluating:  64%|██████▎   | 199/313 [00:32<00:17,  6.37it/s]Evaluating:  64%|██████▍   | 200/313 [00:32<00:17,  6.36it/s]Evaluating:  64%|██████▍   | 201/313 [00:32<00:17,  6.31it/s]Evaluating:  65%|██████▍   | 202/313 [00:33<00:17,  6.25it/s]Evaluating:  65%|██████▍   | 203/313 [00:33<00:17,  6.21it/s]Evaluating:  65%|██████▌   | 204/313 [00:33<00:17,  6.17it/s]Evaluating:  65%|██████▌   | 205/313 [00:33<00:17,  6.12it/s]Evaluating:  66%|██████▌   | 206/313 [00:33<00:17,  6.03it/s]Evaluating:  66%|██████▌   | 207/313 [00:33<00:17,  5.97it/s]Evaluating:  66%|██████▋   | 208/313 [00:34<00:17,  5.94it/s]Evaluating:  67%|██████▋   | 209/313 [00:34<00:17,  5.92it/s]Evaluating:  67%|██████▋   | 210/313 [00:34<00:17,  5.91it/s]Evaluating:  67%|██████▋   | 211/313 [00:34<00:17,  5.94it/s]Evaluating:  68%|██████▊   | 212/313 [00:34<00:16,  5.95it/s]Evaluating:  68%|██████▊   | 213/313 [00:34<00:16,  5.94it/s]Evaluating:  68%|██████▊   | 214/313 [00:35<00:16,  5.92it/s]Evaluating:  69%|██████▊   | 215/313 [00:35<00:16,  5.95it/s]Evaluating:  69%|██████▉   | 216/313 [00:35<00:16,  5.93it/s]Evaluating:  69%|██████▉   | 217/313 [00:35<00:16,  5.89it/s]Evaluating:  70%|██████▉   | 218/313 [00:35<00:16,  5.87it/s]Evaluating:  70%|██████▉   | 219/313 [00:35<00:15,  5.88it/s]Evaluating:  70%|███████   | 220/313 [00:36<00:15,  5.89it/s]Evaluating:  71%|███████   | 221/313 [00:36<00:15,  5.92it/s]Evaluating:  71%|███████   | 222/313 [00:36<00:15,  5.93it/s]Evaluating:  71%|███████   | 223/313 [00:36<00:15,  5.92it/s]Evaluating:  72%|███████▏  | 224/313 [00:36<00:15,  5.93it/s]Evaluating:  72%|███████▏  | 225/313 [00:37<00:14,  5.95it/s]Evaluating:  72%|███████▏  | 226/313 [00:37<00:14,  5.96it/s]Evaluating:  73%|███████▎  | 227/313 [00:37<00:14,  5.97it/s]Evaluating:  73%|███████▎  | 228/313 [00:37<00:14,  5.97it/s]Evaluating:  73%|███████▎  | 229/313 [00:37<00:14,  5.96it/s]Evaluating:  73%|███████▎  | 230/313 [00:37<00:13,  5.95it/s]Evaluating:  74%|███████▍  | 231/313 [00:38<00:13,  6.00it/s]Evaluating:  74%|███████▍  | 232/313 [00:38<00:13,  6.05it/s]Evaluating:  74%|███████▍  | 233/313 [00:38<00:13,  6.09it/s]Evaluating:  75%|███████▍  | 234/313 [00:38<00:12,  6.09it/s]Evaluating:  75%|███████▌  | 235/313 [00:38<00:12,  6.09it/s]Evaluating:  75%|███████▌  | 236/313 [00:38<00:14,  5.41it/s]Evaluating:  76%|███████▌  | 237/313 [00:39<00:13,  5.66it/s]Evaluating:  76%|███████▌  | 238/313 [00:39<00:12,  5.85it/s]Evaluating:  76%|███████▋  | 239/313 [00:39<00:12,  5.99it/s]Evaluating:  77%|███████▋  | 240/313 [00:39<00:12,  6.05it/s]Evaluating:  77%|███████▋  | 241/313 [00:39<00:11,  6.10it/s]Evaluating:  77%|███████▋  | 242/313 [00:39<00:11,  6.13it/s]Evaluating:  78%|███████▊  | 243/313 [00:40<00:11,  6.15it/s]Evaluating:  78%|███████▊  | 244/313 [00:40<00:11,  6.16it/s]Evaluating:  78%|███████▊  | 245/313 [00:40<00:11,  6.18it/s]Evaluating:  79%|███████▊  | 246/313 [00:40<00:10,  6.19it/s]Evaluating:  79%|███████▉  | 247/313 [00:40<00:10,  6.23it/s]Evaluating:  79%|███████▉  | 248/313 [00:40<00:10,  6.25it/s]Evaluating:  80%|███████▉  | 249/313 [00:40<00:10,  6.29it/s]Evaluating:  80%|███████▉  | 250/313 [00:41<00:10,  6.29it/s]Evaluating:  80%|████████  | 251/313 [00:41<00:09,  6.31it/s]Evaluating:  81%|████████  | 252/313 [00:41<00:09,  6.31it/s]Evaluating:  81%|████████  | 253/313 [00:41<00:09,  6.31it/s]Evaluating:  81%|████████  | 254/313 [00:41<00:09,  6.30it/s]Evaluating:  81%|████████▏ | 255/313 [00:41<00:09,  6.30it/s]Evaluating:  82%|████████▏ | 256/313 [00:42<00:09,  6.29it/s]Evaluating:  82%|████████▏ | 257/313 [00:42<00:08,  6.26it/s]Evaluating:  82%|████████▏ | 258/313 [00:42<00:08,  6.21it/s]Evaluating:  83%|████████▎ | 259/313 [00:42<00:08,  6.18it/s]Evaluating:  83%|████████▎ | 260/313 [00:42<00:08,  6.17it/s]Evaluating:  83%|████████▎ | 261/313 [00:42<00:08,  6.18it/s]Evaluating:  84%|████████▎ | 262/313 [00:43<00:08,  6.19it/s]Evaluating:  84%|████████▍ | 263/313 [00:43<00:08,  6.21it/s]Evaluating:  84%|████████▍ | 264/313 [00:43<00:07,  6.24it/s]Evaluating:  85%|████████▍ | 265/313 [00:43<00:07,  6.27it/s]Evaluating:  85%|████████▍ | 266/313 [00:43<00:07,  6.28it/s]Evaluating:  85%|████████▌ | 267/313 [00:43<00:07,  5.89it/s]Evaluating:  86%|████████▌ | 268/313 [00:44<00:07,  5.86it/s]Evaluating:  86%|████████▌ | 269/313 [00:44<00:07,  5.86it/s]Evaluating:  86%|████████▋ | 270/313 [00:44<00:07,  5.89it/s]Evaluating:  87%|████████▋ | 271/313 [00:44<00:07,  5.92it/s]Evaluating:  87%|████████▋ | 272/313 [00:44<00:06,  5.94it/s]Evaluating:  87%|████████▋ | 273/313 [00:44<00:06,  5.96it/s]Evaluating:  88%|████████▊ | 274/313 [00:45<00:06,  5.92it/s]Evaluating:  88%|████████▊ | 275/313 [00:45<00:06,  5.89it/s]Evaluating:  88%|████████▊ | 276/313 [00:45<00:06,  5.88it/s]Evaluating:  88%|████████▊ | 277/313 [00:45<00:06,  5.89it/s]Evaluating:  89%|████████▉ | 278/313 [00:45<00:05,  5.92it/s]Evaluating:  89%|████████▉ | 279/313 [00:45<00:05,  5.94it/s]Evaluating:  89%|████████▉ | 280/313 [00:46<00:05,  5.92it/s]Evaluating:  90%|████████▉ | 281/313 [00:46<00:05,  5.92it/s]Evaluating:  90%|█████████ | 282/313 [00:46<00:05,  5.95it/s]Evaluating:  90%|█████████ | 283/313 [00:46<00:05,  5.98it/s]Evaluating:  91%|█████████ | 284/313 [00:46<00:04,  5.98it/s]Evaluating:  91%|█████████ | 285/313 [00:46<00:04,  6.01it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:04,  6.03it/s]Evaluating:  92%|█████████▏| 287/313 [00:47<00:04,  6.05it/s]Evaluating:  92%|█████████▏| 288/313 [00:47<00:04,  6.06it/s]Evaluating:  92%|█████████▏| 289/313 [00:47<00:03,  6.10it/s]Evaluating:  93%|█████████▎| 290/313 [00:47<00:03,  6.13it/s]Evaluating:  93%|█████████▎| 291/313 [00:47<00:03,  6.17it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  6.20it/s]Evaluating:  94%|█████████▎| 293/313 [00:48<00:03,  6.22it/s]Evaluating:  94%|█████████▍| 294/313 [00:48<00:03,  6.11it/s]Evaluating:  94%|█████████▍| 295/313 [00:48<00:02,  6.07it/s]Evaluating:  95%|█████████▍| 296/313 [00:48<00:02,  6.05it/s]Evaluating:  95%|█████████▍| 297/313 [00:48<00:03,  5.21it/s]Evaluating:  95%|█████████▌| 298/313 [00:49<00:02,  5.44it/s]Evaluating:  96%|█████████▌| 299/313 [00:49<00:02,  5.59it/s]Evaluating:  96%|█████████▌| 300/313 [00:49<00:02,  5.66it/s]Evaluating:  96%|█████████▌| 301/313 [00:49<00:02,  5.71it/s]Evaluating:  96%|█████████▋| 302/313 [00:49<00:01,  5.77it/s]Evaluating:  97%|█████████▋| 303/313 [00:49<00:01,  5.81it/s]Evaluating:  97%|█████████▋| 304/313 [00:50<00:01,  5.86it/s]Evaluating:  97%|█████████▋| 305/313 [00:50<00:01,  5.88it/s]Evaluating:  98%|█████████▊| 306/313 [00:50<00:01,  5.90it/s]Evaluating:  98%|█████████▊| 307/313 [00:50<00:01,  5.91it/s]Evaluating:  98%|█████████▊| 308/313 [00:50<00:00,  5.91it/s]Evaluating:  99%|█████████▊| 309/313 [00:50<00:00,  5.91it/s]Evaluating:  99%|█████████▉| 310/313 [00:51<00:00,  5.91it/s]Evaluating:  99%|█████████▉| 311/313 [00:51<00:00,  5.93it/s]Evaluating: 100%|█████████▉| 312/313 [00:51<00:00,  5.93it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.50it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.06it/s]
10/14/2021 16:25:30 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/14/2021 16:25:30 - INFO - __main__ -     f1 = 0.6953078202995008
10/14/2021 16:25:30 - INFO - __main__ -     loss = 1.2034663605613831
10/14/2021 16:25:30 - INFO - __main__ -     precision = 0.653386703358559
10/14/2021 16:25:30 - INFO - __main__ -     recall = 0.7429770286608349
10/14/2021 16:25:30 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:25:43 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:25:43 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:25:59 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/14/2021 16:26:01 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:26:01 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/14/2021 16:26:01 - INFO - __main__ -   Seed = 3
10/14/2021 16:26:01 - INFO - root -   save model
10/14/2021 16:26:01 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/14/2021 16:26:01 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:26:17 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:26:17 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/14/2021 16:26:17 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
10/14/2021 16:26:17 - INFO - root -   Trying to decide if add adapter
10/14/2021 16:26:17 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/pytorch_model_head.bin
10/14/2021 16:26:17 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/14/2021 16:26:17 - INFO - __main__ -   Language = en
10/14/2021 16:26:17 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/14/2021 16:26:19 - INFO - __main__ -   Language = ru
10/14/2021 16:26:19 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/14/2021 16:26:23 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/14/2021 16:26:23 - INFO - __main__ -   ***** Running evaluation  in be *****
10/14/2021 16:26:23 - INFO - __main__ -     Num examples = 1001
10/14/2021 16:26:23 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  6.03it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.28it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.48it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.57it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.64it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  6.67it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.68it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.70it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.69it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.70it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.70it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  6.70it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  6.71it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.72it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.71it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.71it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.70it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.71it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  6.71it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.72it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.70it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.70it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.68it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.67it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.67it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.67it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.66it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.65it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.64it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.64it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.65it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.80it/s]
10/14/2021 16:26:28 - INFO - __main__ -   ***** Evaluation result  in be *****
10/14/2021 16:26:28 - INFO - __main__ -     f1 = 0.6431888544891641
10/14/2021 16:26:28 - INFO - __main__ -     loss = 1.2483324268832803
10/14/2021 16:26:28 - INFO - __main__ -     precision = 0.6079005120702268
10/14/2021 16:26:28 - INFO - __main__ -     recall = 0.6828266228430567
10/14/2021 16:26:28 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/14/2021 16:26:29 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/14/2021 16:26:29 - INFO - __main__ -     Num examples = 10001
10/14/2021 16:26:29 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:46,  6.74it/s]Evaluating:   1%|          | 2/313 [00:00<00:46,  6.72it/s]Evaluating:   1%|          | 3/313 [00:00<00:46,  6.72it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:46,  6.72it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:45,  6.72it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:45,  6.71it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:45,  6.72it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:45,  6.71it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:45,  6.71it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:45,  6.71it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:45,  6.70it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:44,  6.71it/s]Evaluating:   4%|▍         | 13/313 [00:01<00:44,  6.70it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:44,  6.70it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:44,  6.69it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:44,  6.69it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:44,  6.69it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:44,  6.69it/s]Evaluating:   6%|▌         | 19/313 [00:02<00:43,  6.70it/s]Evaluating:   6%|▋         | 20/313 [00:02<00:43,  6.69it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:43,  6.69it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:43,  6.68it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:43,  6.67it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:43,  6.67it/s]Evaluating:   8%|▊         | 25/313 [00:03<00:43,  6.67it/s]Evaluating:   8%|▊         | 26/313 [00:03<00:43,  6.66it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:42,  6.66it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:42,  6.66it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:42,  6.66it/s]Evaluating:  10%|▉         | 30/313 [00:04<00:42,  6.66it/s]Evaluating:  10%|▉         | 31/313 [00:04<00:42,  6.66it/s]Evaluating:  10%|█         | 32/313 [00:04<00:42,  6.63it/s]Evaluating:  11%|█         | 33/313 [00:04<00:42,  6.63it/s]Evaluating:  11%|█         | 34/313 [00:05<00:42,  6.64it/s]Evaluating:  11%|█         | 35/313 [00:05<00:41,  6.64it/s]Evaluating:  12%|█▏        | 36/313 [00:05<00:41,  6.64it/s]Evaluating:  12%|█▏        | 37/313 [00:05<00:41,  6.64it/s]Evaluating:  12%|█▏        | 38/313 [00:05<00:41,  6.64it/s]Evaluating:  12%|█▏        | 39/313 [00:05<00:41,  6.64it/s]Evaluating:  13%|█▎        | 40/313 [00:05<00:41,  6.63it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:41,  6.63it/s]Evaluating:  13%|█▎        | 42/313 [00:06<00:41,  6.60it/s]Evaluating:  14%|█▎        | 43/313 [00:06<00:40,  6.60it/s]Evaluating:  14%|█▍        | 44/313 [00:06<00:40,  6.61it/s]Evaluating:  14%|█▍        | 45/313 [00:06<00:40,  6.59it/s]Evaluating:  15%|█▍        | 46/313 [00:06<00:40,  6.56it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:40,  6.56it/s]Evaluating:  15%|█▌        | 48/313 [00:07<00:40,  6.56it/s]Evaluating:  16%|█▌        | 49/313 [00:07<00:40,  6.52it/s]Evaluating:  16%|█▌        | 50/313 [00:07<00:40,  6.50it/s]Evaluating:  16%|█▋        | 51/313 [00:07<00:40,  6.49it/s]Evaluating:  17%|█▋        | 52/313 [00:07<00:40,  6.48it/s]Evaluating:  17%|█▋        | 53/313 [00:07<00:40,  6.45it/s]Evaluating:  17%|█▋        | 54/313 [00:08<00:40,  6.45it/s]Evaluating:  18%|█▊        | 55/313 [00:08<00:40,  6.42it/s]Evaluating:  18%|█▊        | 56/313 [00:08<00:39,  6.44it/s]Evaluating:  18%|█▊        | 57/313 [00:08<00:39,  6.49it/s]Evaluating:  19%|█▊        | 58/313 [00:08<00:39,  6.51it/s]Evaluating:  19%|█▉        | 59/313 [00:08<00:38,  6.52it/s]Evaluating:  19%|█▉        | 60/313 [00:09<00:38,  6.52it/s]Evaluating:  19%|█▉        | 61/313 [00:09<00:38,  6.53it/s]Evaluating:  20%|█▉        | 62/313 [00:09<00:38,  6.53it/s]Evaluating:  20%|██        | 63/313 [00:09<00:38,  6.52it/s]Evaluating:  20%|██        | 64/313 [00:09<00:38,  6.54it/s]Evaluating:  21%|██        | 65/313 [00:09<00:37,  6.54it/s]Evaluating:  21%|██        | 66/313 [00:09<00:37,  6.53it/s]Evaluating:  21%|██▏       | 67/313 [00:10<00:37,  6.54it/s]Evaluating:  22%|██▏       | 68/313 [00:10<00:37,  6.50it/s]Evaluating:  22%|██▏       | 69/313 [00:10<00:37,  6.46it/s]Evaluating:  22%|██▏       | 70/313 [00:10<00:37,  6.44it/s]Evaluating:  23%|██▎       | 71/313 [00:10<00:37,  6.43it/s]Evaluating:  23%|██▎       | 72/313 [00:10<00:37,  6.42it/s]Evaluating:  23%|██▎       | 73/313 [00:11<00:37,  6.42it/s]Evaluating:  24%|██▎       | 74/313 [00:11<00:37,  6.40it/s]Evaluating:  24%|██▍       | 75/313 [00:11<00:37,  6.43it/s]Evaluating:  24%|██▍       | 76/313 [00:11<00:36,  6.44it/s]Evaluating:  25%|██▍       | 77/313 [00:11<00:36,  6.44it/s]Evaluating:  25%|██▍       | 78/313 [00:11<00:36,  6.44it/s]Evaluating:  25%|██▌       | 79/313 [00:12<00:36,  6.42it/s]Evaluating:  26%|██▌       | 80/313 [00:12<00:36,  6.40it/s]Evaluating:  26%|██▌       | 81/313 [00:12<00:36,  6.38it/s]Evaluating:  26%|██▌       | 82/313 [00:12<00:36,  6.32it/s]Evaluating:  27%|██▋       | 83/313 [00:12<00:36,  6.26it/s]Evaluating:  27%|██▋       | 84/313 [00:12<00:36,  6.22it/s]Evaluating:  27%|██▋       | 85/313 [00:12<00:36,  6.19it/s]Evaluating:  27%|██▋       | 86/313 [00:13<00:36,  6.15it/s]Evaluating:  28%|██▊       | 87/313 [00:13<00:36,  6.13it/s]Evaluating:  28%|██▊       | 88/313 [00:13<00:36,  6.11it/s]Evaluating:  28%|██▊       | 89/313 [00:13<00:36,  6.09it/s]Evaluating:  29%|██▉       | 90/313 [00:13<00:36,  6.11it/s]Evaluating:  29%|██▉       | 91/313 [00:13<00:36,  6.12it/s]Evaluating:  29%|██▉       | 92/313 [00:14<00:36,  6.11it/s]Evaluating:  30%|██▉       | 93/313 [00:14<00:36,  6.09it/s]Evaluating:  30%|███       | 94/313 [00:14<00:36,  6.05it/s]Evaluating:  30%|███       | 95/313 [00:14<00:36,  6.02it/s]Evaluating:  31%|███       | 96/313 [00:14<00:36,  6.00it/s]Evaluating:  31%|███       | 97/313 [00:14<00:36,  5.98it/s]Evaluating:  31%|███▏      | 98/313 [00:15<00:36,  5.97it/s]Evaluating:  32%|███▏      | 99/313 [00:15<00:35,  5.97it/s]Evaluating:  32%|███▏      | 100/313 [00:15<00:35,  5.96it/s]Evaluating:  32%|███▏      | 101/313 [00:15<00:35,  5.97it/s]Evaluating:  33%|███▎      | 102/313 [00:15<00:35,  5.96it/s]Evaluating:  33%|███▎      | 103/313 [00:15<00:35,  5.96it/s]Evaluating:  33%|███▎      | 104/313 [00:16<00:35,  5.96it/s]Evaluating:  34%|███▎      | 105/313 [00:16<00:34,  5.96it/s]Evaluating:  34%|███▍      | 106/313 [00:16<00:34,  5.95it/s]Evaluating:  34%|███▍      | 107/313 [00:16<00:34,  5.95it/s]Evaluating:  35%|███▍      | 108/313 [00:16<00:34,  5.95it/s]Evaluating:  35%|███▍      | 109/313 [00:16<00:34,  5.96it/s]Evaluating:  35%|███▌      | 110/313 [00:17<00:34,  5.95it/s]Evaluating:  35%|███▌      | 111/313 [00:17<00:33,  5.95it/s]Evaluating:  36%|███▌      | 112/313 [00:17<00:33,  5.95it/s]Evaluating:  36%|███▌      | 113/313 [00:17<00:33,  5.95it/s]Evaluating:  36%|███▋      | 114/313 [00:17<00:33,  5.95it/s]Evaluating:  37%|███▋      | 115/313 [00:17<00:33,  5.95it/s]Evaluating:  37%|███▋      | 116/313 [00:18<00:33,  5.94it/s]Evaluating:  37%|███▋      | 117/313 [00:18<00:33,  5.89it/s]Evaluating:  38%|███▊      | 118/313 [00:18<00:33,  5.91it/s]Evaluating:  38%|███▊      | 119/313 [00:18<00:32,  5.91it/s]Evaluating:  38%|███▊      | 120/313 [00:18<00:32,  5.92it/s]Evaluating:  39%|███▊      | 121/313 [00:18<00:32,  5.92it/s]Evaluating:  39%|███▉      | 122/313 [00:19<00:32,  5.92it/s]Evaluating:  39%|███▉      | 123/313 [00:19<00:32,  5.92it/s]Evaluating:  40%|███▉      | 124/313 [00:19<00:31,  5.92it/s]Evaluating:  40%|███▉      | 125/313 [00:19<00:31,  5.92it/s]Evaluating:  40%|████      | 126/313 [00:19<00:31,  5.93it/s]Evaluating:  41%|████      | 127/313 [00:20<00:31,  5.93it/s]Evaluating:  41%|████      | 128/313 [00:20<00:31,  5.92it/s]Evaluating:  41%|████      | 129/313 [00:20<00:31,  5.93it/s]Evaluating:  42%|████▏     | 130/313 [00:20<00:30,  5.95it/s]Evaluating:  42%|████▏     | 131/313 [00:20<00:30,  5.94it/s]Evaluating:  42%|████▏     | 132/313 [00:20<00:30,  5.94it/s]Evaluating:  42%|████▏     | 133/313 [00:21<00:30,  5.94it/s]Evaluating:  43%|████▎     | 134/313 [00:21<00:30,  5.92it/s]Evaluating:  43%|████▎     | 135/313 [00:21<00:30,  5.92it/s]Evaluating:  43%|████▎     | 136/313 [00:21<00:29,  5.92it/s]Evaluating:  44%|████▍     | 137/313 [00:21<00:29,  5.91it/s]Evaluating:  44%|████▍     | 138/313 [00:21<00:29,  5.91it/s]Evaluating:  44%|████▍     | 139/313 [00:22<00:29,  5.91it/s]Evaluating:  45%|████▍     | 140/313 [00:22<00:29,  5.91it/s]Evaluating:  45%|████▌     | 141/313 [00:22<00:29,  5.82it/s]Evaluating:  45%|████▌     | 142/313 [00:22<00:29,  5.81it/s]Evaluating:  46%|████▌     | 143/313 [00:22<00:29,  5.83it/s]Evaluating:  46%|████▌     | 144/313 [00:22<00:28,  5.86it/s]Evaluating:  46%|████▋     | 145/313 [00:23<00:28,  5.87it/s]Evaluating:  47%|████▋     | 146/313 [00:23<00:28,  5.87it/s]Evaluating:  47%|████▋     | 147/313 [00:23<00:28,  5.88it/s]Evaluating:  47%|████▋     | 148/313 [00:23<00:28,  5.89it/s]Evaluating:  48%|████▊     | 149/313 [00:23<00:27,  5.89it/s]Evaluating:  48%|████▊     | 150/313 [00:23<00:27,  5.90it/s]Evaluating:  48%|████▊     | 151/313 [00:24<00:27,  5.92it/s]Evaluating:  49%|████▊     | 152/313 [00:24<00:27,  5.94it/s]Evaluating:  49%|████▉     | 153/313 [00:24<00:26,  5.98it/s]Evaluating:  49%|████▉     | 154/313 [00:24<00:26,  5.98it/s]Evaluating:  50%|████▉     | 155/313 [00:24<00:26,  5.97it/s]Evaluating:  50%|████▉     | 156/313 [00:24<00:26,  5.98it/s]Evaluating:  50%|█████     | 157/313 [00:25<00:26,  5.98it/s]Evaluating:  50%|█████     | 158/313 [00:25<00:26,  5.96it/s]Evaluating:  51%|█████     | 159/313 [00:25<00:25,  5.94it/s]Evaluating:  51%|█████     | 160/313 [00:25<00:25,  5.92it/s]Evaluating:  51%|█████▏    | 161/313 [00:25<00:25,  5.91it/s]Evaluating:  52%|█████▏    | 162/313 [00:25<00:25,  5.90it/s]Evaluating:  52%|█████▏    | 163/313 [00:26<00:25,  5.90it/s]Evaluating:  52%|█████▏    | 164/313 [00:26<00:25,  5.88it/s]Evaluating:  53%|█████▎    | 165/313 [00:26<00:25,  5.87it/s]Evaluating:  53%|█████▎    | 166/313 [00:26<00:25,  5.87it/s]Evaluating:  53%|█████▎    | 167/313 [00:26<00:24,  5.87it/s]Evaluating:  54%|█████▎    | 168/313 [00:26<00:24,  5.86it/s]Evaluating:  54%|█████▍    | 169/313 [00:27<00:24,  5.86it/s]Evaluating:  54%|█████▍    | 170/313 [00:27<00:24,  5.86it/s]Evaluating:  55%|█████▍    | 171/313 [00:27<00:24,  5.86it/s]Evaluating:  55%|█████▍    | 172/313 [00:27<00:24,  5.86it/s]Evaluating:  55%|█████▌    | 173/313 [00:27<00:23,  5.86it/s]Evaluating:  56%|█████▌    | 174/313 [00:27<00:23,  5.87it/s]Evaluating:  56%|█████▌    | 175/313 [00:28<00:23,  5.86it/s]Evaluating:  56%|█████▌    | 176/313 [00:28<00:23,  5.76it/s]Evaluating:  57%|█████▋    | 177/313 [00:28<00:23,  5.79it/s]Evaluating:  57%|█████▋    | 178/313 [00:28<00:23,  5.81it/s]Evaluating:  57%|█████▋    | 179/313 [00:28<00:22,  5.83it/s]Evaluating:  58%|█████▊    | 180/313 [00:29<00:22,  5.85it/s]Evaluating:  58%|█████▊    | 181/313 [00:29<00:22,  5.84it/s]Evaluating:  58%|█████▊    | 182/313 [00:29<00:22,  5.84it/s]Evaluating:  58%|█████▊    | 183/313 [00:29<00:22,  5.84it/s]Evaluating:  59%|█████▉    | 184/313 [00:29<00:22,  5.84it/s]Evaluating:  59%|█████▉    | 185/313 [00:29<00:21,  5.84it/s]Evaluating:  59%|█████▉    | 186/313 [00:30<00:21,  5.84it/s]Evaluating:  60%|█████▉    | 187/313 [00:30<00:21,  5.86it/s]Evaluating:  60%|██████    | 188/313 [00:30<00:21,  5.87it/s]Evaluating:  60%|██████    | 189/313 [00:30<00:21,  5.85it/s]Evaluating:  61%|██████    | 190/313 [00:30<00:21,  5.85it/s]Evaluating:  61%|██████    | 191/313 [00:30<00:20,  5.85it/s]Evaluating:  61%|██████▏   | 192/313 [00:31<00:20,  5.85it/s]Evaluating:  62%|██████▏   | 193/313 [00:31<00:20,  5.85it/s]Evaluating:  62%|██████▏   | 194/313 [00:31<00:20,  5.86it/s]Evaluating:  62%|██████▏   | 195/313 [00:31<00:20,  5.85it/s]Evaluating:  63%|██████▎   | 196/313 [00:31<00:19,  5.85it/s]Evaluating:  63%|██████▎   | 197/313 [00:31<00:19,  5.85it/s]Evaluating:  63%|██████▎   | 198/313 [00:32<00:19,  5.86it/s]Evaluating:  64%|██████▎   | 199/313 [00:32<00:19,  5.86it/s]Evaluating:  64%|██████▍   | 200/313 [00:32<00:19,  5.87it/s]Evaluating:  64%|██████▍   | 201/313 [00:32<00:19,  5.86it/s]Evaluating:  65%|██████▍   | 202/313 [00:32<00:18,  5.86it/s]Evaluating:  65%|██████▍   | 203/313 [00:32<00:18,  5.86it/s]Evaluating:  65%|██████▌   | 204/313 [00:33<00:18,  5.87it/s]Evaluating:  65%|██████▌   | 205/313 [00:33<00:18,  5.89it/s]Evaluating:  66%|██████▌   | 206/313 [00:33<00:18,  5.78it/s]Evaluating:  66%|██████▌   | 207/313 [00:33<00:18,  5.79it/s]Evaluating:  66%|██████▋   | 208/313 [00:33<00:18,  5.80it/s]Evaluating:  67%|██████▋   | 209/313 [00:33<00:17,  5.82it/s]Evaluating:  67%|██████▋   | 210/313 [00:34<00:17,  5.83it/s]Evaluating:  67%|██████▋   | 211/313 [00:34<00:17,  5.84it/s]Evaluating:  68%|██████▊   | 212/313 [00:34<00:17,  5.86it/s]Evaluating:  68%|██████▊   | 213/313 [00:34<00:16,  5.89it/s]Evaluating:  68%|██████▊   | 214/313 [00:34<00:16,  5.90it/s]Evaluating:  69%|██████▊   | 215/313 [00:34<00:16,  5.90it/s]Evaluating:  69%|██████▉   | 216/313 [00:35<00:16,  5.92it/s]Evaluating:  69%|██████▉   | 217/313 [00:35<00:16,  5.94it/s]Evaluating:  70%|██████▉   | 218/313 [00:35<00:16,  5.93it/s]Evaluating:  70%|██████▉   | 219/313 [00:35<00:15,  5.89it/s]Evaluating:  70%|███████   | 220/313 [00:35<00:15,  5.87it/s]Evaluating:  71%|███████   | 221/313 [00:36<00:15,  5.87it/s]Evaluating:  71%|███████   | 222/313 [00:36<00:15,  5.85it/s]Evaluating:  71%|███████   | 223/313 [00:36<00:15,  5.84it/s]Evaluating:  72%|███████▏  | 224/313 [00:36<00:15,  5.86it/s]Evaluating:  72%|███████▏  | 225/313 [00:36<00:15,  5.86it/s]Evaluating:  72%|███████▏  | 226/313 [00:36<00:14,  5.84it/s]Evaluating:  73%|███████▎  | 227/313 [00:37<00:14,  5.85it/s]Evaluating:  73%|███████▎  | 228/313 [00:37<00:14,  5.84it/s]Evaluating:  73%|███████▎  | 229/313 [00:37<00:14,  5.84it/s]Evaluating:  73%|███████▎  | 230/313 [00:37<00:14,  5.87it/s]Evaluating:  74%|███████▍  | 231/313 [00:37<00:13,  5.87it/s]Evaluating:  74%|███████▍  | 232/313 [00:37<00:13,  5.89it/s]Evaluating:  74%|███████▍  | 233/313 [00:38<00:13,  5.89it/s]Evaluating:  75%|███████▍  | 234/313 [00:38<00:13,  5.87it/s]Evaluating:  75%|███████▌  | 235/313 [00:38<00:13,  5.86it/s]Evaluating:  75%|███████▌  | 236/313 [00:38<00:13,  5.87it/s]Evaluating:  76%|███████▌  | 237/313 [00:38<00:16,  4.69it/s]Evaluating:  76%|███████▌  | 238/313 [00:39<00:14,  5.01it/s]Evaluating:  76%|███████▋  | 239/313 [00:39<00:14,  5.27it/s]Evaluating:  77%|███████▋  | 240/313 [00:39<00:13,  5.47it/s]Evaluating:  77%|███████▋  | 241/313 [00:39<00:12,  5.57it/s]Evaluating:  77%|███████▋  | 242/313 [00:39<00:12,  5.68it/s]Evaluating:  78%|███████▊  | 243/313 [00:39<00:12,  5.77it/s]Evaluating:  78%|███████▊  | 244/313 [00:40<00:11,  5.86it/s]Evaluating:  78%|███████▊  | 245/313 [00:40<00:11,  5.95it/s]Evaluating:  79%|███████▊  | 246/313 [00:40<00:11,  6.04it/s]Evaluating:  79%|███████▉  | 247/313 [00:40<00:10,  6.09it/s]Evaluating:  79%|███████▉  | 248/313 [00:40<00:10,  6.13it/s]Evaluating:  80%|███████▉  | 249/313 [00:40<00:10,  6.13it/s]Evaluating:  80%|███████▉  | 250/313 [00:41<00:10,  6.13it/s]Evaluating:  80%|████████  | 251/313 [00:41<00:10,  6.10it/s]Evaluating:  81%|████████  | 252/313 [00:41<00:10,  6.07it/s]Evaluating:  81%|████████  | 253/313 [00:41<00:09,  6.07it/s]Evaluating:  81%|████████  | 254/313 [00:41<00:09,  6.07it/s]Evaluating:  81%|████████▏ | 255/313 [00:41<00:09,  6.09it/s]Evaluating:  82%|████████▏ | 256/313 [00:42<00:09,  6.10it/s]Evaluating:  82%|████████▏ | 257/313 [00:42<00:09,  6.06it/s]Evaluating:  82%|████████▏ | 258/313 [00:42<00:09,  6.09it/s]Evaluating:  83%|████████▎ | 259/313 [00:42<00:08,  6.03it/s]Evaluating:  83%|████████▎ | 260/313 [00:42<00:08,  6.04it/s]Evaluating:  83%|████████▎ | 261/313 [00:42<00:08,  6.06it/s]Evaluating:  84%|████████▎ | 262/313 [00:43<00:08,  6.05it/s]Evaluating:  84%|████████▍ | 263/313 [00:43<00:08,  6.05it/s]Evaluating:  84%|████████▍ | 264/313 [00:43<00:08,  6.06it/s]Evaluating:  85%|████████▍ | 265/313 [00:43<00:07,  6.06it/s]Evaluating:  85%|████████▍ | 266/313 [00:43<00:07,  6.07it/s]Evaluating:  85%|████████▌ | 267/313 [00:43<00:07,  6.06it/s]Evaluating:  86%|████████▌ | 268/313 [00:44<00:08,  5.42it/s]Evaluating:  86%|████████▌ | 269/313 [00:44<00:07,  5.58it/s]Evaluating:  86%|████████▋ | 270/313 [00:44<00:07,  5.71it/s]Evaluating:  87%|████████▋ | 271/313 [00:44<00:07,  5.77it/s]Evaluating:  87%|████████▋ | 272/313 [00:44<00:07,  5.82it/s]Evaluating:  87%|████████▋ | 273/313 [00:44<00:06,  5.85it/s]Evaluating:  88%|████████▊ | 274/313 [00:45<00:06,  5.85it/s]Evaluating:  88%|████████▊ | 275/313 [00:45<00:06,  5.85it/s]Evaluating:  88%|████████▊ | 276/313 [00:45<00:06,  5.90it/s]Evaluating:  88%|████████▊ | 277/313 [00:45<00:06,  5.94it/s]Evaluating:  89%|████████▉ | 278/313 [00:45<00:06,  5.77it/s]Evaluating:  89%|████████▉ | 279/313 [00:45<00:06,  5.59it/s]Evaluating:  89%|████████▉ | 280/313 [00:46<00:06,  5.45it/s]Evaluating:  90%|████████▉ | 281/313 [00:46<00:05,  5.42it/s]Evaluating:  90%|█████████ | 282/313 [00:46<00:05,  5.42it/s]Evaluating:  90%|█████████ | 283/313 [00:46<00:05,  5.43it/s]Evaluating:  91%|█████████ | 284/313 [00:46<00:05,  5.44it/s]Evaluating:  91%|█████████ | 285/313 [00:47<00:05,  5.47it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:04,  5.50it/s]Evaluating:  92%|█████████▏| 287/313 [00:47<00:04,  5.54it/s]Evaluating:  92%|█████████▏| 288/313 [00:47<00:04,  5.56it/s]Evaluating:  92%|█████████▏| 289/313 [00:47<00:04,  5.56it/s]Evaluating:  93%|█████████▎| 290/313 [00:47<00:04,  5.58it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:03,  5.57it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  5.61it/s]Evaluating:  94%|█████████▎| 293/313 [00:48<00:03,  5.69it/s]Evaluating:  94%|█████████▍| 294/313 [00:48<00:03,  5.77it/s]Evaluating:  94%|█████████▍| 295/313 [00:48<00:03,  5.82it/s]Evaluating:  95%|█████████▍| 296/313 [00:48<00:02,  5.80it/s]Evaluating:  95%|█████████▍| 297/313 [00:49<00:02,  5.73it/s]Evaluating:  95%|█████████▌| 298/313 [00:49<00:02,  5.68it/s]Evaluating:  96%|█████████▌| 299/313 [00:49<00:02,  5.66it/s]Evaluating:  96%|█████████▌| 300/313 [00:49<00:02,  5.67it/s]Evaluating:  96%|█████████▌| 301/313 [00:49<00:02,  5.72it/s]Evaluating:  96%|█████████▋| 302/313 [00:50<00:01,  5.78it/s]Evaluating:  97%|█████████▋| 303/313 [00:50<00:01,  5.71it/s]Evaluating:  97%|█████████▋| 304/313 [00:50<00:01,  5.67it/s]Evaluating:  97%|█████████▋| 305/313 [00:50<00:01,  5.67it/s]Evaluating:  98%|█████████▊| 306/313 [00:50<00:01,  5.68it/s]Evaluating:  98%|█████████▊| 307/313 [00:50<00:01,  5.74it/s]Evaluating:  98%|█████████▊| 308/313 [00:51<00:00,  5.80it/s]Evaluating:  99%|█████████▊| 309/313 [00:51<00:00,  5.82it/s]Evaluating:  99%|█████████▉| 310/313 [00:51<00:00,  5.81it/s]Evaluating:  99%|█████████▉| 311/313 [00:51<00:00,  5.80it/s]Evaluating: 100%|█████████▉| 312/313 [00:51<00:00,  5.83it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.65it/s]Evaluating: 100%|██████████| 313/313 [00:51<00:00,  6.03it/s]
10/14/2021 16:27:22 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/14/2021 16:27:22 - INFO - __main__ -     f1 = 0.5741718674987998
10/14/2021 16:27:22 - INFO - __main__ -     loss = 1.5869220415243326
10/14/2021 16:27:22 - INFO - __main__ -     precision = 0.5317919075144508
10/14/2021 16:27:22 - INFO - __main__ -     recall = 0.6238914971309337
10/14/2021 16:27:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/14/2021 16:27:24 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/14/2021 16:27:24 - INFO - __main__ -     Num examples = 10004
10/14/2021 16:27:24 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/313 [00:00<?, ?it/s]Evaluating:   0%|          | 1/313 [00:00<00:51,  6.03it/s]Evaluating:   1%|          | 2/313 [00:00<00:51,  6.03it/s]Evaluating:   1%|          | 3/313 [00:00<00:51,  6.03it/s]Evaluating:   1%|▏         | 4/313 [00:00<00:51,  6.03it/s]Evaluating:   2%|▏         | 5/313 [00:00<00:51,  6.04it/s]Evaluating:   2%|▏         | 6/313 [00:00<00:50,  6.04it/s]Evaluating:   2%|▏         | 7/313 [00:01<00:50,  6.04it/s]Evaluating:   3%|▎         | 8/313 [00:01<00:50,  6.04it/s]Evaluating:   3%|▎         | 9/313 [00:01<00:50,  6.04it/s]Evaluating:   3%|▎         | 10/313 [00:01<00:50,  6.04it/s]Evaluating:   4%|▎         | 11/313 [00:01<00:50,  6.03it/s]Evaluating:   4%|▍         | 12/313 [00:01<00:49,  6.03it/s]Evaluating:   4%|▍         | 13/313 [00:02<00:49,  6.03it/s]Evaluating:   4%|▍         | 14/313 [00:02<00:49,  6.03it/s]Evaluating:   5%|▍         | 15/313 [00:02<00:49,  6.03it/s]Evaluating:   5%|▌         | 16/313 [00:02<00:49,  6.03it/s]Evaluating:   5%|▌         | 17/313 [00:02<00:49,  6.03it/s]Evaluating:   6%|▌         | 18/313 [00:02<00:48,  6.04it/s]Evaluating:   6%|▌         | 19/313 [00:03<00:48,  6.04it/s]Evaluating:   6%|▋         | 20/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 21/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 22/313 [00:03<00:48,  6.05it/s]Evaluating:   7%|▋         | 23/313 [00:03<00:47,  6.05it/s]Evaluating:   8%|▊         | 24/313 [00:03<00:47,  6.05it/s]Evaluating:   8%|▊         | 25/313 [00:04<00:47,  6.05it/s]Evaluating:   8%|▊         | 26/313 [00:04<00:47,  6.05it/s]Evaluating:   9%|▊         | 27/313 [00:04<00:47,  6.05it/s]Evaluating:   9%|▉         | 28/313 [00:04<00:47,  6.05it/s]Evaluating:   9%|▉         | 29/313 [00:04<00:57,  4.97it/s]Evaluating:  10%|▉         | 30/313 [00:05<00:53,  5.29it/s]Evaluating:  10%|▉         | 31/313 [00:05<00:50,  5.57it/s]Evaluating:  10%|█         | 32/313 [00:05<00:48,  5.76it/s]Evaluating:  11%|█         | 33/313 [00:05<00:47,  5.88it/s]Evaluating:  11%|█         | 34/313 [00:05<00:46,  5.95it/s]Evaluating:  11%|█         | 35/313 [00:05<00:46,  5.98it/s]Evaluating:  12%|█▏        | 36/313 [00:06<00:46,  5.99it/s]Evaluating:  12%|█▏        | 37/313 [00:06<00:46,  5.98it/s]Evaluating:  12%|█▏        | 38/313 [00:06<00:45,  5.99it/s]Evaluating:  12%|█▏        | 39/313 [00:06<00:45,  6.00it/s]Evaluating:  13%|█▎        | 40/313 [00:06<00:45,  6.01it/s]Evaluating:  13%|█▎        | 41/313 [00:06<00:45,  6.01it/s]Evaluating:  13%|█▎        | 42/313 [00:07<00:45,  6.02it/s]Evaluating:  14%|█▎        | 43/313 [00:07<00:44,  6.01it/s]Evaluating:  14%|█▍        | 44/313 [00:07<00:44,  6.02it/s]Evaluating:  14%|█▍        | 45/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▍        | 46/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▌        | 47/313 [00:07<00:44,  6.02it/s]Evaluating:  15%|█▌        | 48/313 [00:08<00:44,  6.02it/s]Evaluating:  16%|█▌        | 49/313 [00:08<00:43,  6.02it/s]Evaluating:  16%|█▌        | 50/313 [00:08<00:43,  6.01it/s]Evaluating:  16%|█▋        | 51/313 [00:08<00:43,  6.01it/s]Evaluating:  17%|█▋        | 52/313 [00:08<00:43,  6.00it/s]Evaluating:  17%|█▋        | 53/313 [00:08<00:43,  6.01it/s]Evaluating:  17%|█▋        | 54/313 [00:09<00:43,  6.01it/s]Evaluating:  18%|█▊        | 55/313 [00:09<00:42,  6.01it/s]Evaluating:  18%|█▊        | 56/313 [00:09<00:42,  6.02it/s]Evaluating:  18%|█▊        | 57/313 [00:09<00:42,  6.01it/s]Evaluating:  19%|█▊        | 58/313 [00:09<00:42,  6.02it/s]Evaluating:  19%|█▉        | 59/313 [00:09<00:42,  6.00it/s]Evaluating:  19%|█▉        | 60/313 [00:10<00:42,  6.00it/s]Evaluating:  19%|█▉        | 61/313 [00:10<00:41,  6.00it/s]Evaluating:  20%|█▉        | 62/313 [00:10<00:41,  6.00it/s]Evaluating:  20%|██        | 63/313 [00:10<00:41,  5.98it/s]Evaluating:  20%|██        | 64/313 [00:10<00:41,  5.98it/s]Evaluating:  21%|██        | 65/313 [00:10<00:41,  5.98it/s]Evaluating:  21%|██        | 66/313 [00:11<00:41,  5.98it/s]Evaluating:  21%|██▏       | 67/313 [00:11<00:41,  5.99it/s]Evaluating:  22%|██▏       | 68/313 [00:11<00:40,  5.99it/s]Evaluating:  22%|██▏       | 69/313 [00:11<00:40,  5.99it/s]Evaluating:  22%|██▏       | 70/313 [00:11<00:40,  5.99it/s]Evaluating:  23%|██▎       | 71/313 [00:11<00:40,  5.99it/s]Evaluating:  23%|██▎       | 72/313 [00:12<00:40,  5.99it/s]Evaluating:  23%|██▎       | 73/313 [00:12<00:40,  5.99it/s]Evaluating:  24%|██▎       | 74/313 [00:12<00:39,  5.99it/s]Evaluating:  24%|██▍       | 75/313 [00:12<00:39,  5.98it/s]Evaluating:  24%|██▍       | 76/313 [00:12<00:39,  5.98it/s]Evaluating:  25%|██▍       | 77/313 [00:12<00:39,  5.98it/s]Evaluating:  25%|██▍       | 78/313 [00:13<00:39,  5.98it/s]Evaluating:  25%|██▌       | 79/313 [00:13<00:39,  5.98it/s]Evaluating:  26%|██▌       | 80/313 [00:13<00:38,  5.98it/s]Evaluating:  26%|██▌       | 81/313 [00:13<00:38,  5.98it/s]Evaluating:  26%|██▌       | 82/313 [00:13<00:38,  5.98it/s]Evaluating:  27%|██▋       | 83/313 [00:13<00:38,  5.98it/s]Evaluating:  27%|██▋       | 84/313 [00:14<00:38,  5.98it/s]Evaluating:  27%|██▋       | 85/313 [00:14<00:38,  5.98it/s]Evaluating:  27%|██▋       | 86/313 [00:14<00:37,  5.98it/s]Evaluating:  28%|██▊       | 87/313 [00:14<00:37,  5.97it/s]Evaluating:  28%|██▊       | 88/313 [00:14<00:37,  5.97it/s]Evaluating:  28%|██▊       | 89/313 [00:15<00:49,  4.55it/s]Evaluating:  29%|██▉       | 90/313 [00:15<00:45,  4.89it/s]Evaluating:  29%|██▉       | 91/313 [00:15<00:42,  5.17it/s]Evaluating:  29%|██▉       | 92/313 [00:15<00:41,  5.39it/s]Evaluating:  30%|██▉       | 93/313 [00:15<00:39,  5.55it/s]Evaluating:  30%|███       | 94/313 [00:15<00:38,  5.66it/s]Evaluating:  30%|███       | 95/313 [00:16<00:37,  5.75it/s]Evaluating:  31%|███       | 96/313 [00:16<00:37,  5.82it/s]Evaluating:  31%|███       | 97/313 [00:16<00:36,  5.85it/s]Evaluating:  31%|███▏      | 98/313 [00:16<00:36,  5.88it/s]Evaluating:  32%|███▏      | 99/313 [00:16<00:36,  5.90it/s]Evaluating:  32%|███▏      | 100/313 [00:16<00:35,  5.92it/s]Evaluating:  32%|███▏      | 101/313 [00:17<00:35,  5.93it/s]Evaluating:  33%|███▎      | 102/313 [00:17<00:35,  5.94it/s]Evaluating:  33%|███▎      | 103/313 [00:17<00:35,  5.94it/s]Evaluating:  33%|███▎      | 104/313 [00:17<00:35,  5.93it/s]Evaluating:  34%|███▎      | 105/313 [00:17<00:35,  5.94it/s]Evaluating:  34%|███▍      | 106/313 [00:17<00:34,  5.96it/s]Evaluating:  34%|███▍      | 107/313 [00:18<00:34,  5.95it/s]Evaluating:  35%|███▍      | 108/313 [00:18<00:34,  5.95it/s]Evaluating:  35%|███▍      | 109/313 [00:18<00:34,  5.95it/s]Evaluating:  35%|███▌      | 110/313 [00:18<00:34,  5.95it/s]Evaluating:  35%|███▌      | 111/313 [00:18<00:33,  5.94it/s]Evaluating:  36%|███▌      | 112/313 [00:18<00:33,  5.96it/s]Evaluating:  36%|███▌      | 113/313 [00:19<00:33,  5.97it/s]Evaluating:  36%|███▋      | 114/313 [00:19<00:33,  5.96it/s]Evaluating:  37%|███▋      | 115/313 [00:19<00:33,  5.98it/s]Evaluating:  37%|███▋      | 116/313 [00:19<00:32,  6.03it/s]Evaluating:  37%|███▋      | 117/313 [00:19<00:32,  6.09it/s]Evaluating:  38%|███▊      | 118/313 [00:20<00:38,  5.06it/s]Evaluating:  38%|███▊      | 119/313 [00:20<00:35,  5.40it/s]Evaluating:  38%|███▊      | 120/313 [00:20<00:33,  5.69it/s]Evaluating:  39%|███▊      | 121/313 [00:20<00:32,  5.90it/s]Evaluating:  39%|███▉      | 122/313 [00:20<00:31,  6.05it/s]Evaluating:  39%|███▉      | 123/313 [00:20<00:30,  6.18it/s]Evaluating:  40%|███▉      | 124/313 [00:20<00:30,  6.27it/s]Evaluating:  40%|███▉      | 125/313 [00:21<00:29,  6.33it/s]Evaluating:  40%|████      | 126/313 [00:21<00:29,  6.35it/s]Evaluating:  41%|████      | 127/313 [00:21<00:29,  6.36it/s]Evaluating:  41%|████      | 128/313 [00:21<00:28,  6.38it/s]Evaluating:  41%|████      | 129/313 [00:21<00:28,  6.38it/s]Evaluating:  42%|████▏     | 130/313 [00:21<00:28,  6.37it/s]Evaluating:  42%|████▏     | 131/313 [00:22<00:28,  6.36it/s]Evaluating:  42%|████▏     | 132/313 [00:22<00:28,  6.36it/s]Evaluating:  42%|████▏     | 133/313 [00:22<00:28,  6.32it/s]Evaluating:  43%|████▎     | 134/313 [00:22<00:28,  6.30it/s]Evaluating:  43%|████▎     | 135/313 [00:22<00:28,  6.28it/s]Evaluating:  43%|████▎     | 136/313 [00:22<00:28,  6.26it/s]Evaluating:  44%|████▍     | 137/313 [00:23<00:28,  6.22it/s]Evaluating:  44%|████▍     | 138/313 [00:23<00:28,  6.17it/s]Evaluating:  44%|████▍     | 139/313 [00:23<00:28,  6.09it/s]Evaluating:  45%|████▍     | 140/313 [00:23<00:28,  6.05it/s]Evaluating:  45%|████▌     | 141/313 [00:23<00:28,  6.04it/s]Evaluating:  45%|████▌     | 142/313 [00:23<00:28,  6.03it/s]Evaluating:  46%|████▌     | 143/313 [00:24<00:28,  6.00it/s]Evaluating:  46%|████▌     | 144/313 [00:24<00:28,  5.97it/s]Evaluating:  46%|████▋     | 145/313 [00:24<00:28,  5.95it/s]Evaluating:  47%|████▋     | 146/313 [00:24<00:28,  5.94it/s]Evaluating:  47%|████▋     | 147/313 [00:24<00:27,  5.95it/s]Evaluating:  47%|████▋     | 148/313 [00:24<00:33,  4.96it/s]Evaluating:  48%|████▊     | 149/313 [00:25<00:30,  5.31it/s]Evaluating:  48%|████▊     | 150/313 [00:25<00:29,  5.58it/s]Evaluating:  48%|████▊     | 151/313 [00:25<00:27,  5.79it/s]Evaluating:  49%|████▊     | 152/313 [00:25<00:27,  5.94it/s]Evaluating:  49%|████▉     | 153/313 [00:25<00:26,  6.05it/s]Evaluating:  49%|████▉     | 154/313 [00:25<00:25,  6.14it/s]Evaluating:  50%|████▉     | 155/313 [00:26<00:25,  6.20it/s]Evaluating:  50%|████▉     | 156/313 [00:26<00:25,  6.23it/s]Evaluating:  50%|█████     | 157/313 [00:26<00:24,  6.25it/s]Evaluating:  50%|█████     | 158/313 [00:26<00:24,  6.26it/s]Evaluating:  51%|█████     | 159/313 [00:26<00:24,  6.28it/s]Evaluating:  51%|█████     | 160/313 [00:26<00:24,  6.26it/s]Evaluating:  51%|█████▏    | 161/313 [00:27<00:24,  6.20it/s]Evaluating:  52%|█████▏    | 162/313 [00:27<00:24,  6.13it/s]Evaluating:  52%|█████▏    | 163/313 [00:27<00:24,  6.07it/s]Evaluating:  52%|█████▏    | 164/313 [00:27<00:24,  6.04it/s]Evaluating:  53%|█████▎    | 165/313 [00:27<00:24,  5.99it/s]Evaluating:  53%|█████▎    | 166/313 [00:27<00:24,  5.96it/s]Evaluating:  53%|█████▎    | 167/313 [00:28<00:24,  5.93it/s]Evaluating:  54%|█████▎    | 168/313 [00:28<00:24,  5.92it/s]Evaluating:  54%|█████▍    | 169/313 [00:28<00:24,  5.90it/s]Evaluating:  54%|█████▍    | 170/313 [00:28<00:24,  5.90it/s]Evaluating:  55%|█████▍    | 171/313 [00:28<00:24,  5.89it/s]Evaluating:  55%|█████▍    | 172/313 [00:28<00:23,  5.88it/s]Evaluating:  55%|█████▌    | 173/313 [00:29<00:23,  5.88it/s]Evaluating:  56%|█████▌    | 174/313 [00:29<00:23,  5.88it/s]Evaluating:  56%|█████▌    | 175/313 [00:29<00:23,  5.89it/s]Evaluating:  56%|█████▌    | 176/313 [00:29<00:23,  5.91it/s]Evaluating:  57%|█████▋    | 177/313 [00:29<00:23,  5.90it/s]Evaluating:  57%|█████▋    | 178/313 [00:29<00:22,  5.89it/s]Evaluating:  57%|█████▋    | 179/313 [00:30<00:22,  5.88it/s]Evaluating:  58%|█████▊    | 180/313 [00:30<00:22,  5.89it/s]Evaluating:  58%|█████▊    | 181/313 [00:30<00:22,  5.90it/s]Evaluating:  58%|█████▊    | 182/313 [00:30<00:22,  5.93it/s]Evaluating:  58%|█████▊    | 183/313 [00:30<00:21,  5.94it/s]Evaluating:  59%|█████▉    | 184/313 [00:30<00:21,  5.91it/s]Evaluating:  59%|█████▉    | 185/313 [00:31<00:21,  5.89it/s]Evaluating:  59%|█████▉    | 186/313 [00:31<00:21,  5.88it/s]Evaluating:  60%|█████▉    | 187/313 [00:31<00:21,  5.87it/s]Evaluating:  60%|██████    | 188/313 [00:31<00:21,  5.87it/s]Evaluating:  60%|██████    | 189/313 [00:31<00:21,  5.88it/s]Evaluating:  61%|██████    | 190/313 [00:31<00:20,  5.90it/s]Evaluating:  61%|██████    | 191/313 [00:32<00:20,  5.91it/s]Evaluating:  61%|██████▏   | 192/313 [00:32<00:20,  5.94it/s]Evaluating:  62%|██████▏   | 193/313 [00:32<00:20,  5.96it/s]Evaluating:  62%|██████▏   | 194/313 [00:32<00:19,  5.99it/s]Evaluating:  62%|██████▏   | 195/313 [00:32<00:19,  5.95it/s]Evaluating:  63%|██████▎   | 196/313 [00:32<00:19,  5.93it/s]Evaluating:  63%|██████▎   | 197/313 [00:33<00:19,  5.92it/s]Evaluating:  63%|██████▎   | 198/313 [00:33<00:19,  5.93it/s]Evaluating:  64%|██████▎   | 199/313 [00:33<00:19,  5.95it/s]Evaluating:  64%|██████▍   | 200/313 [00:33<00:18,  5.96it/s]Evaluating:  64%|██████▍   | 201/313 [00:33<00:18,  5.92it/s]Evaluating:  65%|██████▍   | 202/313 [00:33<00:18,  5.90it/s]Evaluating:  65%|██████▍   | 203/313 [00:34<00:18,  5.91it/s]Evaluating:  65%|██████▌   | 204/313 [00:34<00:18,  5.95it/s]Evaluating:  65%|██████▌   | 205/313 [00:34<00:17,  6.01it/s]Evaluating:  66%|██████▌   | 206/313 [00:34<00:17,  6.06it/s]Evaluating:  66%|██████▌   | 207/313 [00:34<00:18,  5.70it/s]Evaluating:  66%|██████▋   | 208/313 [00:34<00:17,  5.88it/s]Evaluating:  67%|██████▋   | 209/313 [00:35<00:17,  6.01it/s]Evaluating:  67%|██████▋   | 210/313 [00:35<00:16,  6.12it/s]Evaluating:  67%|██████▋   | 211/313 [00:35<00:16,  6.19it/s]Evaluating:  68%|██████▊   | 212/313 [00:35<00:16,  6.21it/s]Evaluating:  68%|██████▊   | 213/313 [00:35<00:16,  6.21it/s]Evaluating:  68%|██████▊   | 214/313 [00:35<00:15,  6.23it/s]Evaluating:  69%|██████▊   | 215/313 [00:36<00:15,  6.26it/s]Evaluating:  69%|██████▉   | 216/313 [00:36<00:15,  6.25it/s]Evaluating:  69%|██████▉   | 217/313 [00:36<00:15,  6.27it/s]Evaluating:  70%|██████▉   | 218/313 [00:36<00:15,  6.29it/s]Evaluating:  70%|██████▉   | 219/313 [00:36<00:14,  6.31it/s]Evaluating:  70%|███████   | 220/313 [00:36<00:14,  6.32it/s]Evaluating:  71%|███████   | 221/313 [00:37<00:14,  6.29it/s]Evaluating:  71%|███████   | 222/313 [00:37<00:14,  6.28it/s]Evaluating:  71%|███████   | 223/313 [00:37<00:14,  6.27it/s]Evaluating:  72%|███████▏  | 224/313 [00:37<00:14,  6.24it/s]Evaluating:  72%|███████▏  | 225/313 [00:37<00:14,  6.18it/s]Evaluating:  72%|███████▏  | 226/313 [00:37<00:14,  6.15it/s]Evaluating:  73%|███████▎  | 227/313 [00:38<00:14,  6.14it/s]Evaluating:  73%|███████▎  | 228/313 [00:38<00:13,  6.14it/s]Evaluating:  73%|███████▎  | 229/313 [00:38<00:13,  6.16it/s]Evaluating:  73%|███████▎  | 230/313 [00:38<00:13,  6.18it/s]Evaluating:  74%|███████▍  | 231/313 [00:38<00:13,  6.21it/s]Evaluating:  74%|███████▍  | 232/313 [00:38<00:12,  6.26it/s]Evaluating:  74%|███████▍  | 233/313 [00:38<00:12,  6.29it/s]Evaluating:  75%|███████▍  | 234/313 [00:39<00:12,  6.31it/s]Evaluating:  75%|███████▌  | 235/313 [00:39<00:12,  6.32it/s]Evaluating:  75%|███████▌  | 236/313 [00:39<00:12,  6.33it/s]Evaluating:  76%|███████▌  | 237/313 [00:39<00:12,  6.33it/s]Evaluating:  76%|███████▌  | 238/313 [00:39<00:11,  6.31it/s]Evaluating:  76%|███████▋  | 239/313 [00:39<00:11,  6.26it/s]Evaluating:  77%|███████▋  | 240/313 [00:40<00:11,  6.22it/s]Evaluating:  77%|███████▋  | 241/313 [00:40<00:11,  6.18it/s]Evaluating:  77%|███████▋  | 242/313 [00:40<00:11,  6.14it/s]Evaluating:  78%|███████▊  | 243/313 [00:40<00:11,  6.08it/s]Evaluating:  78%|███████▊  | 244/313 [00:40<00:11,  6.00it/s]Evaluating:  78%|███████▊  | 245/313 [00:40<00:11,  5.95it/s]Evaluating:  79%|███████▊  | 246/313 [00:41<00:11,  5.91it/s]Evaluating:  79%|███████▉  | 247/313 [00:41<00:11,  5.89it/s]Evaluating:  79%|███████▉  | 248/313 [00:41<00:11,  5.89it/s]Evaluating:  80%|███████▉  | 249/313 [00:41<00:10,  5.87it/s]Evaluating:  80%|███████▉  | 250/313 [00:41<00:10,  5.85it/s]Evaluating:  80%|████████  | 251/313 [00:41<00:10,  5.84it/s]Evaluating:  81%|████████  | 252/313 [00:42<00:10,  5.85it/s]Evaluating:  81%|████████  | 253/313 [00:42<00:10,  5.87it/s]Evaluating:  81%|████████  | 254/313 [00:42<00:10,  5.89it/s]Evaluating:  81%|████████▏ | 255/313 [00:42<00:09,  5.87it/s]Evaluating:  82%|████████▏ | 256/313 [00:42<00:09,  5.85it/s]Evaluating:  82%|████████▏ | 257/313 [00:42<00:09,  5.84it/s]Evaluating:  82%|████████▏ | 258/313 [00:43<00:09,  5.83it/s]Evaluating:  83%|████████▎ | 259/313 [00:43<00:09,  5.82it/s]Evaluating:  83%|████████▎ | 260/313 [00:43<00:09,  5.82it/s]Evaluating:  83%|████████▎ | 261/313 [00:43<00:08,  5.81it/s]Evaluating:  84%|████████▎ | 262/313 [00:43<00:08,  5.81it/s]Evaluating:  84%|████████▍ | 263/313 [00:44<00:08,  5.81it/s]Evaluating:  84%|████████▍ | 264/313 [00:44<00:08,  5.80it/s]Evaluating:  85%|████████▍ | 265/313 [00:44<00:08,  5.80it/s]Evaluating:  85%|████████▍ | 266/313 [00:44<00:08,  5.80it/s]Evaluating:  85%|████████▌ | 267/313 [00:44<00:07,  5.82it/s]Evaluating:  86%|████████▌ | 268/313 [00:44<00:08,  5.50it/s]Evaluating:  86%|████████▌ | 269/313 [00:45<00:07,  5.59it/s]Evaluating:  86%|████████▋ | 270/313 [00:45<00:07,  5.65it/s]Evaluating:  87%|████████▋ | 271/313 [00:45<00:07,  5.69it/s]Evaluating:  87%|████████▋ | 272/313 [00:45<00:07,  5.74it/s]Evaluating:  87%|████████▋ | 273/313 [00:45<00:06,  5.81it/s]Evaluating:  88%|████████▊ | 274/313 [00:45<00:06,  5.86it/s]Evaluating:  88%|████████▊ | 275/313 [00:46<00:06,  5.87it/s]Evaluating:  88%|████████▊ | 276/313 [00:46<00:06,  5.87it/s]Evaluating:  88%|████████▊ | 277/313 [00:46<00:06,  5.87it/s]Evaluating:  89%|████████▉ | 278/313 [00:46<00:05,  5.88it/s]Evaluating:  89%|████████▉ | 279/313 [00:46<00:05,  5.90it/s]Evaluating:  89%|████████▉ | 280/313 [00:46<00:05,  5.90it/s]Evaluating:  90%|████████▉ | 281/313 [00:47<00:05,  5.87it/s]Evaluating:  90%|█████████ | 282/313 [00:47<00:05,  5.84it/s]Evaluating:  90%|█████████ | 283/313 [00:47<00:05,  5.82it/s]Evaluating:  91%|█████████ | 284/313 [00:47<00:04,  5.83it/s]Evaluating:  91%|█████████ | 285/313 [00:47<00:04,  5.84it/s]Evaluating:  91%|█████████▏| 286/313 [00:47<00:04,  5.84it/s]Evaluating:  92%|█████████▏| 287/313 [00:48<00:04,  5.87it/s]Evaluating:  92%|█████████▏| 288/313 [00:48<00:04,  5.89it/s]Evaluating:  92%|█████████▏| 289/313 [00:48<00:04,  5.93it/s]Evaluating:  93%|█████████▎| 290/313 [00:48<00:03,  5.98it/s]Evaluating:  93%|█████████▎| 291/313 [00:48<00:03,  5.97it/s]Evaluating:  93%|█████████▎| 292/313 [00:48<00:03,  6.00it/s]Evaluating:  94%|█████████▎| 293/313 [00:49<00:03,  5.97it/s]Evaluating:  94%|█████████▍| 294/313 [00:49<00:03,  5.91it/s]Evaluating:  94%|█████████▍| 295/313 [00:49<00:03,  5.93it/s]Evaluating:  95%|█████████▍| 296/313 [00:49<00:02,  5.95it/s]Evaluating:  95%|█████████▍| 297/313 [00:50<00:03,  4.09it/s]Evaluating:  95%|█████████▌| 298/313 [00:50<00:03,  4.46it/s]Evaluating:  96%|█████████▌| 299/313 [00:50<00:02,  4.82it/s]Evaluating:  96%|█████████▌| 300/313 [00:50<00:02,  5.04it/s]Evaluating:  96%|█████████▌| 301/313 [00:50<00:02,  5.20it/s]Evaluating:  96%|█████████▋| 302/313 [00:50<00:02,  5.34it/s]Evaluating:  97%|█████████▋| 303/313 [00:51<00:01,  5.48it/s]Evaluating:  97%|█████████▋| 304/313 [00:51<00:01,  5.59it/s]Evaluating:  97%|█████████▋| 305/313 [00:51<00:01,  5.68it/s]Evaluating:  98%|█████████▊| 306/313 [00:51<00:01,  5.76it/s]Evaluating:  98%|█████████▊| 307/313 [00:51<00:01,  5.83it/s]Evaluating:  98%|█████████▊| 308/313 [00:51<00:00,  5.88it/s]Evaluating:  99%|█████████▊| 309/313 [00:52<00:00,  5.92it/s]Evaluating:  99%|█████████▉| 310/313 [00:52<00:00,  5.94it/s]Evaluating:  99%|█████████▉| 311/313 [00:52<00:00,  5.96it/s]Evaluating: 100%|█████████▉| 312/313 [00:52<00:00,  5.91it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  6.44it/s]Evaluating: 100%|██████████| 313/313 [00:52<00:00,  5.93it/s]
10/14/2021 16:28:18 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/14/2021 16:28:18 - INFO - __main__ -     f1 = 0.7146766335179872
10/14/2021 16:28:18 - INFO - __main__ -     loss = 1.1539910480427666
10/14/2021 16:28:18 - INFO - __main__ -     precision = 0.6732474064759509
10/14/2021 16:28:18 - INFO - __main__ -     recall = 0.7615390086053624
10/14/2021 16:28:18 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:28:31 - INFO - __main__ -   Using lang2id = None
10/14/2021 16:28:31 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/14/2021 16:28:50 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/15/2021 19:28:51 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:28:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:28:51 - INFO - __main__ -   Seed = 1
10/15/2021 19:28:51 - INFO - root -   save model
10/15/2021 19:28:51 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:28:51 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:29:05 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:29:05 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/15/2021 19:29:05 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/
10/15/2021 19:29:05 - INFO - root -   Trying to decide if add adapter
10/15/2021 19:29:05 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s1/checkpoint-best/ner/pytorch_model_head.bin
10/15/2021 19:29:05 - INFO - root -   loading lang adpater en/wiki@ukp
10/15/2021 19:29:05 - INFO - __main__ -   Language = en
10/15/2021 19:29:05 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/15/2021 19:29:10 - INFO - __main__ -   Language adapter for bn not found, using en instead
10/15/2021 19:29:10 - INFO - __main__ -   Set active language adapter to en
10/15/2021 19:29:10 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/15/2021 19:29:10 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/15/2021 19:29:10 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:29:10 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.70it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.97it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  7.22it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.41it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.54it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.62it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.67it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.71it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:02,  7.73it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.73it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.73it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.71it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.73it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.73it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.74it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.74it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.75it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.74it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.74it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  6.89it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.12it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.29it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.40it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.50it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.57it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.61it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.65it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.70it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.71it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.72it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.72it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.75it/s]
10/15/2021 19:29:14 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/15/2021 19:29:14 - INFO - __main__ -     f1 = 0.4622093023255814
10/15/2021 19:29:14 - INFO - __main__ -     loss = 4.911349400877953
10/15/2021 19:29:14 - INFO - __main__ -     precision = 0.48923076923076925
10/15/2021 19:29:14 - INFO - __main__ -     recall = 0.4380165289256198
10/15/2021 19:29:14 - INFO - __main__ -   Language adapter for mr not found, using en instead
10/15/2021 19:29:14 - INFO - __main__ -   Set active language adapter to en
10/15/2021 19:29:14 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/15/2021 19:29:14 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/15/2021 19:29:14 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:29:14 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:03,  7.82it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.77it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.78it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.77it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.78it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.77it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.77it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.77it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:02,  7.76it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.73it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.74it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.75it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.76it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.77it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.76it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.76it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.75it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.72it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.72it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.73it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.72it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.73it/s]Evaluating:  72%|███████▏  | 23/32 [00:02<00:01,  7.74it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.75it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.71it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.70it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.71it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.72it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.72it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.69it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.71it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.91it/s]
10/15/2021 19:29:18 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/15/2021 19:29:18 - INFO - __main__ -     f1 = 0.40761575075724793
10/15/2021 19:29:18 - INFO - __main__ -     loss = 3.28140190243721
10/15/2021 19:29:18 - INFO - __main__ -     precision = 0.4498567335243553
10/15/2021 19:29:18 - INFO - __main__ -     recall = 0.372626582278481
10/15/2021 19:29:18 - INFO - __main__ -   Language adapter for ta not found, using en instead
10/15/2021 19:29:18 - INFO - __main__ -   Set active language adapter to en
10/15/2021 19:29:18 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/15/2021 19:29:19 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/15/2021 19:29:19 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:29:19 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:03,  7.82it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.75it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.77it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.77it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.76it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.76it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.75it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.72it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:02,  7.70it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.70it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.72it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.72it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.73it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.73it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.74it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.74it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.73it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.71it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.72it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.72it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.71it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.71it/s]Evaluating:  72%|███████▏  | 23/32 [00:02<00:01,  7.71it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.71it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.70it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.70it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.70it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.70it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.69it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.69it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.05it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.81it/s]
10/15/2021 19:29:23 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/15/2021 19:29:23 - INFO - __main__ -     f1 = 0.15297741273100618
10/15/2021 19:29:23 - INFO - __main__ -     loss = 4.66862840205431
10/15/2021 19:29:23 - INFO - __main__ -     precision = 0.21074964639321075
10/15/2021 19:29:23 - INFO - __main__ -     recall = 0.1200644641418211
10/15/2021 19:29:23 - INFO - __main__ -   Language adapter for is not found, using en instead
10/15/2021 19:29:23 - INFO - __main__ -   Set active language adapter to en
10/15/2021 19:29:23 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
10/15/2021 19:29:23 - INFO - __main__ -   ***** Running evaluation  in is *****
10/15/2021 19:29:23 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:29:23 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:03,  7.77it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.67it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.70it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.69it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.70it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.71it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.71it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.71it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:02,  7.71it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.70it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.70it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.70it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.68it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.66it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.68it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.69it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.70it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.70it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.71it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.71it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.70it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.69it/s]Evaluating:  72%|███████▏  | 23/32 [00:02<00:01,  7.69it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.69it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.69it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.69it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.68it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.63it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.64it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.64it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.65it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.85it/s]
10/15/2021 19:29:27 - INFO - __main__ -   ***** Evaluation result  in is *****
10/15/2021 19:29:27 - INFO - __main__ -     f1 = 0.6306445387881127
10/15/2021 19:29:27 - INFO - __main__ -     loss = 1.4931111615151167
10/15/2021 19:29:27 - INFO - __main__ -     precision = 0.5980966325036603
10/15/2021 19:29:27 - INFO - __main__ -     recall = 0.6669387755102041
10/15/2021 19:29:27 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:29:39 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:29:39 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:29:57 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/15/2021 19:29:59 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:29:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:29:59 - INFO - __main__ -   Seed = 2
10/15/2021 19:29:59 - INFO - root -   save model
10/15/2021 19:29:59 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:29:59 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:30:15 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:30:15 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/15/2021 19:30:15 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/
10/15/2021 19:30:15 - INFO - root -   Trying to decide if add adapter
10/15/2021 19:30:15 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s2/checkpoint-best/ner/pytorch_model_head.bin
10/15/2021 19:30:16 - INFO - root -   loading lang adpater en/wiki@ukp
10/15/2021 19:30:16 - INFO - __main__ -   Language = en
10/15/2021 19:30:16 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/15/2021 19:30:19 - INFO - __main__ -   Language adapter for bn not found, using en instead
10/15/2021 19:30:19 - INFO - __main__ -   Set active language adapter to en
10/15/2021 19:30:19 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/15/2021 19:30:19 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/15/2021 19:30:19 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:30:19 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.83it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.14it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.41it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.53it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.61it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.65it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.68it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.70it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:02,  7.70it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.67it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.69it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.70it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.71it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.71it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.71it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.70it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.68it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.66it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.65it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.65it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.66it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.63it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.65it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.07it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.47it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  6.79it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.02it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.19it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.33it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.44it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.52it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.60it/s]
10/15/2021 19:30:24 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/15/2021 19:30:24 - INFO - __main__ -     f1 = 0.54735883424408
10/15/2021 19:30:24 - INFO - __main__ -     loss = 4.181345410645008
10/15/2021 19:30:24 - INFO - __main__ -     precision = 0.5429087624209575
10/15/2021 19:30:24 - INFO - __main__ -     recall = 0.55188246097337
10/15/2021 19:30:24 - INFO - __main__ -   Language adapter for mr not found, using en instead
10/15/2021 19:30:24 - INFO - __main__ -   Set active language adapter to en
10/15/2021 19:30:24 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/15/2021 19:30:24 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/15/2021 19:30:24 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:30:24 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:03,  7.75it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.74it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.69it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.68it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.69it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.68it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.67it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.67it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:02,  7.68it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.67it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.68it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.66it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.68it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.68it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.67it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.68it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.66it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.62it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.63it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.62it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.62it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.60it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.62it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.63it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.66it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.67it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.65it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.65it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.62it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.62it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.63it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.82it/s]
10/15/2021 19:30:28 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/15/2021 19:30:28 - INFO - __main__ -     f1 = 0.474481658692185
10/15/2021 19:30:28 - INFO - __main__ -     loss = 2.8258582949638367
10/15/2021 19:30:28 - INFO - __main__ -     precision = 0.4782958199356913
10/15/2021 19:30:28 - INFO - __main__ -     recall = 0.47072784810126583
10/15/2021 19:30:28 - INFO - __main__ -   Language adapter for ta not found, using en instead
10/15/2021 19:30:28 - INFO - __main__ -   Set active language adapter to en
10/15/2021 19:30:28 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/15/2021 19:30:28 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/15/2021 19:30:28 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:30:28 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:03,  7.75it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.71it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.71it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.69it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.68it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.68it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.67it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.68it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.67it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.67it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.67it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.65it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.64it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.64it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.66it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.66it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.67it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.67it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.66it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.67it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.64it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.64it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.65it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.65it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.65it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.65it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.64it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.64it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.64it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.64it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.63it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.82it/s]
10/15/2021 19:30:32 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/15/2021 19:30:32 - INFO - __main__ -     f1 = 0.27313566936208444
10/15/2021 19:30:32 - INFO - __main__ -     loss = 3.8481214120984077
10/15/2021 19:30:32 - INFO - __main__ -     precision = 0.3086294416243655
10/15/2021 19:30:32 - INFO - __main__ -     recall = 0.24496373892022563
10/15/2021 19:30:32 - INFO - __main__ -   Language adapter for is not found, using en instead
10/15/2021 19:30:32 - INFO - __main__ -   Set active language adapter to en
10/15/2021 19:30:32 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_is_bert-base-multilingual-cased_128
10/15/2021 19:30:33 - INFO - __main__ -   ***** Running evaluation  in is *****
10/15/2021 19:30:33 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:30:33 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.63it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.63it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.37it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.47it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.54it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.58it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.62it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.62it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.62it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.61it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.61it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.60it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.61it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.62it/s]Evaluating:  47%|████▋     | 15/32 [00:01<00:02,  7.63it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.62it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:01,  7.61it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.61it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.62it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.62it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.62it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.62it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.63it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.63it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.63it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.63it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.61it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.60it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.60it/s]Evaluating:  94%|█████████▍| 30/32 [00:03<00:00,  7.59it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.52it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.76it/s]
10/15/2021 19:30:37 - INFO - __main__ -   ***** Evaluation result  in is *****
10/15/2021 19:30:37 - INFO - __main__ -     f1 = 0.6435185185185185
10/15/2021 19:30:37 - INFO - __main__ -     loss = 1.5105882696807384
10/15/2021 19:30:37 - INFO - __main__ -     precision = 0.6100950987564009
10/15/2021 19:30:37 - INFO - __main__ -     recall = 0.6808163265306122
10/15/2021 19:30:37 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:30:52 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:30:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

PyTorch version 1.9.0+cu102 available.
10/15/2021 19:31:03 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:31:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:31:03 - INFO - __main__ -   Seed = 12
10/15/2021 19:31:03 - INFO - root -   save model
10/15/2021 19:31:03 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:31:03 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:31:25 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:31:25 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/15/2021 19:31:25 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/
10/15/2021 19:31:25 - INFO - root -   Trying to decide if add adapter
10/15/2021 19:31:25 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/pytorch_model_head.bin
10/15/2021 19:31:26 - INFO - root -   loading lang adpater hi/wiki@ukp
10/15/2021 19:31:26 - INFO - __main__ -   Language = hi
10/15/2021 19:31:26 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/15/2021 19:31:27 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/15/2021 19:31:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:31:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:31:29 - INFO - __main__ -   Seed = 3
10/15/2021 19:31:29 - INFO - root -   save model
10/15/2021 19:31:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='bn,mr,ta,is', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:31:29 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
10/15/2021 19:31:30 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/15/2021 19:31:30 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:31:30 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/15/2021 19:31:30 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/15/2021 19:31:30 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:31:30 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.49it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.71it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.96it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.15it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.26it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.34it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.35it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.38it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.40it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.42it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.43it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.44it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.44it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.44it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.42it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.44it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.43it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.42it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.43it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.44it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.45it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.41it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.42it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.42it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.43it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.43it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.43it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  6.84it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.01it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.13it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.21it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.47it/s]
10/15/2021 19:31:34 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/15/2021 19:31:34 - INFO - __main__ -     f1 = 0.5278676734987414
10/15/2021 19:31:34 - INFO - __main__ -     loss = 0.5681666410528123
10/15/2021 19:31:34 - INFO - __main__ -     precision = 0.4838497033618985
10/15/2021 19:31:34 - INFO - __main__ -     recall = 0.5806962025316456
10/15/2021 19:31:34 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/15/2021 19:31:34 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:31:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/15/2021 19:31:34 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/15/2021 19:31:34 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:31:34 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.50it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.47it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.48it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.47it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.46it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.47it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.46it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.44it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.44it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.45it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.44it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.44it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.44it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.45it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.44it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.42it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.43it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.41it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.43it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.44it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.44it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.43it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.43it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.42it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.42it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.41it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.40it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.41it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.40it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.38it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.39it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.60it/s]
10/15/2021 19:31:39 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/15/2021 19:31:39 - INFO - __main__ -     f1 = 0.3344231667445119
10/15/2021 19:31:39 - INFO - __main__ -     loss = 1.3888210225850344
10/15/2021 19:31:39 - INFO - __main__ -     precision = 0.3403041825095057
10/15/2021 19:31:39 - INFO - __main__ -     recall = 0.3287419651056015
10/15/2021 19:31:39 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/15/2021 19:31:39 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:31:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/15/2021 19:31:39 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/15/2021 19:31:39 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:31:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.47it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.45it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.44it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.44it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.44it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.45it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.46it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.36it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.36it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.39it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.39it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.39it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.40it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.40it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.40it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.40it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.41it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.41it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.42it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.42it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.41it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.41it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.41it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.40it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.41it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.40it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.37it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.36it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.37it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.36it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.37it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.56it/s]
10/15/2021 19:31:43 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/15/2021 19:31:43 - INFO - __main__ -     f1 = 0.10427066001109261
10/15/2021 19:31:43 - INFO - __main__ -     loss = 1.4147008582949638
10/15/2021 19:31:43 - INFO - __main__ -     precision = 0.16725978647686832
10/15/2021 19:31:43 - INFO - __main__ -     recall = 0.0757453666398066
10/15/2021 19:31:43 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:31:44 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:31:44 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/15/2021 19:31:44 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/
10/15/2021 19:31:44 - INFO - root -   Trying to decide if add adapter
10/15/2021 19:31:44 - INFO - root -   loading task adapter
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 861, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_tag.py", line 534, in setup_adapter
    load_as=task_name,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 1180, in load_adapter
    **kwargs,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 391, in load
    **kwargs,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_utils.py", line 437, in resolve_adapter_path
    adapter_name_or_path, adapter_type, model_name, adapter_config=adapter_config, version=version, **kwargs
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_utils.py", line 367, in pull_from_hub
    raise EnvironmentError("No adapter with name '{}' was found in the adapter index.".format(specifier))
OSError: No adapter with name 'output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s3/checkpoint-best/ner/' was found in the adapter index.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:32:02 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:32:02 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:32:18 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/15/2021 19:32:20 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:32:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:32:20 - INFO - __main__ -   Seed = 22
10/15/2021 19:32:20 - INFO - root -   save model
10/15/2021 19:32:20 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=22, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:32:20 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:32:34 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:32:34 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/15/2021 19:32:34 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/
10/15/2021 19:32:34 - INFO - root -   Trying to decide if add adapter
10/15/2021 19:32:34 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s22/checkpoint-best/ner/pytorch_model_head.bin
10/15/2021 19:32:34 - INFO - root -   loading lang adpater hi/wiki@ukp
10/15/2021 19:32:34 - INFO - __main__ -   Language = hi
10/15/2021 19:32:34 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/15/2021 19:32:38 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/15/2021 19:32:38 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:32:38 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/15/2021 19:32:38 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/15/2021 19:32:38 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:32:38 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.58it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.76it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.96it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.17it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.30it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.37it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.41it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.45it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.47it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.47it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.45it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.45it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.46it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.48it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.50it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.49it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.49it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.48it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.48it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.48it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.48it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.46it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.46it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.46it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.46it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.45it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.46it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.46it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.47it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.47it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.46it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.58it/s]
10/15/2021 19:32:42 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/15/2021 19:32:42 - INFO - __main__ -     f1 = 0.4896354538956397
10/15/2021 19:32:42 - INFO - __main__ -     loss = 1.9978054463863373
10/15/2021 19:32:42 - INFO - __main__ -     precision = 0.44654498044328556
10/15/2021 19:32:42 - INFO - __main__ -     recall = 0.5419303797468354
10/15/2021 19:32:42 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/15/2021 19:32:42 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:32:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/15/2021 19:32:42 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/15/2021 19:32:42 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:32:42 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.57it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:03,  7.50it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.50it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.49it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.48it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.49it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.49it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.49it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.71it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.93it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.10it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.20it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.28it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.34it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.38it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.41it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.43it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.45it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.45it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.46it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.46it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.46it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.45it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.46it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.46it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.45it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.45it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.44it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.46it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.45it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.44it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.55it/s]
10/15/2021 19:32:47 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/15/2021 19:32:47 - INFO - __main__ -     f1 = 0.3197702249880326
10/15/2021 19:32:47 - INFO - __main__ -     loss = 5.357382453978062
10/15/2021 19:32:47 - INFO - __main__ -     precision = 0.334
10/15/2021 19:32:47 - INFO - __main__ -     recall = 0.30670339761248855
10/15/2021 19:32:47 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/15/2021 19:32:47 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:32:47 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/15/2021 19:32:47 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/15/2021 19:32:47 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:32:47 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.51it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.47it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.45it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.45it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.45it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.45it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.45it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.46it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.44it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.44it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.44it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.45it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.44it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.45it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.44it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.45it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.43it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.43it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.43it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.42it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.43it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.44it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.44it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.43it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.43it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.40it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.41it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.42it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.43it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.40it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.41it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.60it/s]
10/15/2021 19:32:51 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/15/2021 19:32:51 - INFO - __main__ -     f1 = 0.11822660098522168
10/15/2021 19:32:51 - INFO - __main__ -     loss = 4.587707541882992
10/15/2021 19:32:51 - INFO - __main__ -     precision = 0.1520912547528517
10/15/2021 19:32:51 - INFO - __main__ -     recall = 0.09669621273166801
10/15/2021 19:32:51 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:33:05 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:33:05 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:33:26 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/15/2021 19:33:28 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:33:28 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:33:28 - INFO - __main__ -   Seed = 32
10/15/2021 19:33:28 - INFO - root -   save model
10/15/2021 19:33:28 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=32, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:33:28 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:33:42 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:33:42 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/15/2021 19:33:42 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/
10/15/2021 19:33:42 - INFO - root -   Trying to decide if add adapter
10/15/2021 19:33:42 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s32/checkpoint-best/ner/pytorch_model_head.bin
10/15/2021 19:33:42 - INFO - root -   loading lang adpater hi/wiki@ukp
10/15/2021 19:33:42 - INFO - __main__ -   Language = hi
10/15/2021 19:33:42 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/15/2021 19:33:46 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/15/2021 19:33:46 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:33:46 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/15/2021 19:33:46 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/15/2021 19:33:46 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:33:46 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.57it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.90it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  7.15it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.28it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.36it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.41it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.43it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.46it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.47it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.45it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.47it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.46it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.47it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.48it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.47it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.47it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.47it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.47it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.48it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.45it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.47it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.43it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.44it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.44it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.45it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.43it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.43it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.44it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.45it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.44it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.45it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.58it/s]
10/15/2021 19:33:50 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/15/2021 19:33:50 - INFO - __main__ -     f1 = 0.4606851549755302
10/15/2021 19:33:50 - INFO - __main__ -     loss = 2.2967870384454727
10/15/2021 19:33:50 - INFO - __main__ -     precision = 0.3920044419766796
10/15/2021 19:33:50 - INFO - __main__ -     recall = 0.5585443037974683
10/15/2021 19:33:50 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/15/2021 19:33:50 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:33:50 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/15/2021 19:33:50 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/15/2021 19:33:50 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:33:50 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.53it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.48it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.49it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.49it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.48it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.47it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.48it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.47it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.48it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.46it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.45it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.45it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.46it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.45it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.44it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.44it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.45it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.44it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.45it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.45it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.45it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.44it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.44it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.42it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.42it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:01,  5.25it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  5.75it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  6.16it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.49it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.75it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.93it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.29it/s]
10/15/2021 19:33:55 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/15/2021 19:33:55 - INFO - __main__ -     f1 = 0.39519359145527366
10/15/2021 19:33:55 - INFO - __main__ -     loss = 4.545034572482109
10/15/2021 19:33:55 - INFO - __main__ -     precision = 0.38341968911917096
10/15/2021 19:33:55 - INFO - __main__ -     recall = 0.40771349862258954
10/15/2021 19:33:55 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/15/2021 19:33:55 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:33:55 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/15/2021 19:33:55 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/15/2021 19:33:55 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:33:55 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.47it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.45it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.45it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.45it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.44it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.45it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.45it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.43it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.44it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.43it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.45it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.43it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.42it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.42it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.43it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.43it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.43it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.42it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.42it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.43it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.42it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.43it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.41it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.41it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.41it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.42it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.42it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.42it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.42it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.42it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.43it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.59it/s]
10/15/2021 19:33:59 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/15/2021 19:33:59 - INFO - __main__ -     f1 = 0.21976074435090828
10/15/2021 19:33:59 - INFO - __main__ -     loss = 3.7088574543595314
10/15/2021 19:33:59 - INFO - __main__ -     precision = 0.2440944881889764
10/15/2021 19:33:59 - INFO - __main__ -     recall = 0.1998388396454472
10/15/2021 19:33:59 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

PyTorch version 1.9.0+cu102 available.
10/15/2021 19:34:09 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:34:09 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:34:09 - INFO - __main__ -   Seed = 12
10/15/2021 19:34:09 - INFO - root -   save model
10/15/2021 19:34:09 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=12, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s12/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:34:09 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:34:11 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:34:11 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 842, in main
    use_fast=False,
  File "/home/abhijeet/rohan/emea/src/transformers/tokenization_auto.py", line 306, in from_pretrained
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_auto.py", line 333, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_utils.py", line 389, in get_config_dict
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 955, in cached_path
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connection.py", line 366, in connect
    self._tunnel()
  File "/usr/lib/python3.7/http/client.py", line 922, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.7/http/client.py", line 271, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
KeyboardInterrupt
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/__init__.py", line 197, in <module>
    from torch._C import *  # noqa: F403
RuntimeError: KeyboardInterrupt: 
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 32, in <module>
    import torch
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/__init__.py", line 629, in <module>
    from .functional import *  # noqa: F403
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/functional.py", line 6, in <module>
    import torch.nn.functional as F
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/modules/__init__.py", line 2, in <module>
    from .linear import Identity, Linear, Bilinear, LazyLinear
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 6, in <module>
    from .. import functional as F
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/nn/functional.py", line 11, in <module>
    from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/_jit_internal.py", line 23, in <module>
    import torch.distributed.rpc
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/distributed/rpc/__init__.py", line 74, in <module>
    from .server_process_global_profiler import (
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/distributed/rpc/server_process_global_profiler.py", line 6, in <module>
    from torch.autograd.profiler import profile
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/autograd/__init__.py", line 17, in <module>
    from .gradcheck import gradcheck, gradgradcheck
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/autograd/gradcheck.py", line 3, in <module>
    import torch.testing
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/testing/__init__.py", line 3, in <module>
    from ._check_kernel_launches import *  # noqa: F403
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/torch/testing/_check_kernel_launches.py", line 37, in <module>
    """, flags=re.MULTILINE | re.VERBOSE)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/re.py", line 236, in compile
    return _compile(pattern, flags)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/re.py", line 288, in _compile
    p = sre_compile.compile(pattern, flags)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/sre_compile.py", line 764, in compile
    p = sre_parse.parse(p, flags)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/sre_parse.py", line 924, in parse
    p = _parse_sub(source, pattern, flags & SRE_FLAG_VERBOSE, 0)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/sre_parse.py", line 420, in _parse_sub
    not nested and not items))
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/sre_parse.py", line 730, in _parse
    p = _parse_sub(source, state, verbose, nested + 1)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/sre_parse.py", line 420, in _parse_sub
    not nested and not items))
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/sre_parse.py", line 640, in _parse
    item = subpattern[-1:]
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/sre_parse.py", line 166, in __getitem__
    return SubPattern(self.pattern, self.data[index])
KeyboardInterrupt
PyTorch version 1.9.0+cu102 available.
10/15/2021 19:34:41 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:34:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:34:41 - INFO - __main__ -   Seed = 42
10/15/2021 19:34:41 - INFO - root -   save model
10/15/2021 19:34:41 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_is//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:34:41 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 829, in main
    cache_dir=args.cache_dir,
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_auto.py", line 333, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/configuration_utils.py", line 389, in get_config_dict
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 955, in cached_path
    local_files_only=local_files_only,
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connection.py", line 366, in connect
    self._tunnel()
  File "/usr/lib/python3.7/http/client.py", line 922, in _tunnel
    (version, code, message) = response._read_status()
  File "/usr/lib/python3.7/http/client.py", line 271, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
  File "/usr/lib/python3.7/socket.py", line 589, in readinto
    return self._sock.recv_into(b)
KeyboardInterrupt
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 33, in <module>
    from seqeval.metrics import precision_score, recall_score, f1_score
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/__init__.py", line 1, in <module>
    from seqeval.metrics.sequence_labeling import (accuracy_score,
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py", line 14, in <module>
    from seqeval.metrics.v1 import SCORES, _precision_recall_fscore_support
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/v1.py", line 5, in <module>
    from sklearn.exceptions import UndefinedMetricWarning
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/__init__.py", line 82, in <module>
    from .base import clone
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/base.py", line 17, in <module>
    from .utils import _IS_32BIT
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/utils/__init__.py", line 23, in <module>
    from .class_weight import compute_class_weight, compute_sample_weight
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/utils/class_weight.py", line 7, in <module>
    from .validation import _deprecate_positional_args
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/utils/validation.py", line 26, in <module>
    from .fixes import _object_dtype_isnan, parse_version
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/sklearn/utils/fixes.py", line 20, in <module>
    import scipy.stats
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/stats/__init__.py", line 441, in <module>
    from .stats import *
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/stats/stats.py", line 43, in <module>
    from . import distributions
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/stats/distributions.py", line 8, in <module>
    from ._distn_infrastructure import (rv_discrete, rv_continuous, rv_frozen)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py", line 24, in <module>
    from scipy import optimize
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/optimize/__init__.py", line 401, in <module>
    from ._minimize import *
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/optimize/_minimize.py", line 27, in <module>
    from ._trustregion_constr import _minimize_trustregion_constr
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/optimize/_trustregion_constr/__init__.py", line 4, in <module>
    from .minimize_trustregion_constr import _minimize_trustregion_constr
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/optimize/_trustregion_constr/minimize_trustregion_constr.py", line 14, in <module>
    from .report import BasicReport, SQPReport, IPReport
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/optimize/_trustregion_constr/report.py", line 55, in <module>
    class IPReport(ReportBase):
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/scipy/optimize/_trustregion_constr/report.py", line 55, in IPReport
    class IPReport(ReportBase):
KeyboardInterrupt
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:34:43 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/15/2021 19:34:45 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:34:45 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:34:45 - INFO - __main__ -   Seed = 42
10/15/2021 19:34:45 - INFO - root -   save model
10/15/2021 19:34:45 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:34:45 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:35:08 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:35:08 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/15/2021 19:35:08 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/
10/15/2021 19:35:08 - INFO - root -   Trying to decide if add adapter
10/15/2021 19:35:08 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s42/checkpoint-best/ner/pytorch_model_head.bin
10/15/2021 19:35:08 - INFO - root -   loading lang adpater hi/wiki@ukp
10/15/2021 19:35:08 - INFO - __main__ -   Language = hi
10/15/2021 19:35:08 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/15/2021 19:35:11 - INFO - __main__ -   Language adapter for mr not found, using hi instead
10/15/2021 19:35:11 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:35:11 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/15/2021 19:35:12 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/15/2021 19:35:12 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:35:12 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.52it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.85it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  7.12it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.25it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.31it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.36it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.38it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.40it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.42it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.41it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.43it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.43it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.43it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.45it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.43it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.43it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.42it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.42it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.42it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.42it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.42it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.41it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.40it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.40it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.40it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.38it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.36it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.33it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.35it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.36it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.35it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.53it/s]
10/15/2021 19:35:16 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/15/2021 19:35:16 - INFO - __main__ -     f1 = 0.4320862824401753
10/15/2021 19:35:16 - INFO - __main__ -     loss = 2.3186681754887104
10/15/2021 19:35:16 - INFO - __main__ -     precision = 0.3763945977686436
10/15/2021 19:35:16 - INFO - __main__ -     recall = 0.507120253164557
10/15/2021 19:35:16 - INFO - __main__ -   Language adapter for bn not found, using hi instead
10/15/2021 19:35:16 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:35:16 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/15/2021 19:35:16 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/15/2021 19:35:16 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:35:16 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.50it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.42it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.44it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.44it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.44it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.44it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.45it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.43it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.44it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.44it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.42it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.41it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.42it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.41it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.41it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.41it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.42it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.41it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.41it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.40it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.39it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.38it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.38it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.38it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.39it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.38it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.39it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.38it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.37it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.37it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.37it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.57it/s]
10/15/2021 19:35:21 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/15/2021 19:35:21 - INFO - __main__ -     f1 = 0.4220779220779221
10/15/2021 19:35:21 - INFO - __main__ -     loss = 4.491160623729229
10/15/2021 19:35:21 - INFO - __main__ -     precision = 0.42642924086223055
10/15/2021 19:35:21 - INFO - __main__ -     recall = 0.41781450872359965
10/15/2021 19:35:21 - INFO - __main__ -   Language adapter for ta not found, using hi instead
10/15/2021 19:35:21 - INFO - __main__ -   Set active language adapter to hi
10/15/2021 19:35:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/15/2021 19:35:21 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/15/2021 19:35:21 - INFO - __main__ -     Num examples = 1000
10/15/2021 19:35:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  7.48it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  7.43it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:03,  7.41it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:03,  7.41it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:03,  7.41it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:03,  7.41it/s]Evaluating:  22%|██▏       | 7/32 [00:00<00:03,  7.40it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  7.39it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  7.41it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:02,  7.40it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:02,  7.39it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:02,  7.39it/s]Evaluating:  41%|████      | 13/32 [00:01<00:02,  7.38it/s]Evaluating:  44%|████▍     | 14/32 [00:01<00:02,  7.39it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  7.39it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  7.39it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  7.38it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:01,  7.38it/s]Evaluating:  59%|█████▉    | 19/32 [00:02<00:01,  7.38it/s]Evaluating:  62%|██████▎   | 20/32 [00:02<00:01,  7.37it/s]Evaluating:  66%|██████▌   | 21/32 [00:02<00:01,  7.38it/s]Evaluating:  69%|██████▉   | 22/32 [00:02<00:01,  7.38it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  7.37it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  7.36it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:00,  7.38it/s]Evaluating:  81%|████████▏ | 26/32 [00:03<00:00,  7.38it/s]Evaluating:  84%|████████▍ | 27/32 [00:03<00:00,  7.37it/s]Evaluating:  88%|████████▊ | 28/32 [00:03<00:00,  7.37it/s]Evaluating:  91%|█████████ | 29/32 [00:03<00:00,  7.37it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  7.36it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  7.37it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.55it/s]
10/15/2021 19:35:25 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/15/2021 19:35:25 - INFO - __main__ -     f1 = 0.21067548417572035
10/15/2021 19:35:25 - INFO - __main__ -     loss = 4.025712043046951
10/15/2021 19:35:25 - INFO - __main__ -     precision = 0.2545662100456621
10/15/2021 19:35:25 - INFO - __main__ -     recall = 0.17969379532634971
10/15/2021 19:35:25 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:35:37 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:35:37 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:35:53 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/15/2021 19:35:55 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:35:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/15/2021 19:35:55 - INFO - __main__ -   Seed = 52
10/15/2021 19:35:55 - INFO - root -   save model
10/15/2021 19:35:55 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=52, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_hi//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight=None, calc_weight_step=0, predict_save_prefix='')
10/15/2021 19:35:55 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/15/2021 19:36:13 - INFO - __main__ -   Using lang2id = None
10/15/2021 19:36:13 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/15/2021 19:36:13 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/
10/15/2021 19:36:13 - INFO - root -   Trying to decide if add adapter
10/15/2021 19:36:13 - INFO - root -   loading task adapter
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s52/checkpoint-best/ner/pytorch_model_head.bin
10/15/2021 19:36:14 - INFO - root -   loading lang adpater hi/wiki@ukp
10/15/2021 19:36:14 - INFO - __main__ -   Language = hi
10/15/2021 19:36:14 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Traceback (most recent call last):
  File "third_party/my_run_tag.py", line 1090, in <module>
    main()
  File "third_party/my_run_tag.py", line 861, in main
    model, lang_adapter_names, task_name = setup_adapter(args, adapter_args, model, load_adapter=load_adapter, load_lang_adapter=load_lang_adapter)
  File "third_party/my_run_tag.py", line 563, in setup_adapter
    load_as=language,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 1180, in load_adapter
    **kwargs,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 999, in load_adapter
    load_dir, load_name = loader.load(adapter_name_or_path, config, version, model_name, load_as, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_model_mixin.py", line 391, in load
    **kwargs,
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_utils.py", line 437, in resolve_adapter_path
    adapter_name_or_path, adapter_type, model_name, adapter_config=adapter_config, version=version, **kwargs
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_utils.py", line 381, in pull_from_hub
    download_path = download_cached(file_entry["url"], checksum=checksum, checksum_algo=checksum_algo, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/adapter_utils.py", line 137, in download_cached
    output_path = get_from_cache(url, cache_dir=cache_dir, **kwargs)
  File "/home/abhijeet/rohan/emea/src/transformers/file_utils.py", line 1075, in get_from_cache
    r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 102, in head
    return request('head', url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 542, in request
    resp = self.send(prep, **send_kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/sessions.py", line 655, in send
    r = adapter.send(request, **kwargs)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/requests/adapters.py", line 449, in send
    timeout=timeout
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 696, in urlopen
    self._prepare_proxy(conn)
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connectionpool.py", line 964, in _prepare_proxy
    conn.connect()
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/connection.py", line 421, in connect
    tls_in_tls=tls_in_tls,
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/util/ssl_.py", line 450, in ssl_wrap_socket
    sock, context, tls_in_tls, server_hostname=server_hostname
  File "/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/usr/lib/python3.7/ssl.py", line 423, in wrap_socket
    session=session
  File "/usr/lib/python3.7/ssl.py", line 870, in _create
    self.do_handshake()
  File "/usr/lib/python3.7/ssl.py", line 1139, in do_handshake
    self._sslobj.do_handshake()
KeyboardInterrupt
PyTorch version 1.9.0+cu102 available.
10/16/2021 18:20:50 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=4, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s4/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/16/2021 18:20:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/16/2021 18:20:50 - INFO - __main__ -   Seed = 4
10/16/2021 18:20:50 - INFO - root -   save model
10/16/2021 18:20:50 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=100.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=4, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bn,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//panx/my-bert-base-multilingual-cased-MaxLen128_ner_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='ner', predict_task_adapter='output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s4/checkpoint-best/ner/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/16/2021 18:20:50 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/16/2021 18:21:03 - INFO - __main__ -   Using lang2id = None
10/16/2021 18:21:03 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/16/2021 18:21:03 - INFO - __main__ -   Task Adapter will be loaded from this path output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s4/checkpoint-best/ner/
10/16/2021 18:21:03 - INFO - root -   Trying to decide if add adapter
10/16/2021 18:21:03 - INFO - root -   loading task adapter
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s4/checkpoint-best/ner/adapter_config.json
Adding adapter 'ner' of type 'text_task'.
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s4/checkpoint-best/ner/pytorch_adapter.bin
Loading module configuration from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s4/checkpoint-best/ner/head_config.json
Loading module weights from output/panx/my-bert-base-multilingual-cased-LR1e-4-epoch100-MaxLen128-TrainLangen_en_s4/checkpoint-best/ner/pytorch_model_head.bin
10/16/2021 18:21:03 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/16/2021 18:21:03 - INFO - __main__ -   Language = en
10/16/2021 18:21:03 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/16/2021 18:21:04 - INFO - __main__ -   Language = hi
10/16/2021 18:21:04 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/16/2021 18:21:05 - INFO - __main__ -   Language = ar
10/16/2021 18:21:05 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/16/2021 18:21:09 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/16/2021 18:21:09 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/16/2021 18:21:09 - INFO - __main__ -     Num examples = 1000
10/16/2021 18:21:09 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:05,  5.46it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:05,  5.79it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.02it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.14it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.20it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.25it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.27it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.29it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.30it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.31it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.30it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.30it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.29it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.30it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.30it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.31it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.31it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.31it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.29it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.29it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.29it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.28it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.27it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.29it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.29it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.30it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.30it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.30it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.28it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.29it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.29it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.40it/s]
10/16/2021 18:21:14 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/16/2021 18:21:14 - INFO - __main__ -     f1 = 0.5276116778051355
10/16/2021 18:21:14 - INFO - __main__ -     loss = 2.248550545424223
10/16/2021 18:21:14 - INFO - __main__ -     precision = 0.4749841671944269
10/16/2021 18:21:14 - INFO - __main__ -     recall = 0.5933544303797469
10/16/2021 18:21:14 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_bn_bert-base-multilingual-cased_128
10/16/2021 18:21:15 - INFO - __main__ -   ***** Running evaluation  in bn *****
10/16/2021 18:21:15 - INFO - __main__ -     Num examples = 1000
10/16/2021 18:21:15 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.34it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.31it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.31it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.31it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.31it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.31it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.31it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.31it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.31it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.30it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.29it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.30it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.30it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.30it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.30it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.30it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.29it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.28it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.28it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.28it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.29it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.28it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.27it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.27it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.27it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.27it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.27it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.27it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.27it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.27it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.27it/s]Evaluating: 100%|██████████| 32/32 [00:04<00:00,  6.43it/s]
10/16/2021 18:21:20 - INFO - __main__ -   ***** Evaluation result  in bn *****
10/16/2021 18:21:20 - INFO - __main__ -     f1 = 0.5168641750227894
10/16/2021 18:21:20 - INFO - __main__ -     loss = 4.126587770879269
10/16/2021 18:21:20 - INFO - __main__ -     precision = 0.5131221719457013
10/16/2021 18:21:20 - INFO - __main__ -     recall = 0.5206611570247934
10/16/2021 18:21:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//panx/panx_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/16/2021 18:21:20 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/16/2021 18:21:20 - INFO - __main__ -     Num examples = 1000
10/16/2021 18:21:20 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/32 [00:00<00:04,  6.31it/s]Evaluating:   6%|▋         | 2/32 [00:00<00:04,  6.30it/s]Evaluating:   9%|▉         | 3/32 [00:00<00:04,  6.28it/s]Evaluating:  12%|█▎        | 4/32 [00:00<00:04,  6.28it/s]Evaluating:  16%|█▌        | 5/32 [00:00<00:04,  6.29it/s]Evaluating:  19%|█▉        | 6/32 [00:00<00:04,  6.28it/s]Evaluating:  22%|██▏       | 7/32 [00:01<00:03,  6.27it/s]Evaluating:  25%|██▌       | 8/32 [00:01<00:03,  6.27it/s]Evaluating:  28%|██▊       | 9/32 [00:01<00:03,  6.27it/s]Evaluating:  31%|███▏      | 10/32 [00:01<00:03,  6.27it/s]Evaluating:  34%|███▍      | 11/32 [00:01<00:03,  6.28it/s]Evaluating:  38%|███▊      | 12/32 [00:01<00:03,  6.28it/s]Evaluating:  41%|████      | 13/32 [00:02<00:03,  6.27it/s]Evaluating:  44%|████▍     | 14/32 [00:02<00:02,  6.27it/s]Evaluating:  47%|████▋     | 15/32 [00:02<00:02,  6.27it/s]Evaluating:  50%|█████     | 16/32 [00:02<00:02,  6.27it/s]Evaluating:  53%|█████▎    | 17/32 [00:02<00:02,  6.27it/s]Evaluating:  56%|█████▋    | 18/32 [00:02<00:02,  6.26it/s]Evaluating:  59%|█████▉    | 19/32 [00:03<00:02,  6.27it/s]Evaluating:  62%|██████▎   | 20/32 [00:03<00:01,  6.24it/s]Evaluating:  66%|██████▌   | 21/32 [00:03<00:01,  6.24it/s]Evaluating:  69%|██████▉   | 22/32 [00:03<00:01,  6.23it/s]Evaluating:  72%|███████▏  | 23/32 [00:03<00:01,  6.23it/s]Evaluating:  75%|███████▌  | 24/32 [00:03<00:01,  6.24it/s]Evaluating:  78%|███████▊  | 25/32 [00:03<00:01,  6.25it/s]Evaluating:  81%|████████▏ | 26/32 [00:04<00:00,  6.25it/s]Evaluating:  84%|████████▍ | 27/32 [00:04<00:00,  6.26it/s]Evaluating:  88%|████████▊ | 28/32 [00:04<00:00,  6.25it/s]Evaluating:  91%|█████████ | 29/32 [00:04<00:00,  6.23it/s]Evaluating:  94%|█████████▍| 30/32 [00:04<00:00,  6.23it/s]Evaluating:  97%|█████████▋| 31/32 [00:04<00:00,  6.24it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.23it/s]Evaluating: 100%|██████████| 32/32 [00:05<00:00,  6.26it/s]
10/16/2021 18:21:25 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/16/2021 18:21:25 - INFO - __main__ -     f1 = 0.2744776730848013
10/16/2021 18:21:25 - INFO - __main__ -     loss = 3.5885495468974113
10/16/2021 18:21:25 - INFO - __main__ -     precision = 0.2791666666666667
10/16/2021 18:21:25 - INFO - __main__ -     recall = 0.26994359387590655
10/16/2021 18:21:25 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/16/2021 18:21:39 - INFO - __main__ -   Using lang2id = None
10/16/2021 18:21:39 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/16/2021 18:21:56 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/18/2021 15:17:30 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:17:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:17:30 - INFO - __main__ -   Seed = 1
10/18/2021 15:17:30 - INFO - root -   save model
10/18/2021 15:17:30 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:17:30 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:18:09 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:18:09 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:18:09 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
10/18/2021 15:18:09 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:18:09 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:18:09 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/18/2021 15:18:09 - INFO - __main__ -   Language = en
10/18/2021 15:18:09 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:18:11 - INFO - __main__ -   Language = hi
10/18/2021 15:18:11 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/18/2021 15:18:12 - INFO - __main__ -   Language = ar
10/18/2021 15:18:12 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/18/2021 15:18:20 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/18/2021 15:18:20 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/18/2021 15:18:20 - INFO - __main__ -     Num examples = 47
10/18/2021 15:18:20 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]Evaluating:  50%|█████     | 1/2 [00:00<00:00,  2.78it/s]Evaluating: 100%|██████████| 2/2 [00:00<00:00,  3.62it/s]Evaluating: 100%|██████████| 2/2 [00:00<00:00,  3.46it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:18:21 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/18/2021 15:18:21 - INFO - __main__ -     f1 = 0.615146831530139
10/18/2021 15:18:21 - INFO - __main__ -     loss = 1.1531794667243958
10/18/2021 15:18:21 - INFO - __main__ -     precision = 0.6337579617834395
10/18/2021 15:18:21 - INFO - __main__ -     recall = 0.5975975975975976
10/18/2021 15:18:21 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
10/18/2021 15:18:21 - INFO - __main__ -   ***** Running evaluation  in bho *****
10/18/2021 15:18:21 - INFO - __main__ -     Num examples = 361
10/18/2021 15:18:21 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]Evaluating:   8%|▊         | 1/12 [00:00<00:05,  2.02it/s]Evaluating:  17%|█▋        | 2/12 [00:01<00:05,  1.88it/s]Evaluating:  25%|██▌       | 3/12 [00:01<00:04,  1.83it/s]Evaluating:  33%|███▎      | 4/12 [00:02<00:04,  1.80it/s]Evaluating:  42%|████▏     | 5/12 [00:02<00:03,  1.78it/s]Evaluating:  50%|█████     | 6/12 [00:03<00:03,  1.77it/s]Evaluating:  58%|█████▊    | 7/12 [00:03<00:02,  1.78it/s]Evaluating:  67%|██████▋   | 8/12 [00:04<00:02,  1.77it/s]Evaluating:  75%|███████▌  | 9/12 [00:05<00:01,  1.77it/s]Evaluating:  83%|████████▎ | 10/12 [00:05<00:01,  1.77it/s]Evaluating:  92%|█████████▏| 11/12 [00:06<00:00,  1.76it/s]Evaluating: 100%|██████████| 12/12 [00:06<00:00,  1.85it/s]Evaluating: 100%|██████████| 12/12 [00:06<00:00,  1.81it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:18:27 - INFO - __main__ -   ***** Evaluation result  in bho *****
10/18/2021 15:18:27 - INFO - __main__ -     f1 = 0.4694160457459713
10/18/2021 15:18:27 - INFO - __main__ -     loss = 2.3205223381519318
10/18/2021 15:18:27 - INFO - __main__ -     precision = 0.4910277324632953
10/18/2021 15:18:27 - INFO - __main__ -     recall = 0.4496265560165975
10/18/2021 15:18:27 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/18/2021 15:18:28 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/18/2021 15:18:28 - INFO - __main__ -     Num examples = 656
10/18/2021 15:18:28 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]Evaluating:   5%|▍         | 1/21 [00:00<00:11,  1.79it/s]Evaluating:  10%|▉         | 2/21 [00:01<00:10,  1.77it/s]Evaluating:  14%|█▍        | 3/21 [00:01<00:10,  1.78it/s]Evaluating:  19%|█▉        | 4/21 [00:02<00:09,  1.77it/s]Evaluating:  24%|██▍       | 5/21 [00:02<00:09,  1.76it/s]Evaluating:  29%|██▊       | 6/21 [00:03<00:08,  1.78it/s]Evaluating:  33%|███▎      | 7/21 [00:03<00:07,  1.77it/s]Evaluating:  38%|███▊      | 8/21 [00:04<00:07,  1.78it/s]Evaluating:  43%|████▎     | 9/21 [00:05<00:06,  1.78it/s]Evaluating:  48%|████▊     | 10/21 [00:05<00:06,  1.77it/s]Evaluating:  52%|█████▏    | 11/21 [00:06<00:05,  1.78it/s]Evaluating:  57%|█████▋    | 12/21 [00:06<00:05,  1.77it/s]Evaluating:  62%|██████▏   | 13/21 [00:07<00:04,  1.91it/s]Evaluating:  67%|██████▋   | 14/21 [00:07<00:03,  2.08it/s]Evaluating:  71%|███████▏  | 15/21 [00:08<00:03,  1.99it/s]Evaluating:  76%|███████▌  | 16/21 [00:08<00:02,  1.92it/s]Evaluating:  81%|████████  | 17/21 [00:09<00:02,  1.87it/s]Evaluating:  86%|████████▌ | 18/21 [00:09<00:01,  1.84it/s]Evaluating:  90%|█████████ | 19/21 [00:10<00:01,  1.81it/s]Evaluating:  95%|█████████▌| 20/21 [00:10<00:00,  1.80it/s]Evaluating: 100%|██████████| 21/21 [00:11<00:00,  1.98it/s]Evaluating: 100%|██████████| 21/21 [00:11<00:00,  1.85it/s]
10/18/2021 15:18:39 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/18/2021 15:18:39 - INFO - __main__ -     f1 = 0.5977750971719609
10/18/2021 15:18:39 - INFO - __main__ -     loss = 1.1879316029094515
10/18/2021 15:18:39 - INFO - __main__ -     precision = 0.6258770698849284
10/18/2021 15:18:39 - INFO - __main__ -     recall = 0.5720882503848127
10/18/2021 15:18:39 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:18:51 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:18:51 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:19:15 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/18/2021 15:19:17 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:19:17 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:19:17 - INFO - __main__ -   Seed = 2
10/18/2021 15:19:17 - INFO - root -   save model
10/18/2021 15:19:17 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:19:17 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

PyTorch version 1.9.0+cu102 available.
10/18/2021 15:19:32 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:19:32 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:19:32 - INFO - __main__ -   Seed = 1
10/18/2021 15:19:32 - INFO - root -   save model
10/18/2021 15:19:32 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:19:32 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:20:01 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:20:01 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:20:01 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
10/18/2021 15:20:01 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:20:01 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:20:02 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/18/2021 15:20:02 - INFO - __main__ -   Language = en
10/18/2021 15:20:02 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:20:02 - INFO - __main__ -   Language = hi
10/18/2021 15:20:02 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/18/2021 15:20:03 - INFO - __main__ -   Language = ar
10/18/2021 15:20:03 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

10/18/2021 15:20:07 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/18/2021 15:20:07 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/18/2021 15:20:07 - INFO - __main__ -     Num examples = 47
10/18/2021 15:20:07 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]Evaluating:  50%|█████     | 1/2 [00:00<00:00,  5.48it/s]Evaluating: 100%|██████████| 2/2 [00:00<00:00,  7.38it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:20:08 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/18/2021 15:20:08 - INFO - __main__ -     f1 = 0.6367851622874806
10/18/2021 15:20:08 - INFO - __main__ -     loss = 1.166869044303894
10/18/2021 15:20:08 - INFO - __main__ -     precision = 0.6560509554140127
10/18/2021 15:20:08 - INFO - __main__ -     recall = 0.6186186186186187
10/18/2021 15:20:08 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
10/18/2021 15:20:08 - INFO - __main__ -   ***** Running evaluation  in bho *****
10/18/2021 15:20:08 - INFO - __main__ -     Num examples = 361
10/18/2021 15:20:08 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]Evaluating:   8%|▊         | 1/12 [00:00<00:01,  6.17it/s]Evaluating:  17%|█▋        | 2/12 [00:00<00:01,  6.16it/s]Evaluating:  25%|██▌       | 3/12 [00:00<00:01,  6.11it/s]Evaluating:  33%|███▎      | 4/12 [00:00<00:01,  6.13it/s]Evaluating:  42%|████▏     | 5/12 [00:00<00:01,  6.14it/s]Evaluating:  50%|█████     | 6/12 [00:00<00:00,  6.14it/s]Evaluating:  58%|█████▊    | 7/12 [00:01<00:00,  6.13it/s]Evaluating:  67%|██████▋   | 8/12 [00:01<00:00,  6.08it/s]Evaluating:  75%|███████▌  | 9/12 [00:01<00:00,  6.09it/s]Evaluating:  83%|████████▎ | 10/12 [00:01<00:00,  6.09it/s]Evaluating:  92%|█████████▏| 11/12 [00:01<00:00,  6.09it/s]Evaluating: 100%|██████████| 12/12 [00:01<00:00,  6.47it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:20:10 - INFO - __main__ -   ***** Evaluation result  in bho *****
10/18/2021 15:20:10 - INFO - __main__ -     f1 = 0.42363762582436654
10/18/2021 15:20:10 - INFO - __main__ -     loss = 2.6389107604821525
10/18/2021 15:20:10 - INFO - __main__ -     precision = 0.44389889070740135
10/18/2021 15:20:10 - INFO - __main__ -     recall = 0.40514522821576765
10/18/2021 15:20:10 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/18/2021 15:20:10 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/18/2021 15:20:10 - INFO - __main__ -     Num examples = 656
10/18/2021 15:20:10 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]Evaluating:   5%|▍         | 1/21 [00:00<00:03,  6.19it/s]Evaluating:  10%|▉         | 2/21 [00:00<00:03,  6.14it/s]Evaluating:  14%|█▍        | 3/21 [00:00<00:02,  6.15it/s]Evaluating:  19%|█▉        | 4/21 [00:00<00:02,  6.14it/s]Evaluating:  24%|██▍       | 5/21 [00:00<00:02,  6.13it/s]Evaluating:  29%|██▊       | 6/21 [00:00<00:02,  6.13it/s]Evaluating:  33%|███▎      | 7/21 [00:01<00:02,  6.12it/s]Evaluating:  38%|███▊      | 8/21 [00:01<00:02,  5.45it/s]Evaluating:  43%|████▎     | 9/21 [00:01<00:02,  5.64it/s]Evaluating:  48%|████▊     | 10/21 [00:01<00:01,  5.77it/s]Evaluating:  52%|█████▏    | 11/21 [00:01<00:01,  5.87it/s]Evaluating:  57%|█████▋    | 12/21 [00:02<00:01,  5.94it/s]Evaluating:  62%|██████▏   | 13/21 [00:02<00:01,  5.98it/s]Evaluating:  67%|██████▋   | 14/21 [00:02<00:01,  6.01it/s]Evaluating:  71%|███████▏  | 15/21 [00:02<00:00,  6.03it/s]Evaluating:  76%|███████▌  | 16/21 [00:02<00:00,  6.06it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  81%|████████  | 17/21 [00:02<00:00,  6.02it/s]Evaluating:  86%|████████▌ | 18/21 [00:03<00:00,  6.02it/s]10/18/2021 15:20:13 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:20:13 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:20:13 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
10/18/2021 15:20:13 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:20:13 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:20:13 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/18/2021 15:20:13 - INFO - __main__ -   Language = en
10/18/2021 15:20:13 - INFO - __main__ -   Adapter Name = en/wiki@ukp
Evaluating:  90%|█████████ | 19/21 [00:03<00:00,  6.03it/s]No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Evaluating:  95%|█████████▌| 20/21 [00:03<00:00,  6.05it/s]Evaluating: 100%|██████████| 21/21 [00:03<00:00,  6.12it/s]
10/18/2021 15:20:13 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/18/2021 15:20:13 - INFO - __main__ -     f1 = 0.6095364238410597
10/18/2021 15:20:13 - INFO - __main__ -     loss = 1.2234021595546178
10/18/2021 15:20:13 - INFO - __main__ -     precision = 0.630065717415115
10/18/2021 15:20:13 - INFO - __main__ -     recall = 0.590302719343253
10/18/2021 15:20:13 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:20:14 - INFO - __main__ -   Language = is
10/18/2021 15:20:14 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/18/2021 15:20:16 - INFO - __main__ -   Language = de
10/18/2021 15:20:16 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
10/18/2021 15:20:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/18/2021 15:20:22 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/18/2021 15:20:22 - INFO - __main__ -     Num examples = 1516
10/18/2021 15:20:22 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|▏         | 1/48 [00:00<00:07,  5.94it/s]Evaluating:   4%|▍         | 2/48 [00:00<00:07,  5.96it/s]Evaluating:   6%|▋         | 3/48 [00:00<00:07,  5.99it/s]Evaluating:   8%|▊         | 4/48 [00:00<00:07,  5.98it/s]Evaluating:  10%|█         | 5/48 [00:00<00:07,  5.99it/s]Evaluating:  12%|█▎        | 6/48 [00:01<00:07,  5.98it/s]Evaluating:  15%|█▍        | 7/48 [00:01<00:06,  5.97it/s]Evaluating:  17%|█▋        | 8/48 [00:01<00:06,  5.99it/s]Evaluating:  19%|█▉        | 9/48 [00:01<00:06,  5.98it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  21%|██        | 10/48 [00:01<00:06,  5.98it/s]Evaluating:  23%|██▎       | 11/48 [00:01<00:06,  5.97it/s]Evaluating:  25%|██▌       | 12/48 [00:02<00:06,  5.97it/s]Evaluating:  27%|██▋       | 13/48 [00:02<00:05,  5.97it/s]Evaluating:  29%|██▉       | 14/48 [00:02<00:05,  5.96it/s]Evaluating:  31%|███▏      | 15/48 [00:02<00:05,  5.96it/s]Evaluating:  33%|███▎      | 16/48 [00:02<00:05,  5.95it/s]Evaluating:  35%|███▌      | 17/48 [00:02<00:05,  5.96it/s]Evaluating:  38%|███▊      | 18/48 [00:03<00:05,  5.96it/s]Evaluating:  40%|███▉      | 19/48 [00:03<00:04,  5.96it/s]Evaluating:  42%|████▏     | 20/48 [00:03<00:04,  5.96it/s]Evaluating:  44%|████▍     | 21/48 [00:03<00:04,  5.96it/s]Evaluating:  46%|████▌     | 22/48 [00:03<00:04,  5.90it/s]Evaluating:  48%|████▊     | 23/48 [00:03<00:04,  5.92it/s]Evaluating:  50%|█████     | 24/48 [00:04<00:04,  5.92it/s]Evaluating:  52%|█████▏    | 25/48 [00:04<00:03,  5.91it/s]Evaluating:  54%|█████▍    | 26/48 [00:04<00:03,  5.91it/s]Evaluating:  56%|█████▋    | 27/48 [00:04<00:03,  5.91it/s]Evaluating:  58%|█████▊    | 28/48 [00:04<00:03,  5.90it/s]Evaluating:  60%|██████    | 29/48 [00:04<00:03,  5.89it/s]Evaluating:  62%|██████▎   | 30/48 [00:05<00:03,  5.88it/s]Evaluating:  65%|██████▍   | 31/48 [00:05<00:02,  5.88it/s]Evaluating:  67%|██████▋   | 32/48 [00:05<00:02,  5.85it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  69%|██████▉   | 33/48 [00:05<00:02,  5.84it/s]Evaluating:  71%|███████   | 34/48 [00:05<00:02,  5.85it/s]Evaluating:  73%|███████▎  | 35/48 [00:05<00:02,  5.81it/s]Evaluating:  75%|███████▌  | 36/48 [00:06<00:02,  5.73it/s]Evaluating:  77%|███████▋  | 37/48 [00:06<00:01,  5.72it/s]Evaluating:  79%|███████▉  | 38/48 [00:06<00:01,  5.73it/s]Evaluating:  81%|████████▏ | 39/48 [00:06<00:01,  5.74it/s]Evaluating:  83%|████████▎ | 40/48 [00:06<00:01,  5.74it/s]Evaluating:  85%|████████▌ | 41/48 [00:06<00:01,  5.76it/s]Evaluating:  88%|████████▊ | 42/48 [00:07<00:01,  5.76it/s]Evaluating:  90%|████████▉ | 43/48 [00:07<00:00,  5.76it/s]Evaluating:  92%|█████████▏| 44/48 [00:07<00:00,  5.76it/s]Evaluating:  94%|█████████▍| 45/48 [00:07<00:00,  5.76it/s]Evaluating:  96%|█████████▌| 46/48 [00:07<00:00,  5.77it/s]Evaluating:  98%|█████████▊| 47/48 [00:07<00:00,  5.78it/s]Evaluating: 100%|██████████| 48/48 [00:08<00:00,  5.94it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:20:30 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/18/2021 15:20:30 - INFO - __main__ -     f1 = 0.7545962784746753
10/18/2021 15:20:30 - INFO - __main__ -     loss = 0.836134389663736
10/18/2021 15:20:30 - INFO - __main__ -     precision = 0.7604978415652857
10/18/2021 15:20:30 - INFO - __main__ -     recall = 0.7487856038860675
10/18/2021 15:20:30 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/18/2021 15:20:31 - INFO - __main__ -   ***** Running evaluation  in no *****
10/18/2021 15:20:31 - INFO - __main__ -     Num examples = 4408
10/18/2021 15:20:31 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/138 [00:00<?, ?it/s]Evaluating:   1%|          | 1/138 [00:00<00:24,  5.56it/s]Evaluating:   1%|▏         | 2/138 [00:00<00:24,  5.54it/s]Evaluating:   2%|▏         | 3/138 [00:00<00:24,  5.58it/s]Evaluating:   3%|▎         | 4/138 [00:00<00:24,  5.58it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:   4%|▎         | 5/138 [00:00<00:23,  5.58it/s]Evaluating:   4%|▍         | 6/138 [00:01<00:23,  5.57it/s]Evaluating:   5%|▌         | 7/138 [00:01<00:23,  5.55it/s]Evaluating:   6%|▌         | 8/138 [00:01<00:23,  5.51it/s]Evaluating:   7%|▋         | 9/138 [00:01<00:23,  5.48it/s]Evaluating:   7%|▋         | 10/138 [00:01<00:23,  5.44it/s]Evaluating:   8%|▊         | 11/138 [00:01<00:23,  5.43it/s]Evaluating:   9%|▊         | 12/138 [00:02<00:23,  5.43it/s]Evaluating:   9%|▉         | 13/138 [00:02<00:22,  5.45it/s]Evaluating:  10%|█         | 14/138 [00:02<00:22,  5.49it/s]Evaluating:  11%|█         | 15/138 [00:02<00:22,  5.50it/s]Evaluating:  12%|█▏        | 16/138 [00:02<00:22,  5.50it/s]Evaluating:  12%|█▏        | 17/138 [00:03<00:22,  5.48it/s]Evaluating:  13%|█▎        | 18/138 [00:03<00:21,  5.46it/s]Evaluating:  14%|█▍        | 19/138 [00:03<00:21,  5.45it/s]Evaluating:  14%|█▍        | 20/138 [00:03<00:21,  5.44it/s]Evaluating:  15%|█▌        | 21/138 [00:03<00:21,  5.43it/s]Evaluating:  16%|█▌        | 22/138 [00:04<00:21,  5.43it/s]Evaluating:  17%|█▋        | 23/138 [00:04<00:21,  5.42it/s]Evaluating:  17%|█▋        | 24/138 [00:04<00:21,  5.41it/s]Evaluating:  18%|█▊        | 25/138 [00:04<00:20,  5.41it/s]Evaluating:  19%|█▉        | 26/138 [00:04<00:20,  5.41it/s]Evaluating:  20%|█▉        | 27/138 [00:04<00:20,  5.40it/s]Evaluating:  20%|██        | 28/138 [00:05<00:20,  5.39it/s]Evaluating:  21%|██        | 29/138 [00:05<00:20,  5.39it/s]Evaluating:  22%|██▏       | 30/138 [00:05<00:20,  5.39it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  22%|██▏       | 31/138 [00:05<00:19,  5.39it/s]Evaluating:  23%|██▎       | 32/138 [00:05<00:19,  5.39it/s]Evaluating:  24%|██▍       | 33/138 [00:06<00:19,  5.38it/s]Evaluating:  25%|██▍       | 34/138 [00:06<00:19,  5.39it/s]Evaluating:  25%|██▌       | 35/138 [00:06<00:19,  5.38it/s]Evaluating:  26%|██▌       | 36/138 [00:06<00:18,  5.38it/s]Evaluating:  27%|██▋       | 37/138 [00:06<00:18,  5.37it/s]Evaluating:  28%|██▊       | 38/138 [00:06<00:18,  5.38it/s]Evaluating:  28%|██▊       | 39/138 [00:07<00:18,  5.37it/s]Evaluating:  29%|██▉       | 40/138 [00:07<00:18,  5.37it/s]Evaluating:  30%|██▉       | 41/138 [00:07<00:18,  5.36it/s]Evaluating:  30%|███       | 42/138 [00:07<00:17,  5.35it/s]Evaluating:  31%|███       | 43/138 [00:07<00:17,  5.36it/s]Evaluating:  32%|███▏      | 44/138 [00:08<00:17,  5.36it/s]Evaluating:  33%|███▎      | 45/138 [00:08<00:17,  5.35it/s]Evaluating:  33%|███▎      | 46/138 [00:08<00:17,  5.37it/s]Evaluating:  34%|███▍      | 47/138 [00:08<00:16,  5.39it/s]Evaluating:  35%|███▍      | 48/138 [00:08<00:16,  5.39it/s]Evaluating:  36%|███▌      | 49/138 [00:09<00:16,  5.38it/s]Evaluating:  36%|███▌      | 50/138 [00:09<00:16,  5.38it/s]Evaluating:  37%|███▋      | 51/138 [00:09<00:16,  5.36it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  38%|███▊      | 52/138 [00:09<00:16,  5.36it/s]10/18/2021 15:20:41 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:20:41 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  38%|███▊      | 53/138 [00:09<00:15,  5.35it/s]Evaluating:  39%|███▉      | 54/138 [00:09<00:15,  5.36it/s]Evaluating:  40%|███▉      | 55/138 [00:10<00:15,  5.38it/s]Evaluating:  41%|████      | 56/138 [00:10<00:15,  5.39it/s]Evaluating:  41%|████▏     | 57/138 [00:10<00:15,  5.37it/s]Evaluating:  42%|████▏     | 58/138 [00:10<00:14,  5.36it/s]Evaluating:  43%|████▎     | 59/138 [00:10<00:15,  5.03it/s]Evaluating:  43%|████▎     | 60/138 [00:11<00:15,  5.05it/s]Evaluating:  44%|████▍     | 61/138 [00:11<00:14,  5.15it/s]Evaluating:  45%|████▍     | 62/138 [00:11<00:14,  5.22it/s]Evaluating:  46%|████▌     | 63/138 [00:11<00:14,  5.24it/s]Evaluating:  46%|████▋     | 64/138 [00:11<00:14,  5.27it/s]Evaluating:  47%|████▋     | 65/138 [00:12<00:13,  5.29it/s]Evaluating:  48%|████▊     | 66/138 [00:12<00:13,  5.29it/s]Evaluating:  49%|████▊     | 67/138 [00:12<00:13,  5.29it/s]Evaluating:  49%|████▉     | 68/138 [00:12<00:13,  5.30it/s]Evaluating:  50%|█████     | 69/138 [00:12<00:13,  5.30it/s]Evaluating:  51%|█████     | 70/138 [00:13<00:12,  5.33it/s]Evaluating:  51%|█████▏    | 71/138 [00:13<00:12,  5.34it/s]Evaluating:  52%|█████▏    | 72/138 [00:13<00:12,  5.33it/s]Evaluating:  53%|█████▎    | 73/138 [00:13<00:12,  5.32it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  54%|█████▎    | 74/138 [00:13<00:12,  5.31it/s]Evaluating:  54%|█████▍    | 75/138 [00:13<00:11,  5.31it/s]Evaluating:  55%|█████▌    | 76/138 [00:14<00:11,  5.30it/s]Evaluating:  56%|█████▌    | 77/138 [00:14<00:11,  5.31it/s]Evaluating:  57%|█████▋    | 78/138 [00:14<00:11,  5.31it/s]Evaluating:  57%|█████▋    | 79/138 [00:14<00:11,  5.33it/s]Evaluating:  58%|█████▊    | 80/138 [00:14<00:10,  5.33it/s]Evaluating:  59%|█████▊    | 81/138 [00:15<00:10,  5.34it/s]Evaluating:  59%|█████▉    | 82/138 [00:15<00:10,  5.38it/s]Evaluating:  60%|██████    | 83/138 [00:15<00:10,  5.45it/s]Evaluating:  61%|██████    | 84/138 [00:15<00:09,  5.50it/s]Evaluating:  62%|██████▏   | 85/138 [00:16<00:17,  3.08it/s]Evaluating:  62%|██████▏   | 86/138 [00:16<00:18,  2.80it/s]Evaluating:  63%|██████▎   | 87/138 [00:16<00:16,  3.03it/s]Evaluating:  64%|██████▍   | 88/138 [00:17<00:15,  3.30it/s]Evaluating:  64%|██████▍   | 89/138 [00:17<00:13,  3.60it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  65%|██████▌   | 90/138 [00:17<00:12,  3.89it/s]Evaluating:  66%|██████▌   | 91/138 [00:17<00:11,  4.15it/s]Evaluating:  67%|██████▋   | 92/138 [00:18<00:10,  4.39it/s]Evaluating:  67%|██████▋   | 93/138 [00:18<00:09,  4.59it/s]Evaluating:  68%|██████▊   | 94/138 [00:18<00:09,  4.75it/s]Evaluating:  69%|██████▉   | 95/138 [00:18<00:08,  4.89it/s]Evaluating:  70%|██████▉   | 96/138 [00:18<00:08,  5.00it/s]Evaluating:  70%|███████   | 97/138 [00:18<00:08,  5.08it/s]Evaluating:  71%|███████   | 98/138 [00:19<00:07,  5.14it/s]Evaluating:  72%|███████▏  | 99/138 [00:19<00:07,  5.22it/s]Evaluating:  72%|███████▏  | 100/138 [00:19<00:07,  5.27it/s]Evaluating:  73%|███████▎  | 101/138 [00:19<00:07,  5.29it/s]Evaluating:  74%|███████▍  | 102/138 [00:19<00:06,  5.33it/s]Evaluating:  75%|███████▍  | 103/138 [00:20<00:06,  5.38it/s]Evaluating:  75%|███████▌  | 104/138 [00:20<00:06,  5.40it/s]Evaluating:  76%|███████▌  | 105/138 [00:20<00:06,  5.38it/s]Evaluating:  77%|███████▋  | 106/138 [00:20<00:05,  5.37it/s]Evaluating:  78%|███████▊  | 107/138 [00:20<00:05,  5.27it/s]Evaluating:  78%|███████▊  | 108/138 [00:21<00:05,  5.30it/s]Evaluating:  79%|███████▉  | 109/138 [00:21<00:05,  5.33it/s]Evaluating:  80%|███████▉  | 110/138 [00:21<00:05,  5.28it/s]Evaluating:  80%|████████  | 111/138 [00:21<00:05,  5.26it/s]Evaluating:  81%|████████  | 112/138 [00:21<00:04,  5.27it/s]Evaluating:  82%|████████▏ | 113/138 [00:21<00:04,  5.29it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  83%|████████▎ | 114/138 [00:22<00:04,  5.28it/s]Evaluating:  83%|████████▎ | 115/138 [00:22<00:04,  5.28it/s]Evaluating:  84%|████████▍ | 116/138 [00:22<00:04,  4.42it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  85%|████████▍ | 117/138 [00:23<00:05,  3.70it/s]Evaluating:  86%|████████▌ | 118/138 [00:23<00:05,  3.59it/s]Evaluating:  86%|████████▌ | 119/138 [00:23<00:05,  3.68it/s]Evaluating:  87%|████████▋ | 120/138 [00:23<00:04,  3.83it/s]Evaluating:  88%|████████▊ | 121/138 [00:24<00:04,  4.01it/s]Evaluating:  88%|████████▊ | 122/138 [00:24<00:03,  4.21it/s]Evaluating:  89%|████████▉ | 123/138 [00:24<00:03,  4.39it/s]Evaluating:  90%|████████▉ | 124/138 [00:24<00:03,  4.53it/s]Evaluating:  91%|█████████ | 125/138 [00:24<00:02,  4.66it/s]Evaluating:  91%|█████████▏| 126/138 [00:25<00:02,  4.75it/s]Evaluating:  92%|█████████▏| 127/138 [00:25<00:02,  4.83it/s]Evaluating:  93%|█████████▎| 128/138 [00:25<00:02,  4.91it/s]Evaluating:  93%|█████████▎| 129/138 [00:25<00:01,  4.98it/s]Evaluating:  94%|█████████▍| 130/138 [00:25<00:01,  5.03it/s]Evaluating:  95%|█████████▍| 131/138 [00:26<00:01,  5.03it/s]Evaluating:  96%|█████████▌| 132/138 [00:26<00:01,  5.03it/s]Evaluating:  96%|█████████▋| 133/138 [00:26<00:00,  5.03it/s]Evaluating:  97%|█████████▋| 134/138 [00:26<00:00,  5.02it/s]Evaluating:  98%|█████████▊| 135/138 [00:26<00:00,  5.02it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:20:58 - INFO - __main__ -   Using lang2id = None
Evaluating:  99%|█████████▊| 136/138 [00:27<00:00,  5.07it/s]Evaluating:  99%|█████████▉| 137/138 [00:27<00:00,  5.05it/s]Evaluating: 100%|██████████| 138/138 [00:27<00:00,  5.37it/s]Evaluating: 100%|██████████| 138/138 [00:27<00:00,  5.03it/s]PyTorch version 1.9.0+cu102 available.
10/18/2021 15:21:00 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:21:00 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:21:00 - INFO - __main__ -   Seed = 3
10/18/2021 15:21:00 - INFO - root -   save model
10/18/2021 15:21:00 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='mr,bho,ta', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,hi,ar_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:21:00 - INFO - __main__ -   Loading fresh pretrained model and tokenizer

10/18/2021 15:21:00 - INFO - __main__ -   ***** Evaluation result  in no *****
10/18/2021 15:21:00 - INFO - __main__ -     f1 = 0.8390286603553364
10/18/2021 15:21:00 - INFO - __main__ -     loss = 0.5229466760504073
10/18/2021 15:21:00 - INFO - __main__ -     precision = 0.8370340422091048
10/18/2021 15:21:00 - INFO - __main__ -     recall = 0.8410328073966948
10/18/2021 15:21:00 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/18/2021 15:21:00 - INFO - __main__ -   ***** Running evaluation  in da *****
10/18/2021 15:21:00 - INFO - __main__ -     Num examples = 565
10/18/2021 15:21:00 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]Evaluating:   6%|▌         | 1/18 [00:00<00:03,  5.46it/s]Evaluating:  11%|█         | 2/18 [00:00<00:02,  5.45it/s]Evaluating:  17%|█▋        | 3/18 [00:00<00:02,  5.45it/s]Evaluating:  22%|██▏       | 4/18 [00:00<00:02,  5.45it/s]Evaluating:  28%|██▊       | 5/18 [00:00<00:02,  5.45it/s]Evaluating:  33%|███▎      | 6/18 [00:01<00:02,  5.45it/s]Evaluating:  39%|███▉      | 7/18 [00:01<00:02,  5.44it/s]Evaluating:  44%|████▍     | 8/18 [00:01<00:01,  5.44it/s]Evaluating:  50%|█████     | 9/18 [00:01<00:01,  5.30it/s]Evaluating:  56%|█████▌    | 10/18 [00:01<00:01,  5.37it/s]Evaluating:  61%|██████    | 11/18 [00:02<00:01,  5.38it/s]Evaluating:  67%|██████▋   | 12/18 [00:02<00:01,  5.38it/s]Evaluating:  72%|███████▏  | 13/18 [00:02<00:00,  5.39it/s]Evaluating:  78%|███████▊  | 14/18 [00:02<00:00,  5.40it/s]Evaluating:  83%|████████▎ | 15/18 [00:02<00:00,  5.41it/s]Evaluating:  89%|████████▉ | 16/18 [00:02<00:00,  5.41it/s]Evaluating:  94%|█████████▍| 17/18 [00:03<00:00,  5.40it/s]Evaluating: 100%|██████████| 18/18 [00:03<00:00,  5.92it/s]Evaluating: 100%|██████████| 18/18 [00:03<00:00,  5.50it/s]
10/18/2021 15:21:04 - INFO - __main__ -   ***** Evaluation result  in da *****
10/18/2021 15:21:04 - INFO - __main__ -     f1 = 0.8859067522410226
10/18/2021 15:21:04 - INFO - __main__ -     loss = 0.33106522924370235
10/18/2021 15:21:04 - INFO - __main__ -     precision = 0.886141765704584
10/18/2021 15:21:04 - INFO - __main__ -     recall = 0.8856718634001485
10/18/2021 15:21:04 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:21:34 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:21:34 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:21:36 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:21:36 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:21:36 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
10/18/2021 15:21:36 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:21:36 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:21:36 - INFO - root -   loading lang adpater en/wiki@ukp,hi/wiki@ukp,ar/wiki@ukp
10/18/2021 15:21:36 - INFO - __main__ -   Language = en
10/18/2021 15:21:36 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:21:36 - INFO - __main__ -   Language = hi
10/18/2021 15:21:36 - INFO - __main__ -   Adapter Name = hi/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/hi/bert-base-multilingual-cased/pfeiffer/hi_pfeiffer_gelu_nd_200k.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/adapter_config.json
Adding adapter 'hi' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/074d5b7e47a2e35f3d06d1f3bcdba40b0709c9e5ce81b3c3533bc81cd8e35505-6d8198b7d99138cfc02cbf557a60122f7492f9ee1ed400529bf29c8180688fec-extracted'
10/18/2021 15:21:37 - INFO - __main__ -   Language = ar
10/18/2021 15:21:37 - INFO - __main__ -   Adapter Name = ar/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ar/bert-base-multilingual-cased/pfeiffer/ar_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/adapter_config.json
Adding adapter 'ar' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/0b2a547243e82d816959cbe6c9a814aee3716c08d62584aebd7a9044229cc8a8-1ec0387460c90e04c5e70048e0981620ddf52d228c8f806eca3ce770b10ed331-extracted'
10/18/2021 15:21:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_mr_bert-base-multilingual-cased_128
10/18/2021 15:21:42 - INFO - __main__ -   ***** Running evaluation  in mr *****
10/18/2021 15:21:42 - INFO - __main__ -     Num examples = 47
10/18/2021 15:21:42 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  50%|█████     | 1/2 [00:00<00:00,  4.84it/s]Evaluating: 100%|██████████| 2/2 [00:00<00:00,  6.85it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:21:42 - INFO - __main__ -   ***** Evaluation result  in mr *****
10/18/2021 15:21:42 - INFO - __main__ -     f1 = 0.6687306501547987
10/18/2021 15:21:42 - INFO - __main__ -     loss = 1.136005461215973
10/18/2021 15:21:42 - INFO - __main__ -     precision = 0.6900958466453674
10/18/2021 15:21:42 - INFO - __main__ -     recall = 0.6486486486486487
10/18/2021 15:21:42 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_bho_bert-base-multilingual-cased_128
10/18/2021 15:21:42 - INFO - __main__ -   ***** Running evaluation  in bho *****
10/18/2021 15:21:42 - INFO - __main__ -     Num examples = 361
10/18/2021 15:21:42 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]Evaluating:   8%|▊         | 1/12 [00:00<00:01,  6.24it/s]Evaluating:  17%|█▋        | 2/12 [00:00<00:01,  6.20it/s]Evaluating:  25%|██▌       | 3/12 [00:00<00:01,  6.19it/s]Evaluating:  33%|███▎      | 4/12 [00:00<00:01,  6.18it/s]Evaluating:  42%|████▏     | 5/12 [00:00<00:01,  6.17it/s]Evaluating:  50%|█████     | 6/12 [00:00<00:00,  6.16it/s]Evaluating:  58%|█████▊    | 7/12 [00:01<00:00,  6.15it/s]Evaluating:  67%|██████▋   | 8/12 [00:01<00:00,  6.14it/s]Evaluating:  75%|███████▌  | 9/12 [00:01<00:00,  6.14it/s]Evaluating:  83%|████████▎ | 10/12 [00:01<00:00,  6.11it/s]Evaluating:  92%|█████████▏| 11/12 [00:01<00:00,  6.10it/s]Evaluating: 100%|██████████| 12/12 [00:01<00:00,  6.46it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:21:44 - INFO - __main__ -   ***** Evaluation result  in bho *****
10/18/2021 15:21:44 - INFO - __main__ -     f1 = 0.4755220518152673
10/18/2021 15:21:44 - INFO - __main__ -     loss = 2.3132957220077515
10/18/2021 15:21:44 - INFO - __main__ -     precision = 0.49746192893401014
10/18/2021 15:21:44 - INFO - __main__ -     recall = 0.4554356846473029
10/18/2021 15:21:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_ta_bert-base-multilingual-cased_128
10/18/2021 15:21:45 - INFO - __main__ -   ***** Running evaluation  in ta *****
10/18/2021 15:21:45 - INFO - __main__ -     Num examples = 656
10/18/2021 15:21:45 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/21 [00:00<?, ?it/s]Evaluating:   5%|▍         | 1/21 [00:00<00:03,  6.17it/s]Evaluating:  10%|▉         | 2/21 [00:00<00:03,  6.17it/s]Evaluating:  14%|█▍        | 3/21 [00:00<00:02,  6.14it/s]Evaluating:  19%|█▉        | 4/21 [00:00<00:02,  6.14it/s]Evaluating:  24%|██▍       | 5/21 [00:00<00:02,  6.13it/s]Evaluating:  29%|██▊       | 6/21 [00:00<00:02,  6.13it/s]Evaluating:  33%|███▎      | 7/21 [00:01<00:02,  6.12it/s]Evaluating:  38%|███▊      | 8/21 [00:01<00:02,  6.13it/s]Evaluating:  43%|████▎     | 9/21 [00:01<00:01,  6.11it/s]Evaluating:  48%|████▊     | 10/21 [00:01<00:01,  6.10it/s]Evaluating:  52%|█████▏    | 11/21 [00:01<00:01,  6.09it/s]Evaluating:  57%|█████▋    | 12/21 [00:01<00:01,  6.10it/s]Evaluating:  62%|██████▏   | 13/21 [00:02<00:01,  6.10it/s]Evaluating:  67%|██████▋   | 14/21 [00:02<00:01,  6.09it/s]Evaluating:  71%|███████▏  | 15/21 [00:02<00:00,  6.08it/s]Evaluating:  76%|███████▌  | 16/21 [00:02<00:00,  6.08it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  81%|████████  | 17/21 [00:02<00:00,  6.09it/s]Evaluating:  86%|████████▌ | 18/21 [00:02<00:00,  6.08it/s]Evaluating:  90%|█████████ | 19/21 [00:03<00:00,  6.09it/s]Evaluating:  95%|█████████▌| 20/21 [00:03<00:00,  6.09it/s]Evaluating: 100%|██████████| 21/21 [00:03<00:00,  6.24it/s]
10/18/2021 15:21:48 - INFO - __main__ -   ***** Evaluation result  in ta *****
10/18/2021 15:21:48 - INFO - __main__ -     f1 = 0.6479430379746836
10/18/2021 15:21:48 - INFO - __main__ -     loss = 1.0876776490892683
10/18/2021 15:21:48 - INFO - __main__ -     precision = 0.6665762344004341
10/18/2021 15:21:48 - INFO - __main__ -     recall = 0.6303232426885582
10/18/2021 15:21:48 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

PyTorch version 1.9.0+cu102 available.
10/18/2021 15:21:52 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:21:52 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:21:52 - INFO - __main__ -   Seed = 1
10/18/2021 15:21:52 - INFO - root -   save model
10/18/2021 15:21:52 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:21:52 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:22:11 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:22:11 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:22:12 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/18/2021 15:22:14 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:22:14 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:22:14 - INFO - __main__ -   Seed = 2
10/18/2021 15:22:14 - INFO - root -   save model
10/18/2021 15:22:14 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:22:14 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
10/18/2021 15:22:34 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:22:34 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:22:34 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
10/18/2021 15:22:34 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:22:34 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:22:34 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/18/2021 15:22:34 - INFO - __main__ -   Language = en
10/18/2021 15:22:34 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:22:35 - INFO - __main__ -   Language = ru
10/18/2021 15:22:35 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

10/18/2021 15:22:39 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/18/2021 15:22:39 - INFO - __main__ -   ***** Running evaluation  in be *****
10/18/2021 15:22:39 - INFO - __main__ -     Num examples = 932
10/18/2021 15:22:39 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/30 [00:00<00:04,  5.98it/s]Evaluating:   7%|▋         | 2/30 [00:00<00:04,  6.28it/s]Evaluating:  10%|█         | 3/30 [00:00<00:04,  6.46it/s]Evaluating:  13%|█▎        | 4/30 [00:00<00:03,  6.54it/s]Evaluating:  17%|█▋        | 5/30 [00:00<00:03,  6.56it/s]Evaluating:  20%|██        | 6/30 [00:00<00:03,  6.59it/s]Evaluating:  23%|██▎       | 7/30 [00:01<00:03,  6.59it/s]Evaluating:  27%|██▋       | 8/30 [00:01<00:03,  6.60it/s]Evaluating:  30%|███       | 9/30 [00:01<00:03,  6.61it/s]Evaluating:  33%|███▎      | 10/30 [00:01<00:03,  6.61it/s]Evaluating:  37%|███▋      | 11/30 [00:01<00:02,  6.62it/s]Evaluating:  40%|████      | 12/30 [00:01<00:02,  6.60it/s]Evaluating:  43%|████▎     | 13/30 [00:01<00:02,  6.60it/s]Evaluating:  47%|████▋     | 14/30 [00:02<00:02,  6.61it/s]Evaluating:  50%|█████     | 15/30 [00:02<00:02,  6.62it/s]Evaluating:  53%|█████▎    | 16/30 [00:02<00:02,  6.61it/s]Evaluating:  57%|█████▋    | 17/30 [00:02<00:01,  6.61it/s]Evaluating:  60%|██████    | 18/30 [00:02<00:01,  6.61it/s]Evaluating:  63%|██████▎   | 19/30 [00:02<00:01,  6.61it/s]Evaluating:  67%|██████▋   | 20/30 [00:03<00:01,  6.60it/s]Evaluating:  70%|███████   | 21/30 [00:03<00:01,  6.59it/s]Evaluating:  73%|███████▎  | 22/30 [00:03<00:01,  6.60it/s]Evaluating:  77%|███████▋  | 23/30 [00:03<00:01,  6.60it/s]Evaluating:  80%|████████  | 24/30 [00:03<00:00,  6.59it/s]Evaluating:  83%|████████▎ | 25/30 [00:03<00:00,  6.59it/s]Evaluating:  87%|████████▋ | 26/30 [00:03<00:00,  6.51it/s]Evaluating:  90%|█████████ | 27/30 [00:04<00:00,  6.53it/s]Evaluating:  93%|█████████▎| 28/30 [00:04<00:00,  6.54it/s]Evaluating:  97%|█████████▋| 29/30 [00:04<00:00,  6.54it/s]Evaluating: 100%|██████████| 30/30 [00:04<00:00,  6.75it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:22:44 - INFO - __main__ -   ***** Evaluation result  in be *****
10/18/2021 15:22:44 - INFO - __main__ -     f1 = 0.8161305808929429
10/18/2021 15:22:44 - INFO - __main__ -     loss = 0.7507193391521771
10/18/2021 15:22:44 - INFO - __main__ -     precision = 0.8198158702323542
10/18/2021 15:22:44 - INFO - __main__ -     recall = 0.8124782759819256
10/18/2021 15:22:44 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/18/2021 15:22:44 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/18/2021 15:22:44 - INFO - __main__ -     Num examples = 915
10/18/2021 15:22:44 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/29 [00:00<?, ?it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:   3%|▎         | 1/29 [00:00<00:04,  6.73it/s]10/18/2021 15:22:44 - INFO - __main__ -   Using lang2id = None
Evaluating:   7%|▋         | 2/29 [00:00<00:04,  6.69it/s]Evaluating:  10%|█         | 3/29 [00:00<00:03,  6.66it/s]Evaluating:  14%|█▍        | 4/29 [00:00<00:03,  6.66it/s]Evaluating:  17%|█▋        | 5/29 [00:00<00:03,  6.65it/s]Evaluating:  21%|██        | 6/29 [00:00<00:03,  6.62it/s]Evaluating:  24%|██▍       | 7/29 [00:01<00:03,  6.46it/s]Evaluating:  28%|██▊       | 8/29 [00:01<00:03,  6.52it/s]Evaluating:  31%|███       | 9/29 [00:01<00:03,  6.55it/s]Evaluating:  34%|███▍      | 10/29 [00:01<00:02,  6.57it/s]Evaluating:  38%|███▊      | 11/29 [00:01<00:02,  6.58it/s]Evaluating:  41%|████▏     | 12/29 [00:01<00:02,  6.60it/s]Evaluating:  45%|████▍     | 13/29 [00:01<00:02,  6.60it/s]Evaluating:  48%|████▊     | 14/29 [00:02<00:02,  6.60it/s]Evaluating:  52%|█████▏    | 15/29 [00:02<00:02,  6.60it/s]Evaluating:  55%|█████▌    | 16/29 [00:02<00:01,  6.59it/s]Evaluating:  59%|█████▊    | 17/29 [00:02<00:01,  6.60it/s]Evaluating:  62%|██████▏   | 18/29 [00:02<00:01,  6.60it/s]Evaluating:  66%|██████▌   | 19/29 [00:02<00:01,  6.60it/s]Evaluating:  69%|██████▉   | 20/29 [00:03<00:01,  6.59it/s]Evaluating:  72%|███████▏  | 21/29 [00:03<00:01,  6.59it/s]Evaluating:  76%|███████▌  | 22/29 [00:03<00:01,  6.59it/s]Evaluating:  79%|███████▉  | 23/29 [00:03<00:00,  6.58it/s]Evaluating:  83%|████████▎ | 24/29 [00:03<00:00,  6.58it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  86%|████████▌ | 25/29 [00:03<00:00,  6.58it/s]Evaluating:  90%|████████▉ | 26/29 [00:03<00:00,  6.57it/s]10/18/2021 15:22:48 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:22:48 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:22:48 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
10/18/2021 15:22:48 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:22:48 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:22:48 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/18/2021 15:22:48 - INFO - __main__ -   Language = en
10/18/2021 15:22:48 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:  93%|█████████▎| 27/29 [00:04<00:00,  6.57it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Evaluating:  97%|█████████▋| 28/29 [00:04<00:00,  6.57it/s]Evaluating: 100%|██████████| 29/29 [00:04<00:00,  7.30it/s]Evaluating: 100%|██████████| 29/29 [00:04<00:00,  6.66it/s]
10/18/2021 15:22:49 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/18/2021 15:22:49 - INFO - __main__ -     f1 = 0.8200045055192611
10/18/2021 15:22:49 - INFO - __main__ -     loss = 0.6460147777508045
10/18/2021 15:22:49 - INFO - __main__ -     precision = 0.8209291835814163
10/18/2021 15:22:49 - INFO - __main__ -     recall = 0.819081908190819
10/18/2021 15:22:49 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
10/18/2021 15:22:49 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/18/2021 15:22:49 - INFO - __main__ -     Num examples = 1117
10/18/2021 15:22:49 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:22:49 - INFO - __main__ -   Language = is
10/18/2021 15:22:49 - INFO - __main__ -   Adapter Name = is/wiki@ukp
Evaluating:   3%|▎         | 1/35 [00:00<00:05,  6.69it/s]No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
Evaluating:   6%|▌         | 2/35 [00:00<00:04,  6.61it/s]Evaluating:   9%|▊         | 3/35 [00:00<00:04,  6.61it/s]Evaluating:  11%|█▏        | 4/35 [00:00<00:04,  6.61it/s]Evaluating:  14%|█▍        | 5/35 [00:00<00:04,  6.60it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Evaluating:  17%|█▋        | 6/35 [00:00<00:04,  6.59it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/18/2021 15:22:50 - INFO - __main__ -   Language = de
10/18/2021 15:22:50 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Evaluating:  20%|██        | 7/35 [00:01<00:04,  6.57it/s]Evaluating:  23%|██▎       | 8/35 [00:01<00:04,  5.75it/s]Evaluating:  26%|██▌       | 9/35 [00:01<00:04,  5.99it/s]Evaluating:  29%|██▊       | 10/35 [00:01<00:04,  6.16it/s]Evaluating:  31%|███▏      | 11/35 [00:01<00:03,  6.27it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
Evaluating:  34%|███▍      | 12/35 [00:01<00:03,  6.37it/s]Evaluating:  37%|███▋      | 13/35 [00:02<00:03,  6.36it/s]Evaluating:  40%|████      | 14/35 [00:02<00:03,  6.34it/s]Evaluating:  43%|████▎     | 15/35 [00:02<00:03,  6.33it/s]Evaluating:  46%|████▌     | 16/35 [00:02<00:03,  6.27it/s]Evaluating:  49%|████▊     | 17/35 [00:02<00:02,  6.22it/s]Evaluating:  51%|█████▏    | 18/35 [00:02<00:02,  6.24it/s]Evaluating:  54%|█████▍    | 19/35 [00:03<00:02,  6.21it/s]Evaluating:  57%|█████▋    | 20/35 [00:03<00:02,  6.17it/s]Evaluating:  60%|██████    | 21/35 [00:03<00:02,  6.12it/s]Evaluating:  63%|██████▎   | 22/35 [00:03<00:02,  6.07it/s]Evaluating:  66%|██████▌   | 23/35 [00:03<00:01,  6.04it/s]Evaluating:  69%|██████▊   | 24/35 [00:03<00:01,  5.99it/s]Evaluating:  71%|███████▏  | 25/35 [00:04<00:01,  5.93it/s]Evaluating:  74%|███████▍  | 26/35 [00:04<00:01,  5.92it/s]Evaluating:  77%|███████▋  | 27/35 [00:04<00:01,  5.92it/s]Evaluating:  80%|████████  | 28/35 [00:04<00:01,  5.88it/s]Evaluating:  83%|████████▎ | 29/35 [00:04<00:01,  5.88it/s]Evaluating:  86%|████████▌ | 30/35 [00:04<00:00,  5.84it/s]Evaluating:  89%|████████▊ | 31/35 [00:05<00:00,  5.89it/s]Evaluating:  91%|█████████▏| 32/35 [00:05<00:00,  5.90it/s]Evaluating:  94%|█████████▍| 33/35 [00:05<00:00,  5.88it/s]Evaluating:  97%|█████████▋| 34/35 [00:05<00:00,  5.92it/s]Evaluating: 100%|██████████| 35/35 [00:05<00:00,  5.98it/s]Evaluating: 100%|██████████| 35/35 [00:05<00:00,  6.12it/s]
10/18/2021 15:22:55 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/18/2021 15:22:55 - INFO - __main__ -     f1 = 0.8489347536617844
10/18/2021 15:22:55 - INFO - __main__ -     loss = 0.5505974675927844
10/18/2021 15:22:55 - INFO - __main__ -     precision = 0.850690506371339
10/18/2021 15:22:55 - INFO - __main__ -     recall = 0.8471862334728589
10/18/2021 15:22:55 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
10/18/2021 15:22:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/18/2021 15:22:56 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/18/2021 15:22:56 - INFO - __main__ -     Num examples = 1516
10/18/2021 15:22:56 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|▏         | 1/48 [00:00<00:08,  5.26it/s]Evaluating:   4%|▍         | 2/48 [00:00<00:08,  5.36it/s]Evaluating:   6%|▋         | 3/48 [00:00<00:08,  5.39it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   8%|▊         | 4/48 [00:00<00:08,  5.41it/s]Evaluating:  10%|█         | 5/48 [00:00<00:07,  5.41it/s]Evaluating:  12%|█▎        | 6/48 [00:01<00:07,  5.39it/s]Evaluating:  15%|█▍        | 7/48 [00:01<00:07,  5.39it/s]Evaluating:  17%|█▋        | 8/48 [00:01<00:07,  5.41it/s]Evaluating:  19%|█▉        | 9/48 [00:01<00:07,  5.41it/s]Evaluating:  21%|██        | 10/48 [00:01<00:07,  5.41it/s]Evaluating:  23%|██▎       | 11/48 [00:02<00:06,  5.42it/s]Evaluating:  25%|██▌       | 12/48 [00:02<00:06,  5.42it/s]Evaluating:  27%|██▋       | 13/48 [00:02<00:06,  5.42it/s]Evaluating:  29%|██▉       | 14/48 [00:02<00:06,  5.42it/s]Evaluating:  31%|███▏      | 15/48 [00:02<00:06,  5.42it/s]Evaluating:  33%|███▎      | 16/48 [00:02<00:05,  5.42it/s]Evaluating:  35%|███▌      | 17/48 [00:03<00:05,  5.42it/s]Evaluating:  38%|███▊      | 18/48 [00:03<00:05,  5.42it/s]Evaluating:  40%|███▉      | 19/48 [00:03<00:05,  5.42it/s]Evaluating:  42%|████▏     | 20/48 [00:03<00:05,  5.41it/s]Evaluating:  44%|████▍     | 21/48 [00:03<00:04,  5.41it/s]Evaluating:  46%|████▌     | 22/48 [00:04<00:04,  5.41it/s]Evaluating:  48%|████▊     | 23/48 [00:04<00:04,  5.39it/s]Evaluating:  50%|█████     | 24/48 [00:04<00:04,  5.34it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  52%|█████▏    | 25/48 [00:04<00:04,  5.39it/s]Evaluating:  54%|█████▍    | 26/48 [00:04<00:04,  5.39it/s]Evaluating:  56%|█████▋    | 27/48 [00:05<00:03,  5.37it/s]Evaluating:  58%|█████▊    | 28/48 [00:05<00:03,  5.37it/s]Evaluating:  60%|██████    | 29/48 [00:05<00:03,  5.38it/s]Evaluating:  62%|██████▎   | 30/48 [00:05<00:03,  5.35it/s]Evaluating:  65%|██████▍   | 31/48 [00:05<00:03,  5.34it/s]Evaluating:  67%|██████▋   | 32/48 [00:05<00:02,  5.35it/s]Evaluating:  69%|██████▉   | 33/48 [00:06<00:02,  5.37it/s]Evaluating:  71%|███████   | 34/48 [00:06<00:02,  5.38it/s]Evaluating:  73%|███████▎  | 35/48 [00:06<00:02,  5.39it/s]Evaluating:  75%|███████▌  | 36/48 [00:06<00:02,  5.38it/s]Evaluating:  77%|███████▋  | 37/48 [00:06<00:02,  5.38it/s]Evaluating:  79%|███████▉  | 38/48 [00:07<00:01,  5.38it/s]Evaluating:  81%|████████▏ | 39/48 [00:07<00:01,  5.38it/s]Evaluating:  83%|████████▎ | 40/48 [00:07<00:01,  5.38it/s]Evaluating:  85%|████████▌ | 41/48 [00:07<00:01,  5.38it/s]Evaluating:  88%|████████▊ | 42/48 [00:07<00:01,  5.38it/s]Evaluating:  90%|████████▉ | 43/48 [00:07<00:00,  5.38it/s]Evaluating:  92%|█████████▏| 44/48 [00:08<00:00,  5.38it/s]Evaluating:  94%|█████████▍| 45/48 [00:08<00:00,  5.37it/s]Evaluating:  96%|█████████▌| 46/48 [00:08<00:00,  5.38it/s]Evaluating:  98%|█████████▊| 47/48 [00:08<00:00,  5.37it/s]Evaluating: 100%|██████████| 48/48 [00:08<00:00,  5.45it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:23:06 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/18/2021 15:23:06 - INFO - __main__ -     f1 = 0.7445320553899306
10/18/2021 15:23:06 - INFO - __main__ -     loss = 0.9389523106316725
10/18/2021 15:23:06 - INFO - __main__ -     precision = 0.7516736990154712
10/18/2021 15:23:06 - INFO - __main__ -     recall = 0.7375248399205122
10/18/2021 15:23:06 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

10/18/2021 15:23:06 - INFO - __main__ -   ***** Running evaluation  in no *****
10/18/2021 15:23:06 - INFO - __main__ -     Num examples = 4408
10/18/2021 15:23:06 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/138 [00:00<?, ?it/s]Evaluating:   1%|          | 1/138 [00:00<00:25,  5.47it/s]Evaluating:   1%|▏         | 2/138 [00:00<00:24,  5.47it/s]Evaluating:   2%|▏         | 3/138 [00:00<00:24,  5.47it/s]Evaluating:   3%|▎         | 4/138 [00:00<00:24,  5.46it/s]Evaluating:   4%|▎         | 5/138 [00:00<00:24,  5.46it/s]Evaluating:   4%|▍         | 6/138 [00:01<00:24,  5.45it/s]Evaluating:   5%|▌         | 7/138 [00:01<00:24,  5.44it/s]Evaluating:   6%|▌         | 8/138 [00:01<00:23,  5.43it/s]Evaluating:   7%|▋         | 9/138 [00:01<00:23,  5.41it/s]Evaluating:   7%|▋         | 10/138 [00:01<00:23,  5.43it/s]Evaluating:   8%|▊         | 11/138 [00:02<00:23,  5.42it/s]Evaluating:   9%|▊         | 12/138 [00:02<00:23,  5.42it/s]Evaluating:   9%|▉         | 13/138 [00:02<00:23,  5.41it/s]Evaluating:  10%|█         | 14/138 [00:02<00:22,  5.39it/s]Evaluating:  11%|█         | 15/138 [00:02<00:22,  5.40it/s]Evaluating:  12%|█▏        | 16/138 [00:02<00:22,  5.41it/s]Evaluating:  12%|█▏        | 17/138 [00:03<00:22,  5.40it/s]Evaluating:  13%|█▎        | 18/138 [00:03<00:22,  5.41it/s]Evaluating:  14%|█▍        | 19/138 [00:03<00:22,  5.38it/s]Evaluating:  14%|█▍        | 20/138 [00:03<00:21,  5.37it/s]Evaluating:  15%|█▌        | 21/138 [00:03<00:21,  5.38it/s]Evaluating:  16%|█▌        | 22/138 [00:04<00:21,  5.32it/s]Evaluating:  17%|█▋        | 23/138 [00:04<00:21,  5.35it/s]Evaluating:  17%|█▋        | 24/138 [00:04<00:21,  5.35it/s]Evaluating:  18%|█▊        | 25/138 [00:04<00:21,  5.38it/s]Evaluating:  19%|█▉        | 26/138 [00:04<00:20,  5.37it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  20%|█▉        | 27/138 [00:04<00:20,  5.37it/s]10/18/2021 15:23:11 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:23:11 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  20%|██        | 28/138 [00:05<00:20,  5.37it/s]Evaluating:  21%|██        | 29/138 [00:05<00:20,  5.35it/s]Evaluating:  22%|██▏       | 30/138 [00:05<00:20,  5.36it/s]Evaluating:  22%|██▏       | 31/138 [00:05<00:19,  5.39it/s]Evaluating:  23%|██▎       | 32/138 [00:05<00:19,  5.35it/s]Evaluating:  24%|██▍       | 33/138 [00:06<00:19,  5.35it/s]Evaluating:  25%|██▍       | 34/138 [00:06<00:19,  5.40it/s]Evaluating:  25%|██▌       | 35/138 [00:06<00:18,  5.46it/s]Evaluating:  26%|██▌       | 36/138 [00:06<00:18,  5.50it/s]Evaluating:  27%|██▋       | 37/138 [00:06<00:18,  5.47it/s]Evaluating:  28%|██▊       | 38/138 [00:07<00:18,  5.44it/s]Evaluating:  28%|██▊       | 39/138 [00:07<00:18,  5.43it/s]Evaluating:  29%|██▉       | 40/138 [00:07<00:18,  5.41it/s]Evaluating:  30%|██▉       | 41/138 [00:07<00:18,  5.28it/s]Evaluating:  30%|███       | 42/138 [00:07<00:18,  5.26it/s]Evaluating:  31%|███       | 43/138 [00:07<00:17,  5.31it/s]Evaluating:  32%|███▏      | 44/138 [00:08<00:17,  5.35it/s]Evaluating:  33%|███▎      | 45/138 [00:08<00:17,  5.40it/s]Evaluating:  33%|███▎      | 46/138 [00:08<00:16,  5.45it/s]Evaluating:  34%|███▍      | 47/138 [00:08<00:16,  5.46it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  35%|███▍      | 48/138 [00:08<00:16,  5.46it/s]Evaluating:  36%|███▌      | 49/138 [00:09<00:16,  5.43it/s]Evaluating:  36%|███▌      | 50/138 [00:09<00:16,  5.41it/s]Evaluating:  37%|███▋      | 51/138 [00:09<00:16,  5.40it/s]Evaluating:  38%|███▊      | 52/138 [00:09<00:15,  5.41it/s]Evaluating:  38%|███▊      | 53/138 [00:09<00:15,  5.37it/s]Evaluating:  39%|███▉      | 54/138 [00:10<00:15,  5.34it/s]Evaluating:  40%|███▉      | 55/138 [00:10<00:15,  5.34it/s]Evaluating:  41%|████      | 56/138 [00:10<00:15,  5.34it/s]Evaluating:  41%|████▏     | 57/138 [00:10<00:15,  5.34it/s]Evaluating:  42%|████▏     | 58/138 [00:10<00:15,  5.28it/s]Evaluating:  43%|████▎     | 59/138 [00:10<00:15,  5.24it/s]Evaluating:  43%|████▎     | 60/138 [00:11<00:14,  5.30it/s]Evaluating:  44%|████▍     | 61/138 [00:11<00:14,  5.36it/s]Evaluating:  45%|████▍     | 62/138 [00:11<00:14,  5.38it/s]Evaluating:  46%|████▌     | 63/138 [00:11<00:13,  5.37it/s]Evaluating:  46%|████▋     | 64/138 [00:11<00:13,  5.36it/s]Evaluating:  47%|████▋     | 65/138 [00:12<00:13,  5.37it/s]Evaluating:  48%|████▊     | 66/138 [00:12<00:13,  5.36it/s]Evaluating:  49%|████▊     | 67/138 [00:12<00:13,  5.35it/s]Evaluating:  49%|████▉     | 68/138 [00:12<00:13,  5.34it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  50%|█████     | 69/138 [00:12<00:12,  5.34it/s]Evaluating:  51%|█████     | 70/138 [00:13<00:12,  5.30it/s]Evaluating:  51%|█████▏    | 71/138 [00:13<00:12,  5.32it/s]Evaluating:  52%|█████▏    | 72/138 [00:13<00:12,  5.34it/s]Evaluating:  53%|█████▎    | 73/138 [00:13<00:12,  5.39it/s]Evaluating:  54%|█████▎    | 74/138 [00:13<00:11,  5.40it/s]Evaluating:  54%|█████▍    | 75/138 [00:14<00:12,  4.87it/s]Evaluating:  55%|█████▌    | 76/138 [00:14<00:12,  5.00it/s]Evaluating:  56%|█████▌    | 77/138 [00:14<00:11,  5.11it/s]Evaluating:  57%|█████▋    | 78/138 [00:14<00:11,  5.21it/s]Evaluating:  57%|█████▋    | 79/138 [00:14<00:11,  5.27it/s]Evaluating:  58%|█████▊    | 80/138 [00:14<00:10,  5.29it/s]Evaluating:  59%|█████▊    | 81/138 [00:15<00:10,  5.28it/s]Evaluating:  59%|█████▉    | 82/138 [00:15<00:10,  5.26it/s]Evaluating:  60%|██████    | 83/138 [00:15<00:10,  5.24it/s]Evaluating:  61%|██████    | 84/138 [00:15<00:10,  5.24it/s]Evaluating:  62%|██████▏   | 85/138 [00:15<00:10,  5.22it/s]Evaluating:  62%|██████▏   | 86/138 [00:16<00:09,  5.21it/s]Evaluating:  63%|██████▎   | 87/138 [00:16<00:09,  5.21it/s]Evaluating:  64%|██████▍   | 88/138 [00:16<00:09,  5.19it/s]Evaluating:  64%|██████▍   | 89/138 [00:16<00:09,  5.21it/s]Evaluating:  65%|██████▌   | 90/138 [00:16<00:09,  5.28it/s]Evaluating:  66%|██████▌   | 91/138 [00:17<00:08,  5.35it/s]Evaluating:  67%|██████▋   | 92/138 [00:17<00:08,  5.36it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:  67%|██████▋   | 93/138 [00:17<00:08,  5.16it/s]Evaluating:  68%|██████▊   | 94/138 [00:17<00:08,  5.21it/s]Evaluating:  69%|██████▉   | 95/138 [00:17<00:08,  5.25it/s]Evaluating:  70%|██████▉   | 96/138 [00:17<00:07,  5.26it/s]Evaluating:  70%|███████   | 97/138 [00:18<00:07,  5.26it/s]Evaluating:  71%|███████   | 98/138 [00:18<00:07,  5.26it/s]Evaluating:  72%|███████▏  | 99/138 [00:18<00:07,  5.26it/s]Evaluating:  72%|███████▏  | 100/138 [00:18<00:07,  5.27it/s]Evaluating:  73%|███████▎  | 101/138 [00:19<00:08,  4.51it/s]Evaluating:  74%|███████▍  | 102/138 [00:19<00:07,  4.72it/s]Evaluating:  75%|███████▍  | 103/138 [00:19<00:07,  4.87it/s]Evaluating:  75%|███████▌  | 104/138 [00:19<00:06,  4.98it/s]Evaluating:  76%|███████▌  | 105/138 [00:19<00:06,  5.06it/s]Evaluating:  77%|███████▋  | 106/138 [00:19<00:06,  5.13it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  78%|███████▊  | 107/138 [00:20<00:06,  5.09it/s]Evaluating:  78%|███████▊  | 108/138 [00:20<00:05,  5.18it/s]Evaluating:  79%|███████▉  | 109/138 [00:20<00:05,  5.23it/s]Evaluating:  80%|███████▉  | 110/138 [00:20<00:05,  5.28it/s]Evaluating:  80%|████████  | 111/138 [00:20<00:05,  5.33it/s]Evaluating:  81%|████████  | 112/138 [00:21<00:04,  5.38it/s]Evaluating:  82%|████████▏ | 113/138 [00:21<00:04,  5.40it/s]Evaluating:  83%|████████▎ | 114/138 [00:21<00:04,  5.38it/s]Evaluating:  83%|████████▎ | 115/138 [00:21<00:04,  5.33it/s]Evaluating:  84%|████████▍ | 116/138 [00:21<00:04,  5.31it/s]Evaluating:  85%|████████▍ | 117/138 [00:22<00:03,  5.31it/s]Evaluating:  86%|████████▌ | 118/138 [00:22<00:03,  5.29it/s]Evaluating:  86%|████████▌ | 119/138 [00:22<00:03,  5.27it/s]Evaluating:  87%|████████▋ | 120/138 [00:22<00:03,  5.27it/s]Evaluating:  88%|████████▊ | 121/138 [00:22<00:03,  5.28it/s]Evaluating:  88%|████████▊ | 122/138 [00:22<00:03,  5.31it/s]Evaluating:  89%|████████▉ | 123/138 [00:23<00:02,  5.36it/s]Evaluating:  90%|████████▉ | 124/138 [00:23<00:02,  5.39it/s]Evaluating:  91%|█████████ | 125/138 [00:23<00:02,  5.37it/s]Evaluating:  91%|█████████▏| 126/138 [00:23<00:02,  5.32it/s]Evaluating:  92%|█████████▏| 127/138 [00:23<00:02,  5.28it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  93%|█████████▎| 128/138 [00:24<00:01,  5.29it/s]10/18/2021 15:23:31 - INFO - __main__ -   Using lang2id = None
Evaluating:  93%|█████████▎| 129/138 [00:24<00:01,  5.29it/s]Evaluating:  94%|█████████▍| 130/138 [00:24<00:01,  5.27it/s]Evaluating:  95%|█████████▍| 131/138 [00:24<00:01,  5.24it/s]Evaluating:  96%|█████████▌| 132/138 [00:24<00:01,  5.24it/s]Evaluating:  96%|█████████▋| 133/138 [00:25<00:00,  5.10it/s]Evaluating:  97%|█████████▋| 134/138 [00:25<00:00,  5.10it/s]Evaluating:  98%|█████████▊| 135/138 [00:25<00:00,  5.15it/s]Evaluating:  99%|█████████▊| 136/138 [00:25<00:00,  5.14it/s]Evaluating:  99%|█████████▉| 137/138 [00:25<00:00,  5.15it/s]Evaluating: 100%|██████████| 138/138 [00:26<00:00,  5.51it/s]Evaluating: 100%|██████████| 138/138 [00:26<00:00,  5.30it/s]PyTorch version 1.9.0+cu102 available.
10/18/2021 15:23:33 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:23:33 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:23:33 - INFO - __main__ -   Seed = 2
10/18/2021 15:23:33 - INFO - root -   save model
10/18/2021 15:23:33 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=2, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:23:33 - INFO - __main__ -   Loading fresh pretrained model and tokenizer

10/18/2021 15:23:34 - INFO - __main__ -   ***** Evaluation result  in no *****
10/18/2021 15:23:34 - INFO - __main__ -     f1 = 0.8384856303652857
10/18/2021 15:23:34 - INFO - __main__ -     loss = 0.5763262819120849
10/18/2021 15:23:34 - INFO - __main__ -     precision = 0.83737136374686
10/18/2021 15:23:34 - INFO - __main__ -     recall = 0.8396028663817617
10/18/2021 15:23:34 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/18/2021 15:23:34 - INFO - __main__ -   ***** Running evaluation  in da *****
10/18/2021 15:23:34 - INFO - __main__ -     Num examples = 565
10/18/2021 15:23:34 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]Evaluating:   6%|▌         | 1/18 [00:00<00:03,  5.48it/s]Evaluating:  11%|█         | 2/18 [00:00<00:02,  5.47it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  17%|█▋        | 3/18 [00:00<00:02,  5.47it/s]Evaluating:  22%|██▏       | 4/18 [00:00<00:02,  5.46it/s]Evaluating:  28%|██▊       | 5/18 [00:00<00:02,  5.45it/s]Evaluating:  33%|███▎      | 6/18 [00:01<00:02,  5.44it/s]Evaluating:  39%|███▉      | 7/18 [00:01<00:02,  4.91it/s]Evaluating:  44%|████▍     | 8/18 [00:01<00:01,  5.07it/s]Evaluating:  50%|█████     | 9/18 [00:01<00:01,  5.18it/s]Evaluating:  56%|█████▌    | 10/18 [00:01<00:01,  5.25it/s]Evaluating:  61%|██████    | 11/18 [00:02<00:01,  5.28it/s]Evaluating:  67%|██████▋   | 12/18 [00:02<00:01,  5.32it/s]Evaluating:  72%|███████▏  | 13/18 [00:02<00:00,  5.35it/s]Evaluating:  78%|███████▊  | 14/18 [00:02<00:00,  5.37it/s]Evaluating:  83%|████████▎ | 15/18 [00:02<00:00,  5.38it/s]Evaluating:  89%|████████▉ | 16/18 [00:03<00:00,  5.39it/s]Evaluating:  94%|█████████▍| 17/18 [00:03<00:00,  5.40it/s]Evaluating: 100%|██████████| 18/18 [00:03<00:00,  5.92it/s]Evaluating: 100%|██████████| 18/18 [00:03<00:00,  5.42it/s]
10/18/2021 15:23:38 - INFO - __main__ -   ***** Evaluation result  in da *****
10/18/2021 15:23:38 - INFO - __main__ -     f1 = 0.8714680263437434
10/18/2021 15:23:38 - INFO - __main__ -     loss = 0.41142405072848004
10/18/2021 15:23:38 - INFO - __main__ -     precision = 0.8728588147675285
10/18/2021 15:23:38 - INFO - __main__ -     recall = 0.8700816629547142
10/18/2021 15:23:38 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:23:55 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:23:55 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:23:55 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/
10/18/2021 15:23:55 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:23:55 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s2/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:23:55 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/18/2021 15:23:55 - INFO - __main__ -   Language = en
10/18/2021 15:23:55 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:23:56 - INFO - __main__ -   Language = ru
10/18/2021 15:23:56 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

10/18/2021 15:24:01 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
10/18/2021 15:24:01 - INFO - __main__ -   ***** Running evaluation  in be *****
10/18/2021 15:24:01 - INFO - __main__ -     Num examples = 932
10/18/2021 15:24:01 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/30 [00:00<00:05,  5.42it/s]Evaluating:   7%|▋         | 2/30 [00:00<00:04,  5.97it/s]Evaluating:  10%|█         | 3/30 [00:00<00:04,  6.26it/s]Evaluating:  13%|█▎        | 4/30 [00:00<00:04,  6.41it/s]Evaluating:  17%|█▋        | 5/30 [00:00<00:03,  6.48it/s]Evaluating:  20%|██        | 6/30 [00:00<00:03,  6.53it/s]Evaluating:  23%|██▎       | 7/30 [00:01<00:03,  6.54it/s]Evaluating:  27%|██▋       | 8/30 [00:01<00:03,  6.57it/s]Evaluating:  30%|███       | 9/30 [00:01<00:03,  6.59it/s]Evaluating:  33%|███▎      | 10/30 [00:01<00:03,  6.58it/s]Evaluating:  37%|███▋      | 11/30 [00:01<00:02,  6.58it/s]Evaluating:  40%|████      | 12/30 [00:01<00:02,  6.59it/s]Evaluating:  43%|████▎     | 13/30 [00:02<00:02,  6.58it/s]Evaluating:  47%|████▋     | 14/30 [00:02<00:02,  6.58it/s]Evaluating:  50%|█████     | 15/30 [00:02<00:02,  6.59it/s]Evaluating:  53%|█████▎    | 16/30 [00:02<00:02,  6.59it/s]Evaluating:  57%|█████▋    | 17/30 [00:02<00:01,  6.59it/s]Evaluating:  60%|██████    | 18/30 [00:02<00:01,  6.55it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  63%|██████▎   | 19/30 [00:02<00:01,  6.54it/s]10/18/2021 15:24:04 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:24:04 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  67%|██████▋   | 20/30 [00:03<00:01,  6.54it/s]Evaluating:  70%|███████   | 21/30 [00:03<00:01,  6.54it/s]Evaluating:  73%|███████▎  | 22/30 [00:03<00:01,  6.54it/s]Evaluating:  77%|███████▋  | 23/30 [00:03<00:01,  6.55it/s]Evaluating:  80%|████████  | 24/30 [00:03<00:00,  6.55it/s]Evaluating:  83%|████████▎ | 25/30 [00:03<00:00,  6.56it/s]Evaluating:  87%|████████▋ | 26/30 [00:03<00:00,  6.55it/s]Evaluating:  90%|█████████ | 27/30 [00:04<00:00,  6.54it/s]Evaluating:  93%|█████████▎| 28/30 [00:04<00:00,  6.49it/s]Evaluating:  97%|█████████▋| 29/30 [00:04<00:00,  6.48it/s]Evaluating: 100%|██████████| 30/30 [00:04<00:00,  6.69it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:24:06 - INFO - __main__ -   ***** Evaluation result  in be *****
10/18/2021 15:24:06 - INFO - __main__ -     f1 = 0.805916590083585
10/18/2021 15:24:06 - INFO - __main__ -     loss = 0.7873556271195412
10/18/2021 15:24:06 - INFO - __main__ -     precision = 0.8117781891915719
10/18/2021 15:24:06 - INFO - __main__ -     recall = 0.8001390337156761
10/18/2021 15:24:06 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
10/18/2021 15:24:06 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/18/2021 15:24:06 - INFO - __main__ -     Num examples = 915
10/18/2021 15:24:06 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/29 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/29 [00:00<00:04,  6.66it/s]Evaluating:   7%|▋         | 2/29 [00:00<00:04,  6.63it/s]Evaluating:  10%|█         | 3/29 [00:00<00:03,  6.62it/s]Evaluating:  14%|█▍        | 4/29 [00:00<00:03,  6.62it/s]Evaluating:  17%|█▋        | 5/29 [00:00<00:03,  6.61it/s]Evaluating:  21%|██        | 6/29 [00:00<00:03,  6.60it/s]Evaluating:  24%|██▍       | 7/29 [00:01<00:03,  6.59it/s]Evaluating:  28%|██▊       | 8/29 [00:01<00:03,  6.57it/s]Evaluating:  31%|███       | 9/29 [00:01<00:03,  6.54it/s]Evaluating:  34%|███▍      | 10/29 [00:01<00:02,  6.51it/s]Evaluating:  38%|███▊      | 11/29 [00:01<00:02,  6.50it/s]Evaluating:  41%|████▏     | 12/29 [00:01<00:02,  6.45it/s]Evaluating:  45%|████▍     | 13/29 [00:01<00:02,  6.41it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  48%|████▊     | 14/29 [00:02<00:02,  6.39it/s]Evaluating:  52%|█████▏    | 15/29 [00:02<00:02,  6.37it/s]Evaluating:  55%|█████▌    | 16/29 [00:02<00:02,  6.36it/s]Evaluating:  59%|█████▊    | 17/29 [00:02<00:01,  6.35it/s]Evaluating:  62%|██████▏   | 18/29 [00:02<00:01,  6.34it/s]Evaluating:  66%|██████▌   | 19/29 [00:02<00:01,  6.33it/s]Evaluating:  69%|██████▉   | 20/29 [00:03<00:01,  6.33it/s]Evaluating:  72%|███████▏  | 21/29 [00:03<00:01,  6.33it/s]Evaluating:  76%|███████▌  | 22/29 [00:03<00:01,  6.31it/s]Evaluating:  79%|███████▉  | 23/29 [00:03<00:00,  6.30it/s]Evaluating:  83%|████████▎ | 24/29 [00:03<00:00,  6.28it/s]Evaluating:  86%|████████▌ | 25/29 [00:03<00:00,  6.24it/s]Evaluating:  90%|████████▉ | 26/29 [00:04<00:00,  6.21it/s]Evaluating:  93%|█████████▎| 27/29 [00:04<00:00,  6.20it/s]Evaluating:  97%|█████████▋| 28/29 [00:04<00:00,  6.17it/s]Evaluating: 100%|██████████| 29/29 [00:04<00:00,  6.82it/s]Evaluating: 100%|██████████| 29/29 [00:04<00:00,  6.45it/s]
10/18/2021 15:24:11 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/18/2021 15:24:11 - INFO - __main__ -     f1 = 0.8078093634656479
10/18/2021 15:24:11 - INFO - __main__ -     loss = 0.7334953659567339
10/18/2021 15:24:11 - INFO - __main__ -     precision = 0.8136577093660319
10/18/2021 15:24:11 - INFO - __main__ -     recall = 0.802044490163302
10/18/2021 15:24:11 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
10/18/2021 15:24:11 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/18/2021 15:24:11 - INFO - __main__ -     Num examples = 1117
10/18/2021 15:24:11 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/35 [00:00<00:05,  6.26it/s]Evaluating:   6%|▌         | 2/35 [00:00<00:05,  6.11it/s]Evaluating:   9%|▊         | 3/35 [00:00<00:05,  6.07it/s]Evaluating:  11%|█▏        | 4/35 [00:00<00:05,  6.05it/s]Evaluating:  14%|█▍        | 5/35 [00:00<00:04,  6.03it/s]Evaluating:  17%|█▋        | 6/35 [00:00<00:04,  6.03it/s]Evaluating:  20%|██        | 7/35 [00:01<00:04,  6.02it/s]Evaluating:  23%|██▎       | 8/35 [00:01<00:04,  6.00it/s]Evaluating:  26%|██▌       | 9/35 [00:01<00:04,  6.00it/s]Evaluating:  29%|██▊       | 10/35 [00:01<00:04,  6.00it/s]Evaluating:  31%|███▏      | 11/35 [00:01<00:04,  5.99it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  34%|███▍      | 12/35 [00:01<00:03,  5.99it/s]Evaluating:  37%|███▋      | 13/35 [00:02<00:03,  6.00it/s]Evaluating:  40%|████      | 14/35 [00:02<00:03,  5.99it/s]Evaluating:  43%|████▎     | 15/35 [00:02<00:03,  6.00it/s]Evaluating:  46%|████▌     | 16/35 [00:02<00:03,  5.99it/s]Evaluating:  49%|████▊     | 17/35 [00:02<00:03,  5.97it/s]Evaluating:  51%|█████▏    | 18/35 [00:02<00:02,  5.98it/s]Evaluating:  54%|█████▍    | 19/35 [00:03<00:02,  5.97it/s]Evaluating:  57%|█████▋    | 20/35 [00:03<00:02,  5.96it/s]Evaluating:  60%|██████    | 21/35 [00:03<00:02,  5.96it/s]Evaluating:  63%|██████▎   | 22/35 [00:03<00:02,  5.96it/s]Evaluating:  66%|██████▌   | 23/35 [00:03<00:02,  5.96it/s]Evaluating:  69%|██████▊   | 24/35 [00:04<00:01,  5.97it/s]Evaluating:  71%|███████▏  | 25/35 [00:04<00:01,  5.96it/s]Evaluating:  74%|███████▍  | 26/35 [00:04<00:01,  5.95it/s]Evaluating:  77%|███████▋  | 27/35 [00:04<00:01,  5.96it/s]Evaluating:  80%|████████  | 28/35 [00:04<00:01,  5.96it/s]Evaluating:  83%|████████▎ | 29/35 [00:04<00:01,  5.94it/s]Evaluating:  86%|████████▌ | 30/35 [00:05<00:00,  5.94it/s]Evaluating:  89%|████████▊ | 31/35 [00:05<00:00,  5.95it/s]Evaluating:  91%|█████████▏| 32/35 [00:05<00:00,  5.94it/s]Evaluating:  94%|█████████▍| 33/35 [00:05<00:00,  5.97it/s]Evaluating:  97%|█████████▋| 34/35 [00:05<00:00,  6.00it/s]Evaluating: 100%|██████████| 35/35 [00:05<00:00,  6.07it/s]Evaluating: 100%|██████████| 35/35 [00:05<00:00,  5.99it/s]
10/18/2021 15:24:17 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/18/2021 15:24:17 - INFO - __main__ -     f1 = 0.846035873841435
10/18/2021 15:24:17 - INFO - __main__ -     loss = 0.5796676286629268
10/18/2021 15:24:17 - INFO - __main__ -     precision = 0.8490932209061098
10/18/2021 15:24:17 - INFO - __main__ -     recall = 0.8430004650853764
10/18/2021 15:24:17 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:24:28 - INFO - __main__ -   Using lang2id = None
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
PyTorch version 1.9.0+cu102 available.
10/18/2021 15:24:29 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:24:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:24:29 - INFO - __main__ -   Seed = 3
10/18/2021 15:24:29 - INFO - root -   save model
10/18/2021 15:24:29 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='fo,no,da', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,is,de_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.33,0.33,0.33', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:24:29 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:24:42 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:24:42 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:25:07 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:25:07 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:25:07 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
10/18/2021 15:25:07 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:25:07 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:25:07 - INFO - root -   loading lang adpater en/wiki@ukp,is/wiki@ukp,de/wiki@ukp
10/18/2021 15:25:07 - INFO - __main__ -   Language = en
10/18/2021 15:25:07 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:25:08 - INFO - __main__ -   Language = is
10/18/2021 15:25:08 - INFO - __main__ -   Adapter Name = is/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/is/bert-base-multilingual-cased/pfeiffer/is_pfeiffer_gelu_nd.zip.
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:25:09 - INFO - __main__ -   Using lang2id = None
Loading module configuration from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/adapter_config.json
Adding adapter 'is' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/9e33d05df2faefa3bc0c9362c76f6d832a092fa705c554c913660889d25971bd-ca498748f27d97f5ac394ca8b673871d44c30209e39a0678c92f272435c2e7db-extracted'
10/18/2021 15:25:09 - INFO - __main__ -   Language = de
10/18/2021 15:25:09 - INFO - __main__ -   Adapter Name = de/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/de/bert-base-multilingual-cased/pfeiffer/de_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/adapter_config.json
Adding adapter 'de' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/08ef10ebc1fdaef8ab51afedb140cb376629ba3d6d003ebb84d671fc6c449137-323d2b3cb18cc775356074e7ae17939f1dbe46a8adb8d13e3929f68b0c67e907-extracted'
PyTorch version 1.9.0+cu102 available.
10/18/2021 15:25:11 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:25:11 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/18/2021 15:25:11 - INFO - __main__ -   Seed = 3
10/18/2021 15:25:11 - INFO - root -   save model
10/18/2021 15:25:11 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=3, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/my-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/18/2021 15:25:11 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
10/18/2021 15:25:13 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_fo_bert-base-multilingual-cased_128
10/18/2021 15:25:13 - INFO - __main__ -   ***** Running evaluation  in fo *****
10/18/2021 15:25:13 - INFO - __main__ -     Num examples = 1516
10/18/2021 15:25:13 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]Evaluating:   2%|▏         | 1/48 [00:00<00:08,  5.42it/s]Evaluating:   4%|▍         | 2/48 [00:00<00:08,  5.62it/s]Evaluating:   6%|▋         | 3/48 [00:00<00:07,  5.81it/s]Evaluating:   8%|▊         | 4/48 [00:00<00:07,  5.91it/s]Evaluating:  10%|█         | 5/48 [00:00<00:07,  5.96it/s]Evaluating:  12%|█▎        | 6/48 [00:01<00:07,  5.99it/s]Evaluating:  15%|█▍        | 7/48 [00:01<00:06,  6.00it/s]Evaluating:  17%|█▋        | 8/48 [00:01<00:06,  6.01it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:  19%|█▉        | 9/48 [00:01<00:06,  6.01it/s]Evaluating:  21%|██        | 10/48 [00:01<00:06,  6.00it/s]Evaluating:  23%|██▎       | 11/48 [00:01<00:06,  6.01it/s]Evaluating:  25%|██▌       | 12/48 [00:02<00:05,  6.02it/s]Evaluating:  27%|██▋       | 13/48 [00:02<00:05,  6.01it/s]Evaluating:  29%|██▉       | 14/48 [00:02<00:05,  6.01it/s]Evaluating:  31%|███▏      | 15/48 [00:02<00:05,  6.01it/s]Evaluating:  33%|███▎      | 16/48 [00:02<00:05,  6.00it/s]Evaluating:  35%|███▌      | 17/48 [00:02<00:05,  6.00it/s]Evaluating:  38%|███▊      | 18/48 [00:03<00:05,  6.00it/s]Evaluating:  40%|███▉      | 19/48 [00:03<00:04,  5.99it/s]Evaluating:  42%|████▏     | 20/48 [00:03<00:04,  5.99it/s]Evaluating:  44%|████▍     | 21/48 [00:03<00:04,  5.99it/s]Evaluating:  46%|████▌     | 22/48 [00:03<00:04,  5.98it/s]Evaluating:  48%|████▊     | 23/48 [00:03<00:04,  5.99it/s]Evaluating:  50%|█████     | 24/48 [00:04<00:04,  5.99it/s]Evaluating:  52%|█████▏    | 25/48 [00:04<00:03,  5.97it/s]Evaluating:  54%|█████▍    | 26/48 [00:04<00:03,  5.97it/s]Evaluating:  56%|█████▋    | 27/48 [00:04<00:03,  5.98it/s]Evaluating:  58%|█████▊    | 28/48 [00:04<00:03,  5.97it/s]Evaluating:  60%|██████    | 29/48 [00:04<00:03,  5.97it/s]Evaluating:  62%|██████▎   | 30/48 [00:05<00:03,  5.97it/s]Evaluating:  65%|██████▍   | 31/48 [00:05<00:02,  5.95it/s]loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Evaluating:  67%|██████▋   | 32/48 [00:05<00:02,  5.95it/s]Evaluating:  69%|██████▉   | 33/48 [00:05<00:02,  5.96it/s]Evaluating:  71%|███████   | 34/48 [00:05<00:02,  5.95it/s]Evaluating:  73%|███████▎  | 35/48 [00:05<00:02,  5.95it/s]Evaluating:  75%|███████▌  | 36/48 [00:06<00:02,  5.96it/s]Evaluating:  77%|███████▋  | 37/48 [00:06<00:01,  5.91it/s]Evaluating:  79%|███████▉  | 38/48 [00:06<00:01,  5.92it/s]Evaluating:  81%|████████▏ | 39/48 [00:06<00:01,  5.93it/s]Evaluating:  83%|████████▎ | 40/48 [00:06<00:01,  5.94it/s]Evaluating:  85%|████████▌ | 41/48 [00:06<00:01,  5.93it/s]Evaluating:  88%|████████▊ | 42/48 [00:07<00:01,  5.92it/s]Evaluating:  90%|████████▉ | 43/48 [00:07<00:00,  5.92it/s]Evaluating:  92%|█████████▏| 44/48 [00:07<00:00,  5.92it/s]Evaluating:  94%|█████████▍| 45/48 [00:07<00:00,  5.91it/s]Evaluating:  96%|█████████▌| 46/48 [00:07<00:00,  5.91it/s]Evaluating:  98%|█████████▊| 47/48 [00:07<00:00,  5.91it/s]Evaluating: 100%|██████████| 48/48 [00:07<00:00,  6.02it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:25:22 - INFO - __main__ -   ***** Evaluation result  in fo *****
10/18/2021 15:25:22 - INFO - __main__ -     f1 = 0.7577168669659536
10/18/2021 15:25:22 - INFO - __main__ -     loss = 0.8452212108920018
10/18/2021 15:25:22 - INFO - __main__ -     precision = 0.7655941849326646
10/18/2021 15:25:22 - INFO - __main__ -     recall = 0.75
10/18/2021 15:25:22 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_no_bert-base-multilingual-cased_128
10/18/2021 15:25:22 - INFO - __main__ -   ***** Running evaluation  in no *****
10/18/2021 15:25:22 - INFO - __main__ -     Num examples = 4408
10/18/2021 15:25:22 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/138 [00:00<?, ?it/s]Evaluating:   1%|          | 1/138 [00:00<00:22,  6.08it/s]Evaluating:   1%|▏         | 2/138 [00:00<00:22,  6.04it/s]Evaluating:   2%|▏         | 3/138 [00:00<00:22,  6.03it/s]Evaluating:   3%|▎         | 4/138 [00:00<00:22,  6.03it/s]Evaluating:   4%|▎         | 5/138 [00:00<00:22,  6.01it/s]Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Evaluating:   4%|▍         | 6/138 [00:00<00:22,  6.00it/s]Evaluating:   5%|▌         | 7/138 [00:01<00:21,  5.99it/s]Evaluating:   6%|▌         | 8/138 [00:01<00:21,  5.99it/s]Evaluating:   7%|▋         | 9/138 [00:01<00:21,  6.00it/s]Evaluating:   7%|▋         | 10/138 [00:01<00:21,  5.99it/s]loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

Evaluating:   8%|▊         | 11/138 [00:01<00:21,  6.00it/s]Evaluating:   9%|▊         | 12/138 [00:01<00:21,  5.98it/s]Evaluating:   9%|▉         | 13/138 [00:02<00:20,  5.96it/s]Evaluating:  10%|█         | 14/138 [00:02<00:20,  5.93it/s]Evaluating:  11%|█         | 15/138 [00:02<00:20,  5.91it/s]Evaluating:  12%|█▏        | 16/138 [00:02<00:20,  5.90it/s]Evaluating:  12%|█▏        | 17/138 [00:02<00:20,  5.90it/s]Evaluating:  13%|█▎        | 18/138 [00:03<00:20,  5.90it/s]Evaluating:  14%|█▍        | 19/138 [00:03<00:20,  5.89it/s]Evaluating:  14%|█▍        | 20/138 [00:03<00:20,  5.89it/s]Evaluating:  15%|█▌        | 21/138 [00:03<00:19,  5.89it/s]Evaluating:  16%|█▌        | 22/138 [00:03<00:19,  5.90it/s]Evaluating:  17%|█▋        | 23/138 [00:03<00:19,  5.91it/s]Evaluating:  17%|█▋        | 24/138 [00:04<00:19,  5.91it/s]Evaluating:  18%|█▊        | 25/138 [00:04<00:19,  5.90it/s]Evaluating:  19%|█▉        | 26/138 [00:04<00:18,  5.91it/s]Evaluating:  20%|█▉        | 27/138 [00:04<00:18,  5.91it/s]loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
Evaluating:  20%|██        | 28/138 [00:04<00:18,  5.92it/s]10/18/2021 15:25:27 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:25:27 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/18/2021 15:25:27 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/
10/18/2021 15:25:27 - INFO - root -   Trying to decide if add adapter
10/18/2021 15:25:27 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Evaluating:  21%|██        | 29/138 [00:04<00:18,  5.93it/s]Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s3/checkpoint-best/udpos/pytorch_model_head.bin
10/18/2021 15:25:27 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/18/2021 15:25:27 - INFO - __main__ -   Language = en
10/18/2021 15:25:27 - INFO - __main__ -   Adapter Name = en/wiki@ukp
Evaluating:  22%|██▏       | 30/138 [00:05<00:18,  5.92it/s]Evaluating:  22%|██▏       | 31/138 [00:05<00:18,  5.93it/s]No exactly matching adapter config found for this specifier, falling back to default.
Evaluating:  23%|██▎       | 32/138 [00:05<00:17,  5.93it/s]Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Evaluating:  24%|██▍       | 33/138 [00:05<00:17,  5.90it/s]Evaluating:  25%|██▍       | 34/138 [00:05<00:17,  5.90it/s]Evaluating:  25%|██▌       | 35/138 [00:05<00:17,  5.87it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Evaluating:  26%|██▌       | 36/138 [00:06<00:17,  5.83it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/18/2021 15:25:28 - INFO - __main__ -   Language = ru
10/18/2021 15:25:28 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Evaluating:  27%|██▋       | 37/138 [00:06<00:17,  5.81it/s]Evaluating:  28%|██▊       | 38/138 [00:06<00:17,  5.79it/s]Evaluating:  28%|██▊       | 39/138 [00:06<00:17,  5.76it/s]Evaluating:  29%|██▉       | 40/138 [00:06<00:17,  5.75it/s]Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Evaluating:  30%|██▉       | 41/138 [00:06<00:16,  5.74it/s]Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
Evaluating:  30%|███       | 42/138 [00:07<00:16,  5.71it/s]Evaluating:  31%|███       | 43/138 [00:07<00:16,  5.62it/s]Evaluating:  32%|███▏      | 44/138 [00:07<00:17,  5.52it/s]Evaluating:  33%|███▎      | 45/138 [00:07<00:16,  5.49it/s]Evaluating:  33%|███▎      | 46/138 [00:07<00:16,  5.50it/s]Evaluating:  34%|███▍      | 47/138 [00:08<00:16,  5.48it/s]Evaluating:  35%|███▍      | 48/138 [00:08<00:16,  5.48it/s]Evaluating:  36%|███▌      | 49/138 [00:08<00:16,  5.50it/s]Evaluating:  36%|███▌      | 50/138 [00:08<00:16,  5.40it/s]Evaluating:  37%|███▋      | 51/138 [00:08<00:16,  5.33it/s]Evaluating:  38%|███▊      | 52/138 [00:08<00:16,  5.26it/s]Evaluating:  38%|███▊      | 53/138 [00:09<00:16,  5.21it/s]Evaluating:  39%|███▉      | 54/138 [00:09<00:16,  5.20it/s]Evaluating:  40%|███▉      | 55/138 [00:09<00:15,  5.20it/s]Evaluating:  41%|████      | 56/138 [00:09<00:15,  5.19it/s]Evaluating:  41%|████▏     | 57/138 [00:09<00:15,  5.14it/s]Evaluating:  42%|████▏     | 58/138 [00:10<00:15,  5.17it/s]Evaluating:  43%|████▎     | 59/138 [00:10<00:15,  5.21it/s]Evaluating:  43%|████▎     | 60/138 [00:10<00:14,  5.21it/s]Evaluating:  44%|████▍     | 61/138 [00:10<00:14,  5.22it/s]Evaluating:  45%|████▍     | 62/138 [00:10<00:14,  5.26it/s]Evaluating:  46%|████▌     | 63/138 [00:11<00:14,  5.26it/s]Evaluating:  46%|████▋     | 64/138 [00:11<00:14,  5.17it/s]Evaluating:  47%|████▋     | 65/138 [00:11<00:14,  5.10it/s]Evaluating:  48%|████▊     | 66/138 [00:11<00:14,  5.07it/s]Evaluating:  49%|████▊     | 67/138 [00:11<00:13,  5.09it/s]Evaluating:  49%|████▉     | 68/138 [00:12<00:13,  5.09it/s]Evaluating:  50%|█████     | 69/138 [00:12<00:13,  5.11it/s]Evaluating:  51%|█████     | 70/138 [00:12<00:13,  5.18it/s]Evaluating:  51%|█████▏    | 71/138 [00:12<00:12,  5.17it/s]Evaluating:  52%|█████▏    | 72/138 [00:12<00:12,  5.12it/s]Evaluating:  53%|█████▎    | 73/138 [00:13<00:12,  5.08it/s]10/18/2021 15:25:35 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
Evaluating:  54%|█████▎    | 74/138 [00:13<00:12,  5.15it/s]10/18/2021 15:25:36 - INFO - __main__ -   ***** Running evaluation  in be *****
10/18/2021 15:25:36 - INFO - __main__ -     Num examples = 932
10/18/2021 15:25:36 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]Evaluating:  54%|█████▍    | 75/138 [00:13<00:15,  4.13it/s]Evaluating:   3%|▎         | 1/30 [00:00<00:10,  2.86it/s]Evaluating:   7%|▋         | 2/30 [00:00<00:10,  2.80it/s]Evaluating:  55%|█████▌    | 76/138 [00:14<00:17,  3.50it/s]Evaluating:  10%|█         | 3/30 [00:01<00:09,  2.86it/s]Evaluating:  56%|█████▌    | 77/138 [00:14<00:19,  3.13it/s]Evaluating:  13%|█▎        | 4/30 [00:01<00:09,  2.89it/s]Evaluating:  57%|█████▋    | 78/138 [00:14<00:20,  2.92it/s]Evaluating:  17%|█▋        | 5/30 [00:01<00:08,  2.89it/s]Evaluating:  57%|█████▋    | 79/138 [00:15<00:21,  2.80it/s]Evaluating:  20%|██        | 6/30 [00:02<00:08,  2.89it/s]Evaluating:  58%|█████▊    | 80/138 [00:15<00:21,  2.72it/s]Evaluating:  23%|██▎       | 7/30 [00:02<00:07,  2.89it/s]Evaluating:  59%|█████▊    | 81/138 [00:15<00:21,  2.66it/s]Evaluating:  27%|██▋       | 8/30 [00:02<00:07,  2.89it/s]Evaluating:  59%|█████▉    | 82/138 [00:16<00:21,  2.62it/s]Evaluating:  30%|███       | 9/30 [00:03<00:07,  2.90it/s]Evaluating:  33%|███▎      | 10/30 [00:03<00:06,  2.87it/s]Evaluating:  60%|██████    | 83/138 [00:16<00:21,  2.59it/s]Evaluating:  37%|███▋      | 11/30 [00:03<00:06,  2.88it/s]Evaluating:  61%|██████    | 84/138 [00:17<00:20,  2.58it/s]Evaluating:  40%|████      | 12/30 [00:04<00:06,  2.89it/s]Evaluating:  62%|██████▏   | 85/138 [00:17<00:20,  2.56it/s]Evaluating:  43%|████▎     | 13/30 [00:04<00:05,  2.90it/s]Evaluating:  62%|██████▏   | 86/138 [00:17<00:20,  2.55it/s]Evaluating:  47%|████▋     | 14/30 [00:04<00:05,  2.90it/s]Evaluating:  63%|██████▎   | 87/138 [00:18<00:20,  2.54it/s]Evaluating:  50%|█████     | 15/30 [00:05<00:05,  2.91it/s]Evaluating:  64%|██████▍   | 88/138 [00:18<00:19,  2.54it/s]Evaluating:  53%|█████▎    | 16/30 [00:05<00:04,  2.90it/s]Evaluating:  64%|██████▍   | 89/138 [00:19<00:19,  2.55it/s]Evaluating:  57%|█████▋    | 17/30 [00:05<00:04,  2.90it/s]Evaluating:  65%|██████▌   | 90/138 [00:19<00:18,  2.54it/s]Evaluating:  60%|██████    | 18/30 [00:06<00:04,  2.87it/s]Evaluating:  63%|██████▎   | 19/30 [00:06<00:03,  2.85it/s]Evaluating:  66%|██████▌   | 91/138 [00:19<00:18,  2.56it/s]Evaluating:  67%|██████▋   | 20/30 [00:06<00:03,  2.87it/s]Evaluating:  67%|██████▋   | 92/138 [00:20<00:18,  2.55it/s]Evaluating:  70%|███████   | 21/30 [00:07<00:03,  2.88it/s]Evaluating:  67%|██████▋   | 93/138 [00:20<00:17,  2.55it/s]Evaluating:  73%|███████▎  | 22/30 [00:07<00:02,  2.89it/s]Evaluating:  68%|██████▊   | 94/138 [00:21<00:17,  2.55it/s]Evaluating:  77%|███████▋  | 23/30 [00:07<00:02,  2.89it/s]Evaluating:  69%|██████▉   | 95/138 [00:21<00:16,  2.54it/s]Evaluating:  80%|████████  | 24/30 [00:08<00:02,  2.90it/s]Evaluating:  70%|██████▉   | 96/138 [00:21<00:16,  2.54it/s]Evaluating:  83%|████████▎ | 25/30 [00:08<00:01,  2.90it/s]Evaluating:  70%|███████   | 97/138 [00:22<00:16,  2.54it/s]Evaluating:  87%|████████▋ | 26/30 [00:09<00:01,  2.90it/s]Evaluating:  90%|█████████ | 27/30 [00:09<00:01,  2.85it/s]Evaluating:  71%|███████   | 98/138 [00:22<00:15,  2.55it/s]Evaluating:  93%|█████████▎| 28/30 [00:09<00:00,  2.88it/s]Evaluating:  72%|███████▏  | 99/138 [00:23<00:15,  2.55it/s]Evaluating:  97%|█████████▋| 29/30 [00:10<00:00,  2.89it/s]Evaluating:  72%|███████▏  | 100/138 [00:23<00:14,  2.62it/s]Evaluating: 100%|██████████| 30/30 [00:10<00:00,  3.56it/s]Evaluating: 100%|██████████| 30/30 [00:10<00:00,  2.95it/s]Evaluating:  73%|███████▎  | 101/138 [00:23<00:12,  3.06it/s]
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADV seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: VERB seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PROPN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PUNCT seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NOUN seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ADP seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PRON seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: CCONJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PART seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NUM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: AUX seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DET seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SYM seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: X seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
/home/abhijeet/rohan/venvs/emea/lib/python3.7/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: INTJ seems not to be NE tag.
  warnings.warn('{} seems not to be NE tag.'.format(chunk))
10/18/2021 15:25:46 - INFO - __main__ -   ***** Evaluation result  in be *****
10/18/2021 15:25:46 - INFO - __main__ -     f1 = 0.8148505726949576
10/18/2021 15:25:46 - INFO - __main__ -     loss = 0.7116822396715482
10/18/2021 15:25:46 - INFO - __main__ -     precision = 0.8231226172533026
10/18/2021 15:25:46 - INFO - __main__ -     recall = 0.8067431352102885
10/18/2021 15:25:46 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_uk_bert-base-multilingual-cased_128
Evaluating:  74%|███████▍  | 102/138 [00:23<00:10,  3.51it/s]10/18/2021 15:25:46 - INFO - __main__ -   ***** Running evaluation  in uk *****
10/18/2021 15:25:46 - INFO - __main__ -     Num examples = 915
10/18/2021 15:25:46 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/29 [00:00<?, ?it/s]Evaluating:  75%|███████▍  | 103/138 [00:24<00:10,  3.44it/s]Evaluating:   3%|▎         | 1/29 [00:00<00:09,  2.93it/s]Evaluating:  75%|███████▌  | 104/138 [00:24<00:10,  3.10it/s]Evaluating:   7%|▋         | 2/29 [00:00<00:09,  2.92it/s]Evaluating:  76%|███████▌  | 105/138 [00:24<00:11,  2.90it/s]Evaluating:  10%|█         | 3/29 [00:01<00:08,  2.92it/s]Evaluating:  14%|█▍        | 4/29 [00:01<00:08,  2.85it/s]Evaluating:  77%|███████▋  | 106/138 [00:25<00:11,  2.80it/s]Evaluating:  17%|█▋        | 5/29 [00:01<00:08,  2.88it/s]Evaluating:  78%|███████▊  | 107/138 [00:25<00:11,  2.71it/s]Evaluating:  21%|██        | 6/29 [00:02<00:07,  2.89it/s]Evaluating:  78%|███████▊  | 108/138 [00:26<00:11,  2.65it/s]Evaluating:  24%|██▍       | 7/29 [00:02<00:07,  2.90it/s]Evaluating:  79%|███████▉  | 109/138 [00:26<00:11,  2.61it/s]Evaluating:  28%|██▊       | 8/29 [00:02<00:07,  2.90it/s]Evaluating:  80%|███████▉  | 110/138 [00:26<00:10,  2.58it/s]Evaluating:  31%|███       | 9/29 [00:03<00:06,  2.91it/s]Evaluating:  80%|████████  | 111/138 [00:27<00:10,  2.56it/s]Evaluating:  34%|███▍      | 10/29 [00:03<00:06,  2.91it/s]Evaluating:  81%|████████  | 112/138 [00:27<00:10,  2.55it/s]Evaluating:  38%|███▊      | 11/29 [00:03<00:06,  2.91it/s]Evaluating:  41%|████▏     | 12/29 [00:04<00:05,  2.86it/s]Evaluating:  82%|████████▏ | 113/138 [00:28<00:09,  2.57it/s]Evaluating:  45%|████▍     | 13/29 [00:04<00:05,  2.88it/s]Evaluating:  83%|████████▎ | 114/138 [00:28<00:09,  2.56it/s]Evaluating:  48%|████▊     | 14/29 [00:04<00:05,  2.89it/s]Evaluating:  83%|████████▎ | 115/138 [00:28<00:09,  2.52it/s]Evaluating:  52%|█████▏    | 15/29 [00:05<00:04,  2.95it/s]Evaluating:  84%|████████▍ | 116/138 [00:29<00:08,  2.51it/s]Evaluating:  55%|█████▌    | 16/29 [00:05<00:04,  2.94it/s]Evaluating:  85%|████████▍ | 117/138 [00:29<00:08,  2.52it/s]Evaluating:  59%|█████▊    | 17/29 [00:05<00:04,  2.93it/s]Evaluating:  86%|████████▌ | 118/138 [00:30<00:07,  2.52it/s]Evaluating:  62%|██████▏   | 18/29 [00:06<00:03,  2.94it/s]Evaluating:  66%|██████▌   | 19/29 [00:06<00:03,  2.88it/s]Evaluating:  86%|████████▌ | 119/138 [00:30<00:07,  2.53it/s]Evaluating:  69%|██████▉   | 20/29 [00:06<00:03,  2.90it/s]Evaluating:  87%|████████▋ | 120/138 [00:30<00:07,  2.53it/s]Evaluating:  72%|███████▏  | 21/29 [00:07<00:02,  2.91it/s]Evaluating:  88%|████████▊ | 121/138 [00:31<00:06,  2.53it/s]Evaluating:  76%|███████▌  | 22/29 [00:07<00:02,  2.91it/s]Evaluating:  88%|████████▊ | 122/138 [00:31<00:06,  2.53it/s]Evaluating:  79%|███████▉  | 23/29 [00:07<00:02,  2.91it/s]Evaluating:  89%|████████▉ | 123/138 [00:32<00:05,  2.53it/s]Evaluating:  83%|████████▎ | 24/29 [00:08<00:01,  2.92it/s]Evaluating:  90%|████████▉ | 124/138 [00:32<00:05,  2.52it/s]Evaluating:  86%|████████▌ | 25/29 [00:08<00:01,  2.92it/s]Evaluating:  91%|█████████ | 125/138 [00:32<00:05,  2.52it/s]Evaluating:  90%|████████▉ | 26/29 [00:08<00:01,  2.91it/s]Evaluating:  93%|█████████▎| 27/29 [00:09<00:00,  2.86it/s]Evaluating:  91%|█████████▏| 126/138 [00:33<00:04,  2.54it/s]Evaluating:  97%|█████████▋| 28/29 [00:09<00:00,  2.89it/s]Evaluating:  92%|█████████▏| 127/138 [00:33<00:04,  2.54it/s]Evaluating: 100%|██████████| 29/29 [00:09<00:00,  3.24it/s]Evaluating: 100%|██████████| 29/29 [00:09<00:00,  2.94it/s]Evaluating:  93%|█████████▎| 128/138 [00:33<00:03,  2.79it/s]Evaluating:  93%|█████████▎| 129/138 [00:34<00:02,  3.25it/s]
10/18/2021 15:25:56 - INFO - __main__ -   ***** Evaluation result  in uk *****
10/18/2021 15:25:56 - INFO - __main__ -     f1 = 0.8136131079593291
10/18/2021 15:25:56 - INFO - __main__ -     loss = 0.6401112757880112
10/18/2021 15:25:56 - INFO - __main__ -     precision = 0.8196111691022965
10/18/2021 15:25:56 - INFO - __main__ -     recall = 0.8077021987913077
10/18/2021 15:25:56 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_bg_bert-base-multilingual-cased_128
Evaluating:  94%|█████████▍| 130/138 [00:34<00:02,  3.68it/s]10/18/2021 15:25:57 - INFO - __main__ -   ***** Running evaluation  in bg *****
10/18/2021 15:25:57 - INFO - __main__ -     Num examples = 1117
10/18/2021 15:25:57 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/35 [00:00<?, ?it/s]Evaluating:   3%|▎         | 1/35 [00:00<00:11,  2.98it/s]Evaluating:  95%|█████████▍| 131/138 [00:34<00:02,  3.36it/s]Evaluating:   6%|▌         | 2/35 [00:00<00:11,  2.98it/s]Evaluating:  96%|█████████▌| 132/138 [00:35<00:01,  3.03it/s]Evaluating:   9%|▊         | 3/35 [00:00<00:10,  3.01it/s]Evaluating:  96%|█████████▋| 133/138 [00:35<00:01,  2.84it/s]Evaluating:  11%|█▏        | 4/35 [00:01<00:10,  3.01it/s]Evaluating:  97%|█████████▋| 134/138 [00:35<00:01,  2.71it/s]Evaluating:  14%|█▍        | 5/35 [00:01<00:09,  3.02it/s]Evaluating:  98%|█████████▊| 135/138 [00:36<00:01,  2.68it/s]Evaluating:  17%|█▋        | 6/35 [00:02<00:09,  2.96it/s]Evaluating:  99%|█████████▊| 136/138 [00:36<00:00,  2.61it/s]Evaluating:  20%|██        | 7/35 [00:02<00:09,  2.96it/s]Evaluating:  23%|██▎       | 8/35 [00:02<00:09,  2.89it/s]Evaluating:  99%|█████████▉| 137/138 [00:37<00:00,  2.60it/s]Evaluating:  26%|██▌       | 9/35 [00:03<00:08,  2.93it/s]Evaluating: 100%|██████████| 138/138 [00:37<00:00,  2.72it/s]Evaluating: 100%|██████████| 138/138 [00:37<00:00,  3.70it/s]Evaluating:  29%|██▊       | 10/35 [00:03<00:07,  3.48it/s]Evaluating:  31%|███▏      | 11/35 [00:03<00:05,  4.00it/s]Evaluating:  34%|███▍      | 12/35 [00:03<00:05,  4.47it/s]Evaluating:  37%|███▋      | 13/35 [00:03<00:04,  4.85it/s]Evaluating:  40%|████      | 14/35 [00:03<00:04,  5.16it/s]Evaluating:  43%|████▎     | 15/35 [00:04<00:03,  5.40it/s]Evaluating:  46%|████▌     | 16/35 [00:04<00:03,  5.58it/s]Evaluating:  49%|████▊     | 17/35 [00:04<00:03,  5.71it/s]Evaluating:  51%|█████▏    | 18/35 [00:04<00:02,  5.81it/s]
10/18/2021 15:26:01 - INFO - __main__ -   ***** Evaluation result  in no *****
10/18/2021 15:26:01 - INFO - __main__ -     f1 = 0.8427823970796998
10/18/2021 15:26:01 - INFO - __main__ -     loss = 0.5421005683964577
10/18/2021 15:26:01 - INFO - __main__ -     precision = 0.841464994978456
10/18/2021 15:26:01 - INFO - __main__ -     recall = 0.844103930712858
Evaluating:  54%|█████▍    | 19/35 [00:04<00:02,  5.87it/s]10/18/2021 15:26:01 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_da_bert-base-multilingual-cased_128
10/18/2021 15:26:01 - INFO - __main__ -   ***** Running evaluation  in da *****
10/18/2021 15:26:01 - INFO - __main__ -     Num examples = 565
10/18/2021 15:26:01 - INFO - __main__ -     Batch size = 32
Evaluating:   0%|          | 0/18 [00:00<?, ?it/s]Evaluating:  57%|█████▋    | 20/35 [00:04<00:02,  5.49it/s]Evaluating:   6%|▌         | 1/18 [00:00<00:06,  2.59it/s]Evaluating:  60%|██████    | 21/35 [00:05<00:03,  4.29it/s]Evaluating:  11%|█         | 2/18 [00:00<00:06,  2.58it/s]Evaluating:  63%|██████▎   | 22/35 [00:05<00:03,  3.73it/s]Evaluating:  17%|█▋        | 3/18 [00:01<00:05,  2.60it/s]Evaluating:  66%|██████▌   | 23/35 [00:05<00:03,  3.36it/s]Evaluating:  69%|██████▊   | 24/35 [00:06<00:03,  3.14it/s]Evaluating:  22%|██▏       | 4/18 [00:01<00:05,  2.63it/s]Evaluating:  71%|███████▏  | 25/35 [00:06<00:03,  3.06it/s]Evaluating:  28%|██▊       | 5/18 [00:01<00:04,  2.64it/s]Evaluating:  74%|███████▍  | 26/35 [00:07<00:03,  2.96it/s]Evaluating:  33%|███▎      | 6/18 [00:02<00:04,  2.62it/s]Evaluating:  77%|███████▋  | 27/35 [00:07<00:02,  2.92it/s]Evaluating:  39%|███▉      | 7/18 [00:02<00:04,  2.60it/s]Evaluating:  80%|████████  | 28/35 [00:07<00:02,  2.90it/s]Evaluating:  44%|████▍     | 8/18 [00:03<00:03,  2.59it/s]Evaluating:  83%|████████▎ | 29/35 [00:08<00:02,  2.88it/s]Evaluating:  50%|█████     | 9/18 [00:03<00:03,  2.59it/s]Evaluating:  86%|████████▌ | 30/35 [00:08<00:01,  2.87it/s]Evaluating:  56%|█████▌    | 10/18 [00:03<00:03,  2.58it/s]Evaluating:  89%|████████▊ | 31/35 [00:08<00:01,  2.86it/s]Evaluating:  61%|██████    | 11/18 [00:04<00:02,  2.58it/s]Evaluating:  91%|█████████▏| 32/35 [00:09<00:01,  2.86it/s]Evaluating:  67%|██████▋   | 12/18 [00:04<00:02,  2.58it/s]Evaluating:  94%|█████████▍| 33/35 [00:09<00:00,  2.85it/s]Evaluating:  72%|███████▏  | 13/18 [00:05<00:01,  2.58it/s]Evaluating:  97%|█████████▋| 34/35 [00:09<00:00,  2.85it/s]Evaluating:  78%|███████▊  | 14/18 [00:05<00:01,  2.59it/s]Evaluating: 100%|██████████| 35/35 [00:10<00:00,  2.86it/s]Evaluating: 100%|██████████| 35/35 [00:10<00:00,  3.43it/s]Evaluating:  83%|████████▎ | 15/18 [00:05<00:00,  3.07it/s]
10/18/2021 15:26:07 - INFO - __main__ -   ***** Evaluation result  in bg *****
10/18/2021 15:26:07 - INFO - __main__ -     f1 = 0.8469268520682403
10/18/2021 15:26:07 - INFO - __main__ -     loss = 0.5341164750712258
10/18/2021 15:26:07 - INFO - __main__ -     precision = 0.8511609179975842
10/18/2021 15:26:07 - INFO - __main__ -     recall = 0.8427347020131553
10/18/2021 15:26:07 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Evaluating:  89%|████████▉ | 16/18 [00:05<00:00,  3.54it/s]Evaluating:  94%|█████████▍| 17/18 [00:05<00:00,  3.95it/s]Evaluating: 100%|██████████| 18/18 [00:06<00:00,  4.63it/s]Evaluating: 100%|██████████| 18/18 [00:06<00:00,  2.96it/s]
10/18/2021 15:26:08 - INFO - __main__ -   ***** Evaluation result  in da *****
10/18/2021 15:26:08 - INFO - __main__ -     f1 = 0.8861512319456245
10/18/2021 15:26:08 - INFO - __main__ -     loss = 0.3311268703805076
10/18/2021 15:26:08 - INFO - __main__ -     precision = 0.8873763692438583
10/18/2021 15:26:08 - INFO - __main__ -     recall = 0.8849294729027468
10/18/2021 15:26:08 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:26:30 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:26:30 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:26:40 - INFO - __main__ -   Using lang2id = None
10/18/2021 15:26:40 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:27:03 - INFO - __main__ -   Using lang2id = None
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/18/2021 15:27:12 - INFO - __main__ -   Using lang2id = None
PyTorch version 1.9.0+cu102 available.
10/19/2021 20:07:13 - INFO - root -   Input args: ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/ignore-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/ignore-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/19/2021 20:07:13 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
10/19/2021 20:07:13 - INFO - __main__ -   Seed = 1
10/19/2021 20:07:13 - INFO - root -   save model
10/19/2021 20:07:13 - INFO - __main__ -   Training/evaluation parameters ModelArguments(model_name_or_path='bert-base-multilingual-cased', model_type='bert', config_name=None, tokenizer_name=None, cache_dir=None, labels='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128//labels.txt', data_dir='/home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/', output_dir='/home/abhijeet/rohan/emea/outputs//udpos/ignore-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble/', max_seq_length=128, do_train=False, do_eval=False, do_predict=True, do_adapter_predict=False, do_predict_dev=False, do_predict_train=False, init_checkpoint=None, evaluate_during_training=False, do_lower_case=False, few_shot=-1, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=32, gradient_accumulation_steps=4, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, save_steps=1000, warmup_steps=0, logging_steps=50, save_only_best_checkpoint=True, eval_all_checkpoints=True, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=1, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', predict_langs='be,uk,bg', train_langs='en', log_file='/home/abhijeet/rohan/emea/outputs//udpos/ignore-bert-base-multilingual-cased-MaxLen128_udpos_en,ru_ensemble//train.log', eval_patience=-1, bpe_dropout=0, do_save_adapter_fusions=False, task_name='udpos', predict_task_adapter='output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/', predict_lang_adapter=None, test_adapter=True, adapter_weight='0.5,0.5', calc_weight_step=0, predict_save_prefix='')
10/19/2021 20:07:13 - INFO - __main__ -   Loading fresh pretrained model and tokenizer
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_11": 11,
    "LABEL_12": 12,
    "LABEL_13": 13,
    "LABEL_14": 14,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_2": 2,
    "LABEL_3": 3,
    "LABEL_4": 4,
    "LABEL_5": 5,
    "LABEL_6": 6,
    "LABEL_7": 7,
    "LABEL_8": 8,
    "LABEL_9": 9
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading weights file https://huggingface.co/bert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /home/abhijeet/.cache/torch/transformers/0a3fd51713dcbb4def175c7f85bddc995d5976ce1dde327f99104e4d33069f17.aa7be4c79d76f4066d9b354496ea477c9ee39c5d889156dd1efb680643c2b052
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /home/abhijeet/.cache/torch/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c
Model config BertConfig {
  "adapters": {
    "adapters": {},
    "config_map": {}
  },
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 119547
}

loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /home/abhijeet/.cache/torch/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29
10/19/2021 20:07:22 - INFO - __main__ -   Using lang2id = None
10/19/2021 20:07:22 - INFO - __main__ -   Evaluating the model on test set of all the languages specified
10/19/2021 20:07:22 - INFO - __main__ -   Task Adapter will be loaded from this path output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/
10/19/2021 20:07:22 - INFO - root -   Trying to decide if add adapter
10/19/2021 20:07:22 - INFO - root -   loading task adapter
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/adapter_config.json
Adding adapter 'udpos' of type 'text_task'.
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_adapter.bin
Loading module configuration from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/head_config.json
Loading module weights from output/udpos/my-bert-base-multilingual-cased-LR1e-4-epoch50-MaxLen128-TrainLangen_en_s1/checkpoint-best/udpos/pytorch_model_head.bin
10/19/2021 20:07:23 - INFO - root -   loading lang adpater en/wiki@ukp,ru/wiki@ukp
10/19/2021 20:07:23 - INFO - __main__ -   Language = en
10/19/2021 20:07:23 - INFO - __main__ -   Adapter Name = en/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/en/bert-base-multilingual-cased/pfeiffer/en_relu_2.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/adapter_config.json
Adding adapter 'en' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/209973079df3cb5d155df4e290ec86070f257721693ce7618ff84b89b4aae2cf-3bd7c13f02e43f33b6ab727158d366c50d676646774b1c975dea5f965352474a-extracted'
10/19/2021 20:07:25 - INFO - __main__ -   Language = ru
10/19/2021 20:07:25 - INFO - __main__ -   Adapter Name = ru/wiki@ukp
No exactly matching adapter config found for this specifier, falling back to default.
Resolved adapter files at https://public.ukp.informatik.tu-darmstadt.de/AdapterHub/text_lang/ru/bert-base-multilingual-cased/pfeiffer/ru_pfeiffer_gelu_nd.zip.
Loading module configuration from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/adapter_config.json
Adding adapter 'ru' of type 'text_lang'.
Loading module weights from /home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted/pytorch_adapter.bin
No matching prediction head found in '/home/abhijeet/.cache/torch/adapters/c75dab93655d93c2bd93a15037950ab3afd6bbc79d9eb44a196c78db8d2d63e6-1575ecfe64066684ac57c97e4062f60b25b36bfea6113c644ba5181776da92fe-extracted'
10/19/2021 20:07:31 - INFO - __main__ -   Loading features from cached file /home/abhijeet/rohan/emea/data//udpos/udpos_processed_maxlen128/cached_test_be_bert-base-multilingual-cased_128
Average f1 score over 3 seeds for language ar = 0.29193214397939166 +- 0.013235004110666613
Average f1 score over 3 seeds for language bn = 0.5171137051443931 +- 0.03884143900014221
Average f1 score over 3 seeds for language mr = 0.4187350569369556 +- 0.02044047402137415
Average f1 score over 3 seeds for language ta = 0.23786094888696332 +- 0.05922606391930954
Average f1 score over 3 seeds for language bh = 0.4058856414353977 +- 0.024241886600848944
Average f1 score over 3 seeds for language hi = 0.48523276336517734 +- 0.0331369544453859
Average f1 score over 3 seeds for language is = 0.6548431242568219 +- 0.008301025406159892
Average f1 score over 3 seeds for language fo = 0.6428155413686091 +- 0.01387854141613133
Average f1 score over 3 seeds for language no = 0.7255096375115492 +- 0.009434054862503545
Average f1 score over 3 seeds for language da = 0.797273983473089 +- 0.0029020559703510996
Average f1 score over 3 seeds for language ru = 0.5703516494958284 +- 0.005620949776670872
Average f1 score over 3 seeds for language bg = 0.6914005402695004 +- 0.006871347856965511
Average f1 score over 3 seeds for language uk = 0.5797476562557734 +- 0.013161818675424977
Average f1 score over 3 seeds for language be = 0.5885549151854407 +- 0.040800582366318026
1.99user 2.81system 0:00.20elapsed 2355%CPU (0avgtext+0avgdata 30316maxresident)k
0inputs+8outputs (0major+6666minor)pagefaults 0swaps
